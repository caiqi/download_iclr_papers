<!DOCTYPE html><html xmlns="http://www.w3.org/1999/xhtml" class="no-js"><head>
  <meta charset="utf-8" />
  <meta http-equiv="X-UA-Compatible" content="IE=edge,chrome=1" />
  <meta name="viewport" content="width=device-width, initial-scale=1" />

  <title>On Tighter Generalization Bounds for Deep Neural Networks: CNNs, ResNets, and Beyond | OpenReview</title>
  <meta name="description" content="" />

      <meta name="citation_title" content="On Tighter Generalization Bounds for Deep Neural Networks: CNNs, ResNets, and Beyond" />
        <meta name="citation_author" content="Anonymous" />
      <meta name="citation_publication_date" content="2018/09/27" />
      <meta name="citation_online_date" content="2018/09/27" />
      <meta name="citation_pdf_url" content="https://openreview.net/pdf?id=SJzwvoCqF7" />
      <meta name="twitter:card" content="summary" />
      <meta name="twitter:site" content="@openreviewnet" />
      <meta name="og:title" content="On Tighter Generalization Bounds for Deep Neural Networks: CNNs..." />
      <meta name="og:description" content="We propose a generalization error bound for a general family of deep neural networks based on the depth and width of the networks, as well as the spectral norm of weight matrices. Through..." />
      <meta name="og:image" content="https://openreview.net/static/images/openreview_logo_512.png" />

  <link rel="stylesheet" href="https://fonts.googleapis.com/css?family=Noto+Sans:400,400i,700,700i" />

  <link rel="stylesheet" href="/static/css/bootstrap.min.css" />
  <link rel="stylesheet" href="/static/css/jquery-ui.min.css" />
  <link rel="stylesheet" href="/static/css/main.min.css" />
</head>

<body class="forum">
  <nav class="navbar navbar-inverse navbar-fixed-top" role="navigation">
    <div class="container">
  
      <div class="navbar-header">
        <button type="button" class="navbar-toggle collapsed" data-toggle="collapse" data-target="#navbar" aria-expanded="false" aria-controls="navbar">
          <span class="sr-only">Toggle navigation</span>
          <span class="icon-bar"></span>
          <span class="icon-bar"></span>
          <span class="icon-bar"></span>
        </button>
        <a class="navbar-brand home push-link" href="/"><strong>OpenReview</strong>.net</a>
      </div>
  
      <div id="navbar" class="navbar-collapse collapse">
        <form class="navbar-form navbar-left profile-search" role="search">
          <div class="form-group has-feedback">
            <input id="search_input" type="text" class="form-control ui-autocomplete-input" placeholder="Search ICLR 2019 Conference" autocomplete="off" />
            <span class="glyphicon glyphicon-search form-control-feedback" aria-hidden="true"></span>
          </div>
  
          <input id="search_group" type="hidden" value="ICLR.cc/2019/Conference" />
          <input id="search_content" type="hidden" value="all" />
        <ul id="ui-id-1" tabindex="0" class="ui-menu ui-widget ui-widget-content ui-autocomplete ui-front" style="display: none;"></ul></form>
  
        <ul class="nav navbar-nav navbar-right">
        
            <li id="user-menu"><a href="/login">Login</a></li>
        </ul>
      </div>
  
    </div>
  </nav>

  <div id="or-banner" class="banner" style="">
  <div class="container">
    <div class="row">
      <div class="col-xs-12"><a href="/group?id=ICLR.cc/2019/Conference" title="Venue Homepage"><img class="icon" src="/static/images/arrow_left.svg" /> Go to <strong>ICLR 2019 Conference</strong> homepage</a></div>
    </div>
  </div>
</div>
<div id="flash-message-container" class="alert alert-danger" role="alert" style="display: none;">
  <div class="container">
    <div class="row">
      <div class="col-xs-12">
        <div class="alert-content">
          <button type="button" class="close" aria-label="Close"><span aria-hidden="true">×</span></button>
        </div>
      </div>
    </div>
  </div>
</div>

<div class="container">
  <div class="row">
    <div class="col-xs-12">
      <main id="content" class="clearfix openbanner-visible legacy-styles"><div id="note_SJzwvoCqF7" class="note panel"><div class="title_pdf_row clearfix"><h2 class="note_content_title"><a>On Tighter Generalization Bounds for Deep Neural Networks: CNNs, ResNets, and Beyond</a> <a class="note_content_pdf" href="/pdf?id=SJzwvoCqF7" title="Download PDF" target="_blank"><img src="/static/images/pdf_icon_blue.svg" /></a> </h2></div><div class="meta_row"><span class="signatures">Anonymous</span></div><div class="clearfix"><div class="meta_row pull-left"><span class="date item">28 Sep 2018 (modified: 20 Nov 2018)</span><span class="item">ICLR 2019 Conference Blind Submission</span><span class="item">Readers: <span class="readers-icon glyphicon glyphicon-globe"></span> Everyone</span><span class="item"><a class="action-bibtex-modal" data-bibtex="@inproceedings{    &#10;anonymous2019on,    &#10;title={On Tighter Generalization Bounds for Deep Neural Networks: CNNs, ResNets, and Beyond},    &#10;author={Anonymous},    &#10;booktitle={Submitted to International Conference on Learning Representations},    &#10;year={2019},    &#10;url={https://openreview.net/forum?id=SJzwvoCqF7},    &#10;note={under review}    &#10;}">Show Bibtex</a></span><a class="note_content_pdf item" href="/revisions?id=SJzwvoCqF7" target="_blank">Show Revisions</a></div><div class="meta_row meta_actions"></div></div><div class="note_contents"><span class="note_content_field">Abstract: </span><span class="note_content_value">We propose a generalization error bound for a general family of deep neural networks based on the depth and width of the networks, as well as the spectral norm of weight matrices. Through introducing a novel characterization of the Lipschitz properties of neural network family, we achieve a tighter generalization error bound. We further obtain a result that is free of linear dependence on norms for bounded losses. Besides the general deep neural networks, our results can be applied to derive new bounds for several popular architectures, including convolutional neural networks (CNNs), residual networks (ResNets), and hyperspherical networks (SphereNets).  When achieving same generalization errors with previous arts, our bounds allow for the choice of much larger parameter spaces of weight matrices, inducing potentially stronger expressive ability for neural networks.</span></div><div class="note_contents"><span class="note_content_field">Keywords: </span><span class="note_content_value">deep learning, generalization error bound, convolutional neural networks</span></div><div class="reply_row clearfix"><div class="item" id="reply_count">8 Replies</div></div></div><hr class="small" /><div id="note_children"><div class="note_with_children"><div id="note_BJluyTu3Tm" class="note panel"><div class="title_pdf_row clearfix"><h2 class="note_content_title"><a>Why is the dependence on parameters in Corollary 1 \sqrt{Dp^2} and not \sqrt{D^2 p^2}?</a>  <button class="btn btn-xs btn-default permalink-button" title="Link to this comment" data-permalink-url="https://openreview.net/forum?id=SJzwvoCqF7&amp;noteId=BJluyTu3Tm"><span class="glyphicon glyphicon-link" aria-hidden="true"></span></button></h2></div><div class="meta_row"><span class="signatures">(anonymous)</span></div><div class="clearfix"><div class="meta_row pull-left"><span class="date item">17 Nov 2018</span><span class="item">ICLR 2019 Conference Paper272 Public Comment</span><span class="item">Readers: <span class="readers-icon glyphicon glyphicon-globe"></span> Everyone</span></div><div class="meta_row meta_actions"></div></div><div class="note_contents"><span class="note_content_field">Comment: </span><span class="note_content_value">I've a couple of questions about the proof of Lemma 3 and Corollary 1 and I'll be glad if the authors can help clarify.

First, at the end of page 12, there's an upper bound of the form  alpha log (1/alpha).  Equation 15 upper bounds alpha. How do you use this upper bound on alpha to upper bound the 1/alpha within the log term later? Perhaps I'm missing something here, is it possible to directly plug in the upper bound on alpha in alpha log(1/alpha)?

Second, in the proof of corollary 2, assuming we can directly plug in the tighter upper bound on alpha, we will have a log (L_w/ alpha) term where L_w scales with the product of spectral norms (which wouldn't get cancelled now because alpha is potentially much smaller than that). Wouldn't this result in an extra dependence on D as sqrt{D*log max_d B_{d,2}}?</span></div><div class="reply_row clearfix"></div></div><div class="children"><div class="note_with_children"><div id="note_Sye88ggb0X" class="note panel"><div class="title_pdf_row clearfix"><h2 class="note_content_title"><a>Refined analysis</a>  <button class="btn btn-xs btn-default permalink-button" title="Link to this comment" data-permalink-url="https://openreview.net/forum?id=SJzwvoCqF7&amp;noteId=Sye88ggb0X"><span class="glyphicon glyphicon-link" aria-hidden="true"></span></button></h2></div><div class="meta_row"><span class="signatures">ICLR 2019 Conference Paper272 Authors</span></div><div class="clearfix"><div class="meta_row pull-left"><span class="date item">20 Nov 2018</span><span class="item">ICLR 2019 Conference Paper272 Official Comment</span><span class="item">Readers: <span class="readers-icon glyphicon glyphicon-globe"></span> Everyone</span></div><div class="meta_row meta_actions"></div></div><div class="note_contents"><span class="note_content_field">Comment: </span><span class="note_content_value">We updated the proof of Theorem 1 in terms of the L_w using the spectral norm of Jacobian operators instead of the product of spectral norms of weight matrices. This is a tighter result since the spectral norm of Jacobian is significantly smaller than the product of spectral norms of weight matrices. Specifically, a recent result [1] guarantees that a trained network from a random initialization using (stochastic) gradient descent has the spectral norm of Jacobian bounded by sqrt(depth), which is significantly smaller than the product of spectral norms of weight matrices that is exponential on depth in general. When we consider the network functions obtained using (stochastic) gradient descent, we do have sqrt(D) dependence in ERC rather than D. The same argument applies to Corollary 2 for CNNs. We also provided details of quantities alpha, L_w, and K in all cases for completeness.

[1] Allen-Zhu et al. A convergence theory for deep learning via over-parameterization.</span></div><div class="reply_row clearfix"></div></div><div class="children"></div></div></div></div><div class="note_with_children"><div id="note_HkeC6lVonX" class="note panel"><div class="title_pdf_row clearfix"><h2 class="note_content_title"><a>Different Take on Generalization - Size Dependent Bounds</a>  <button class="btn btn-xs btn-default permalink-button" title="Link to this comment" data-permalink-url="https://openreview.net/forum?id=SJzwvoCqF7&amp;noteId=HkeC6lVonX"><span class="glyphicon glyphicon-link" aria-hidden="true"></span></button></h2></div><div class="meta_row"><span class="signatures">ICLR 2019 Conference Paper272 AnonReviewer1</span></div><div class="clearfix"><div class="meta_row pull-left"><span class="date item">03 Nov 2018 (modified: 07 Nov 2018)</span><span class="item">ICLR 2019 Conference Paper272 Official Review</span><span class="item">Readers: <span class="readers-icon glyphicon glyphicon-globe"></span> Everyone</span></div><div class="meta_row meta_actions"></div></div><div class="note_contents"><span class="note_content_field">Review: </span><span class="note_content_value">The paper provides a generalization bound for multi-layered deep neural networks in terms of dimensions rather than norms. The bound is derived by controlling Rademacher complexity of the Ramp loss under the Lipschitzness of the network as a parametric function in Depth * Width ^2 number of parameters, and then using standard L-2 covering and Dudley Integral. They extend this technique for CNNs, Resnets, Hyper-spherical Networks, etc and provide specialized bounds for each case. In the end, the authors provide comparisons to the existing bounds. 

Although intended, the bound in Theorem-1 depends on the number of parameters and hold only if m &gt; d * (p^2) = number of parameters (from the last line of proof of Lemma 3, we need \beta &lt; \alpha and thus m &gt; h). Such bounds are already know in the literature (see Anthony and Bartlett, 1999). Adaptive (completely norm dependent, like Bartlett et. al. 2017) bounds will be better than explicit dimension dependent bounds. The comparison in Figure-1 which suggest their bound to be better is unfair because they are comparing their specialized bounds for CNN to generic bounds for standard feedforward networks. Same for comparison in Table-2. 

It was already established in Theorem 3.4 (Bartlett et al. 2017) that spectral norms are necessary for any generalization bounds for Deep Neural Networks, thus voiding the claims made in the paper (and discussion) about the importance of spectral norms. 

Typos / Errors : 
1. Statement of Lemma 2 does not contain the spectral norms terms. 
2. The third equation in Page 13 should be K &lt;= \sqrt{pD} max B_{d, 2}; and this changes the bound further. 

The paper introduces some new techniques on mathematical analysis of specialized neural networks. </span></div><div class="note_contents"><span class="note_content_field">Rating: </span><span class="note_content_value">5: Marginally below acceptance threshold</span></div><div class="note_contents"><span class="note_content_field">Confidence: </span><span class="note_content_value">3: The reviewer is fairly confident that the evaluation is correct</span></div><div class="reply_row clearfix"></div></div><div class="children"><div class="note_with_children"><div id="note_SklcK0k-RX" class="note panel"><div class="title_pdf_row clearfix"><h2 class="note_content_title"><a>Refined analysis and extended numerical results</a>  <button class="btn btn-xs btn-default permalink-button" title="Link to this comment" data-permalink-url="https://openreview.net/forum?id=SJzwvoCqF7&amp;noteId=SklcK0k-RX"><span class="glyphicon glyphicon-link" aria-hidden="true"></span></button></h2></div><div class="meta_row"><span class="signatures">ICLR 2019 Conference Paper272 Authors</span></div><div class="clearfix"><div class="meta_row pull-left"><span class="date item">20 Nov 2018</span><span class="item">ICLR 2019 Conference Paper272 Official Comment</span><span class="item">Readers: <span class="readers-icon glyphicon glyphicon-globe"></span> Everyone</span></div><div class="meta_row meta_actions"></div></div><div class="note_contents"><span class="note_content_field">Comment: </span><span class="note_content_value">We thank the reviewer for careful reading and helpful comments. We do have the same dependence with the VC type of bound in terms of the number of parameters, but we are considering essentially a smaller class of network functions (than that considered in the VC type of bound) with norm constraints. In the case when all norms are bounded (by 1) or the loss function is bounded (e.g., the ramp loss &lt; 1), then our resulting bound can be smaller than the VC type of bound. 

We want to remark that we have updated our analysis for DNNs in terms of the rank of weight matrices, which may lead to tighter results when the weight matrices are low-rank, analogous to [1,2]. Our result for CNNs is a direct derivation from Theorem 1 (for DNNs). This is one benefit of our analysis based on the Lipschitz property on the parameters, which allows us to directly reduce from pr (r is the rank of W) to k^2 in the CNNs case. For the other comparing results in Figure 1, to the best of our knowledge, there is no such direct simplification from the general case to the CNNs case from their analysis. In other words, they have the same order of bounds for DNNs and CNNs. Note that the ranks are of the same order with width p in CNNs when the filters are linear independent, and we already simplified the norms in the other comparing results as in our result for CNNs in Table 2. 

Moreover, we want to remark that Thm 3.4 in [1] is the lower bound of the ERC for the neural network function, which did not take the loss function into consideration. In other words, it does not conflict our case when the loss is bounded, which can lead to tighter dependence in terms of the loss function bound rather than the product of norm. Note that our result shown in Figure 1 is the worse case of Corollary 1. In practice, the loss function for a trained network has a significantly smaller scale than \prod B_{d,2}, which mean our result in Figure 1 is in fact significantly smaller than what is shown. This is another benefit of our analysis that allows us to only depend on the output of loss function, rather than the product of norms that the comparing results in Figure 1 cannot avoid due to the nature of their analyses. We have added an extended experiment in Appendix F for the bounded loss case. One can find that the generalization bound in the bounded loss case can be significantly smaller than the norm based result. We also added a discussion in Section 3.2 and Appendix F to compare with existing output based bounds [3,4] and show that our output based result is tighter.

In addition, from our observation on the trained network using real data, the dependence on the network sizes in other norm based results (e.g., the terms in Bound 1 and 2 in Figure 1 excluding the term of the product of norms) are significantly larger than the number of parameters. Based on the updated Theorem 1, [1,2] are always larger than our bound by a margin at least D, including the low-rank cases.

The typos are corrected. 

[1] Bartlett et al. Spectrally-normalized margin bounds for neural networks.
[2] Neyshabur et al. A pac-bayesian approach to spectrally-normalized margin bounds for neural networks.
[3] Arora et al. Stronger generalization bounds for deep nets via a compression approach.
[4] Zhou and Feng. Understanding generalization and optimization performance of deep cnns.</span></div><div class="reply_row clearfix"></div></div><div class="children"></div></div></div></div><div class="note_with_children"><div id="note_H1gtrvTL37" class="note panel"><div class="title_pdf_row clearfix"><h2 class="note_content_title"><a>An improved and new characterization of generalization bound </a>  <button class="btn btn-xs btn-default permalink-button" title="Link to this comment" data-permalink-url="https://openreview.net/forum?id=SJzwvoCqF7&amp;noteId=H1gtrvTL37"><span class="glyphicon glyphicon-link" aria-hidden="true"></span></button></h2></div><div class="meta_row"><span class="signatures">ICLR 2019 Conference Paper272 AnonReviewer2</span></div><div class="clearfix"><div class="meta_row pull-left"><span class="date item">31 Oct 2018 (modified: 07 Nov 2018)</span><span class="item">ICLR 2019 Conference Paper272 Official Review</span><span class="item">Readers: <span class="readers-icon glyphicon glyphicon-globe"></span> Everyone</span></div><div class="meta_row meta_actions"></div></div><div class="note_contents"><span class="note_content_field">Review: </span><span class="note_content_value">The paper presents a new characterization of generalization error bound for general deep neural networks in terms of the depth and width of the networks and the spectral norm of weight matrices. The proof follows the setting of Bartlett et al. 2017 with new development on the Lipschitz properties of neural networks.

Pros:
1. The paper provides a solid improvement over previous bounds on generalization error.
2. The presentation of the result and proofs is clear and easy to follow.
3. It does case studies specially on widely used network structures CNN, ResNet, etc.

Con:
1. It is not clear whether the bound is vacuous or not, as discussed in Arora et al. 2018.  If it is vacuous, it is hard to justify the claims that given the generalization error, which is vacuous for all bounds, the paper's bound allows the choices of larger dimensions of parameters and larger spectral norms of weight matrices.
2.  The L_w has the factor "products of B_{d,2}s" which, however, does not show up in the final generalization bound (The equation right above Appendix B). This products may introduce an additional $D$ under sqrt changing D to D^2 under the sqrt, which changes the order. The authors should give some explanation on this.

3. Is the assumption on the orthogonal and normalized filters in CNN a must thing for the argument or just for convenience of the presentation? The paper should be clearer about this point.

4. The RHS of the equation  in Lemma 2 misses terms related with B_{d,2}.
5. Typos: Find one typo in Page 3  "deﬁed as"
</span></div><div class="note_contents"><span class="note_content_field">Rating: </span><span class="note_content_value">7: Good paper, accept</span></div><div class="note_contents"><span class="note_content_field">Confidence: </span><span class="note_content_value">4: The reviewer is confident but not absolutely certain that the evaluation is correct</span></div><div class="reply_row clearfix"></div></div><div class="children"><div class="note_with_children"><div id="note_rygmMJlZAQ" class="note panel"><div class="title_pdf_row clearfix"><h2 class="note_content_title"><a>Extended discussion, experiments, and refined analysis</a>  <button class="btn btn-xs btn-default permalink-button" title="Link to this comment" data-permalink-url="https://openreview.net/forum?id=SJzwvoCqF7&amp;noteId=rygmMJlZAQ"><span class="glyphicon glyphicon-link" aria-hidden="true"></span></button></h2></div><div class="meta_row"><span class="signatures">ICLR 2019 Conference Paper272 Authors</span></div><div class="clearfix"><div class="meta_row pull-left"><span class="date item">20 Nov 2018</span><span class="item">ICLR 2019 Conference Paper272 Official Comment</span><span class="item">Readers: <span class="readers-icon glyphicon glyphicon-globe"></span> Everyone</span></div><div class="meta_row meta_actions"></div></div><div class="note_contents"><span class="note_content_field">Comment: </span><span class="note_content_value">We thank the reviewer for recognizing our contributions. 

1. As in existing norm based bounds, it can be vacuous (e.g., when the norms of weight matrices are large) if we do not have further structural conditions on the networks. Our effort here is to tighten the potentially vacuous bound from two directions: (1) reduce the dependence in terms of the depth and width; (2) reduce the product of norms to the loss function output when it is bounded. With these improvements, we can obtain non-vacuous bounds. For example, in the case of CNNs with a bounded loss, our generalization bound can be &lt;&lt; 1 given a moderate training sample size. An extended numerical result is provided in Appendix F to support this claim. 

2. We updated the proof of Theorem 1 in terms of the L_w using the spectral norm of Jacobian operators instead of the product of spectral norms of weight matrices. This is a tighter result since the spectral norm of Jacobian is significantly smaller than the product of spectral norms of weight matrices. Specifically, a recent result [1] guarantees that a trained network from a random initialization using (stochastic) gradient descent has the spectral norm of Jacobian bounded by sqrt(depth), which is significantly smaller than the product of spectral norms of weight matrices that is exponential on depth in general. When we consider the network functions obtained using (stochastic) gradient descent, we do have sqrt(D) dependence in ERC rather than D. 

3. The orthogonal and normalized filters in CNNs have shown comparable or even improved empirical results than their non-orthogonal counterpart [2,3]. This motivates us to analyze this setting for CNNs. In addition, the orthogonal filters also provide a way to quantify the spectral of weight matrices for CNNs tightly based on the filter size and stride size. We further clarified this in the revision. On the other hand, the orthogonality is not a must as we discussed in the numerical evaluation (Section 4.5). When the filters are not orthogonal, our bound reduces to the number of total parameters in filters (# filters * filter size). This is one of the benefits of our analytic pipeline that allows obtaining potentially tighter bounds based on the number of parameters rather than the width of weight matrices.

4 and 5. The typos are corrected. 

[1] Allen-Zhu et al. A convergence theory for deep learning via over-parameterization.
[2] Huang et al. Orthogonal weight normalization: Solution to optimization over multiple dependent stiefel manifolds in deep neural networks.
[3] Xie et al. All you need is beyond a good init: Exploring better solution for training extremely deep convolutional neural networks with orthonormality and modulation.</span></div><div class="reply_row clearfix"></div></div><div class="children"></div></div></div></div><div class="note_with_children"><div id="note_BklVMpXrnm" class="note panel"><div class="title_pdf_row clearfix"><h2 class="note_content_title"><a>review</a>  <button class="btn btn-xs btn-default permalink-button" title="Link to this comment" data-permalink-url="https://openreview.net/forum?id=SJzwvoCqF7&amp;noteId=BklVMpXrnm"><span class="glyphicon glyphicon-link" aria-hidden="true"></span></button></h2></div><div class="meta_row"><span class="signatures">ICLR 2019 Conference Paper272 AnonReviewer3</span></div><div class="clearfix"><div class="meta_row pull-left"><span class="date item">30 Oct 2018 (modified: 07 Nov 2018)</span><span class="item">ICLR 2019 Conference Paper272 Official Review</span><span class="item">Readers: <span class="readers-icon glyphicon glyphicon-globe"></span> Everyone</span></div><div class="meta_row meta_actions"></div></div><div class="note_contents"><span class="note_content_field">Review: </span><span class="note_content_value">This paper proposes a generalization error bound for DNNs (and their generalizations) based on the depth and width of the networks, as well as the spectral norm of weight matrices. The proposed work is shown to provide a tighter generalization error bounds compared with a few existing literatures. 

Pros: This paper makes theoretical contributions to the understanding of DNNs. This is an important but difficult task. As a theoretical paper, this one is relatively easy to follow. 

Cons: In spite of its theoretical contributions, this paper has a few major issues. 

Q1: This paper fails to fairly compare with the most recent work, Arora et al. (2018), Zhou and Feng (2018). For instance, Arora et al. (2018) uses error-resilience parameters instead of the norms of weight matrices to obtain a better generalization error. The authors claim that the error-resilience parameters are less interpretable than the norms of weight matrices. This claim could be subjective and is not convincing. 

Q2: The error bounds of Bartlett et al. (2017), Neyshabur et al. (2017) could be improved for low-rank weight matrices, in which case the proposed Theorem 1 is tighter only if $p \le D^2$. This holds only when DNN is very deep. Can theorem 1 be improved by similarly considering the low-rankness of weight matrices?

Q3: In Corollary 2, the error bound for CNN, the authors assume that the filters are orthogonal with unit norm. Can the authors provide some justification on the orthogonal filters? In addition, Zhou and Feng (2018) have achieved similar bound for CNN. Can the authors provide some justification why this latest result is not included in Table 2? 

</span></div><div class="note_contents"><span class="note_content_field">Rating: </span><span class="note_content_value">6: Marginally above acceptance threshold</span></div><div class="note_contents"><span class="note_content_field">Confidence: </span><span class="note_content_value">4: The reviewer is confident but not absolutely certain that the evaluation is correct</span></div><div class="reply_row clearfix"></div></div><div class="children"><div class="note_with_children"><div id="note_H1lv6JlZ0X" class="note panel"><div class="title_pdf_row clearfix"><h2 class="note_content_title"><a>Extended discussion, numerical results, and refined analysis</a>  <button class="btn btn-xs btn-default permalink-button" title="Link to this comment" data-permalink-url="https://openreview.net/forum?id=SJzwvoCqF7&amp;noteId=H1lv6JlZ0X"><span class="glyphicon glyphicon-link" aria-hidden="true"></span></button></h2></div><div class="meta_row"><span class="signatures">ICLR 2019 Conference Paper272 Authors</span></div><div class="clearfix"><div class="meta_row pull-left"><span class="date item">20 Nov 2018</span><span class="item">ICLR 2019 Conference Paper272 Official Comment</span><span class="item">Readers: <span class="readers-icon glyphicon glyphicon-globe"></span> Everyone</span></div><div class="meta_row meta_actions"></div></div><div class="note_contents"><span class="note_content_field">Comment: </span><span class="note_content_value">We thank the reviewer for the careful reading and recognizing our contributions. 

1. We have added some discussion to compare with the [1,2] after corollary 1 in the bounded loss case. Moreover, we extended the numerical result to compare with [1,2] in Appendix F for the trained network on CIFAR10. Please refer further details therein. Our results show that our bound is tighter than the existing works based on the bounded function [1,2].

2. Thanks to the reviewer for pointing this out, we updated our analysis to incorporate the rank of weight matrices in the revision. In the updated results, we have sqrt(pr) (r is the rank) dependence, which is of the same order with [3,4] in terms of the width. To this end, our bound is strictly better than [3,4] by a margin at least D in all scenarios of ranks. 

3. The orthogonal and normalized filters in CNNs have shown comparable or even improved empirical results than their non-orthogonal counterpart [5,6]. This motivates us to analyze this setting for CNNs, which also provide a way to quantify the spectral of weight matrices for CNNs tightly based on the filter size and stride size. We further clarified this in the revision. In addition, we provide further discussion to compare with [2] in Section 3.2 and extended numerical evaluations in Appendix F in the bounded loss case. This support that our new bound is tighter than [2] in this case. Also, note that the orthogonality is not a must as we discussed in the numerical evaluation (Section 4.5). When the filters are not orthogonal, our bound reduces to the number of total parameters in filters (# filters * filter size). This is one of the benefits of our analytic pipeline that allows obtaining tighter bounds based on the number of parameters rather than the width of weight matrices.

[1] Arora et al. Stronger generalization bounds for deep nets via a compression approach.
[2] Zhou and Feng. Understanding generalization and optimization performance of deep cnns.
[3] Bartlett et al. Spectrally-normalized margin bounds for neural networks.
[4] Neyshabur et al. A pac-bayesian approach to spectrally-normalized margin bounds for neural networks.
[5] Huang et al. Orthogonal weight normalization: Solution to optimization over multiple dependent stiefel manifolds in deep neural networks.
[6] Xie et al. All you need is beyond a good init: Exploring better solution for training extremely deep convolutional neural networks with orthonormality and modulation.</span></div><div class="reply_row clearfix"></div></div><div class="children"></div></div></div></div></div></main></div>
  </div>
</div>


    <!-- Footer -->
    <footer class="sitemap">
      <div class="container">
        <div class="row hidden-xs">
          <div class="col-sm-4">
            <ul class="list-unstyled">
              <li><a href="/" class="home">Home</a></li>
              <li><a href="/venues">All Venues</a></li>
            </ul>
          </div>
    
          <div class="col-sm-4">
            <ul class="list-unstyled">
              <li><a href="/about">About OpenReview</a></li>
              <li><a href="/contact">Contact</a></li>
              <li><a href="#" data-toggle="modal" data-target="#feedback-modal">Feedback</a></li>
            </ul>
          </div>
    
          <div class="col-sm-4">
            <ul class="list-unstyled">
              <li><a href="/terms">Terms of Service</a></li>
              <li><a href="/privacy">Privacy Policy</a></li>
            </ul>
          </div>
        </div>
    
        <div class="row visible-xs-block">
          <div class="col-xs-6">
            <ul class="list-unstyled">
              <li><a href="/" class="home">Home</a></li>
              <li><a href="/about">About OpenReview</a></li>
              <li><a href="#" data-toggle="modal" data-target="#feedback-modal">Feedback</a></li>
            </ul>
          </div>
    
          <div class="col-xs-6">
            <ul class="list-unstyled">
              <li><a href="/contact">Contact</a></li>
              <li><a href="/terms">Terms of Service</a></li>
              <li><a href="/privacy">Privacy Policy</a></li>
            </ul>
          </div>
        </div>
      </div>
    </footer>
    
    <footer class="sponsor">
      <div class="container">
        <div class="row">
          <div class="col-sm-10 col-sm-offset-1">
            <p class="text-center">
              OpenReview is created by the <a href="http://www.iesl.cs.umass.edu/" target="_blank">Information Extraction and Synthesis Laboratory</a>, College of Information and Computer Science, University of Massachusetts Amherst. We gratefully acknowledge the support of the OpenReview sponsors:  Google,  Facebook, NSF, the University of Massachusetts Amherst Center for Data Science, and Center for Intelligent Information Retrieval, as well as the Google Cloud Platform for donating the computing and networking services on which OpenReview.net runs.
            </p>
          </div>
        </div>
      </div>
    </footer>

  <div id="feedback-modal" class="modal fade" tabindex="-1" role="dialog">
    <div class="modal-dialog">
      <div class="modal-content">
        <div class="modal-header">
          <button type="button" class="close" data-dismiss="modal" aria-label="Close"><span aria-hidden="true">×</span></button>
          <h3 class="modal-title">Send Feedback</h3>
        </div>
        <div class="modal-body">
          <p>Enter your feedback below and we'll get back to you as soon as possible.</p>
          <form action="/feedback" method="POST">
            <div class="form-group">
              <input type="email" name="from" class="form-control" placeholder="Email" />
            </div>
            <div class="form-group">
              <input type="text" name="subject" class="form-control" placeholder="Subject" />
            </div>
            <div class="form-group">
              <textarea name="message" class="form-control feedback-input" rows="5" placeholder="Message" required=""></textarea>
            </div>
          <ul id="ui-id-2" tabindex="0" class="ui-menu ui-widget ui-widget-content" style="display: none;"></ul></form>
        </div>
        <div class="modal-footer">
          <button type="button" class="btn btn-default" data-dismiss="modal">Cancel</button>
          <button type="button" class="btn btn-primary">Send</button>
        </div>
      </div>
    </div>
  </div>

  <script type="text/javascript" async="" src="https://www.google-analytics.com/analytics.js"></script><script src="//cdnjs.cloudflare.com/ajax/libs/jquery/2.2.4/jquery.min.js"></script>
  <script>window.jQuery || document.write('&lt;script src="/static/js/vendor/jquery-2.2.4.min.js"&gt;&lt;\/script&gt;')</script>

  <script src="/static/dist/vendor.min.js"></script>
  <script src="/static/dist/templates.min.js"></script>
  <script src="/static/dist/openreview.min.js"></script>

    <script>window.legacyScripts = true;</script>


    <script async="" src="https://www.googletagmanager.com/gtag/js?id=UA-108703919-1"></script>
    <script>
      window.dataLayer = window.dataLayer || [];
      function gtag() { dataLayer.push(arguments); }
      gtag('js', new Date());
      gtag('config', 'UA-108703919-1');
    </script>


<div role="status" aria-live="assertive" aria-relevant="additions" class="ui-helper-hidden-accessible"></div></body></html>