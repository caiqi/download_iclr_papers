<!DOCTYPE html><html xmlns="http://www.w3.org/1999/xhtml" class="no-js"><head>
  <meta charset="utf-8" />
  <meta http-equiv="X-UA-Compatible" content="IE=edge,chrome=1" />
  <meta name="viewport" content="width=device-width, initial-scale=1" />

  <title>Value Propagation Networks | OpenReview</title>
  <meta name="description" content="" />

      <meta name="citation_title" content="Value Propagation Networks" />
        <meta name="citation_author" content="Anonymous" />
      <meta name="citation_publication_date" content="2018/09/27" />
      <meta name="citation_online_date" content="2018/09/27" />
      <meta name="citation_pdf_url" content="https://openreview.net/pdf?id=SJG6G2RqtX" />
      <meta name="twitter:card" content="summary" />
      <meta name="twitter:site" content="@openreviewnet" />
      <meta name="og:title" content="Value Propagation Networks" />
      <meta name="og:description" content="We present Value Propagation (VProp), a set of parameter-efficient differentiable planning modules built on Value Iteration which can successfully be trained using reinforcement learning to solve..." />
      <meta name="og:image" content="https://openreview.net/static/images/openreview_logo_512.png" />

  <link rel="stylesheet" href="https://fonts.googleapis.com/css?family=Noto+Sans:400,400i,700,700i" />

  <link rel="stylesheet" href="/static/css/bootstrap.min.css" />
  <link rel="stylesheet" href="/static/css/jquery-ui.min.css" />
  <link rel="stylesheet" href="/static/css/main.min.css" />
</head>

<body class="forum">
  <nav class="navbar navbar-inverse navbar-fixed-top" role="navigation">
    <div class="container">
  
      <div class="navbar-header">
        <button type="button" class="navbar-toggle collapsed" data-toggle="collapse" data-target="#navbar" aria-expanded="false" aria-controls="navbar">
          <span class="sr-only">Toggle navigation</span>
          <span class="icon-bar"></span>
          <span class="icon-bar"></span>
          <span class="icon-bar"></span>
        </button>
        <a class="navbar-brand home push-link" href="/"><strong>OpenReview</strong>.net</a>
      </div>
  
      <div id="navbar" class="navbar-collapse collapse">
        <form class="navbar-form navbar-left profile-search" role="search">
          <div class="form-group has-feedback">
            <input id="search_input" type="text" class="form-control ui-autocomplete-input" placeholder="Search ICLR 2019 Conference" autocomplete="off" />
            <span class="glyphicon glyphicon-search form-control-feedback" aria-hidden="true"></span>
          </div>
  
          <input id="search_group" type="hidden" value="ICLR.cc/2019/Conference" />
          <input id="search_content" type="hidden" value="all" />
        <ul id="ui-id-1" tabindex="0" class="ui-menu ui-widget ui-widget-content ui-autocomplete ui-front" style="display: none;"></ul></form>
  
        <ul class="nav navbar-nav navbar-right">
        
            <li id="user-menu"><a href="/login">Login</a></li>
        </ul>
      </div>
  
    </div>
  </nav>

  <div id="or-banner" class="banner" style="">
  <div class="container">
    <div class="row">
      <div class="col-xs-12"><a href="/group?id=ICLR.cc/2019/Conference" title="Venue Homepage"><img class="icon" src="/static/images/arrow_left.svg" /> Go to <strong>ICLR 2019 Conference</strong> homepage</a></div>
    </div>
  </div>
</div>
<div id="flash-message-container" class="alert alert-danger" role="alert" style="display: none;">
  <div class="container">
    <div class="row">
      <div class="col-xs-12">
        <div class="alert-content">
          <button type="button" class="close" aria-label="Close"><span aria-hidden="true">×</span></button>
        </div>
      </div>
    </div>
  </div>
</div>

<div class="container">
  <div class="row">
    <div class="col-xs-12">
      <main id="content" class="clearfix openbanner-visible legacy-styles"><div id="note_SJG6G2RqtX" class="note panel"><div class="title_pdf_row clearfix"><h2 class="note_content_title"><a>Value Propagation Networks</a> <a class="note_content_pdf" href="/pdf?id=SJG6G2RqtX" title="Download PDF" target="_blank"><img src="/static/images/pdf_icon_blue.svg" /></a> </h2></div><div class="meta_row"><span class="signatures">Anonymous</span></div><div class="clearfix"><div class="meta_row pull-left"><span class="date item">28 Sep 2018 (modified: 11 Oct 2018)</span><span class="item">ICLR 2019 Conference Blind Submission</span><span class="item">Readers: <span class="readers-icon glyphicon glyphicon-globe"></span> Everyone</span><span class="item"><a class="action-bibtex-modal" data-bibtex="@inproceedings{    &#10;anonymous2019value,    &#10;title={Value Propagation Networks},    &#10;author={Anonymous},    &#10;booktitle={Submitted to International Conference on Learning Representations},    &#10;year={2019},    &#10;url={https://openreview.net/forum?id=SJG6G2RqtX},    &#10;note={under review}    &#10;}">Show Bibtex</a></span></div><div class="meta_row meta_actions"></div></div><div class="note_contents"><span class="note_content_field">Abstract: </span><span class="note_content_value">We present Value Propagation (VProp), a set of parameter-efficient differentiable planning modules built on Value Iteration which can successfully be trained using reinforcement learning to solve unseen tasks, has the capability to generalize to larger map sizes, and can learn to navigate in dynamic environments. We show that the modules enable learning to plan when the environment also includes stochastic elements, providing a cost-efficient learning system to build low-level size-invariant planners for a variety of interactive navigation problems. We evaluate on static and dynamic configurations of MazeBase grid-worlds, with randomly generated environments of several different sizes, and on a StarCraft navigation scenario, with more complex dynamics, and pixels as input.</span></div><div class="note_contents"><span class="note_content_field">Keywords: </span><span class="note_content_value">Reinforcement Learning, Value Iteration, Navigation, Convolutional Neural Networks, Learning to plan</span></div><div class="note_contents"><span class="note_content_field">TL;DR: </span><span class="note_content_value">We present planners based on convnets that are sample-efficient and that generalize to larger instances of navigation and pathfinding problems.</span></div><div class="reply_row clearfix"><div class="item" id="reply_count">3 Replies</div></div></div><hr class="small" /><div id="note_children"><div class="note_with_children"><div id="note_B1lNYtd03m" class="note panel"><div class="title_pdf_row clearfix"><h2 class="note_content_title"><a>Interesting extension of the original value iteration networks (VIN), promising work</a>  <button class="btn btn-xs btn-default permalink-button" title="Link to this comment" data-permalink-url="https://openreview.net/forum?id=SJG6G2RqtX&amp;noteId=B1lNYtd03m"><span class="glyphicon glyphicon-link" aria-hidden="true"></span></button></h2></div><div class="meta_row"><span class="signatures">ICLR 2019 Conference Paper1308 AnonReviewer1</span></div><div class="clearfix"><div class="meta_row pull-left"><span class="date item">06 Nov 2018 (modified: 07 Nov 2018)</span><span class="item">ICLR 2019 Conference Paper1308 Official Review</span><span class="item">Readers: <span class="readers-icon glyphicon glyphicon-globe"></span> Everyone</span></div><div class="meta_row meta_actions"></div></div><div class="note_contents"><span class="note_content_field">Review: </span><span class="note_content_value">The paper presents an extension of the original value iteration networks (VIN) by considering state-dependent transition function, which alleviates the limitation of VIN to translation-invariant transition functions and further constraining the reward function parametrization to improve sample efficiency of learning to plan algorithms. The first problem is addressed  by interpreting transition probabilities as state-dependent discount factors, given by a sigmoid function that takes as input state features. The second problem is addressed by defining the reward function as the difference between an input reward and an output cost. Obstacle states are given a high cost. The proposed method is evaluated on random grids of different sizes, of the same type as the grids considered in the VIN paper. Comparaisons with VIN show that the proposed MVProp approach outperforms VIN by several orders of magnitude and can learn optimal plans in less than a thousand episodes, compared to VIN that doesn't seem here to learn much even after 30 thousands episodes. 
The paper is well-written in general. Certain aspects of value iteration networks were explained too briefly and the reviewer had to re-read the original VIN paper to grasp certain details of the proposed approach. This work is an interesting improvement of VIN, but somehow incremental in nature as the improvement is limited to slightly changing the reward and transition representations. However, the resulting performance seems very impressive, especially for larger grids. One question that needs to be clarified is: how is this work situated with respect to the body of work on RL? How does this method compare empirically to model-free algorithms such as DDPG and PPO?</span></div><div class="note_contents"><span class="note_content_field">Rating: </span><span class="note_content_value">7: Good paper, accept</span></div><div class="note_contents"><span class="note_content_field">Confidence: </span><span class="note_content_value">3: The reviewer is fairly confident that the evaluation is correct</span></div><div class="reply_row clearfix"></div></div><div class="children"></div></div><div class="note_with_children"><div id="note_rkexJb4i2X" class="note panel"><div class="title_pdf_row clearfix"><h2 class="note_content_title"><a>Missing information in the exposition</a>  <button class="btn btn-xs btn-default permalink-button" title="Link to this comment" data-permalink-url="https://openreview.net/forum?id=SJG6G2RqtX&amp;noteId=rkexJb4i2X"><span class="glyphicon glyphicon-link" aria-hidden="true"></span></button></h2></div><div class="meta_row"><span class="signatures">ICLR 2019 Conference Paper1308 AnonReviewer2</span></div><div class="clearfix"><div class="meta_row pull-left"><span class="date item">03 Nov 2018 (modified: 07 Nov 2018)</span><span class="item">ICLR 2019 Conference Paper1308 Official Review</span><span class="item">Readers: <span class="readers-icon glyphicon glyphicon-globe"></span> Everyone</span></div><div class="meta_row meta_actions"></div></div><div class="note_contents"><span class="note_content_field">Review: </span><span class="note_content_value">The paper proposes two new modules to overcome some limitations of VIN, but the additional or alternative hypotheses used compared to VIN are not clearly stated and explained in my opinion. 

    Pros :
    - experiments are numerous and advanced
    - transition probabilities are not transition-invariant compared to VIN
    - do not need pretraining trajectories

    Cons :
    - limitation and hypotheses are not very explicit

    Questions/remarks :
    - d_{rew} is not defined 
    - the shared weights should be explained in more details
    - sometimes \psi(s) is written as parametrized by \theta, sometime not
    - is it normal that the \gamma never appears in your formula to update the \theta and w? yet reading the background part I feel that you optimize the discounted sum of the rewards, is it the case?
    - I think there is a mistake in the definition of 1_{s' \neq \emptyset }, it is 1 if s' is NOT terminal and 0 otherwise, am I wrong?
    - why do you need the parameters w to represent the value function V, if you already have v^k_{i,j} available? is it just to say that your NN is updated with two distinct cost functions? 
    - I did not understand the assumptions made by VProp, do you consider that the transition function T is known? this seems to be the case when you explain that transitions are deterministic and that there is a mapping between the actions and the positions, but is never really said
    - Compared to VIN, VProp uses an extra maximum to compute v^k_{i, j}, why? In this case, the approximation of the value function can never decrease.
    - How is R_{a, i, j, i ', j'} broken into r^{in}_{i ', j'} - r^{out}_{i, j} in VProp? Is the reward function known to the agent at all points?
    - In MVProp, can r_{i, j} be negative?
    - In MVProp, how does the rewriting in p * v + r * (1-p) shows that only positive rewards are propagated? Does not it come only from the max?
    - In the experiments, S is not fully described, \phi(s) neither
</span></div><div class="note_contents"><span class="note_content_field">Rating: </span><span class="note_content_value">5: Marginally below acceptance threshold</span></div><div class="note_contents"><span class="note_content_field">Confidence: </span><span class="note_content_value">3: The reviewer is fairly confident that the evaluation is correct</span></div><div class="reply_row clearfix"></div></div><div class="children"></div></div><div class="note_with_children"><div id="note_B1lJJiUc27" class="note panel"><div class="title_pdf_row clearfix"><h2 class="note_content_title"><a>Official review</a>  <button class="btn btn-xs btn-default permalink-button" title="Link to this comment" data-permalink-url="https://openreview.net/forum?id=SJG6G2RqtX&amp;noteId=B1lJJiUc27"><span class="glyphicon glyphicon-link" aria-hidden="true"></span></button></h2></div><div class="meta_row"><span class="signatures">ICLR 2019 Conference Paper1308 AnonReviewer3</span></div><div class="clearfix"><div class="meta_row pull-left"><span class="date item">03 Nov 2018 (modified: 07 Nov 2018)</span><span class="item">ICLR 2019 Conference Paper1308 Official Review</span><span class="item">Readers: <span class="readers-icon glyphicon glyphicon-globe"></span> Everyone</span></div><div class="meta_row meta_actions"></div></div><div class="note_contents"><span class="note_content_field">Review: </span><span class="note_content_value">The paper proposes a learnable planning model based on value iteration. The proposed methods can be seen as modifications of Value Iteration Networks (VIN), with some improvements aimed at improving sample efficiency and generalization to large environment sizes. The method is validated on gridworld-type environments, as well as on a more complex StarCraft-based domain with raw pixel input.

Pros:
1) The topic of the paper is interesting: combining the advantages of learning and planning seems like a promising direction to achieving adaptive and generalizable systems.
2) The presentation is quite good, although some details are missing.
3) The proposed method can be effectively trained with reinforcement learning and generalizes well to much larger environments than trained on. It beats vanilla VIN by a large margin. The MVProp variant of the method is especially successful.

Cons:
1) I would like to see a more complete discussion of the MVProp method. Propagation of only positive rewards seems like somewhat of a hack. Is this a general solution or is it only applicable to gridworld navigation-type tasks? Why? If not, is the area of applicability of MVProp different from VProp? Also, is the area of applicability of VProp different from VIN? It’s important to discuss this in detail.
2) I wonder how would the method behave in more realistic gridworld environments, for instance similar in layout to those used in RL navigation literature (DMLab, ViZDoom, MINOS, etc). The presented environments are quite artificial and seem to basically only require “obstacle avoidance”, not so much deliberate long-distance planning.
3) Some details are missing. For instance, I was not able to find the exact network architectures used in different tasks. 
Related to this, I was confused by the phrase “As these new environments are not static, the agent needs to re-plan at every step, forcing us to train on 8x8 maps to reduce the time spent rolling-out the recurrent modules.” I might be misunderstanding something, but is there any recurrent network in VProp? Isn’t it just predicting the parameters once and then rolling our value iteration forward without any learning? Is this so time-consuming?
4) Why does the performance even of the best method not reach 100% even in the simpler environments in Figure 2? Why is the performance plateauing far from 100% in the more difficult case? It would be interesting to see more analysis of how the method works, when it fails, and which parts still need improvement. On a related topic, it would be good to see more qualitative results both in MazeBaze and StarCraft - in the form of images or videos.
5) Novelty is somewhat limited: the method is conceptually similar to VIN. 

To conclude, I think the paper is interesting and the proposed method seems to perform well in the tested environments. I am quite positive about the paper, and I will gladly raise the rating if my questions are addressed satisfactorily.</span></div><div class="note_contents"><span class="note_content_field">Rating: </span><span class="note_content_value">6: Marginally above acceptance threshold</span></div><div class="note_contents"><span class="note_content_field">Confidence: </span><span class="note_content_value">3: The reviewer is fairly confident that the evaluation is correct</span></div><div class="reply_row clearfix"></div></div><div class="children"></div></div></div></main></div>
  </div>
</div>


    <!-- Footer -->
    <footer class="sitemap">
      <div class="container">
        <div class="row hidden-xs">
          <div class="col-sm-4">
            <ul class="list-unstyled">
              <li><a href="/" class="home">Home</a></li>
              <li><a href="/venues">All Venues</a></li>
            </ul>
          </div>
    
          <div class="col-sm-4">
            <ul class="list-unstyled">
              <li><a href="/about">About OpenReview</a></li>
              <li><a href="/contact">Contact</a></li>
              <li><a href="#" data-toggle="modal" data-target="#feedback-modal">Feedback</a></li>
            </ul>
          </div>
    
          <div class="col-sm-4">
            <ul class="list-unstyled">
              <li><a href="/terms">Terms of Service</a></li>
              <li><a href="/privacy">Privacy Policy</a></li>
            </ul>
          </div>
        </div>
    
        <div class="row visible-xs-block">
          <div class="col-xs-6">
            <ul class="list-unstyled">
              <li><a href="/" class="home">Home</a></li>
              <li><a href="/about">About OpenReview</a></li>
              <li><a href="#" data-toggle="modal" data-target="#feedback-modal">Feedback</a></li>
            </ul>
          </div>
    
          <div class="col-xs-6">
            <ul class="list-unstyled">
              <li><a href="/contact">Contact</a></li>
              <li><a href="/terms">Terms of Service</a></li>
              <li><a href="/privacy">Privacy Policy</a></li>
            </ul>
          </div>
        </div>
      </div>
    </footer>
    
    <footer class="sponsor">
      <div class="container">
        <div class="row">
          <div class="col-sm-10 col-sm-offset-1">
            <p class="text-center">
              OpenReview is created by the <a href="http://www.iesl.cs.umass.edu/" target="_blank">Information Extraction and Synthesis Laboratory</a>, College of Information and Computer Science, University of Massachusetts Amherst. We gratefully acknowledge the support of the OpenReview sponsors:  Google,  Facebook, NSF, the University of Massachusetts Amherst Center for Data Science, and Center for Intelligent Information Retrieval, as well as the Google Cloud Platform for donating the computing and networking services on which OpenReview.net runs.
            </p>
          </div>
        </div>
      </div>
    </footer>

  <div id="feedback-modal" class="modal fade" tabindex="-1" role="dialog">
    <div class="modal-dialog">
      <div class="modal-content">
        <div class="modal-header">
          <button type="button" class="close" data-dismiss="modal" aria-label="Close"><span aria-hidden="true">×</span></button>
          <h3 class="modal-title">Send Feedback</h3>
        </div>
        <div class="modal-body">
          <p>Enter your feedback below and we'll get back to you as soon as possible.</p>
          <form action="/feedback" method="POST">
            <div class="form-group">
              <input type="email" name="from" class="form-control" placeholder="Email" />
            </div>
            <div class="form-group">
              <input type="text" name="subject" class="form-control" placeholder="Subject" />
            </div>
            <div class="form-group">
              <textarea name="message" class="form-control feedback-input" rows="5" placeholder="Message" required=""></textarea>
            </div>
          <ul id="ui-id-2" tabindex="0" class="ui-menu ui-widget ui-widget-content" style="display: none;"></ul></form>
        </div>
        <div class="modal-footer">
          <button type="button" class="btn btn-default" data-dismiss="modal">Cancel</button>
          <button type="button" class="btn btn-primary">Send</button>
        </div>
      </div>
    </div>
  </div>

  <script type="text/javascript" async="" src="https://www.google-analytics.com/analytics.js"></script><script src="//cdnjs.cloudflare.com/ajax/libs/jquery/2.2.4/jquery.min.js"></script>
  <script>window.jQuery || document.write('&lt;script src="/static/js/vendor/jquery-2.2.4.min.js"&gt;&lt;\/script&gt;')</script>

  <script src="/static/dist/vendor.min.js"></script>
  <script src="/static/dist/templates.min.js"></script>
  <script src="/static/dist/openreview.min.js"></script>

    <script>window.legacyScripts = true;</script>


    <script async="" src="https://www.googletagmanager.com/gtag/js?id=UA-108703919-1"></script>
    <script>
      window.dataLayer = window.dataLayer || [];
      function gtag() { dataLayer.push(arguments); }
      gtag('js', new Date());
      gtag('config', 'UA-108703919-1');
    </script>


<div role="status" aria-live="assertive" aria-relevant="additions" class="ui-helper-hidden-accessible"></div></body></html>