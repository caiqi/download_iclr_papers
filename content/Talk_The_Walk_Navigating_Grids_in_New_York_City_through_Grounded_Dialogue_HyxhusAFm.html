<!DOCTYPE html><html xmlns="http://www.w3.org/1999/xhtml" class="no-js"><head>
  <meta charset="utf-8" />
  <meta http-equiv="X-UA-Compatible" content="IE=edge,chrome=1" />
  <meta name="viewport" content="width=device-width, initial-scale=1" />

  <title>Talk The Walk: Navigating Grids in New York City through Grounded Dialogue | OpenReview</title>
  <meta name="description" content="" />

      <meta name="citation_title" content="Talk The Walk: Navigating Grids in New York City through Grounded Dialogue" />
        <meta name="citation_author" content="Anonymous" />
      <meta name="citation_publication_date" content="2018/09/27" />
      <meta name="citation_online_date" content="2018/09/27" />
      <meta name="citation_pdf_url" content="https://openreview.net/pdf?id=HyxhusA9Fm" />
      <meta name="twitter:card" content="summary" />
      <meta name="twitter:site" content="@openreviewnet" />
      <meta name="og:title" content="Talk The Walk: Navigating Grids in New York City through Grounded..." />
      <meta name="og:description" content="We introduce `" talk="" the="" walk",="" first="" large-scale="" dialogue="" dataset="" grounded="" in="" action="" and="" perception.="" task="" involves="" two="" agents="" (a="" 'guide'="" a="" 'tourist')="" that="" communicate="" via="" natural..."="" />
      <meta name="og:image" content="https://openreview.net/static/images/openreview_logo_512.png" />

  <link rel="stylesheet" href="https://fonts.googleapis.com/css?family=Noto+Sans:400,400i,700,700i" />

  <link rel="stylesheet" href="/static/css/bootstrap.min.css" />
  <link rel="stylesheet" href="/static/css/jquery-ui.min.css" />
  <link rel="stylesheet" href="/static/css/main.min.css" />
</head>

<body class="forum">
  <nav class="navbar navbar-inverse navbar-fixed-top" role="navigation">
    <div class="container">
  
      <div class="navbar-header">
        <button type="button" class="navbar-toggle collapsed" data-toggle="collapse" data-target="#navbar" aria-expanded="false" aria-controls="navbar">
          <span class="sr-only">Toggle navigation</span>
          <span class="icon-bar"></span>
          <span class="icon-bar"></span>
          <span class="icon-bar"></span>
        </button>
        <a class="navbar-brand home push-link" href="/"><strong>OpenReview</strong>.net</a>
      </div>
  
      <div id="navbar" class="navbar-collapse collapse">
        <form class="navbar-form navbar-left profile-search" role="search">
          <div class="form-group has-feedback">
            <input id="search_input" type="text" class="form-control ui-autocomplete-input" placeholder="Search ICLR 2019 Conference" autocomplete="off" />
            <span class="glyphicon glyphicon-search form-control-feedback" aria-hidden="true"></span>
          </div>
  
          <input id="search_group" type="hidden" value="ICLR.cc/2019/Conference" />
          <input id="search_content" type="hidden" value="all" />
        <ul id="ui-id-1" tabindex="0" class="ui-menu ui-widget ui-widget-content ui-autocomplete ui-front" style="display: none;"></ul></form>
  
        <ul class="nav navbar-nav navbar-right">
        
            <li id="user-menu"><a href="/login">Login</a></li>
        </ul>
      </div>
  
    </div>
  </nav>

  <div id="or-banner" class="banner" style="">
  <div class="container">
    <div class="row">
      <div class="col-xs-12"><a href="/group?id=ICLR.cc/2019/Conference" title="Venue Homepage"><img class="icon" src="/static/images/arrow_left.svg" /> Go to <strong>ICLR 2019 Conference</strong> homepage</a></div>
    </div>
  </div>
</div>
<div id="flash-message-container" class="alert alert-danger" role="alert" style="display: none;">
  <div class="container">
    <div class="row">
      <div class="col-xs-12">
        <div class="alert-content">
          <button type="button" class="close" aria-label="Close"><span aria-hidden="true">×</span></button>
        </div>
      </div>
    </div>
  </div>
</div>

<div class="container">
  <div class="row">
    <div class="col-xs-12">
      <main id="content" class="clearfix openbanner-visible legacy-styles"><div id="note_HyxhusA9Fm" class="note panel"><div class="title_pdf_row clearfix"><h2 class="note_content_title"><a>Talk The Walk: Navigating Grids in New York City through Grounded Dialogue</a> <a class="note_content_pdf" href="/pdf?id=HyxhusA9Fm" title="Download PDF" target="_blank"><img src="/static/images/pdf_icon_blue.svg" /></a> </h2></div><div class="meta_row"><span class="signatures">Anonymous</span></div><div class="clearfix"><div class="meta_row pull-left"><span class="date item">28 Sep 2018 (modified: 11 Oct 2018)</span><span class="item">ICLR 2019 Conference Blind Submission</span><span class="item">Readers: <span class="readers-icon glyphicon glyphicon-globe"></span> Everyone</span><span class="item"><a class="action-bibtex-modal" data-bibtex="@inproceedings{    &#10;anonymous2019talk,    &#10;title={Talk The Walk: Navigating Grids in New York City through Grounded Dialogue},    &#10;author={Anonymous},    &#10;booktitle={Submitted to International Conference on Learning Representations},    &#10;year={2019},    &#10;url={https://openreview.net/forum?id=HyxhusA9Fm},    &#10;note={under review}    &#10;}">Show Bibtex</a></span></div><div class="meta_row meta_actions"></div></div><div class="note_contents"><span class="note_content_field">Abstract: </span><span class="note_content_value">We introduce `"Talk The Walk", the first large-scale dialogue dataset grounded in action and perception. The task involves two agents (a 'guide' and a 'tourist') that communicate via natural language in order to achieve a common goal: having the tourist navigate to a given target location. The task and dataset, which are described in detail, are challenging and their full solution is an open problem that we pose to the community. We (i) focus on the task of tourist localization and develop the novel Masked Attention for Spatial Convolutions (MASC) mechanism that allows for grounding tourist utterances into the guide's map, (ii) show it yields significant improvements for both emergent and natural language communication, and (iii) using this method, we establish non-trivial baselines on the full task. </span></div><div class="note_contents"><span class="note_content_field">Keywords: </span><span class="note_content_value">Dialogue, Navigation, Grounded Language Learning</span></div><div class="note_contents"><span class="note_content_field">TL;DR: </span><span class="note_content_value">First large-scale dialogue dataset grounded in action and perception</span></div><div class="reply_row clearfix"><div class="item" id="reply_count">5 Replies</div></div></div><hr class="small" /><div id="note_children"><div class="note_with_children"><div id="note_r1lkCfhc3m" class="note panel"><div class="title_pdf_row clearfix"><h2 class="note_content_title"><a>A realistic dataset on dialogs for navigation, with a report of some early studies</a>  <button class="btn btn-xs btn-default permalink-button" title="Link to this comment" data-permalink-url="https://openreview.net/forum?id=HyxhusA9Fm&amp;noteId=r1lkCfhc3m"><span class="glyphicon glyphicon-link" aria-hidden="true"></span></button></h2></div><div class="meta_row"><span class="signatures">ICLR 2019 Conference Paper389 AnonReviewer2</span></div><div class="clearfix"><div class="meta_row pull-left"><span class="date item">03 Nov 2018 (modified: 07 Nov 2018)</span><span class="item">ICLR 2019 Conference Paper389 Official Review</span><span class="item">Readers: <span class="readers-icon glyphicon glyphicon-globe"></span> Everyone</span></div><div class="meta_row meta_actions"></div></div><div class="note_contents"><span class="note_content_field">Review: </span><span class="note_content_value">The paper introduces a new dataset "Talk the Walk" that are dialogs
between a guide and a tourist, where the guide is to help the tourist
navigate to a target location.  The guide has access to a map and the
target location, but he relies on the tourist to communicate her
state (location) by natural language.

Pros.:

The task represented in the dataset can be highly challenging, and
respectable effort went into creating the dataset based on real city
neighborhoods.   The description and analysis of the dataset are
detailed.  The paper is well written up to the end of page 3.

The dataset by itself is a good contribution to the scientific
community when it is shared.  There could be many topics open
for studying within the same data.

The list of references and related work is exceptionally thorough
and useful for researchers interested in the topic.

Cons.:

The description of the experiments done with the dataset, however,
suffers from being overly cryptic.  The methods are not sufficiently
motivated, very few alternatives are presented and argued against,
and the several sections give a dry report of the sequences of things
the authors did.  It is not clear how others may find value in the
results and conclusions.

While the paper opens with the emphasis of a real-world setting,
after a series of simplifications (e.g. landmark typing, perfect perception)
it seems that much of the full complexity of the natural task is taken out,
and the main goal of the study is no longer clear.
For example, since there are only 9 types of landmarks,
in a small neighborhood there are not many combinations to draw reference to.
Simple observation sequences of such can easily narrow down the location
uncertainty.  It is important to highlight what the remaining open
issues are that make the task still challenging.

Misc.:

Examples are missing in the discussion of the experimental tasks.
e.g. in the study of emergent language, what could be a message that a
tourist may generate to describe his location?  what makes it hard for
the guide to decode it?  Likewise, what could be an example state of
the tourist and the description of that state in natural language?
Without the examples, it is difficult for the reader to have a sense
of the challenges in each task.

Table 8 is the first place where (finally) some utterances are presented.
However the description in the table or in the text is not sufficient
to convey the point that is supposed to be explained by the example.

The descriptions of the landmarks are restricted to the type of business
at the location with 9 possibilities.  Is the list of 9 exhaustive?
Are there any exceptions  (e.g. schools)?  How are such exceptions
represented and treated in the dialogs?

To what extent the difficulty of the tasks depends on the variability
of the combination of landmarks visible at each location?

What could be a simplest way to do this without neural-modeling?
e.g. with the many limitations that are built into the task and its
representations, will a simple decision tree based instruction method suffice?
Or a traditional algorithm that relies on repeated exploration and evaluation?
It is surprising that such possibilities are not even mentioned.
A complex neural architecture does not seem to be well justified unless
it is motivated by the need to overcome limitations of a classical method.

Irrelevant to the research effort, a thought about the dataset is that,
in these days with popular uses of GPS, the reliance on such dialogs for
navigation feels a bit backwards.
</span></div><div class="note_contents"><span class="note_content_field">Rating: </span><span class="note_content_value">6: Marginally above acceptance threshold</span></div><div class="note_contents"><span class="note_content_field">Confidence: </span><span class="note_content_value">4: The reviewer is confident but not absolutely certain that the evaluation is correct</span></div><div class="reply_row clearfix"></div></div><div class="children"></div></div><div class="note_with_children"><div id="note_rklAz2j92m" class="note panel"><div class="title_pdf_row clearfix"><h2 class="note_content_title"><a>A challenging new task and dataset</a>  <button class="btn btn-xs btn-default permalink-button" title="Link to this comment" data-permalink-url="https://openreview.net/forum?id=HyxhusA9Fm&amp;noteId=rklAz2j92m"><span class="glyphicon glyphicon-link" aria-hidden="true"></span></button></h2></div><div class="meta_row"><span class="signatures">ICLR 2019 Conference Paper389 AnonReviewer1</span></div><div class="clearfix"><div class="meta_row pull-left"><span class="date item">03 Nov 2018 (modified: 07 Nov 2018)</span><span class="item">ICLR 2019 Conference Paper389 Official Review</span><span class="item">Readers: <span class="readers-icon glyphicon glyphicon-globe"></span> Everyone</span></div><div class="meta_row meta_actions"></div></div><div class="note_contents"><span class="note_content_field">Review: </span><span class="note_content_value">The paper introduces a new task called "Talk the Walk", where a tourist and a guide has to communicate in natural language to reach a common goal. It also introduces strong baselines for the task. The descriptions are thorough and clear. My only worry is that the task is too hard and has too many complexities to be a stand alone task.  Future work will probably focus on sub-parts of the task.</span></div><div class="note_contents"><span class="note_content_field">Rating: </span><span class="note_content_value">7: Good paper, accept</span></div><div class="note_contents"><span class="note_content_field">Confidence: </span><span class="note_content_value">4: The reviewer is confident but not absolutely certain that the evaluation is correct</span></div><div class="reply_row clearfix"></div></div><div class="children"></div></div><div class="note_with_children"><div id="note_rkgGwTlF27" class="note panel"><div class="title_pdf_row clearfix"><h2 class="note_content_title"><a>Emergent Language is Easier than Natural Language</a>  <button class="btn btn-xs btn-default permalink-button" title="Link to this comment" data-permalink-url="https://openreview.net/forum?id=HyxhusA9Fm&amp;noteId=rkgGwTlF27"><span class="glyphicon glyphicon-link" aria-hidden="true"></span></button></h2></div><div class="meta_row"><span class="signatures">ICLR 2019 Conference Paper389 AnonReviewer3</span></div><div class="clearfix"><div class="meta_row pull-left"><span class="date item">02 Nov 2018 (modified: 07 Nov 2018)</span><span class="item">ICLR 2019 Conference Paper389 Official Review</span><span class="item">Readers: <span class="readers-icon glyphicon glyphicon-globe"></span> Everyone</span></div><div class="meta_row meta_actions"></div></div><div class="note_contents"><span class="note_content_field">Review: </span><span class="note_content_value">The primary contribution of this work is a dataset for action following through dialogue.  The authors collect a comparatively small dataset in terms of language but one which contains real images and dialogue. 

There are a number of aspects of the proposed approach which I found hard to follow/justify.  First off, I was unclear on the details of the collected data (e.g. average action sequence length, dialogue length, lexical types/tokens, etc).  There's a claim of 62 acts which sums both dialogue and actions with averages of 8 and 9 dialogue acts for tourist/guide implying 45 move actions?  on a 4x4 grid?  Is it safe therefore to assume that the example dialogue is therefore atypical? It's very hard to figure out based on the number of steps across the different tables what the model should be aiming for.  Also, in 2.3 does the claim that they "successfully complete the task" mean in the 76.74% of cases where they succeed or did they succeed in 100% of cases and then a new human eval was run afterwards which performed worse?

The primary modeling result appears to be the success of emergent language and the bold claim that humans are bad at localizing.  This doesn't feel intuitively true from the example dialogue, but the NLG system samples to appear to be quite bad which makes me worried that it's not so much that humans are bad localizers but that the model's NLU/NLG system is quite weak and maybe there's a problem with the data-collection procedure.  Additional justification and analysis would be appreciated.

As I understand the paper right now:
1. Humans talking to one another do very well on the task and achieve success very quickly. 
2. Emergent language can do better at the task though their approach is very sub-optimal (requiring 2-3x the number of steps).
3. The currently proposed NLU/NLG mechanisms are very weak and cannot produce or correctly interpret actual language.

There are many moving pieces in this paper (e.g. extracting text from images vs detections), there doesn't seem to be any pretraining of the decoder, etc which makes it very hard for me to understand what's going wrong.  The results in this paper, don't convince me that emergent language is better than natural language or that agents are better communicators than humans, but that the data-collection methodology was faulty leading to lots of failures. 

I haven't touched on the MASC aspect and how this compares to existing work on interpretable spatial relations and questions as to why various architectural choices were made though the paper would obviously benefit from that discussion as well.

I found this paper very confusing to read.  It relies heavily on 11 pages of appendices (where it puts all of the related work) and still fails to clearly explain its contributions or justify its claims.  

Minor: URLs intermittently anonymized page 12 vs 19</span></div><div class="note_contents"><span class="note_content_field">Rating: </span><span class="note_content_value">4: Ok but not good enough - rejection</span></div><div class="note_contents"><span class="note_content_field">Confidence: </span><span class="note_content_value">3: The reviewer is fairly confident that the evaluation is correct</span></div><div class="reply_row clearfix"></div></div><div class="children"></div></div><div class="note_with_children"><div id="note_Hygemsnz5Q" class="note panel"><div class="title_pdf_row clearfix"><h2 class="note_content_title"><a>Great task but is the Perfect Perception oversimplified the problem?</a>  <button class="btn btn-xs btn-default permalink-button" title="Link to this comment" data-permalink-url="https://openreview.net/forum?id=HyxhusA9Fm&amp;noteId=Hygemsnz5Q"><span class="glyphicon glyphicon-link" aria-hidden="true"></span></button></h2></div><div class="meta_row"><span class="signatures">(anonymous)</span></div><div class="clearfix"><div class="meta_row pull-left"><span class="date item">04 Oct 2018</span><span class="item">ICLR 2019 Conference Paper389 Public Comment</span><span class="item">Readers: <span class="readers-icon glyphicon glyphicon-globe"></span> Everyone</span></div><div class="meta_row meta_actions"></div></div><div class="note_contents"><span class="note_content_field">Comment: </span><span class="note_content_value">As mentioned in the paper, "To ensure that perception is not the limiting factor when investigating localization capabilities of models, we assume “perfect perception”: in lieu of the 360 image view, the tourist is given the landmarks at its current location," does this mean that the tourist does not need to learn the grounding between language and vision? In this way, doesn't it make the task too easy (even surpass human) because the tourist just need to say every landmarks provided one-by-one instead of any other real conversation?In the human evaluation, did they also get all the landmarks?  And once the guides get all the names of the landmarks, it is also easy for them to find the location by matching the words (or corresponding vectors). 

Can some examples of your model's natural language generation text be provided? That will be helpful :) </span></div><div class="reply_row clearfix"></div></div><div class="children"><div class="note_with_children"><div id="note_BkgwVGyN5X" class="note panel"><div class="title_pdf_row clearfix"><h2 class="note_content_title"><a>Perfect Perception is to establish sensible baselines, but is not part of the full task</a>  <button class="btn btn-xs btn-default permalink-button" title="Link to this comment" data-permalink-url="https://openreview.net/forum?id=HyxhusA9Fm&amp;noteId=BkgwVGyN5X"><span class="glyphicon glyphicon-link" aria-hidden="true"></span></button></h2></div><div class="meta_row"><span class="signatures">ICLR 2019 Conference Paper389 Authors</span></div><div class="clearfix"><div class="meta_row pull-left"><span class="date item">05 Oct 2018</span><span class="item">ICLR 2019 Conference Paper389 Official Comment</span><span class="item">Readers: <span class="readers-icon glyphicon glyphicon-globe"></span> Everyone</span></div><div class="meta_row meta_actions"></div></div><div class="note_contents"><span class="note_content_field">Comment: </span><span class="note_content_value">Thank you for your interest in our work!

The perfect perception assumption is not part of the full task that we propose (i.e. the gathered human data is using 360-degree navigation with two-way natural language communication). We simplified the perception aspect in our experiments because initial findings suggested that perceptual grounding is difficult for simple baseline models (see Section 12 of the Appendix). This indeed makes it easier for the tourist to produce a message about the observed landmarks. Nevertheless, the localization task is still challenging because only communicating the list of observed landmarks is not sufficient for accurate localization (see Table 2:  the upper bound on localization performance is ~56% for taking T=3 actions). In other words, the tourist also needs to communicate their actions to the guide, who is responsible for grounding the entire trajectory (landmarks + actions) in the overhead map before predicting the location. We believe that this sub-task is non-trivial and develop a novel action-grounding mechanism (MASC) that is essential to obtain high localization accuracy with emergent language, as well as improves performance for natural language. 

You can find samples of the natural language model (for different decoding strategies) in Table 8 of the Appendix :)</span></div><div class="reply_row clearfix"></div></div><div class="children"></div></div></div></div></div></main></div>
  </div>
</div>


    <!-- Footer -->
    <footer class="sitemap">
      <div class="container">
        <div class="row hidden-xs">
          <div class="col-sm-4">
            <ul class="list-unstyled">
              <li><a href="/" class="home">Home</a></li>
              <li><a href="/venues">All Venues</a></li>
            </ul>
          </div>
    
          <div class="col-sm-4">
            <ul class="list-unstyled">
              <li><a href="/about">About OpenReview</a></li>
              <li><a href="/contact">Contact</a></li>
              <li><a href="#" data-toggle="modal" data-target="#feedback-modal">Feedback</a></li>
            </ul>
          </div>
    
          <div class="col-sm-4">
            <ul class="list-unstyled">
              <li><a href="/terms">Terms of Service</a></li>
              <li><a href="/privacy">Privacy Policy</a></li>
            </ul>
          </div>
        </div>
    
        <div class="row visible-xs-block">
          <div class="col-xs-6">
            <ul class="list-unstyled">
              <li><a href="/" class="home">Home</a></li>
              <li><a href="/about">About OpenReview</a></li>
              <li><a href="#" data-toggle="modal" data-target="#feedback-modal">Feedback</a></li>
            </ul>
          </div>
    
          <div class="col-xs-6">
            <ul class="list-unstyled">
              <li><a href="/contact">Contact</a></li>
              <li><a href="/terms">Terms of Service</a></li>
              <li><a href="/privacy">Privacy Policy</a></li>
            </ul>
          </div>
        </div>
      </div>
    </footer>
    
    <footer class="sponsor">
      <div class="container">
        <div class="row">
          <div class="col-sm-10 col-sm-offset-1">
            <p class="text-center">
              OpenReview is created by the <a href="http://www.iesl.cs.umass.edu/" target="_blank">Information Extraction and Synthesis Laboratory</a>, College of Information and Computer Science, University of Massachusetts Amherst. We gratefully acknowledge the support of the OpenReview sponsors:  Google,  Facebook, NSF, the University of Massachusetts Amherst Center for Data Science, and Center for Intelligent Information Retrieval, as well as the Google Cloud Platform for donating the computing and networking services on which OpenReview.net runs.
            </p>
          </div>
        </div>
      </div>
    </footer>

  <div id="feedback-modal" class="modal fade" tabindex="-1" role="dialog">
    <div class="modal-dialog">
      <div class="modal-content">
        <div class="modal-header">
          <button type="button" class="close" data-dismiss="modal" aria-label="Close"><span aria-hidden="true">×</span></button>
          <h3 class="modal-title">Send Feedback</h3>
        </div>
        <div class="modal-body">
          <p>Enter your feedback below and we'll get back to you as soon as possible.</p>
          <form action="/feedback" method="POST">
            <div class="form-group">
              <input type="email" name="from" class="form-control" placeholder="Email" />
            </div>
            <div class="form-group">
              <input type="text" name="subject" class="form-control" placeholder="Subject" />
            </div>
            <div class="form-group">
              <textarea name="message" class="form-control feedback-input" rows="5" placeholder="Message" required=""></textarea>
            </div>
          <ul id="ui-id-2" tabindex="0" class="ui-menu ui-widget ui-widget-content" style="display: none;"></ul></form>
        </div>
        <div class="modal-footer">
          <button type="button" class="btn btn-default" data-dismiss="modal">Cancel</button>
          <button type="button" class="btn btn-primary">Send</button>
        </div>
      </div>
    </div>
  </div>

  <script type="text/javascript" async="" src="https://www.google-analytics.com/analytics.js"></script><script src="//cdnjs.cloudflare.com/ajax/libs/jquery/2.2.4/jquery.min.js"></script>
  <script>window.jQuery || document.write('&lt;script src="/static/js/vendor/jquery-2.2.4.min.js"&gt;&lt;\/script&gt;')</script>

  <script src="/static/dist/vendor.min.js"></script>
  <script src="/static/dist/templates.min.js"></script>
  <script src="/static/dist/openreview.min.js"></script>

    <script>window.legacyScripts = true;</script>


    <script async="" src="https://www.googletagmanager.com/gtag/js?id=UA-108703919-1"></script>
    <script>
      window.dataLayer = window.dataLayer || [];
      function gtag() { dataLayer.push(arguments); }
      gtag('js', new Date());
      gtag('config', 'UA-108703919-1');
    </script>


<div role="status" aria-live="assertive" aria-relevant="additions" class="ui-helper-hidden-accessible"></div></body></html>