<!DOCTYPE html><html xmlns="http://www.w3.org/1999/xhtml" class="no-js"><head>
  <meta charset="utf-8" />
  <meta http-equiv="X-UA-Compatible" content="IE=edge,chrome=1" />
  <meta name="viewport" content="width=device-width, initial-scale=1" />

  <title>Spreading vectors for similarity search | OpenReview</title>
  <meta name="description" content="" />

      <meta name="citation_title" content="Spreading vectors for similarity search" />
        <meta name="citation_author" content="Anonymous" />
      <meta name="citation_publication_date" content="2018/09/27" />
      <meta name="citation_online_date" content="2018/09/27" />
      <meta name="citation_pdf_url" content="https://openreview.net/pdf?id=SkGuG2R5tm" />
      <meta name="twitter:card" content="summary" />
      <meta name="twitter:site" content="@openreviewnet" />
      <meta name="og:title" content="Spreading vectors for similarity search" />
      <meta name="og:description" content="Discretizing floating-point vectors is a fundamental step of modern indexing methods. State-of-the-art techniques learn parameters of the quantizers on training data for optimal performance, thus..." />
      <meta name="og:image" content="https://openreview.net/static/images/openreview_logo_512.png" />

  <link rel="stylesheet" href="https://fonts.googleapis.com/css?family=Noto+Sans:400,400i,700,700i" />

  <link rel="stylesheet" href="/static/css/bootstrap.min.css" />
  <link rel="stylesheet" href="/static/css/jquery-ui.min.css" />
  <link rel="stylesheet" href="/static/css/main.min.css" />
</head>

<body class="forum">
  <nav class="navbar navbar-inverse navbar-fixed-top" role="navigation">
    <div class="container">
  
      <div class="navbar-header">
        <button type="button" class="navbar-toggle collapsed" data-toggle="collapse" data-target="#navbar" aria-expanded="false" aria-controls="navbar">
          <span class="sr-only">Toggle navigation</span>
          <span class="icon-bar"></span>
          <span class="icon-bar"></span>
          <span class="icon-bar"></span>
        </button>
        <a class="navbar-brand home push-link" href="/"><strong>OpenReview</strong>.net</a>
      </div>
  
      <div id="navbar" class="navbar-collapse collapse">
        <form class="navbar-form navbar-left profile-search" role="search">
          <div class="form-group has-feedback">
            <input id="search_input" type="text" class="form-control ui-autocomplete-input" placeholder="Search ICLR 2019 Conference" autocomplete="off" />
            <span class="glyphicon glyphicon-search form-control-feedback" aria-hidden="true"></span>
          </div>
  
          <input id="search_group" type="hidden" value="ICLR.cc/2019/Conference" />
          <input id="search_content" type="hidden" value="all" />
        <ul id="ui-id-1" tabindex="0" class="ui-menu ui-widget ui-widget-content ui-autocomplete ui-front" style="display: none;"></ul></form>
  
        <ul class="nav navbar-nav navbar-right">
        
            <li id="user-menu"><a href="/login">Login</a></li>
        </ul>
      </div>
  
    </div>
  </nav>

  <div id="or-banner" class="banner" style="">
  <div class="container">
    <div class="row">
      <div class="col-xs-12"><a href="/group?id=ICLR.cc/2019/Conference" title="Venue Homepage"><img class="icon" src="/static/images/arrow_left.svg" /> Go to <strong>ICLR 2019 Conference</strong> homepage</a></div>
    </div>
  </div>
</div>
<div id="flash-message-container" class="alert alert-danger" role="alert" style="display: none;">
  <div class="container">
    <div class="row">
      <div class="col-xs-12">
        <div class="alert-content">
          <button type="button" class="close" aria-label="Close"><span aria-hidden="true">×</span></button>
        </div>
      </div>
    </div>
  </div>
</div>

<div class="container">
  <div class="row">
    <div class="col-xs-12">
      <main id="content" class="clearfix openbanner-visible legacy-styles"><div id="note_SkGuG2R5tm" class="note panel"><div class="title_pdf_row clearfix"><h2 class="note_content_title"><a>Spreading vectors for similarity search</a> <a class="note_content_pdf" href="/pdf?id=SkGuG2R5tm" title="Download PDF" target="_blank"><img src="/static/images/pdf_icon_blue.svg" /></a> </h2></div><div class="meta_row"><span class="signatures">Anonymous</span></div><div class="clearfix"><div class="meta_row pull-left"><span class="date item">28 Sep 2018 (modified: 11 Oct 2018)</span><span class="item">ICLR 2019 Conference Blind Submission</span><span class="item">Readers: <span class="readers-icon glyphicon glyphicon-globe"></span> Everyone</span><span class="item"><a class="action-bibtex-modal" data-bibtex="@inproceedings{    &#10;anonymous2019spreading,    &#10;title={Spreading vectors for similarity search},    &#10;author={Anonymous},    &#10;booktitle={Submitted to International Conference on Learning Representations},    &#10;year={2019},    &#10;url={https://openreview.net/forum?id=SkGuG2R5tm},    &#10;note={under review}    &#10;}">Show Bibtex</a></span></div><div class="meta_row meta_actions"></div></div><div class="note_contents"><span class="note_content_field">Abstract: </span><span class="note_content_value">Discretizing floating-point vectors is a fundamental step of modern indexing methods. State-of-the-art techniques learn parameters of the quantizers on training data for optimal performance, thus adapting quantizers to the data. In this work, we propose to reverse this paradigm and adapt the data to the quantizer: we train a neural net whose last layers form a fixed parameter-free quantizer, such as pre-defined points of a sphere. As a proxy objective, we design and train a neural network that favors uniformity in the spherical latent space, while preserving the neighborhood structure after the mapping.  For this purpose, we propose a new regularizer derived from the Kozachenko-Leonenko differential entropy estimator and combine it with a locality-aware triplet loss. 
Experiments show that our end-to-end approach outperforms most learned quantization methods, and is competitive with the state of the art on widely adopted benchmarks. Further more, we show that training without the quantization step results in almost no difference in accuracy, but yields a generic catalyser that can be applied with any subsequent quantization technique.
</span></div><div class="note_contents"><span class="note_content_field">Keywords: </span><span class="note_content_value">dimensionality reduction, similarity search, indexing, differential entropy</span></div><div class="note_contents"><span class="note_content_field">TL;DR: </span><span class="note_content_value">We learn a neural network that uniformizes the input distribution, which leads to competitive indexing performance in high-dimensional space</span></div><div class="reply_row clearfix"><div class="item" id="reply_count">6 Replies</div></div></div><hr class="small" /><div id="note_children"><div class="note_with_children"><div id="note_Bkxs4_VP2m" class="note panel"><div class="title_pdf_row clearfix"><h2 class="note_content_title"><a>Spreading vectors for similarity search</a>  <button class="btn btn-xs btn-default permalink-button" title="Link to this comment" data-permalink-url="https://openreview.net/forum?id=SkGuG2R5tm&amp;noteId=Bkxs4_VP2m"><span class="glyphicon glyphicon-link" aria-hidden="true"></span></button></h2></div><div class="meta_row"><span class="signatures">ICLR 2019 Conference Paper1282 AnonReviewer2</span></div><div class="clearfix"><div class="meta_row pull-left"><span class="date item">31 Oct 2018 (modified: 07 Nov 2018)</span><span class="item">ICLR 2019 Conference Paper1282 Official Review</span><span class="item">Readers: <span class="readers-icon glyphicon glyphicon-globe"></span> Everyone</span></div><div class="meta_row meta_actions"></div></div><div class="note_contents"><span class="note_content_field">Review: </span><span class="note_content_value">The authors propose a method to adapt the data to the quantizer, instead of having to work with a difficult to optimize discretization function. The contribution is interesting.

Additional comments and suggestions:

- in the related work overview it would be good to also check possible connections with optimal transport methods using entropy regularization.

- at some points in the paper, e.g. section 3.3, the authors mention Voronoi cells. However, in the related work in section 2 vector quantization and self-organizing maps have not been mentioned.

- more details on the optimization or learning algorithms for eq (3)(4) should be given. The loss function is non-smooth and rather complicated. What are the implications on the learning algorithm when training neural networks? Is it important to have a good initialization or not?

- How reproducible are the results? In Table 1 only one number in each column is shown while eqs (3)(4) are non-convex problems. Is it the best result of several runs or an average that is reported in the Table? 

</span></div><div class="note_contents"><span class="note_content_field">Rating: </span><span class="note_content_value">6: Marginally above acceptance threshold</span></div><div class="note_contents"><span class="note_content_field">Confidence: </span><span class="note_content_value">3: The reviewer is fairly confident that the evaluation is correct</span></div><div class="reply_row clearfix"></div></div><div class="children"><div class="note_with_children"><div id="note_BygLXqBd6m" class="note panel"><div class="title_pdf_row clearfix"><h2 class="note_content_title"><a>Response to Review #2</a>  <button class="btn btn-xs btn-default permalink-button" title="Link to this comment" data-permalink-url="https://openreview.net/forum?id=SkGuG2R5tm&amp;noteId=BygLXqBd6m"><span class="glyphicon glyphicon-link" aria-hidden="true"></span></button></h2></div><div class="meta_row"><span class="signatures">ICLR 2019 Conference Paper1282 Authors</span></div><div class="clearfix"><div class="meta_row pull-left"><span class="date item">13 Nov 2018</span><span class="item">ICLR 2019 Conference Paper1282 Official Comment</span><span class="item">Readers: <span class="readers-icon glyphicon glyphicon-globe"></span> Everyone</span></div><div class="meta_row meta_actions"></div></div><div class="note_contents"><span class="note_content_field">Comment: </span><span class="note_content_value">We thank the reviewer for their review. Upon publication of the paper, we will open-source the code that replicates the experiments. In the meantime, we provide more details below:

1) “in the related work overview it would be good to also check possible connections with optimal transport methods using entropy regularization.“ 
In the related work, we mention Bojanowski &amp; Joulin (2017), who use optimal transport (without entropy regularization) to match images with random points on the sphere. The entropy regularization in optimal transport is a bit different than our entropy regularization as it is used mainly for speed purposes, whereas our entropy regularization provides a trade-off between the quality of nearest neighbors and how spread-out the output of the neural network is.

2) “at some points in the paper, e.g. section 3.3, the authors mention Voronoi cells. However, in the related work in section 2 vector quantization and self-organizing maps have not been mentioned.”
We will update the related work with these references. In the context of section 3.3, Voronoi cells correspond to the quantization cells of the lattice.

3) “more details on the optimization or learning algorithms for eq (3)(4) should be given. The loss function is non-smooth and rather complicated. What are the implications on the learning algorithm when training neural networks? Is it important to have a good initialization or not?”
We found that standard practice for training neural networks worked quite well in our setting (even though we have no guarantee of getting to the global minimum of the objective function). More specifically, we train our networks with Stochastic Gradient descent with an initial learning rate of 0.5, momentum of 0.9, and decay the learning rate when the validation accuracy does not go up for an epoch. We did not need specific initialization to make the networks converge.

4) “How reproducible are the results? In Table 1 only one number in each column is shown while eqs (3)(4) are non-convex problems. Is it the best result of several runs or an average that is reported in the Table? “
Our preliminary experiments have shown that the difference in performance between different trainings is very small (despite the problems being non-convex).  Therefore we train only once per set of hyper-parameters (d_out and lambda), and report the corresponding result. Our open-source code will reproduce these results up to the (very small) variations due to random initialization and mini-batch sampling.

</span></div><div class="reply_row clearfix"></div></div><div class="children"></div></div></div></div><div class="note_with_children"><div id="note_B1x0Ka5M2X" class="note panel"><div class="title_pdf_row clearfix"><h2 class="note_content_title"><a>Well motivated novel idea; excellent results</a>  <button class="btn btn-xs btn-default permalink-button" title="Link to this comment" data-permalink-url="https://openreview.net/forum?id=SkGuG2R5tm&amp;noteId=B1x0Ka5M2X"><span class="glyphicon glyphicon-link" aria-hidden="true"></span></button></h2></div><div class="meta_row"><span class="signatures">ICLR 2019 Conference Paper1282 AnonReviewer3</span></div><div class="clearfix"><div class="meta_row pull-left"><span class="date item">28 Oct 2018 (modified: 07 Nov 2018)</span><span class="item">ICLR 2019 Conference Paper1282 Official Review</span><span class="item">Readers: <span class="readers-icon glyphicon glyphicon-globe"></span> Everyone</span></div><div class="meta_row meta_actions"></div></div><div class="note_contents"><span class="note_content_field">Review: </span><span class="note_content_value">Pros
----

[Originality]
The authors propose a novel idea of learning representations that improves the performance of the subsequent fixed discretization method.

[Clarity]
The authors clearly motivate their solution and explain the different ideas and enhancements introduced. The manuscript is fairly easy to follow. The different terms in the optimization problem are clearly explained and their individual behaviour are presented for the better understanding.

[Significance]
The empirical results for the proposed scheme are compared against various baselines under various scenarios and the results demonstrate the significant utility of the proposed scheme.

Limitations
-----------

[Clarity]
The training times for the catalyzer is never discussed in this manuscript (even relative to the training times of the considered baselines). Moreover, it is not clear if the inference time of the catalyzer is included in the results such as Table 1. Even if, PQ and the catalyzer+lattice might have comparable search recalls, it would be good to understand the relative search times to get similar accuracy especially since the inference time for the catalyzer (which is part of the search time) can be fairly significant.

[Clarity/Significance]
One important point not discussed in this manuscript is the choice of the structure (architechture) of the catalyzer. Is the catalyzer architecture dependent on the data?
  - If yes, how to find an appropriate architecture?
  - If no, what is it about the proposed architecture that makes it sufficient for all data sets?
In my opinion, this is extremely important since this drives the applicability of the proposed scheme beyond the presented examples.

[Minor question]
- Is the parameter r in the rank loss same as the norm r in the lattice quantizer? This is a bit confusing.</span></div><div class="note_contents"><span class="note_content_field">Rating: </span><span class="note_content_value">7: Good paper, accept</span></div><div class="note_contents"><span class="note_content_field">Confidence: </span><span class="note_content_value">4: The reviewer is confident but not absolutely certain that the evaluation is correct</span></div><div class="reply_row clearfix"></div></div><div class="children"><div class="note_with_children"><div id="note_SJlQccS_TQ" class="note panel"><div class="title_pdf_row clearfix"><h2 class="note_content_title"><a>Response to Review #3</a>  <button class="btn btn-xs btn-default permalink-button" title="Link to this comment" data-permalink-url="https://openreview.net/forum?id=SkGuG2R5tm&amp;noteId=SJlQccS_TQ"><span class="glyphicon glyphicon-link" aria-hidden="true"></span></button></h2></div><div class="meta_row"><span class="signatures">ICLR 2019 Conference Paper1282 Authors</span></div><div class="clearfix"><div class="meta_row pull-left"><span class="date item">13 Nov 2018</span><span class="item">ICLR 2019 Conference Paper1282 Official Comment</span><span class="item">Readers: <span class="readers-icon glyphicon glyphicon-globe"></span> Everyone</span></div><div class="meta_row meta_actions"></div></div><div class="note_contents"><span class="note_content_field">Comment: </span><span class="note_content_value">We thank the reviewer for their comments. 

“The training times for the catalyzer is never discussed in this manuscript. [...] Moreover, it is not clear if the inference time of the catalyzer is included in the results such as Table 1.” 
Training takes between 2 and 3 hours on a CPU machine using 20 cores, and the reported query timings take into account inference. 

“One important point not discussed in this manuscript is the choice of the structure (architecture) of the catalyzer. Is the catalyzer architecture dependent on the data?”
Generally, we observe that beyond 3 layers there is no improvement in accuracy. The performance improves when augmenting the width of the network, but with diminishing returns. We use the same architecture across datasets. We successfully used the same architecture on other datasets, but we report results here on the standard datasets of the field.

“What is it about the proposed architecture that makes it sufficient for all data sets?”
We have observed that the dimensions of the hidden layers in our architecture provide enough representation power for the model to be performant across all the datasets we have tested (those of the paper plus some internal datasets).

“Is the parameter r in the rank loss same as the norm r in the lattice quantizer?”
The parameter r is not the same as the norm r of the lattice quantizer, we thank the reviewer for spotting this, we will update the paper to lift this ambiguity.
</span></div><div class="reply_row clearfix"></div></div><div class="children"></div></div></div></div><div class="note_with_children"><div id="note_HkeDt3GOsQ" class="note panel"><div class="title_pdf_row clearfix"><h2 class="note_content_title"><a>Problematic  experimental results</a>  <button class="btn btn-xs btn-default permalink-button" title="Link to this comment" data-permalink-url="https://openreview.net/forum?id=SkGuG2R5tm&amp;noteId=HkeDt3GOsQ"><span class="glyphicon glyphicon-link" aria-hidden="true"></span></button></h2></div><div class="meta_row"><span class="signatures">ICLR 2019 Conference Paper1282 AnonReviewer1</span></div><div class="clearfix"><div class="meta_row pull-left"><span class="date item">20 Oct 2018 (modified: 07 Nov 2018)</span><span class="item">ICLR 2019 Conference Paper1282 Official Review</span><span class="item">Readers: <span class="readers-icon glyphicon glyphicon-globe"></span> Everyone</span></div><div class="meta_row meta_actions"></div></div><div class="note_contents"><span class="note_content_field">Review: </span><span class="note_content_value">The idea, transforming the input data to an out space in which the data is distributed uniformly and thus indexing is easier, is interesting. 

My main concerns come from experimental results.

(1) Table 1: where are the results of OPQ and LSQ from? run the codes by the authors of this paper? or from the original paper?

It is not consist to the LSQ paper (<a href="https://www.cs.ubc.ca/~julm/papers/eccv16.pdf)." target="_blank" rel="nofollow">https://www.cs.ubc.ca/~julm/papers/eccv16.pdf).</a> For BigANN1M, from the LSQ paper, the result is &gt;29 recall at 1 for 64 bits. 

(2) Figure 5: similarly, how did you get the results of PQ and OPQ?

(3) There are some other advanced algorithms: e.g.,  additive quantization (Babenko &amp; Lempitsky, 2014) and composite quantization (https://arxiv.org/abs/1712.00955)

The above points make it hard to judge this paper.</span></div><div class="note_contents"><span class="note_content_field">Rating: </span><span class="note_content_value">5: Marginally below acceptance threshold</span></div><div class="note_contents"><span class="note_content_field">Confidence: </span><span class="note_content_value">4: The reviewer is confident but not absolutely certain that the evaluation is correct</span></div><div class="reply_row clearfix"></div></div><div class="children"><div class="note_with_children"><div id="note_S1epFsSOpm" class="note panel"><div class="title_pdf_row clearfix"><h2 class="note_content_title"><a>Response to Review #1</a>  <button class="btn btn-xs btn-default permalink-button" title="Link to this comment" data-permalink-url="https://openreview.net/forum?id=SkGuG2R5tm&amp;noteId=S1epFsSOpm"><span class="glyphicon glyphicon-link" aria-hidden="true"></span></button></h2></div><div class="meta_row"><span class="signatures">ICLR 2019 Conference Paper1282 Authors</span></div><div class="clearfix"><div class="meta_row pull-left"><span class="date item">13 Nov 2018</span><span class="item">ICLR 2019 Conference Paper1282 Official Comment</span><span class="item">Readers: <span class="readers-icon glyphicon glyphicon-globe"></span> Everyone</span></div><div class="meta_row meta_actions"></div></div><div class="note_contents"><span class="note_content_field">Comment: </span><span class="note_content_value">We thank the reviewer for their review and comments. We provide detailed answers below.

"My main concerns come from experimental results."
Upon publication of the paper, we will release the code that replicates the experiments. 

"(1) Table 1: where are the results of OPQ and LSQ from?  [...] It is not consist to the LSQ paper"
In our experiments, we used the reference public implementation from the authors of LSQ [1]. The discrepancy in the reported 64-bit recall1@1 comes from the fact that the datasets are different: we use Bigann1m (28.4 recall) whereas the LSQ paper reports results on Sift1m. We conducted experiments on Bigann1m because the training set associated with Sift1m is too small (100k vectors) for learning the catalyzer. As a sanity check, we re-ran the code of [1] on Sift1m and obtained 28.99, which is consistent with the results reported by [A] (Table 1, LSQ, 29.37) and [Martinez et al, 2018] (Figure 3, LSQ SR-C and SR-D, ~28; Table 4 corresponds to another experimental setting).

“(2) Figure 5: similarly, how did you get the results of PQ and OPQ?”
We used the open-source Faiss library [2] to obtain the results of PQ and OPQ. This library is used as a reference implementation in recent papers like [D, E]. There is a comparison point with [F] on Deep1M at 64 bit: the Faiss implementation of OPQ obtains recall@1 = 15.6 vs 16.1 in [F] (table 1). 

“(3) There are some other advanced algorithms: e.g., additive quantization [B] and composite quantization [C]”
We did not compare directly to AQ and CQ as it was shown that they underperform LSQ by some margin (Table 1 in [A]). Besides, in general, we insist in the paper that the encoding time for additive quantization methods is at least an order of magnitude slower than product quantization and our catalyzer + lattice (122s for LSQ vs &lt; 10s for PQ/Lattice, cf. Table 1).

References
[1] <a href="https://github.com/una-dinosauria/local-search-quantization" target="_blank" rel="nofollow">https://github.com/una-dinosauria/local-search-quantization</a>
[2] https://github.com/facebookresearch/faiss

[A] Revisiting additive quantization, Martinez et al., ECCV'2016
[B] Additive Quantization for Extreme Vector Compression, Babenko &amp; Lempitsky, CVPR'2014
[C] Composite Quantization, J Wang et al. ICML'14
[D] Link and code: Fast indexing with graphs and compact regression codes, Douze et al, CVPR'18
[E] Revisiting the Inverted Indices for Billion-Scale Approximate Nearest Neighbors, Baranchuk et al, ECCV'18
[F] AnnArbor: Approximate Nearest Neighbors Using Arborescence Coding, Babenko &amp; Lempitsky, ICCV'17
</span></div><div class="reply_row clearfix"></div></div><div class="children"></div></div></div></div></div></main></div>
  </div>
</div>


    <!-- Footer -->
    <footer class="sitemap">
      <div class="container">
        <div class="row hidden-xs">
          <div class="col-sm-4">
            <ul class="list-unstyled">
              <li><a href="/" class="home">Home</a></li>
              <li><a href="/venues">All Venues</a></li>
            </ul>
          </div>
    
          <div class="col-sm-4">
            <ul class="list-unstyled">
              <li><a href="/about">About OpenReview</a></li>
              <li><a href="/contact">Contact</a></li>
              <li><a href="#" data-toggle="modal" data-target="#feedback-modal">Feedback</a></li>
            </ul>
          </div>
    
          <div class="col-sm-4">
            <ul class="list-unstyled">
              <li><a href="/terms">Terms of Service</a></li>
              <li><a href="/privacy">Privacy Policy</a></li>
            </ul>
          </div>
        </div>
    
        <div class="row visible-xs-block">
          <div class="col-xs-6">
            <ul class="list-unstyled">
              <li><a href="/" class="home">Home</a></li>
              <li><a href="/about">About OpenReview</a></li>
              <li><a href="#" data-toggle="modal" data-target="#feedback-modal">Feedback</a></li>
            </ul>
          </div>
    
          <div class="col-xs-6">
            <ul class="list-unstyled">
              <li><a href="/contact">Contact</a></li>
              <li><a href="/terms">Terms of Service</a></li>
              <li><a href="/privacy">Privacy Policy</a></li>
            </ul>
          </div>
        </div>
      </div>
    </footer>
    
    <footer class="sponsor">
      <div class="container">
        <div class="row">
          <div class="col-sm-10 col-sm-offset-1">
            <p class="text-center">
              OpenReview is created by the <a href="http://www.iesl.cs.umass.edu/" target="_blank">Information Extraction and Synthesis Laboratory</a>, College of Information and Computer Science, University of Massachusetts Amherst. We gratefully acknowledge the support of the OpenReview sponsors:  Google,  Facebook, NSF, the University of Massachusetts Amherst Center for Data Science, and Center for Intelligent Information Retrieval, as well as the Google Cloud Platform for donating the computing and networking services on which OpenReview.net runs.
            </p>
          </div>
        </div>
      </div>
    </footer>

  <div id="feedback-modal" class="modal fade" tabindex="-1" role="dialog">
    <div class="modal-dialog">
      <div class="modal-content">
        <div class="modal-header">
          <button type="button" class="close" data-dismiss="modal" aria-label="Close"><span aria-hidden="true">×</span></button>
          <h3 class="modal-title">Send Feedback</h3>
        </div>
        <div class="modal-body">
          <p>Enter your feedback below and we'll get back to you as soon as possible.</p>
          <form action="/feedback" method="POST">
            <div class="form-group">
              <input type="email" name="from" class="form-control" placeholder="Email" />
            </div>
            <div class="form-group">
              <input type="text" name="subject" class="form-control" placeholder="Subject" />
            </div>
            <div class="form-group">
              <textarea name="message" class="form-control feedback-input" rows="5" placeholder="Message" required=""></textarea>
            </div>
          <ul id="ui-id-2" tabindex="0" class="ui-menu ui-widget ui-widget-content" style="display: none;"></ul></form>
        </div>
        <div class="modal-footer">
          <button type="button" class="btn btn-default" data-dismiss="modal">Cancel</button>
          <button type="button" class="btn btn-primary">Send</button>
        </div>
      </div>
    </div>
  </div>

  <script type="text/javascript" async="" src="https://www.google-analytics.com/analytics.js"></script><script src="//cdnjs.cloudflare.com/ajax/libs/jquery/2.2.4/jquery.min.js"></script>
  <script>window.jQuery || document.write('&lt;script src="/static/js/vendor/jquery-2.2.4.min.js"&gt;&lt;\/script&gt;')</script>

  <script src="/static/dist/vendor.min.js"></script>
  <script src="/static/dist/templates.min.js"></script>
  <script src="/static/dist/openreview.min.js"></script>

    <script>window.legacyScripts = true;</script>


    <script async="" src="https://www.googletagmanager.com/gtag/js?id=UA-108703919-1"></script>
    <script>
      window.dataLayer = window.dataLayer || [];
      function gtag() { dataLayer.push(arguments); }
      gtag('js', new Date());
      gtag('config', 'UA-108703919-1');
    </script>


<div role="status" aria-live="assertive" aria-relevant="additions" class="ui-helper-hidden-accessible"></div></body></html>