<!DOCTYPE html><html xmlns="http://www.w3.org/1999/xhtml" class="no-js"><head>
  <meta charset="utf-8" />
  <meta http-equiv="X-UA-Compatible" content="IE=edge,chrome=1" />
  <meta name="viewport" content="width=device-width, initial-scale=1" />

  <title>Mimicking actions is a good strategy for beginners: Fast Reinforcement Learning with Expert Action Sequences | OpenReview</title>
  <meta name="description" content="" />

      <meta name="citation_title" content="Mimicking actions is a good strategy for beginners: Fast Reinforcement Learning with Expert Action Sequences" />
        <meta name="citation_author" content="Anonymous" />
      <meta name="citation_publication_date" content="2018/09/27" />
      <meta name="citation_online_date" content="2018/09/27" />
      <meta name="citation_pdf_url" content="https://openreview.net/pdf?id=HJfxbhR9KQ" />
      <meta name="twitter:card" content="summary" />
      <meta name="twitter:site" content="@openreviewnet" />
      <meta name="og:title" content="Mimicking actions is a good strategy for beginners: Fast..." />
      <meta name="og:description" content="Imitation Learning is the task of mimicking the behavior of an expert player in a Reinforcement Learning(RL) Environment to enhance the training of a fresh agent (called novice) beginning from..." />
      <meta name="og:image" content="https://openreview.net/static/images/openreview_logo_512.png" />

  <link rel="stylesheet" href="https://fonts.googleapis.com/css?family=Noto+Sans:400,400i,700,700i" />

  <link rel="stylesheet" href="/static/css/bootstrap.min.css" />
  <link rel="stylesheet" href="/static/css/jquery-ui.min.css" />
  <link rel="stylesheet" href="/static/css/main.min.css" />
</head>

<body class="forum">
  <nav class="navbar navbar-inverse navbar-fixed-top" role="navigation">
    <div class="container">
  
      <div class="navbar-header">
        <button type="button" class="navbar-toggle collapsed" data-toggle="collapse" data-target="#navbar" aria-expanded="false" aria-controls="navbar">
          <span class="sr-only">Toggle navigation</span>
          <span class="icon-bar"></span>
          <span class="icon-bar"></span>
          <span class="icon-bar"></span>
        </button>
        <a class="navbar-brand home push-link" href="/"><strong>OpenReview</strong>.net</a>
      </div>
  
      <div id="navbar" class="navbar-collapse collapse">
        <form class="navbar-form navbar-left profile-search" role="search">
          <div class="form-group has-feedback">
            <input id="search_input" type="text" class="form-control ui-autocomplete-input" placeholder="Search ICLR 2019 Conference" autocomplete="off" />
            <span class="glyphicon glyphicon-search form-control-feedback" aria-hidden="true"></span>
          </div>
  
          <input id="search_group" type="hidden" value="ICLR.cc/2019/Conference" />
          <input id="search_content" type="hidden" value="all" />
        <ul id="ui-id-1" tabindex="0" class="ui-menu ui-widget ui-widget-content ui-autocomplete ui-front" style="display: none;"></ul></form>
  
        <ul class="nav navbar-nav navbar-right">
        
            <li id="user-menu"><a href="/login">Login</a></li>
        </ul>
      </div>
  
    </div>
  </nav>

  <div id="or-banner" class="banner" style="">
  <div class="container">
    <div class="row">
      <div class="col-xs-12"><a href="/group?id=ICLR.cc/2019/Conference" title="Venue Homepage"><img class="icon" src="/static/images/arrow_left.svg" /> Go to <strong>ICLR 2019 Conference</strong> homepage</a></div>
    </div>
  </div>
</div>
<div id="flash-message-container" class="alert alert-danger" role="alert" style="display: none;">
  <div class="container">
    <div class="row">
      <div class="col-xs-12">
        <div class="alert-content">
          <button type="button" class="close" aria-label="Close"><span aria-hidden="true">×</span></button>
        </div>
      </div>
    </div>
  </div>
</div>

<div class="container">
  <div class="row">
    <div class="col-xs-12">
      <main id="content" class="clearfix openbanner-visible legacy-styles"><div id="note_HJfxbhR9KQ" class="note panel"><div class="title_pdf_row clearfix"><h2 class="note_content_title"><a>Mimicking actions is a good strategy for beginners: Fast Reinforcement Learning with Expert Action Sequences</a> <a class="note_content_pdf" href="/pdf?id=HJfxbhR9KQ" title="Download PDF" target="_blank"><img src="/static/images/pdf_icon_blue.svg" /></a> </h2></div><div class="meta_row"><span class="signatures">Anonymous</span></div><div class="clearfix"><div class="meta_row pull-left"><span class="date item">28 Sep 2018 (modified: 18 Nov 2018)</span><span class="item">ICLR 2019 Conference Blind Submission</span><span class="item">Readers: <span class="readers-icon glyphicon glyphicon-globe"></span> Everyone</span><span class="item"><a class="action-bibtex-modal" data-bibtex="@inproceedings{    &#10;anonymous2019mimicking,    &#10;title={Mimicking actions is a good strategy for beginners: Fast Reinforcement Learning with Expert Action Sequences},    &#10;author={Anonymous},    &#10;booktitle={Submitted to International Conference on Learning Representations},    &#10;year={2019},    &#10;url={https://openreview.net/forum?id=HJfxbhR9KQ},    &#10;note={under review}    &#10;}">Show Bibtex</a></span><a class="note_content_pdf item" href="/revisions?id=HJfxbhR9KQ" target="_blank">Show Revisions</a></div><div class="meta_row meta_actions"></div></div><div class="note_contents"><span class="note_content_field">Abstract: </span><span class="note_content_value">Imitation Learning is the task of mimicking the behavior of an expert player in a Reinforcement Learning(RL) Environment to enhance the training of a fresh agent (called novice) beginning from scratch. Most of the Reinforcement Learning environments are stochastic in nature, i.e., the state sequences that an agent may encounter usually follow a Markov Decision Process (MDP). This makes the task of mimicking difficult as it is very unlikely that a new agent may encounter same or similar state sequences as an expert. Prior research in Imitation Learning proposes various ways to learn a mapping between the states encountered and the respective actions taken by the expert while mostly being agnostic to the order in which these were performed. Most of these methods need considerable number of states-action pairs to achieve good results. We propose a simple alternative to Imitation Learning by appending the novice’s action space with the frequent short action sequences that the expert has taken. This simple modification, surprisingly improves the exploration and significantly outperforms alternative approaches like Dataset Aggregation. We experiment with several popular Atari games and show significant and consistent growth in the score that the new agents achieve using just a few expert action sequences.</span></div><div class="note_contents"><span class="note_content_field">Keywords: </span><span class="note_content_value">Reinforcement Learning, Imitation Learning, Atari, A3C, GA3C</span></div><div class="note_contents"><span class="note_content_field">TL;DR: </span><span class="note_content_value">Appending most frequent action pairs from an expert player to a novice RL agent's action space improves the scores by huge margin.</span></div><div class="reply_row clearfix"><div class="item" id="reply_count">7 Replies</div></div></div><hr class="small" /><div id="note_children"><div class="note_with_children"><div id="note_BkxqR35J07" class="note panel"><div class="title_pdf_row clearfix"><h2 class="note_content_title"><a>Additional experiments and revision</a>  <button class="btn btn-xs btn-default permalink-button" title="Link to this comment" data-permalink-url="https://openreview.net/forum?id=HJfxbhR9KQ&amp;noteId=BkxqR35J07"><span class="glyphicon glyphicon-link" aria-hidden="true"></span></button></h2></div><div class="meta_row"><span class="signatures">ICLR 2019 Conference Paper1140 Authors</span></div><div class="clearfix"><div class="meta_row pull-left"><span class="date item">19 Nov 2018</span><span class="item">ICLR 2019 Conference Paper1140 Official Comment</span><span class="item">Readers: <span class="readers-icon glyphicon glyphicon-globe"></span> Everyone</span></div><div class="meta_row meta_actions"></div></div><div class="note_contents"><span class="note_content_field">Comment: </span><span class="note_content_value">We have added the plots for another baseline which is to choose a random subset of action pairs and append to the original action space. Please check the new plots in magenta in Figure 2. These plots strengthen our hypothesis that the frequent action pairs have useful information that random action pairs do not.</span></div><div class="reply_row clearfix"></div></div><div class="children"></div></div><div class="note_with_children"><div id="note_BJgTCSrEp7" class="note panel"><div class="title_pdf_row clearfix"><h2 class="note_content_title"><a>need more in-depth analysis</a>  <button class="btn btn-xs btn-default permalink-button" title="Link to this comment" data-permalink-url="https://openreview.net/forum?id=HJfxbhR9KQ&amp;noteId=BJgTCSrEp7"><span class="glyphicon glyphicon-link" aria-hidden="true"></span></button></h2></div><div class="meta_row"><span class="signatures">ICLR 2019 Conference Paper1140 AnonReviewer4</span></div><div class="clearfix"><div class="meta_row pull-left"><span class="date item">10 Nov 2018</span><span class="item">ICLR 2019 Conference Paper1140 Official Review</span><span class="item">Readers: <span class="readers-icon glyphicon glyphicon-globe"></span> Everyone</span></div><div class="meta_row meta_actions"></div></div><div class="note_contents"><span class="note_content_field">Review: </span><span class="note_content_value">[Summary]

This paper presents an interesting idea that to append the agent's action space with the expert's most frequent action pairs, by which the agent can perform better exploration as to achieve the same performance in a shorter time. The authors show performance gain by comparing their method with two baselines - Dagger and InfoGAIL.


[Stengths]

The proposed method is simple yet effective, and I really like the analogy to mini-moves in sports as per the motivation section.


[Concerns]

- How to choose the number and length of the action sequences?
The authors empirically add the same number of expert's action sequences as the basic ones and select the length k as 2. However, no ablation studies are performed to demonstrate the sensitivity of the selected hyperparameters. Although the authors claim that "we limit the size of meta-actions k to 2 because large action spaces may lead to poor convergence", a more systematic evaluation is needed. How will the performance change if we add more and longer action sequences? When will the performance reach a plateau? How does it vary between different environments?

- Analysis of the selected action sequences.
It might be better to add more analysis of the selected action sequences. What are the most frequent action pairs? How does it differ from game to game? What if the action pairs are selected in a random fashion?

- Justification of the motivation
The major motivation of the method is to release the burden of memory overheads. However, no quantitative evaluations are provided as to justify the claim. Considering that the input images are resized to 84x84, storing them should not be particularly expensive.

- The choice of baseline.
InfoGAIL (Li et al., 2017) is proposed to identify the latent structures in the expert's demonstration, hence it is not clear to me how it suits the tasks in the paper. The paper also lacks details describing how they implemented the baselines, e.g. beta in Dagger and the length of the latent vector in InfoGAIL.

- The authors only show experiments in Atari games, where the action space is discrete. It would be interesting to see if the idea can generalize to continuous action space. Is it possible to cluster the expert action sequences and form some basis for the agent to select?

- Typos
{LRR, RLR/RRL} --&gt; {LRR, RLR, RRL}
sclability --&gt; scalability
we don't need train a ... --&gt; we don't need to train a ...
Atmost --&gt; At most


[Recommendation]

The idea presents in the paper is simple yet seemingly effective. However, the paper lacks a proper evaluation of the proposed method, and I don't think this paper is ready with the current set of experiments. I will decide my final rating based on the authors' response to the above concerns.</span></div><div class="note_contents"><span class="note_content_field">Rating: </span><span class="note_content_value">5: Marginally below acceptance threshold</span></div><div class="note_contents"><span class="note_content_field">Confidence: </span><span class="note_content_value">3: The reviewer is fairly confident that the evaluation is correct</span></div><div class="reply_row clearfix"></div></div><div class="children"><div class="note_with_children"><div id="note_Skejj-UR67" class="note panel"><div class="title_pdf_row clearfix"><h2 class="note_content_title"><a>Thank you for the review. Clarifications are provided below.</a>  <button class="btn btn-xs btn-default permalink-button" title="Link to this comment" data-permalink-url="https://openreview.net/forum?id=HJfxbhR9KQ&amp;noteId=Skejj-UR67"><span class="glyphicon glyphicon-link" aria-hidden="true"></span></button></h2></div><div class="meta_row"><span class="signatures">(anonymous)</span></div><div class="clearfix"><div class="meta_row pull-left"><span class="date item">18 Nov 2018</span><span class="item">ICLR 2019 Conference Paper1140 Public Comment</span><span class="item">Readers: <span class="readers-icon glyphicon glyphicon-globe"></span> Everyone</span></div><div class="meta_row meta_actions"></div></div><div class="note_contents"><span class="note_content_field">Comment: </span><span class="note_content_value">Q1. Action triplets are inconsistent and statistically insignificant with limited demonstration: Our focus was on using very limited (small) demonstration. The number of episodes that we use is quite small (25 episodes each with actions ranging from 700 to 7000) as we wanted very limited demonstration. We observe that with such limited demonstration, only action-pairs are reliable. An expert should be consistent with the frequent action pairs/triplets over a set of episodes. In our analysis, we found that the frequent action pairs are consistent after 12 hrs, 13 hrs, 14 hrs and 15 hrs of training the expert network. The action pairs at different time instants in training were just permutations of each other. The same was not true for action triplets. Furthermore, for the game FishingDerby with 18 basic actions, the top 18 action pairs account for 33.85% of all the action-pairs in the 25 expert episodes. The top 18 action triplets account to just 7.36% of the all triplets in the same 25 episodes. Even for other games, we have similar discrepancy for action pairs vs triplets (DemonAttack-27.21% vs 8.87%, Asteroids-21.67% vs 6.37%, Atlantis 31.32% vs 9.61%, SpaceInvaders-28.82% vs 10.24%, BeamRider-14.78% vs 2.81%, TimePilot-15.41% vs 2.05%, Qbert-67% vs 51%). We still experimented with 3-step actions and noticed that for Atlantis, action triplets outperform action-pairs which Is great. But for other games, action-triplets perform worse than action-pairs.

Q2. Thank you for the suggestion. It is an interesting exercise to interpret frequent action pairs.

Q3. The memory advantage of our approach is quite straight forward. Out of all imitation baseline, only our method does not need to store state information at all. Even when we are resizing images to 84*84*4, we need several thousands of those images to get noticeable advantage when compared to having no information at all.

Q4. InfoGAIL in one of the most recent techniques in Imitation Learning. Hence, we wanted to compare against InfoGAIL and ensure that we are not missing any subtleties. For our Dagger implementation, we used the simple parameter free version of beta=Indicator(i=1), i.e., 1 for the first episode and then 0 from the second episode.

Q5. Thanks for the great suggestion! We were intending to explore this direction in future on continuous action spaces by binning continuous values to discrete.

Thank you for spotting typos, we have corrected them in the current version. Since our idea is simple and very effective, it needs more visibility so that more investigation can be made on this idea. Simplicity is the very reason why we can beat GA3C by significant margin. If the idea is not computationally simple, most likely it won’t beat GA3C (a highly optimized implementation on GPUs) on running time.  We hope you will change your opinion about the overall score.</span></div><div class="reply_row clearfix"></div></div><div class="children"></div></div></div></div><div class="note_with_children"><div id="note_Hkgw1asmhX" class="note panel"><div class="title_pdf_row clearfix"><h2 class="note_content_title"><a>Border line paper</a>  <button class="btn btn-xs btn-default permalink-button" title="Link to this comment" data-permalink-url="https://openreview.net/forum?id=HJfxbhR9KQ&amp;noteId=Hkgw1asmhX"><span class="glyphicon glyphicon-link" aria-hidden="true"></span></button></h2></div><div class="meta_row"><span class="signatures">ICLR 2019 Conference Paper1140 AnonReviewer1</span></div><div class="clearfix"><div class="meta_row pull-left"><span class="date item">29 Oct 2018 (modified: 07 Nov 2018)</span><span class="item">ICLR 2019 Conference Paper1140 Official Review</span><span class="item">Readers: <span class="readers-icon glyphicon glyphicon-globe"></span> Everyone</span></div><div class="meta_row meta_actions"></div></div><div class="note_contents"><span class="note_content_field">Review: </span><span class="note_content_value">The paper proposes an idea of using the most frequent expert action sequence to assist the novice, which, as claimed, has lower memory overhead than other imitation learning methodologies. The authors present comparison of their proposed method with state-of-the-art and show its superior performance. However I do have the following few questions. 

1. The proposed method requires a long time of GA3C training. How is that a fair comparison in Figure 2, where proposed method already has a lead over GA3C? It could be argued that it's not using all of the training outcome, but have the authors considered other form of experts and see how that works?

2. The authors claimed one of the advantages of their method is reducing the memory overhead. Some supporting experiments will be more convincing.

3. In Figure 3, atlantis panel, the score shows huge variance, which is not seen in Figure 2. Are they generated from the same runs? Could the authors give some explanation on the phenomenon in Figure 3?

Overall, I think the paper has an interesting idea. But the above unresolved questions raises some challenge on its credibility and reproducibility. </span></div><div class="note_contents"><span class="note_content_field">Rating: </span><span class="note_content_value">5: Marginally below acceptance threshold</span></div><div class="note_contents"><span class="note_content_field">Confidence: </span><span class="note_content_value">2: The reviewer is willing to defend the evaluation, but it is quite likely that the reviewer did not understand central parts of the paper</span></div><div class="reply_row clearfix"></div></div><div class="children"><div class="note_with_children"><div id="note_Hyx-qfI0pQ" class="note panel"><div class="title_pdf_row clearfix"><h2 class="note_content_title"><a>Thank you for the review. Clarifications are given below.</a>  <button class="btn btn-xs btn-default permalink-button" title="Link to this comment" data-permalink-url="https://openreview.net/forum?id=HJfxbhR9KQ&amp;noteId=Hyx-qfI0pQ"><span class="glyphicon glyphicon-link" aria-hidden="true"></span></button></h2></div><div class="meta_row"><span class="signatures">(anonymous)</span></div><div class="clearfix"><div class="meta_row pull-left"><span class="date item">18 Nov 2018</span><span class="item">ICLR 2019 Conference Paper1140 Public Comment</span><span class="item">Readers: <span class="readers-icon glyphicon glyphicon-globe"></span> Everyone</span></div><div class="meta_row meta_actions"></div></div><div class="note_contents"><span class="note_content_field">Comment: </span><span class="note_content_value">Q1. We would like to stress that our setting, also clearly mentioned in the paper at several places, is standard imitation learning setting, where access to expert information is given input to the algorithm. We do not need any GA3C training. It is a proxy to generate very few expert action sequences.  For the other imitation learning baselines, the same pretrained GA3C training is used as a proxy for expert. Hence, it is a fair comparison.

Q2.  The memory advantage of our approach is quite straight forward. Out of all imitation baseline, only our method does not need to store state information at all. We only need few action sequences for ~25 episodes (each with a few 1000 integers) which takes trivially low memory. On the other hand, to store any reasonable (say 10000) state-action pairs of an expert in an environment, we will need at least 4032MB memory. Please note that each state is an image is originally 210*160*3 dimensional.

Q3. As we understand, you’re concerned about difference in variance when we plot episode-wise and time-wise.  Please note that we ran all the 5 runs of each game for 15 hrs. But the number of episodes in each run is different. For Atlantis game, the number of episodes range between 9114 to 10366. In our episode-wise plots, we only show the mean and variance of first 9114 episodes for each run. Hence, even though the time-wise and episode-wise plots are generated from the same output, the variance is higher for episode-wise plot. This is more glaring on Atlantis game as our idea gets much higher score than the baselines.

We believe we have answered all your questions. If you have any questions on reproducibility, we’ve our code ready for release once the review period is over. Since our idea is simple and very effective, it needs more visibility so that more investigation can be made on this idea. Simplicity is the very reason why we can beat GA3C by significant margin. If the idea is not computationally simple, most likely it won’t beat GA3C (a highly optimized implementation on GPUs) on running time.  We hope you will change your opinion about the overall score.</span></div><div class="reply_row clearfix"></div></div><div class="children"></div></div></div></div><div class="note_with_children"><div id="note_ByxMU9pvjm" class="note panel"><div class="title_pdf_row clearfix"><h2 class="note_content_title"><a>A well written paper with a simple, yet powerfull, idea that needs further analysis</a>  <button class="btn btn-xs btn-default permalink-button" title="Link to this comment" data-permalink-url="https://openreview.net/forum?id=HJfxbhR9KQ&amp;noteId=ByxMU9pvjm"><span class="glyphicon glyphicon-link" aria-hidden="true"></span></button></h2></div><div class="meta_row"><span class="signatures">ICLR 2019 Conference Paper1140 AnonReviewer2</span></div><div class="clearfix"><div class="meta_row pull-left"><span class="date item">20 Oct 2018 (modified: 07 Nov 2018)</span><span class="item">ICLR 2019 Conference Paper1140 Official Review</span><span class="item">Readers: <span class="readers-icon glyphicon glyphicon-globe"></span> Everyone</span></div><div class="meta_row meta_actions"></div></div><div class="note_contents"><span class="note_content_field">Review: </span><span class="note_content_value">The paper describes an imitation reinforcement learning approach where
the primitive actions of the agent are augmented with the most common
sequences of actions perform by experts. It is experimentally shown
how this simple change has clear improvements in the performance of
the system in Atari games. In practice, the authors double the number
of primitive actions with the most frequent double actions perform by
experts. 

A positive aspect of this paper comes from the simplicity of the
idea. There are however several issues that should be taken into
account:
- It is not clear how to determine when the distribution of action
  pair saturates. This is relevant for the use of the proposed approach.
- The total training time should consider both the initial time to
  obtain the extra pairs of frequent actions plus the subsequent
  training time used by the system. Either obtained from a learning
  system (15 hours) or by collecting traces of human experts (&lt; 1
  hour?). 
- It would be interesting to see the performance of the system with
  all the possible pairs of primitive actions and with a random subset
  of these pairs, to show the benefits of choosing the most frequent
  pairs used by the expert.
- This analysis could be easily extended to triplets and so on, as
  long as they are the most frequently used by experts.
- The inclusion of macro-actions has been extensively studied in
  search algorithms. In general, the utility of those macros depends on
  the effectiveness of the heuristic function. Perhaps the authors
  could revise some of the literature.
- Choosing the most frequent pairs in all the game may not be a
  suitable strategy. Some sequences of actions may be more frequent
  (important) at certain stage of the game (e.g., at the beginning/end
  of the game) and the most frequent sequences over all the game may
  introduce additional noise in those cases.

The paper is well written and easy to follow, there are however, some
small typos:
- expert(whose =&gt; expert (whose
% there are several places where there is no space between a word and
% its following right parenthesis 
- don't need train =&gt; don't need to train
- experiments4. =&gt; experiments.
- Atmost =&gt; At most
</span></div><div class="note_contents"><span class="note_content_field">Rating: </span><span class="note_content_value">6: Marginally above acceptance threshold</span></div><div class="note_contents"><span class="note_content_field">Confidence: </span><span class="note_content_value">4: The reviewer is confident but not absolutely certain that the evaluation is correct</span></div><div class="reply_row clearfix"></div></div><div class="children"><div class="note_with_children"><div id="note_ByeHI78Apm" class="note panel"><div class="title_pdf_row clearfix"><h2 class="note_content_title"><a>Thank you very much for the positive comments and suggestion about another baseline. Clarifications are given below.</a>  <button class="btn btn-xs btn-default permalink-button" title="Link to this comment" data-permalink-url="https://openreview.net/forum?id=HJfxbhR9KQ&amp;noteId=ByeHI78Apm"><span class="glyphicon glyphicon-link" aria-hidden="true"></span></button></h2></div><div class="meta_row"><span class="signatures">(anonymous)</span></div><div class="clearfix"><div class="meta_row pull-left"><span class="date item">18 Nov 2018</span><span class="item">ICLR 2019 Conference Paper1140 Public Comment</span><span class="item">Readers: <span class="readers-icon glyphicon glyphicon-globe"></span> Everyone</span></div><div class="meta_row meta_actions"></div></div><div class="note_contents"><span class="note_content_field">Comment: </span><span class="note_content_value">Please check the updated figures in our paper that include the comparison of a random subset of action pairs vs the most frequent action pairs (the line in magenta). The new plots strengthen our proposal that the most-frequent action pairs have useful information.

Q1. Ideally, an expert should be consistent with the action pair distribution over a set of few episodes. In our analysis, we found that the frequent action pairs after 12 hrs, 13 hrs, 14 hrs and 15 hrs of training the expert network are consistent. Hence, it is evident that after training the expert network for reasonable time, the top action pairs saturate. We have made our choice more concrete by training all expert networks for 15 hrs.

Q2. As mentioned in the paper, imitation learning algorithms presume that the expert information is available beforehand. We just substitute human data with a pre-trained network. Collecting traces of human data is a fast and viable but it is highly dependent on the task/game. In our case, assuming access to expert action sequences, calculating the frequency distribution and obtaining top action sequences is a trivial task with few seconds of time. 

Q3. Please check the new plots for random subset of action-pairs. As for adding all possible action-pairs, the action space grows exponentially, and the network must classify lot more classes with the same information. With games like FishingDerby and Asteroids (18 and 14 actions), it become too hard for network to classify hundreds of classes with same information.

Q4. Action triplets are inconsistent and statistically insignificant with limited demonstration: Our focus was on using very limited (small) demonstration. The number of episodes that we use is quite small (25 episodes each with actions ranging from 700 to 7000) as we wanted very limited demonstration. We observe that with such limited demonstration, only action-pairs are reliable. The frequent action triplets after 12 hrs, 13 hrs, 14 hrs and 15 hrs of training expert network are different each time. Furthermore, for the game FishingDerby with 18 basic actions, the top 18 action pairs account for 33.85% of all the action-pairs in the 25 expert episodes. The top 18 action triplets account to just 7.36% of the all triplets in the same 25 episodes. Even for other games, we have similar discrepancy for action pairs vs triplets (DemonAttack-27.21% vs 8.87%, Asteroids-21.67% vs 6.37%, Atlantis 31.32% vs 9.61%, SpaceInvaders-28.82% vs 10.24%, BeamRider-14.78% vs 2.81%, TimePilot-15.41% vs 2.05%, Qbert-67% vs 51%). We still experimented with 3-step actions and noticed that for Atlantis, action triplets outperform action-pairs which Is great. But for other games, action-triplets perform worse than action-pairs. 

Q5. Thank you for the suggestion. We’ll investigate search algorithms in the future to identify informative action-sequences. One class of models that we mentioned in the paper is ‘Options Framework’. The main drawback of Options Framework is that we need human designed options. Our work is a generic way of identifying options.

Thank you for spotting typos. We have fixed them in the latest revision.</span></div><div class="reply_row clearfix"></div></div><div class="children"></div></div></div></div></div></main></div>
  </div>
</div>


    <!-- Footer -->
    <footer class="sitemap">
      <div class="container">
        <div class="row hidden-xs">
          <div class="col-sm-4">
            <ul class="list-unstyled">
              <li><a href="/" class="home">Home</a></li>
              <li><a href="/venues">All Venues</a></li>
            </ul>
          </div>
    
          <div class="col-sm-4">
            <ul class="list-unstyled">
              <li><a href="/about">About OpenReview</a></li>
              <li><a href="/contact">Contact</a></li>
              <li><a href="#" data-toggle="modal" data-target="#feedback-modal">Feedback</a></li>
            </ul>
          </div>
    
          <div class="col-sm-4">
            <ul class="list-unstyled">
              <li><a href="/terms">Terms of Service</a></li>
              <li><a href="/privacy">Privacy Policy</a></li>
            </ul>
          </div>
        </div>
    
        <div class="row visible-xs-block">
          <div class="col-xs-6">
            <ul class="list-unstyled">
              <li><a href="/" class="home">Home</a></li>
              <li><a href="/about">About OpenReview</a></li>
              <li><a href="#" data-toggle="modal" data-target="#feedback-modal">Feedback</a></li>
            </ul>
          </div>
    
          <div class="col-xs-6">
            <ul class="list-unstyled">
              <li><a href="/contact">Contact</a></li>
              <li><a href="/terms">Terms of Service</a></li>
              <li><a href="/privacy">Privacy Policy</a></li>
            </ul>
          </div>
        </div>
      </div>
    </footer>
    
    <footer class="sponsor">
      <div class="container">
        <div class="row">
          <div class="col-sm-10 col-sm-offset-1">
            <p class="text-center">
              OpenReview is created by the <a href="http://www.iesl.cs.umass.edu/" target="_blank">Information Extraction and Synthesis Laboratory</a>, College of Information and Computer Science, University of Massachusetts Amherst. We gratefully acknowledge the support of the OpenReview sponsors:  Google,  Facebook, NSF, the University of Massachusetts Amherst Center for Data Science, and Center for Intelligent Information Retrieval, as well as the Google Cloud Platform for donating the computing and networking services on which OpenReview.net runs.
            </p>
          </div>
        </div>
      </div>
    </footer>

  <div id="feedback-modal" class="modal fade" tabindex="-1" role="dialog">
    <div class="modal-dialog">
      <div class="modal-content">
        <div class="modal-header">
          <button type="button" class="close" data-dismiss="modal" aria-label="Close"><span aria-hidden="true">×</span></button>
          <h3 class="modal-title">Send Feedback</h3>
        </div>
        <div class="modal-body">
          <p>Enter your feedback below and we'll get back to you as soon as possible.</p>
          <form action="/feedback" method="POST">
            <div class="form-group">
              <input type="email" name="from" class="form-control" placeholder="Email" />
            </div>
            <div class="form-group">
              <input type="text" name="subject" class="form-control" placeholder="Subject" />
            </div>
            <div class="form-group">
              <textarea name="message" class="form-control feedback-input" rows="5" placeholder="Message" required=""></textarea>
            </div>
          <ul id="ui-id-2" tabindex="0" class="ui-menu ui-widget ui-widget-content" style="display: none;"></ul></form>
        </div>
        <div class="modal-footer">
          <button type="button" class="btn btn-default" data-dismiss="modal">Cancel</button>
          <button type="button" class="btn btn-primary">Send</button>
        </div>
      </div>
    </div>
  </div>

  <script type="text/javascript" async="" src="https://www.google-analytics.com/analytics.js"></script><script src="//cdnjs.cloudflare.com/ajax/libs/jquery/2.2.4/jquery.min.js"></script>
  <script>window.jQuery || document.write('&lt;script src="/static/js/vendor/jquery-2.2.4.min.js"&gt;&lt;\/script&gt;')</script>

  <script src="/static/dist/vendor.min.js"></script>
  <script src="/static/dist/templates.min.js"></script>
  <script src="/static/dist/openreview.min.js"></script>

    <script>window.legacyScripts = true;</script>


    <script async="" src="https://www.googletagmanager.com/gtag/js?id=UA-108703919-1"></script>
    <script>
      window.dataLayer = window.dataLayer || [];
      function gtag() { dataLayer.push(arguments); }
      gtag('js', new Date());
      gtag('config', 'UA-108703919-1');
    </script>


<div role="status" aria-live="assertive" aria-relevant="additions" class="ui-helper-hidden-accessible"></div></body></html>