<!DOCTYPE html><html xmlns="http://www.w3.org/1999/xhtml" class="no-js"><head>
  <meta charset="utf-8" />
  <meta http-equiv="X-UA-Compatible" content="IE=edge,chrome=1" />
  <meta name="viewport" content="width=device-width, initial-scale=1" />

  <title>End-to-end Learning of a Convolutional Neural Network via Deep Tensor Decomposition | OpenReview</title>
  <meta name="description" content="" />

      <meta name="citation_title" content="End-to-end Learning of a Convolutional Neural Network via Deep Tensor Decomposition" />
        <meta name="citation_author" content="Anonymous" />
      <meta name="citation_publication_date" content="2018/09/27" />
      <meta name="citation_online_date" content="2018/09/27" />
      <meta name="citation_pdf_url" content="https://openreview.net/pdf?id=BJlhEs09YQ" />
      <meta name="twitter:card" content="summary" />
      <meta name="twitter:site" content="@openreviewnet" />
      <meta name="og:title" content="End-to-end Learning of a Convolutional Neural Network via Deep..." />
      <meta name="og:description" content="In this paper we study the problem of learning the weights of a deep convolutional neural network. We consider a network where convolutions are carried out over non-overlapping patches with a..." />
      <meta name="og:image" content="https://openreview.net/static/images/openreview_logo_512.png" />

  <link rel="stylesheet" href="https://fonts.googleapis.com/css?family=Noto+Sans:400,400i,700,700i" />

  <link rel="stylesheet" href="/static/css/bootstrap.min.css" />
  <link rel="stylesheet" href="/static/css/jquery-ui.min.css" />
  <link rel="stylesheet" href="/static/css/main.min.css" />
</head>

<body class="forum">
  <nav class="navbar navbar-inverse navbar-fixed-top" role="navigation">
    <div class="container">
  
      <div class="navbar-header">
        <button type="button" class="navbar-toggle collapsed" data-toggle="collapse" data-target="#navbar" aria-expanded="false" aria-controls="navbar">
          <span class="sr-only">Toggle navigation</span>
          <span class="icon-bar"></span>
          <span class="icon-bar"></span>
          <span class="icon-bar"></span>
        </button>
        <a class="navbar-brand home push-link" href="/"><strong>OpenReview</strong>.net</a>
      </div>
  
      <div id="navbar" class="navbar-collapse collapse">
        <form class="navbar-form navbar-left profile-search" role="search">
          <div class="form-group has-feedback">
            <input id="search_input" type="text" class="form-control ui-autocomplete-input" placeholder="Search ICLR 2019 Conference" autocomplete="off" />
            <span class="glyphicon glyphicon-search form-control-feedback" aria-hidden="true"></span>
          </div>
  
          <input id="search_group" type="hidden" value="ICLR.cc/2019/Conference" />
          <input id="search_content" type="hidden" value="all" />
        <ul id="ui-id-1" tabindex="0" class="ui-menu ui-widget ui-widget-content ui-autocomplete ui-front" style="display: none;"></ul></form>
  
        <ul class="nav navbar-nav navbar-right">
        
            <li id="user-menu"><a href="/login">Login</a></li>
        </ul>
      </div>
  
    </div>
  </nav>

  <div id="or-banner" class="banner" style="">
  <div class="container">
    <div class="row">
      <div class="col-xs-12"><a href="/group?id=ICLR.cc/2019/Conference" title="Venue Homepage"><img class="icon" src="/static/images/arrow_left.svg" /> Go to <strong>ICLR 2019 Conference</strong> homepage</a></div>
    </div>
  </div>
</div>
<div id="flash-message-container" class="alert alert-danger" role="alert" style="display: none;">
  <div class="container">
    <div class="row">
      <div class="col-xs-12">
        <div class="alert-content">
          <button type="button" class="close" aria-label="Close"><span aria-hidden="true">×</span></button>
        </div>
      </div>
    </div>
  </div>
</div>

<div class="container">
  <div class="row">
    <div class="col-xs-12">
      <main id="content" class="clearfix openbanner-visible legacy-styles"><div id="note_BJlhEs09YQ" class="note panel"><div class="title_pdf_row clearfix"><h2 class="note_content_title"><a>End-to-end Learning of a Convolutional Neural Network via Deep Tensor Decomposition</a> <a class="note_content_pdf" href="/pdf?id=BJlhEs09YQ" title="Download PDF" target="_blank"><img src="/static/images/pdf_icon_blue.svg" /></a> </h2></div><div class="meta_row"><span class="signatures">Anonymous</span></div><div class="clearfix"><div class="meta_row pull-left"><span class="date item">28 Sep 2018 (modified: 11 Oct 2018)</span><span class="item">ICLR 2019 Conference Blind Submission</span><span class="item">Readers: <span class="readers-icon glyphicon glyphicon-globe"></span> Everyone</span><span class="item"><a class="action-bibtex-modal" data-bibtex="@inproceedings{    &#10;anonymous2019end-to-end,    &#10;title={End-to-end Learning of a Convolutional Neural Network via Deep Tensor Decomposition},    &#10;author={Anonymous},    &#10;booktitle={Submitted to International Conference on Learning Representations},    &#10;year={2019},    &#10;url={https://openreview.net/forum?id=BJlhEs09YQ},    &#10;note={under review}    &#10;}">Show Bibtex</a></span></div><div class="meta_row meta_actions"></div></div><div class="note_contents"><span class="note_content_field">Abstract: </span><span class="note_content_value">In this paper we study the problem of learning the weights of a deep convolutional neural network. We consider a network where convolutions are carried out over non-overlapping patches with a single kernel in each layer. We develop an algorithm for simultaneously learning all the kernels from the training data. Our approach dubbed Deep Tensor Decomposition (DeepTD) is based on a rank-1 tensor decomposition. We theoretically investigate DeepTD under a realizable model for the training data where the inputs are chosen i.i.d. from a Gaussian distribution and the labels are generated according to planted convolutional kernels. We show that DeepTD is data-efficient and provably works as soon as the sample size exceeds the total number of convolutional weights in the network. Our numerical experiments demonstrate the effectiveness of DeepTD and verify our theoretical findings.</span></div><div class="note_contents"><span class="note_content_field">Keywords: </span><span class="note_content_value">convolutional neural network, tensor decomposition, sample complexity, approximation</span></div><div class="note_contents"><span class="note_content_field">TL;DR: </span><span class="note_content_value">We consider a simplified deep convolutional neural network model. We show that all layers of this network can be approximately learned with a proper application of tensor decomposition.</span></div><div class="reply_row clearfix"><div class="item" id="reply_count">3 Replies</div></div></div><hr class="small" /><div id="note_children"><div class="note_with_children"><div id="note_r1epEmhUTX" class="note panel"><div class="title_pdf_row clearfix"><h2 class="note_content_title"><a>Nice analysis but needs strong assumptions</a>  <button class="btn btn-xs btn-default permalink-button" title="Link to this comment" data-permalink-url="https://openreview.net/forum?id=BJlhEs09YQ&amp;noteId=r1epEmhUTX"><span class="glyphicon glyphicon-link" aria-hidden="true"></span></button></h2></div><div class="meta_row"><span class="signatures">ICLR 2019 Conference Paper34 AnonReviewer4</span></div><div class="clearfix"><div class="meta_row pull-left"><span class="date item">12 Nov 2018</span><span class="item">ICLR 2019 Conference Paper34 Official Review</span><span class="item">Readers: <span class="readers-icon glyphicon glyphicon-globe"></span> Everyone</span></div><div class="meta_row meta_actions"></div></div><div class="note_contents"><span class="note_content_field">Review: </span><span class="note_content_value">This paper provides theoretical guarantees for learning deep convolutional neural networks using rank-one tensor decomposition. In particular,  a tensor is first constructed from the given data. Then the authors show that the error between this tensor and its corresponding population counterpart can be bounded. They also show that the population tensor can be approximated by a rank-one tensor whose components are convolutional kernels. Finally, by doing decomposition of this tensor, the kernels can be recovered up to some error. 

The paper is in general easy to read even if there are a lot of technical notations in it. The analyses also look rigorous. The tensor formulation provides some theoretical insights for deep neural networks since most existing tensor approaches for neural networks can only handle one-layer neural networks. 

However, I have several concerns about this paper. 

1. The assumptions are very strong and mostly unrealistic. For example, 1) the input needs to follow Gaussian distribution; 2) the stride is equal to the kernel size; 3) the input and activations are one-dimensional. 4) the data comes from a planted model. 

2. Another caveat of the analyses is that it assumes the constructed tensor can be approximated by a rank-one tensor. However, according to Theorem 4.7, this error highly depends on the ground truth kernels as well as the activation function. This error does not converge to zero asymptotically and is not proved to be a sufficiently small number. In other words, this error may be a significant number that invalidates the proposed tensor decomposition approach for learning deep networks. 

Recovery guarantees for deep convolutional neural networks are challenging. This paper provides a tensor approach with rigorous analyses. However, the assumptions are too strong and the error bound does not converge to zero asymptotically . 
</span></div><div class="note_contents"><span class="note_content_field">Rating: </span><span class="note_content_value">5: Marginally below acceptance threshold</span></div><div class="note_contents"><span class="note_content_field">Confidence: </span><span class="note_content_value">3: The reviewer is fairly confident that the evaluation is correct</span></div><div class="reply_row clearfix"></div></div><div class="children"></div></div><div class="note_with_children"><div id="note_HJxyp5qyaQ" class="note panel"><div class="title_pdf_row clearfix"><h2 class="note_content_title"><a>Interesting direction, but lacks practical relevance</a>  <button class="btn btn-xs btn-default permalink-button" title="Link to this comment" data-permalink-url="https://openreview.net/forum?id=BJlhEs09YQ&amp;noteId=HJxyp5qyaQ"><span class="glyphicon glyphicon-link" aria-hidden="true"></span></button></h2></div><div class="meta_row"><span class="signatures">ICLR 2019 Conference Paper34 AnonReviewer1</span></div><div class="clearfix"><div class="meta_row pull-left"><span class="date item">07 Nov 2018</span><span class="item">ICLR 2019 Conference Paper34 Official Review</span><span class="item">Readers: <span class="readers-icon glyphicon glyphicon-globe"></span> Everyone</span></div><div class="meta_row meta_actions"></div></div><div class="note_contents"><span class="note_content_field">Review: </span><span class="note_content_value">==== Summary ====

This paper proposes a learning method for a restricted case of deep convolutional networks, where the convolutional layers are limited to the non-overlapping case (receptive field = stride) and having just a single output channel per layer. Additionally, the method is analyzed only for the case of normally distributed inputs and realizable settings, i.e., there exists some set of weights that correctly solve the learning problem. The proposed method is based on a rank-1 tensor approximation to a tensor “summary” of the training set and is proven to result in a good estimation to the ground-truth parameters when the number of samples exceeds the number of parameters.

==== Detailed Review ====

The paper is well written, easy to follow, and I generally liked reading it. More specifically, I have found the method of aggregating the training set into a tensor that directly reflects the parameters of the model to be quite elegant. Though the general learning scheme is quite similar to previous works (see below), the way it leverages tensor decomposition is unique to the best of my knowledge. Additionally, unlike previous works which dealt with "shallow" non-linear models (e.g., neural networks with a single hidden layer), this work manages to address the learning of deep non-linear models, at least to some degree. Despite the above, the paper has two significant shortcomings:

1. The results on their own are limited to a very restrictive case of neural networks that has little to no practical relevance. While it is known that non-overlapping ConvNets are significantly less expressive than overlapping ones [1], they can at least approximate any function if given a sufficient number of hidden channels [2], but the restriction to a single channel per layer means that they can represent only a limited set of functions. I would have expected the paper to include a short discussion on these facts given its subject. The lack of relevance to practical networks would not be that bad if the paper included a sketch of how DeepTD could be extended to more complex networks. It would be helpful if the authors would comment on how might this method could be generalized to these cases.

2. The novelty of the method and the strength of the theoretical results are a bit lacking. First, the use of Stein’s lemma with E[y * x] for the estimation of the parameters of a non-linear model was used before in [3,4], and moreover in both cases they were able to extend their results to non-gaussian inputs and more complex models using higher moments, i.e. E[y * x * x] and E[y * x * x * x]. The submitted paper does cite these papers but does not directly discuss the similarities between them. While these works also rely on tensor decompositions and Stein's theorem,  DeepTD makes a different and novel use of tensor decompositions, but it does so in a way that is highly tuned to non-overlapping single-channel networks (i.e., the tensor T(x) is shaped according to the size of the kernels), and does not seem to be easily extendable to other settings. Additionally, the actual guarantees of theorem 4.8 show that for non-linear networks the bound on the error does not decrease to zero (even with infinite samples). Moreover, it seems that under common settings the resulting bound is meaningless (i.e., a negative lower bound for a positive quantity). I actually found it interesting that despite this bound, the reported empirical results were actually quite good — perhaps there is more to uncover here and maybe this method has potential beyond what can currently be proven. The only case where it seems to be relevant is when the activations are linear (or with smoothing factor so low which they are practically linear). However, under this trivial case the population tensor T = E[y*x] = E[grad(f_cnn)] is precisely the rank-1 tensor of the kernels, which makes me think that perhaps DeepTD can merely be seen as a kind of linear relaxation to the learning problem. The general point here that though there is a clear distinction between DeepTD and prior works, the delta is not significant, and though the results seem promising, they do not seem fully fleshed out.

In conclusion, while I do like the general direction that the paper takes with learning the parameters of a neural network, and it does seem like it could lead to exciting results in the future, I do not think it is ready for publication in its current form. Nevertheless, I strongly encourage the authors to keep developing this work (e.g., extend it to multi-channel networks, improve the bounds, drop gaussian input assumption).

[1] Sharir et al. On the Expressive Power of Overlapping Architectures of Deep Learning. ICLR 2018. 
[2] Cohen et al. On the Expressive Power of Deep Learning: A Tensor Analysis. COLT 2016. 
[3] Janzamin et al. Beating the Perils of Non-Convexity:
Guaranteed Training of Neural Networks using Tensor Methods. Arxiv preprint.
[4] Sedghi et al. Provable Tensor Methods for Learning Mixtures of Generalized Linear Models. AISTATS 2016.</span></div><div class="note_contents"><span class="note_content_field">Rating: </span><span class="note_content_value">5: Marginally below acceptance threshold</span></div><div class="note_contents"><span class="note_content_field">Confidence: </span><span class="note_content_value">3: The reviewer is fairly confident that the evaluation is correct</span></div><div class="reply_row clearfix"></div></div><div class="children"></div></div><div class="note_with_children"><div id="note_SygjJH2hh7" class="note panel"><div class="title_pdf_row clearfix"><h2 class="note_content_title"><a>Recovery of true weights for a special class of CNNs by reduction to tensor decomposition</a>  <button class="btn btn-xs btn-default permalink-button" title="Link to this comment" data-permalink-url="https://openreview.net/forum?id=BJlhEs09YQ&amp;noteId=SygjJH2hh7"><span class="glyphicon glyphicon-link" aria-hidden="true"></span></button></h2></div><div class="meta_row"><span class="signatures">ICLR 2019 Conference Paper34 AnonReviewer2</span></div><div class="clearfix"><div class="meta_row pull-left"><span class="date item">05 Nov 2018 (modified: 07 Nov 2018)</span><span class="item">ICLR 2019 Conference Paper34 Official Review</span><span class="item">Readers: <span class="readers-icon glyphicon glyphicon-globe"></span> Everyone</span></div><div class="meta_row meta_actions"></div></div><div class="note_contents"><span class="note_content_field">Review: </span><span class="note_content_value">Summary:

This paper analyzes the problem of learning a very special class of CNNs: each layer consists of a single filter, applied to non-overlapping patches of the input. Specifically, the task is: given data generated from a planted CNN of this kind with gaussian inputs, recover the ground truth weights. In the main result (Theorem 4.8), the authors show that the true weights can be approximately recovered, with high probability, by applying rank-one tensor decomposition to a special tensor derived from the generated data. Importantly, achieving a good approximation requires (a) observing more examples than the total number of network parameters, and (b) (somewhat confusingly) the kernels must be large relative to the square of the network depth.

Review:

The paper is clearly written. The results seem mathematically interesting and substantial. The connection to practical deep CNNs may be overstated however.  The nature of the result is ultimately that this very special CNN learning problem is really tensor decomposition in disguise. But why does the reduction in this special case teach us something fundamental about CNN training? One approach to addressing this issue would be to demonstrate that this class of networks can achieve reasonable performance on a baseline task (hence captures something important about the general case). However, this evaluation is not given.

Comments:

1. It seems that a stronger result than 4.7 (hence 4.8) is true. Looking at (A.14) on p. 14, I believe you should have that for $i=1, \dotsc, p$, $\phi_{\i(i)}^\prime (x)$ are iid copies, since $x \sim N(0, I)$. Thus, the vector $v$ itself is a constant, and the population tensor $T$ is rank-one.  So, if you define instead $\alpha_{CNN} = \EE[\phi_{\i(1)}^\prime(x)]$, you get $T = L_{CNN}$. If I'm confused, can the authors please help clear up my confusion?

2. In order for the bound in 4.8 to be meaningful, you must have $D^2 \leq d_\min$. But since $d_\min \leq p^{1/D}$, this implies awkwardly that the input dimension be exponential in the depth of the network: $p \geq D^{2D}$.  This should be mentioned, as it significantly challenges the claim that the results apply to arbitrarily deep networks.

3. Can the authors comment on the challenges for extending these results, say to multiple kernels per layer, or non-gaussian inputs? The latter seems hard, but maybe the former translates to higher rank tensor decomposition?

4. Can the authors include a line in the numerical experiment figures indicating the lower bound from 4.8? This way it is possible to judge how sharp the result is.

5. Perhaps a different activation than ReLU should be used in the numerical experiments, since it violates the smoothness assumption.
</span></div><div class="note_contents"><span class="note_content_field">Rating: </span><span class="note_content_value">5: Marginally below acceptance threshold</span></div><div class="note_contents"><span class="note_content_field">Confidence: </span><span class="note_content_value">3: The reviewer is fairly confident that the evaluation is correct</span></div><div class="reply_row clearfix"></div></div><div class="children"></div></div></div></main></div>
  </div>
</div>


    <!-- Footer -->
    <footer class="sitemap">
      <div class="container">
        <div class="row hidden-xs">
          <div class="col-sm-4">
            <ul class="list-unstyled">
              <li><a href="/" class="home">Home</a></li>
              <li><a href="/venues">All Venues</a></li>
            </ul>
          </div>
    
          <div class="col-sm-4">
            <ul class="list-unstyled">
              <li><a href="/about">About OpenReview</a></li>
              <li><a href="/contact">Contact</a></li>
              <li><a href="#" data-toggle="modal" data-target="#feedback-modal">Feedback</a></li>
            </ul>
          </div>
    
          <div class="col-sm-4">
            <ul class="list-unstyled">
              <li><a href="/terms">Terms of Service</a></li>
              <li><a href="/privacy">Privacy Policy</a></li>
            </ul>
          </div>
        </div>
    
        <div class="row visible-xs-block">
          <div class="col-xs-6">
            <ul class="list-unstyled">
              <li><a href="/" class="home">Home</a></li>
              <li><a href="/about">About OpenReview</a></li>
              <li><a href="#" data-toggle="modal" data-target="#feedback-modal">Feedback</a></li>
            </ul>
          </div>
    
          <div class="col-xs-6">
            <ul class="list-unstyled">
              <li><a href="/contact">Contact</a></li>
              <li><a href="/terms">Terms of Service</a></li>
              <li><a href="/privacy">Privacy Policy</a></li>
            </ul>
          </div>
        </div>
      </div>
    </footer>
    
    <footer class="sponsor">
      <div class="container">
        <div class="row">
          <div class="col-sm-10 col-sm-offset-1">
            <p class="text-center">
              OpenReview is created by the <a href="http://www.iesl.cs.umass.edu/" target="_blank">Information Extraction and Synthesis Laboratory</a>, College of Information and Computer Science, University of Massachusetts Amherst. We gratefully acknowledge the support of the OpenReview sponsors:  Google,  Facebook, NSF, the University of Massachusetts Amherst Center for Data Science, and Center for Intelligent Information Retrieval, as well as the Google Cloud Platform for donating the computing and networking services on which OpenReview.net runs.
            </p>
          </div>
        </div>
      </div>
    </footer>

  <div id="feedback-modal" class="modal fade" tabindex="-1" role="dialog">
    <div class="modal-dialog">
      <div class="modal-content">
        <div class="modal-header">
          <button type="button" class="close" data-dismiss="modal" aria-label="Close"><span aria-hidden="true">×</span></button>
          <h3 class="modal-title">Send Feedback</h3>
        </div>
        <div class="modal-body">
          <p>Enter your feedback below and we'll get back to you as soon as possible.</p>
          <form action="/feedback" method="POST">
            <div class="form-group">
              <input type="email" name="from" class="form-control" placeholder="Email" />
            </div>
            <div class="form-group">
              <input type="text" name="subject" class="form-control" placeholder="Subject" />
            </div>
            <div class="form-group">
              <textarea name="message" class="form-control feedback-input" rows="5" placeholder="Message" required=""></textarea>
            </div>
          <ul id="ui-id-2" tabindex="0" class="ui-menu ui-widget ui-widget-content" style="display: none;"></ul></form>
        </div>
        <div class="modal-footer">
          <button type="button" class="btn btn-default" data-dismiss="modal">Cancel</button>
          <button type="button" class="btn btn-primary">Send</button>
        </div>
      </div>
    </div>
  </div>

  <script type="text/javascript" async="" src="https://www.google-analytics.com/analytics.js"></script><script src="//cdnjs.cloudflare.com/ajax/libs/jquery/2.2.4/jquery.min.js"></script>
  <script>window.jQuery || document.write('&lt;script src="/static/js/vendor/jquery-2.2.4.min.js"&gt;&lt;\/script&gt;')</script>

  <script src="/static/dist/vendor.min.js"></script>
  <script src="/static/dist/templates.min.js"></script>
  <script src="/static/dist/openreview.min.js"></script>

    <script>window.legacyScripts = true;</script>


    <script async="" src="https://www.googletagmanager.com/gtag/js?id=UA-108703919-1"></script>
    <script>
      window.dataLayer = window.dataLayer || [];
      function gtag() { dataLayer.push(arguments); }
      gtag('js', new Date());
      gtag('config', 'UA-108703919-1');
    </script>


<div role="status" aria-live="assertive" aria-relevant="additions" class="ui-helper-hidden-accessible"></div></body></html>