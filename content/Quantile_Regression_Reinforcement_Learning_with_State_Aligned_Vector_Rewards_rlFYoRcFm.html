<!DOCTYPE html><html xmlns="http://www.w3.org/1999/xhtml" class="no-js"><head>
  <meta charset="utf-8" />
  <meta http-equiv="X-UA-Compatible" content="IE=edge,chrome=1" />
  <meta name="viewport" content="width=device-width, initial-scale=1" />

  <title>Quantile Regression Reinforcement Learning with State Aligned Vector Rewards | OpenReview</title>
  <meta name="description" content="" />

      <meta name="citation_title" content="Quantile Regression Reinforcement Learning with State Aligned Vector Rewards" />
        <meta name="citation_author" content="Oliver Richter" />
        <meta name="citation_author" content="Roger Wattenhofer" />
      <meta name="citation_publication_date" content="2018/09/27" />
      <meta name="citation_online_date" content="2018/09/27" />
      <meta name="citation_pdf_url" content="https://openreview.net/pdf?id=r1lFYoRcFm" />
      <meta name="twitter:card" content="summary" />
      <meta name="twitter:site" content="@openreviewnet" />
      <meta name="og:title" content="Quantile Regression Reinforcement Learning with State Aligned..." />
      <meta name="og:description" content="Learning from a scalar reward in continuous action space environments is difficult and often requires millions if not billions of interactions.  We introduce state aligned vector rewards, which are..." />
      <meta name="og:image" content="https://openreview.net/static/images/openreview_logo_512.png" />

  <link rel="stylesheet" href="https://fonts.googleapis.com/css?family=Noto+Sans:400,400i,700,700i" />

  <link rel="stylesheet" href="/static/css/bootstrap.min.css" />
  <link rel="stylesheet" href="/static/css/jquery-ui.min.css" />
  <link rel="stylesheet" href="/static/css/main.min.css" />
</head>

<body class="forum">
  <nav class="navbar navbar-inverse navbar-fixed-top" role="navigation">
    <div class="container">
  
      <div class="navbar-header">
        <button type="button" class="navbar-toggle collapsed" data-toggle="collapse" data-target="#navbar" aria-expanded="false" aria-controls="navbar">
          <span class="sr-only">Toggle navigation</span>
          <span class="icon-bar"></span>
          <span class="icon-bar"></span>
          <span class="icon-bar"></span>
        </button>
        <a class="navbar-brand home push-link" href="/"><strong>OpenReview</strong>.net</a>
      </div>
  
      <div id="navbar" class="navbar-collapse collapse">
        <form class="navbar-form navbar-left profile-search" role="search">
          <div class="form-group has-feedback">
            <input id="search_input" type="text" class="form-control ui-autocomplete-input" placeholder="Search ICLR 2019 Conference" autocomplete="off" />
            <span class="glyphicon glyphicon-search form-control-feedback" aria-hidden="true"></span>
          </div>
  
          <input id="search_group" type="hidden" value="ICLR.cc/2019/Conference" />
          <input id="search_content" type="hidden" value="all" />
        <ul id="ui-id-1" tabindex="0" class="ui-menu ui-widget ui-widget-content ui-autocomplete ui-front" style="display: none;"></ul></form>
  
        <ul class="nav navbar-nav navbar-right">
        
            <li id="user-menu"><a href="/login">Login</a></li>
        </ul>
      </div>
  
    </div>
  </nav>

  <div id="or-banner" class="banner" style="">
  <div class="container">
    <div class="row">
      <div class="col-xs-12"><a href="/group?id=ICLR.cc/2019/Conference" title="Venue Homepage"><img class="icon" src="/static/images/arrow_left.svg" /> Go to <strong>ICLR 2019 Conference</strong> homepage</a></div>
    </div>
  </div>
</div>
<div id="flash-message-container" class="alert alert-danger" role="alert" style="display: none;">
  <div class="container">
    <div class="row">
      <div class="col-xs-12">
        <div class="alert-content">
          <button type="button" class="close" aria-label="Close"><span aria-hidden="true">×</span></button>
        </div>
      </div>
    </div>
  </div>
</div>

<div class="container">
  <div class="row">
    <div class="col-xs-12">
      <main id="content" class="clearfix openbanner-visible legacy-styles"><div id="note_r1lFYoRcFm" class="note panel"><div class="title_pdf_row clearfix"><h2 class="note_content_title"><a>Quantile Regression Reinforcement Learning with State Aligned Vector Rewards</a> <a class="note_content_pdf" href="/pdf?id=r1lFYoRcFm" title="Download PDF" target="_blank"><img src="/static/images/pdf_icon_blue.svg" /></a> </h2></div><div class="meta_row"><span class="signatures"><a href="/profile?email=richtero%40ethz.ch" class="profile-link" data-toggle="tooltip" data-placement="bottom" title="" data-original-title="richtero@ethz.ch">Oliver Richter</a>, <a href="/profile?email=wattenhofer%40ethz.ch" class="profile-link" data-toggle="tooltip" data-placement="bottom" title="" data-original-title="wattenhofer@ethz.ch">Roger Wattenhofer</a></span></div><div class="clearfix"><div class="meta_row pull-left"><span class="date item">28 Sep 2018 (modified: 17 Nov 2018)</span><span class="item">ICLR 2019 Conference Withdrawn Submission</span><span class="item">Readers: <span class="readers-icon glyphicon glyphicon-globe"></span> Everyone</span></div><div class="meta_row meta_actions"></div></div><div class="note_contents"><span class="note_content_field">Abstract: </span><span class="note_content_value">Learning from a scalar reward in continuous action space environments is difficult and often requires millions if not billions of interactions.  We introduce state aligned vector rewards, which are easily defined in metric state spaces and allow our deep reinforcement learning agent to tackle the curse of dimensionality.  Our agent learns to map from action distributions to state change distributions implicitly defined in a quantile function neural network.   We further introduce a new reinforcement learning technique inspired by quantile regression which does not limit agents to explicitly parameterized action distributions.  Our results in high dimensional state spaces show that training with vector rewards allows our agent to learn multiple times faster than an agent training with scalar rewards.</span></div><div class="note_contents"><span class="note_content_field">Keywords: </span><span class="note_content_value">deep reinforcement learning, quantile regression, vector reward</span></div><div class="note_contents"><span class="note_content_field">TL;DR: </span><span class="note_content_value">We train with state aligned vector rewards an agent predicting state changes from action distributions, using a new reinforcement learning technique inspired by quantile regression.</span></div><div class="reply_row clearfix"><div class="item" id="reply_count">12 Replies</div></div></div><hr class="small" /><div id="note_children"><div class="note_with_children"><div id="note_SJgGWBDupX" class="note panel"><div class="title_pdf_row clearfix"><h2 class="note_content_title"><a>Submission Withdrawn by the Authors</a>  <button class="btn btn-xs btn-default permalink-button" title="Link to this comment" data-permalink-url="https://openreview.net/forum?id=r1lFYoRcFm&amp;noteId=SJgGWBDupX"><span class="glyphicon glyphicon-link" aria-hidden="true"></span></button></h2></div><div class="meta_row"><span class="signatures">ICLR 2019 Conference Paper462 Authors</span></div><div class="clearfix"><div class="meta_row pull-left"><span class="date item">13 Nov 2018</span><span class="item">ICLR 2019 Conference Paper462 Withdraw Submission</span><span class="item">Readers: <span class="readers-icon glyphicon glyphicon-globe"></span> Everyone</span></div><div class="meta_row meta_actions"></div></div><div class="note_contents"><span class="note_content_field">Withdrawal confirmation: </span><span class="note_content_value">I have read and agree with the withdrawal statement on behalf of myself and my co-authors.</span></div><div class="reply_row clearfix"></div></div><div class="children"><div class="note_with_children"><div id="note_r1e2avPuTm" class="note panel"><div class="title_pdf_row clearfix"><h2 class="note_content_title"><a>Note on our decision.</a>  <button class="btn btn-xs btn-default permalink-button" title="Link to this comment" data-permalink-url="https://openreview.net/forum?id=r1lFYoRcFm&amp;noteId=r1e2avPuTm"><span class="glyphicon glyphicon-link" aria-hidden="true"></span></button></h2></div><div class="meta_row"><span class="signatures">ICLR 2019 Conference Paper462 Authors</span></div><div class="clearfix"><div class="meta_row pull-left"><span class="date item">13 Nov 2018</span><span class="item">ICLR 2019 Conference Paper462 Official Comment</span><span class="item">Readers: <span class="readers-icon glyphicon glyphicon-globe"></span> Everyone</span></div><div class="meta_row meta_actions"></div></div><div class="note_contents"><span class="note_content_field">Comment: </span><span class="note_content_value">We are thankful for the very constructive reviews we got and agree with the reviewers that the evaluation of our approach is incomplete. Since we do not have enough resources to address all issues within the rebuttal period, we decided to withdraw the submission from ICLR 2019, such that we can address all concerns with the attention they deserve. If you like our idea and want to apply it to a given problem or explore it further, you are welcome to get in touch with us. Also you are invited to cite our NIPS workshop paper on the topic: <a href="https://openreview.net/forum?id=S1fuWRYCFm" target="_blank" rel="nofollow">https://openreview.net/forum?id=S1fuWRYCFm</a> which I'll present in the workshop on modeling and decision-making in the spatiotemporal domain.
In the name of the authors
Oliver Richter</span></div><div class="reply_row clearfix"></div></div><div class="children"></div></div></div></div><div class="note_with_children"><div id="note_ryeS0azk6Q" class="note panel"><div class="title_pdf_row clearfix"><h2 class="note_content_title"><a>Thanks for the constructive reviews - clarifications upcoming</a>  <button class="btn btn-xs btn-default permalink-button" title="Link to this comment" data-permalink-url="https://openreview.net/forum?id=r1lFYoRcFm&amp;noteId=ryeS0azk6Q"><span class="glyphicon glyphicon-link" aria-hidden="true"></span></button></h2></div><div class="meta_row"><span class="signatures">ICLR 2019 Conference Paper462 Authors</span></div><div class="clearfix"><div class="meta_row pull-left"><span class="date item">06 Nov 2018</span><span class="item">ICLR 2019 Conference Paper462 Official Comment</span><span class="item">Readers: <span class="readers-icon glyphicon glyphicon-globe"></span> Everyone</span></div><div class="meta_row meta_actions"></div></div><div class="note_contents"><span class="note_content_field">Comment: </span><span class="note_content_value">We want to thank the reviewers for their detailed and constructive reviews, we really appreciate the high quality of the reviews. As the main author is currently on a conference, we just wanted to let you know that we will give a detailed answer and clarification to all reviews next week and will also revise our paper based on the suggestions. For now, we are thankful for the detailed reviews we got.</span></div><div class="reply_row clearfix"></div></div><div class="children"></div></div><div class="note_with_children"><div id="note_BJx-jW6Y3X" class="note panel"><div class="title_pdf_row clearfix"><h2 class="note_content_title"><a>An attempt at exploiting the reward structure that unfortunately lacks both strong theoretical and empirical support</a>  <button class="btn btn-xs btn-default permalink-button" title="Link to this comment" data-permalink-url="https://openreview.net/forum?id=r1lFYoRcFm&amp;noteId=BJx-jW6Y3X"><span class="glyphicon glyphicon-link" aria-hidden="true"></span></button></h2></div><div class="meta_row"><span class="signatures">ICLR 2019 Conference Paper462 AnonReviewer3</span></div><div class="clearfix"><div class="meta_row pull-left"><span class="date item">02 Nov 2018 (modified: 07 Nov 2018)</span><span class="item">ICLR 2019 Conference Paper462 Official Review</span><span class="item">Readers: <span class="readers-icon glyphicon glyphicon-globe"></span> Everyone</span></div><div class="meta_row meta_actions"></div></div><div class="note_contents"><span class="note_content_field">Review: </span><span class="note_content_value">The algorithm presented here aims at speeding up (reinforcement) learning in situations where the reward is “aligned” with the state space, i.e. can be decomposed into multiple components where each component depends only on one dimension of the state. Such a situation is found e.g. in tasks where the agent needs to reach a specific position in a 2D or 3D space, and the reward can be decomposed into a 2D / 3D reward vector associated to reaching the target coordinate along each axis. The algorithm consists in first pre-training (by collecting data obtained with random action distributions) a so-called “Position Change Prediction Network” (PCPN) to map an action distribution (e.g. mu and sigma if the agent’s policy is Gaussian) to the corresponding state change distribution along each relevant state dimension (for which there is an associated reward). The PCPN is an Implicit Quantile Network (Dabney et al, 2018), so as to model the state change distribution without assuming a specific parametric form. Once trained, the PCPN can be backpropagated through to train the agent distribution so that state changes that lead to better rewards (along each axis) become more frequent, while those leading to worse reward become less frequent (where “better” and “worse” are defined by the advantage, computed with a critic V). Experiments are performed on three toy tasks, and comparisons to A2C show faster learning for the proposed algorithm, especially in higher dimensional state spaces.

The proposed approach combines two interesting ideas: (1) decomposing the reward into multiple components, and (2) aligning the agent’s action distribution with these components (through the PCPN). The second point is novel to the best of my knowledge. However, the first one is not since there exists a large body of literature on multi-objective reinforcement learning. The authors briefly mention some of it in the “Related Work” section, but unfortunately there is no comparison being made in experiments (the only algorithm being compared against is a “vanilla” A2C that uses a single scalar reward). Although previous work on multi-objective RL may not necessarily take advantage of the alignment between reward components and state dimensions, it is still relevant since the decomposition of the reward function by itself might be enough to speed-up learning, regardless of whether or not it is aligned with the state space. Experiments need to show that using the alignment as proposed here actually brings some benefits over existing multi-objective techniques.

In addition to the lack of comparison to related multi-objective methods, the empirical evaluation is not very convincing since it is done on toy problems, with the most “realistic” one being a simple 2D task where the improvement brought by the proposed technique is not obvious (Fig. 5). Better results are obtained on high dimensional toy problems but (a) in these problems the relationship between actions and reward components is somewhat trivial, and (b) since the paper was originally motivated by spatial positioning tasks, it is not clear how often in practice one will need to go beyond 2 or 3 dimensions... Finally, since there is no theoretical analysis either (except for some intuition as to what happens in a limit case at the bottom of p. 4), and the optimized objective is essentially heuristically motivated, it is hard to tell how the algorithm would behave across a larger range of applications.

I have two concerns in particular regarding the methodology:
- Pre-training the PCPN assumes that randomly sampling action distributions will give us enough coverage of the state and action distribution spaces to generalize to those seen during training of the agent. This may not always be the case, requiring training the PCPN online: it would have been nice to verify whether this is feasible in practice.
- I am afraid that forcing the agent to learn only from the PCPN gradient may prevent it from learning an optimal policy. As a dummy illustrative example, imagine an infinite 2D gridworld where the agent has two actions (action A = increase x coordinate by 1 only if y = 0, action B = increase both x and y by 1 only if y = 0), the reward is +1 when x increases and the agent starts at the origin. The reward is aligned with the x axis, and the optimal policy is to always take action A since as soon as action B is taken the agent can’t move anymore. With the PCPN output being the change along the x axis, its gradient wrt the agent action distribution is zero since both actions A and B increase x equally. Thus the agent will never be able to learn that A is to be preferred. In other words, it seems to me that the agent is limited to learn from changes in the state coordinates aligned with the reward, even if other coordinates are important for transition dynamics that may impact future rewards.

Regarding the clarity of the paper, I found it good enough overall except that it is a bit difficult to follow the narrative. In particular Section 3 breaks the flow by considering a more general setting than the one introduced previously (the note about “action” and “policy” meaning something else is particularly confusing). Using Q to denote the action distribution is also a poor idea since Q is pretty much a “reserved” letter in RL. Overall the generic idea of “Quantile Regression RL” from Section 3 seems potentially interesting, and may be worth exploring on its own, but the current presentation (in the application to multi-objective state-aligned rewards) makes it more a distraction than an asset.

Minor points:
- The “biological insights” from the introduction are difficult to grasp since a reader versed in RL is unlikely to know what are “viral vector strategies” or “effector specific value estimations”
- The definition of aligned spaces (“their dimensions correlate”) is pretty vague, a more precise mathematical definition would be helpful (even if the intuition is easy to grasp)
- In the first one-line equation p. 4 there is a tau that should be a_tau
- In the definition of R_t,n on p. 5, the sum should be up to t+n-1 (and same with the vector version at the bottom of the page)
- I wonder if the loss L_mon is really needed, since the quantile regression loss should naturally lead to a monotonous function (assuming enough data and capacity). Are results significantly worse without it?
- The definition of the Huber loss is incorrect 
- It is not explicitly stated that there is no backpropagation through V_psi in L_a
- “QRRL can easily be adapted to the off-policy setting”: this is not obvious since you are using n-step returns bootstrapped with a critic V, which requires on-policy data (unless appropriate corrections are applied)
- Experiments do not clearly define the rewards used by both algorithms 
- Figures would be easier to read if axes were labeled
- I am surprised by the absence of entropy loss in A2C (beta = 0), was this hyper-parameter carefully tuned?
- Having diagrams of the networks in Appendix A would be clearer than text explanations
- The “cosine embedding” should be defined to avoid having to refer to Dabney et al
- Note that the proposed model ends up having significantly more capacity than the single A2C network, which could raise concerns wrt the fairness of the comparison</span></div><div class="note_contents"><span class="note_content_field">Rating: </span><span class="note_content_value">4: Ok but not good enough - rejection</span></div><div class="note_contents"><span class="note_content_field">Confidence: </span><span class="note_content_value">4: The reviewer is confident but not absolutely certain that the evaluation is correct</span></div><div class="reply_row clearfix"></div></div><div class="children"><div class="note_with_children"><div id="note_ryxkQ5SO6Q" class="note panel"><div class="title_pdf_row clearfix"><h2 class="note_content_title"><a>Thank you for the detailed constructive feedback!</a>  <button class="btn btn-xs btn-default permalink-button" title="Link to this comment" data-permalink-url="https://openreview.net/forum?id=r1lFYoRcFm&amp;noteId=ryxkQ5SO6Q"><span class="glyphicon glyphicon-link" aria-hidden="true"></span></button></h2></div><div class="meta_row"><span class="signatures">ICLR 2019 Conference Paper462 Authors</span></div><div class="clearfix"><div class="meta_row pull-left"><span class="date item">13 Nov 2018</span><span class="item">ICLR 2019 Conference Paper462 Official Comment</span><span class="item">Readers: <span class="readers-icon glyphicon glyphicon-globe"></span> Everyone</span></div><div class="meta_row meta_actions"></div></div><div class="note_contents"><span class="note_content_field">Comment: </span><span class="note_content_value">Thanks for the feedback! We really appreciate the high quality of your review! Here some clarifications/answers from our side:

"Experiments need to show that using the alignment as proposed here actually brings some benefits over existing multi-objective techniques."
Although there exists a large body of literature on multi-objective reinforcement learning, we only found a few papers (mentioned in our related work) that apply the concepts to deep reinforcement learning (if you know more, we'd be happy if you can point us to them). However, we agree that a comparison would make the paper more complete and we will consider it when revising the paper.

"Better results are obtained on high dimensional toy problems but (a) in these problems the relationship between actions and reward components is somewhat trivial, and (b) since the paper was originally motivated by spatial positioning tasks, it is not clear how often in practice one will need to go beyond 2 or 3 dimensions..."
We agree that the paper currently lacks a relation to practical applicability. We already have some better experiments in mind and will work on evaluating our approach on these.

"[...] training the PCPN online: it would have been nice to verify whether this is feasible in practice."
We did conduct some informal experiments on this, but settled for the easier approach of pretraining to avoid destructive interference between the learning objectives. However, we plan to investigate this further and will also look into the relationship between PCPN error and training performance (as suggested by reviewer 1).

"I am afraid that forcing the agent to learn only from the PCPN gradient may prevent it from learning an optimal policy. [...]"
You point out an interesting limitation here we honestly didn't think about. To circumvent this one could additionally use the reward sum to train the agent network with normal reinforcement learning. This however introduces another loss term. We will address this issue in the revision.

"Regarding the clarity of the paper [...]"
Yes, we noticed that Q is a bad choice for the notation. Also we found that QRRL is worth investigating on its own and are currently working on a separate project that investigates exactly this. We see that its presentation here clutters the understanding which is one of the reasons why we are going to withdraw the paper.

"Minor points: [...]"
Thanks for the many things you point out. As you might have noticed, the paper was written in a rush and we apologize for the typos and vague definitions. As you took the time for such a detailed revision, the least we can do is answer your questions:

"I wonder if the loss L_mon is really needed [...]. Are results significantly worse without it?"
Yes to both. Quantile regression will converge to a monotonic function as long as you decrease the quantile loss rho_tau for a given sample. However, when you want to decrease the probability of a given sample, i.e., increase the quantile loss (which happens when the advantage is negative), you might end up with a non-monotonic function if you don't constrain the objective.
 
"I am surprised by the absence of entropy loss in A2C (beta = 0), was this hyper-parameter carefully tuned?"
Yes, since our agent takes continuous actions in environments with high randomness (since the starting position is sampled randomly), we found that further encouraging randomness let to agents that did not learn at all.

Thanks again for the many suggestions and detailed review! If you have further questions, do not hesitate to get in contact with us.</span></div><div class="reply_row clearfix"></div></div><div class="children"><div class="note_with_children"><div id="note_H1evo1POam" class="note panel"><div class="title_pdf_row clearfix"><h2 class="note_content_title"><a>Thanks for the detailed answer</a>  <button class="btn btn-xs btn-default permalink-button" title="Link to this comment" data-permalink-url="https://openreview.net/forum?id=r1lFYoRcFm&amp;noteId=H1evo1POam"><span class="glyphicon glyphicon-link" aria-hidden="true"></span></button></h2></div><div class="meta_row"><span class="signatures">ICLR 2019 Conference Paper462 AnonReviewer3</span></div><div class="clearfix"><div class="meta_row pull-left"><span class="date item">13 Nov 2018</span><span class="item">ICLR 2019 Conference Paper462 Official Comment</span><span class="item">Readers: <span class="readers-icon glyphicon glyphicon-globe"></span> Everyone</span></div><div class="meta_row meta_actions"></div></div><div class="note_contents"><span class="note_content_field">Comment: </span><span class="note_content_value">I appreciate the reply -- in particular the clarification on L_mon. Unfortunately I can't point to to the most relevant work on multi-objective RL as it's not really my area of expertise, but I think a comparison to at least one of the few papers you mention regarding multi-objective deep RL methods would help build a more convincing case for the potential benefits of state alignment.

Best of luck with the revision!</span></div><div class="reply_row clearfix"></div></div><div class="children"></div></div></div></div></div></div><div class="note_with_children"><div id="note_rkxnj29uhQ" class="note panel"><div class="title_pdf_row clearfix"><h2 class="note_content_title"><a>Paper is smooth to read, but important issues remian.</a>  <button class="btn btn-xs btn-default permalink-button" title="Link to this comment" data-permalink-url="https://openreview.net/forum?id=r1lFYoRcFm&amp;noteId=rkxnj29uhQ"><span class="glyphicon glyphicon-link" aria-hidden="true"></span></button></h2></div><div class="meta_row"><span class="signatures">ICLR 2019 Conference Paper462 AnonReviewer2</span></div><div class="clearfix"><div class="meta_row pull-left"><span class="date item">01 Nov 2018 (modified: 07 Nov 2018)</span><span class="item">ICLR 2019 Conference Paper462 Official Review</span><span class="item">Readers: <span class="readers-icon glyphicon glyphicon-globe"></span> Everyone</span></div><div class="meta_row meta_actions"></div></div><div class="note_contents"><span class="note_content_field">Review: </span><span class="note_content_value">The paper addresses RL in the continuous action space. The scope is right for ICLR. The main two claimed contributions of the paper are the use of a re-parametrised policy and the use of a novel vector-based training objective (similar to solving each component of a factored MDP).

I have the following concerns about the paper.

(i) First, section 3 seems to re-invent the notion of a re-parametrised probability distribution. Of course you can get any well-behaved probability distribution from the uniform distribution by applying some non-linear transformation. That has, AFAIK been known since probability theory with continuous random variables has been formulated. There is a lot of work in the variational auto-encoder community right now on learning transformations like this. I do not exactly see what the purported novelty of section 3 is. The last equation on page (4) states that the derivative of the inverse function is the reciprocal of the derivative of a function, a well-known fact from calculus (and you cite a paper from 1992 to corroborate it?!). The other equations on page 4 and on top of page 5 seem to manually re-do the math of back-propagating through networks with random inputs, for a special case - this has been known for quite some time as well.

(ii) I do not understand why the learned function \hat{Q} has to be monotonic (the point of the constraint in equation (2)). Sure, you won't get a correspondence with a CDF if it's not, but why is this a problem? You would still be learning some probability distribution that does what you want?

(iii) Section 4 augments the information available to the agent with the component-wise absolute value between the current position and the goal position and uses that to (purportedly) improve performance. I see two problems with this. On a practical side, this restricts the applicability of the algorithm to settings with clear goal states where the state space is Euclidean (or similar). Also, if we know the goal state, there is the question of not just feed it to the agent directly (as a part of the observation) and avoid having the complicated vector reward structure at all? I do not see a compelling reason for this. On a theatrical side, this can only really work if the state space is (a subset of) R^n. Also, if we can improve each action without regard for other actions, (as the algorithm seems to assume) doesn't this correspond to some kind of assumption that the MDP is factorised?

(iv) The paper has problems with presentation. It is well-written as far as the English goes and it is superficially smooth to read, but the concepts which are crucial to understand the main point are very confounded. There is no pseudocode that explains how the algorithm works. I feel that the paper should include an equation literately states all the loss terms being optimised.

(v) The paper takes a cavalier approach to formalism. It mentions arbitrary metric spaces several times, as well as the notion of dimension induced purely by metric structure, but the equations only really address the Euclidean vector space. This is OK and this is what most RL algorithm do, but you shouldn't claim that you algorithm is more general than what is actually defined in the paper.

Unfortunately, as the paper stands now, I have some concerns about whether it is suitable for publication.

I wanted to encourage the authors to address the following points during the discussion phase. Also, if you feel the review misunderstands the paper, please clarify - I am willing to update my score if the reply is convincing.

Suggestions:

1. Include an equation that mentions all the loss terms in the algorithm.
2. Figure 2 should be augmented to include where the loss signal comes from.
3. Contrast section 3 with variational auto-encoders [2].
4. Contrast section 3 with re-parameterised policy gradients. 
5. Explain why the monotonicity constraint is necessary in section 3.
5. Contrast the setting of vector reward with literature on factored MDPs [1].
6. Remove inflated claims about applicability to general metric spaces.

[1] Guestrin et al. Eﬃcient Solution Algorithms for Factored MDPs
[2] Doersch. Tutorial on variational autoencoders (and further references therein)



</span></div><div class="note_contents"><span class="note_content_field">Rating: </span><span class="note_content_value">3: Clear rejection</span></div><div class="note_contents"><span class="note_content_field">Confidence: </span><span class="note_content_value">3: The reviewer is fairly confident that the evaluation is correct</span></div><div class="reply_row clearfix"></div></div><div class="children"><div class="note_with_children"><div id="note_rygozZvdTm" class="note panel"><div class="title_pdf_row clearfix"><h2 class="note_content_title"><a>Thanks for the feedback - some clarifications</a>  <button class="btn btn-xs btn-default permalink-button" title="Link to this comment" data-permalink-url="https://openreview.net/forum?id=r1lFYoRcFm&amp;noteId=rygozZvdTm"><span class="glyphicon glyphicon-link" aria-hidden="true"></span></button></h2></div><div class="meta_row"><span class="signatures">ICLR 2019 Conference Paper462 Authors</span></div><div class="clearfix"><div class="meta_row pull-left"><span class="date item">13 Nov 2018</span><span class="item">ICLR 2019 Conference Paper462 Official Comment</span><span class="item">Readers: <span class="readers-icon glyphicon glyphicon-globe"></span> Everyone</span></div><div class="meta_row meta_actions"></div></div><div class="note_contents"><span class="note_content_field">Comment: </span><span class="note_content_value">Thanks a lot for your review. Some clarifications on your concerns:

"(i) First, section 3 seems to re-invent the notion of a re-parametrised probability distribution. "
The novel/less well studied insight here is to approximate the probability distribution implicitly in all network parameters instead of explicitly giving the parameters of a parameterized probability distribution (e.g., a gaussian) as output. Yes, there has been a lot of interesting work recently on VAEs and variations thereof. Note however, that these explicitly give the parameters of a priori fixed parameterized probability distribution (prior) as output. In contrast, we do not have a prior, our network can approximate any probability distribution (within the capacity limitations of the network). We would like to give credit to Dabney et al (2018) at this point, since the idea of implicit quantile networks is not our invention (as stated in the paper). However, besides the papers we reference we are unaware of others using this approach, therefore we stretched its aspects in section 3.

"The last equation on page (4) states that the derivative of the inverse function is the reciprocal of the derivative of a function, a well-known fact from calculus (and you cite a paper from 1992 to corroborate it?!)"
We would be happy if you can point us to a more appropriate reference here.

"The other equations on page 4 and on top of page 5 seem to manually re-do the math of back-propagating through networks with random inputs, for a special case - this has been known for quite some time as well."
Our point here was to show that the gradient is correlated with the probability of an action and therefore gradient ascent on the quantile regression loss (additionally to gradient descent) also makes sense, given that we stay within the set of monotonic functions. We will try to give a better theoretical explanation in a revised version of the paper.

"(ii) I do not understand why the learned function \hat{Q} has to be monotonic (the point of the constraint in equation (2)). Sure, you won't get a correspondence with a CDF if it's not, but why is this a problem? You would still be learning some probability distribution that does what you want?"
As we assume that our network models a quantile function (and sample as such from it), we have to enforce the monotonicity. Otherwise, gradient ascent on the quantile regression loss leads to divergence.

"(iii) [...] Also, if we know the goal state, there is the question of not just feed it to the agent directly (as a part of the observation) and avoid having the complicated vector reward structure at all?"
We do feed the goal state (implicitly in experiments 1 and 2 and explicitly in experiment 3) to the agent as part of the observation. However, since state and action space are not aligned, the agent cannot directly translate this observation into the optimal action. Our vector reward approach helps the agent to learn this mapping efficiently.

"Also, if we can improve each action without regard for other actions, (as the algorithm seems to assume) doesn't this correspond to some kind of assumption that the MDP is factorised?"
What we essentially do is that we learn to factorise the MDP. The original MDP is not factorised (since an action can affect multiple dimensions). However, the PCPN learns to decouple the effects of the action such that we can learn on the factorised MDP (where we regard the position change as action) with vector rewards and the backpropagation  through the PCPN translates this into the appropriate adjustments of the action in the original (entangled) MDP.

"I feel that the paper should include an equation literately states all the loss terms being optimised."
Thanks for the suggestion, we will consider it in the revision.

"(v) [...] It mentions arbitrary metric spaces [...], but the equations only really address the Euclidean vector space. [...] you shouldn't claim that you algorithm is more general than what is actually defined in the paper."
Thanks for pointing out that a more general derivation is missing in the paper. We will address this in the revision.

"Suggestions: "
Thanks, we will address these in the revision

"Contrast the setting of vector reward with literature on factored MDPs [1]."
On a first glance, the paper you mention seems to focus on the tabular RL setting with finite state space (whereas we focus on infinite continuous state spaces). However, we will read it more carefully to address the similarities to our approach in our revision. Thanks for pointing out this relation.</span></div><div class="reply_row clearfix"></div></div><div class="children"><div class="note_with_children"><div id="note_BJlptY-iTm" class="note panel"><div class="title_pdf_row clearfix"><h2 class="note_content_title"><a>Reply to comments.</a>  <button class="btn btn-xs btn-default permalink-button" title="Link to this comment" data-permalink-url="https://openreview.net/forum?id=r1lFYoRcFm&amp;noteId=BJlptY-iTm"><span class="glyphicon glyphicon-link" aria-hidden="true"></span></button></h2></div><div class="meta_row"><span class="signatures">ICLR 2019 Conference Paper462 AnonReviewer2</span></div><div class="clearfix"><div class="meta_row pull-left"><span class="date item">15 Nov 2018</span><span class="item">ICLR 2019 Conference Paper462 Official Comment</span><span class="item">Readers: <span class="readers-icon glyphicon glyphicon-globe"></span> Everyone</span></div><div class="meta_row meta_actions"></div></div><div class="note_contents"><span class="note_content_field">Comment: </span><span class="note_content_value">1. VAEs with enough neurons can in principle represent any distribution which has a pdf with close-enough accuracy.
2. You do not need a reference results from an undergraduate calculus class.
3. Finite MDPs are a useful tool for visualising algorithms and figuring out how they work. You can have factored MDPs both with continuous and finite state spaces. In either case, you can compare this to having what you call a "vector reward". </span></div><div class="reply_row clearfix"></div></div><div class="children"><div class="note_with_children"><div id="note_rye6tkMnpX" class="note panel"><div class="title_pdf_row clearfix"><h2 class="note_content_title"><a>Thanks for keeping up the discussion</a>  <button class="btn btn-xs btn-default permalink-button" title="Link to this comment" data-permalink-url="https://openreview.net/forum?id=r1lFYoRcFm&amp;noteId=rye6tkMnpX"><span class="glyphicon glyphicon-link" aria-hidden="true"></span></button></h2></div><div class="meta_row"><span class="signatures">ICLR 2019 Conference Paper462 Authors</span></div><div class="clearfix"><div class="meta_row pull-left"><span class="date item">16 Nov 2018</span><span class="item">ICLR 2019 Conference Paper462 Official Comment</span><span class="item">Readers: <span class="readers-icon glyphicon glyphicon-globe"></span> Everyone</span></div><div class="meta_row meta_actions"></div></div><div class="note_contents"><span class="note_content_field">Comment: </span><span class="note_content_value">1. Correct. However, one has to determine how many neurons to assign to each distribution one wants to approximate (in our case one distribution per state dimension). We will include a comparison between multi-variate Gaussian networks and implicit Quantile networks in a revision of this work. Thanks for the suggestion.
2. Noted, we'll remove it then.
3. I'll definitely read the paper. Thanks again for the reference.</span></div><div class="reply_row clearfix"></div></div><div class="children"></div></div></div></div></div></div></div></div><div class="note_with_children"><div id="note_r1xOFXoVnQ" class="note panel"><div class="title_pdf_row clearfix"><h2 class="note_content_title"><a>The idea is interesting but the paper should backed with more complete experiments</a>  <button class="btn btn-xs btn-default permalink-button" title="Link to this comment" data-permalink-url="https://openreview.net/forum?id=r1lFYoRcFm&amp;noteId=r1xOFXoVnQ"><span class="glyphicon glyphicon-link" aria-hidden="true"></span></button></h2></div><div class="meta_row"><span class="signatures">ICLR 2019 Conference Paper462 AnonReviewer1</span></div><div class="clearfix"><div class="meta_row pull-left"><span class="date item">29 Oct 2018 (modified: 07 Nov 2018)</span><span class="item">ICLR 2019 Conference Paper462 Official Review</span><span class="item">Readers: <span class="readers-icon glyphicon glyphicon-globe"></span> Everyone</span></div><div class="meta_row meta_actions"></div></div><div class="note_contents"><span class="note_content_field">Review: </span><span class="note_content_value">This work proposes to mix distributional RL with a net in charge of modeling the evolution of the world in terms of quantiles (PCPN in the paper) . The claim is that this improves the sample efficiency because the backpropagation of collected rewards takes advantage of distributional knowledge we have about the evolution of the world.

I like the idea but if we have a good model of the dynamics of the world (the PCPN) then (stochastic) dynamic programming is likely to be a very hard to beat baseline. This is one of the reason why many RL approaches are avoiding an explicit model of the world.  The presence of this model is very central in the performance of the proposed approach and I do not think that the experiments are conclusive enough. More specifically I think the paper would be better if it would answer some questions 
- How the performance is affected wrt the error made by the PCPN. In the current architecture if the PCPN is bad for some reason then there is no way to recover. Thus the approach cannot be used if we are not able to fit the dynamics
- More baselines would be added. In particular in this kind setting PPO is known to perform well (but many other could be added ACKTR, interpolated policy gradient, PPO with Stein control variate,...). I know there is possibly too many to run a reasonable experiment but with only A2C as a competitor it is not even possible to know if the extra performance comes from the distributional learning or from the use of the "aligned states"
- The task is not very appealing: classical control often work better on theses tasks (with are variants of acrobot already available as Gym environments). To assert the validity of the approach it could be worth to use Atari tasks (but this is expensive) or some Mujuco/Robotics ones </span></div><div class="note_contents"><span class="note_content_field">Rating: </span><span class="note_content_value">4: Ok but not good enough - rejection</span></div><div class="note_contents"><span class="note_content_field">Confidence: </span><span class="note_content_value">4: The reviewer is confident but not absolutely certain that the evaluation is correct</span></div><div class="reply_row clearfix"></div></div><div class="children"><div class="note_with_children"><div id="note_ryxt5VPdpX" class="note panel"><div class="title_pdf_row clearfix"><h2 class="note_content_title"><a>Thanks for the feedback and suggestions</a>  <button class="btn btn-xs btn-default permalink-button" title="Link to this comment" data-permalink-url="https://openreview.net/forum?id=r1lFYoRcFm&amp;noteId=ryxt5VPdpX"><span class="glyphicon glyphicon-link" aria-hidden="true"></span></button></h2></div><div class="meta_row"><span class="signatures">ICLR 2019 Conference Paper462 Authors</span></div><div class="clearfix"><div class="meta_row pull-left"><span class="date item">13 Nov 2018</span><span class="item">ICLR 2019 Conference Paper462 Official Comment</span><span class="item">Readers: <span class="readers-icon glyphicon glyphicon-globe"></span> Everyone</span></div><div class="meta_row meta_actions"></div></div><div class="note_contents"><span class="note_content_field">Comment: </span><span class="note_content_value">Thank you for the constructive review! We agree that the paper as it stands now is not yet suitable for publication at ICLR due to missing evaluation and are thankful for your suggestions.</span></div><div class="reply_row clearfix"></div></div><div class="children"></div></div></div></div></div></main></div>
  </div>
</div>


    <!-- Footer -->
    <footer class="sitemap">
      <div class="container">
        <div class="row hidden-xs">
          <div class="col-sm-4">
            <ul class="list-unstyled">
              <li><a href="/" class="home">Home</a></li>
              <li><a href="/venues">All Venues</a></li>
            </ul>
          </div>
    
          <div class="col-sm-4">
            <ul class="list-unstyled">
              <li><a href="/about">About OpenReview</a></li>
              <li><a href="/contact">Contact</a></li>
              <li><a href="#" data-toggle="modal" data-target="#feedback-modal">Feedback</a></li>
            </ul>
          </div>
    
          <div class="col-sm-4">
            <ul class="list-unstyled">
              <li><a href="/terms">Terms of Service</a></li>
              <li><a href="/privacy">Privacy Policy</a></li>
            </ul>
          </div>
        </div>
    
        <div class="row visible-xs-block">
          <div class="col-xs-6">
            <ul class="list-unstyled">
              <li><a href="/" class="home">Home</a></li>
              <li><a href="/about">About OpenReview</a></li>
              <li><a href="#" data-toggle="modal" data-target="#feedback-modal">Feedback</a></li>
            </ul>
          </div>
    
          <div class="col-xs-6">
            <ul class="list-unstyled">
              <li><a href="/contact">Contact</a></li>
              <li><a href="/terms">Terms of Service</a></li>
              <li><a href="/privacy">Privacy Policy</a></li>
            </ul>
          </div>
        </div>
      </div>
    </footer>
    
    <footer class="sponsor">
      <div class="container">
        <div class="row">
          <div class="col-sm-10 col-sm-offset-1">
            <p class="text-center">
              OpenReview is created by the <a href="http://www.iesl.cs.umass.edu/" target="_blank">Information Extraction and Synthesis Laboratory</a>, College of Information and Computer Science, University of Massachusetts Amherst. We gratefully acknowledge the support of the OpenReview sponsors:  Google,  Facebook, NSF, the University of Massachusetts Amherst Center for Data Science, and Center for Intelligent Information Retrieval, as well as the Google Cloud Platform for donating the computing and networking services on which OpenReview.net runs.
            </p>
          </div>
        </div>
      </div>
    </footer>

  <div id="feedback-modal" class="modal fade" tabindex="-1" role="dialog">
    <div class="modal-dialog">
      <div class="modal-content">
        <div class="modal-header">
          <button type="button" class="close" data-dismiss="modal" aria-label="Close"><span aria-hidden="true">×</span></button>
          <h3 class="modal-title">Send Feedback</h3>
        </div>
        <div class="modal-body">
          <p>Enter your feedback below and we'll get back to you as soon as possible.</p>
          <form action="/feedback" method="POST">
            <div class="form-group">
              <input type="email" name="from" class="form-control" placeholder="Email" />
            </div>
            <div class="form-group">
              <input type="text" name="subject" class="form-control" placeholder="Subject" />
            </div>
            <div class="form-group">
              <textarea name="message" class="form-control feedback-input" rows="5" placeholder="Message" required=""></textarea>
            </div>
          <ul id="ui-id-2" tabindex="0" class="ui-menu ui-widget ui-widget-content" style="display: none;"></ul></form>
        </div>
        <div class="modal-footer">
          <button type="button" class="btn btn-default" data-dismiss="modal">Cancel</button>
          <button type="button" class="btn btn-primary">Send</button>
        </div>
      </div>
    </div>
  </div>

  <script type="text/javascript" async="" src="https://www.google-analytics.com/analytics.js"></script><script src="//cdnjs.cloudflare.com/ajax/libs/jquery/2.2.4/jquery.min.js"></script>
  <script>window.jQuery || document.write('&lt;script src="/static/js/vendor/jquery-2.2.4.min.js"&gt;&lt;\/script&gt;')</script>

  <script src="/static/dist/vendor.min.js"></script>
  <script src="/static/dist/templates.min.js"></script>
  <script src="/static/dist/openreview.min.js"></script>

    <script>window.legacyScripts = true;</script>


    <script async="" src="https://www.googletagmanager.com/gtag/js?id=UA-108703919-1"></script>
    <script>
      window.dataLayer = window.dataLayer || [];
      function gtag() { dataLayer.push(arguments); }
      gtag('js', new Date());
      gtag('config', 'UA-108703919-1');
    </script>


<div role="status" aria-live="assertive" aria-relevant="additions" class="ui-helper-hidden-accessible"></div></body></html>