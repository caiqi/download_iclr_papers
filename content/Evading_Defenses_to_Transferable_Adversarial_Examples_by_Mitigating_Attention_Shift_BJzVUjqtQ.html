<!DOCTYPE html><html xmlns="http://www.w3.org/1999/xhtml" class="no-js"><head>
  <meta charset="utf-8" />
  <meta http-equiv="X-UA-Compatible" content="IE=edge,chrome=1" />
  <meta name="viewport" content="width=device-width, initial-scale=1" />

  <title>Evading Defenses to Transferable Adversarial Examples by Mitigating Attention Shift | OpenReview</title>
  <meta name="description" content="" />

      <meta name="citation_title" content="Evading Defenses to Transferable Adversarial Examples by Mitigating Attention Shift" />
        <meta name="citation_author" content="Anonymous" />
      <meta name="citation_publication_date" content="2018/09/27" />
      <meta name="citation_online_date" content="2018/09/27" />
      <meta name="citation_pdf_url" content="https://openreview.net/pdf?id=BJzVUj0qtQ" />
      <meta name="twitter:card" content="summary" />
      <meta name="twitter:site" content="@openreviewnet" />
      <meta name="og:title" content="Evading Defenses to Transferable Adversarial Examples by Mitigating..." />
      <meta name="og:description" content="Deep neural networks are vulnerable to adversarial examples, which can mislead classifiers by adding imperceptible perturbations. An intriguing property of adversarial examples is their good..." />
      <meta name="og:image" content="https://openreview.net/static/images/openreview_logo_512.png" />

  <link rel="stylesheet" href="https://fonts.googleapis.com/css?family=Noto+Sans:400,400i,700,700i" />

  <link rel="stylesheet" href="/static/css/bootstrap.min.css" />
  <link rel="stylesheet" href="/static/css/jquery-ui.min.css" />
  <link rel="stylesheet" href="/static/css/main.min.css" />
</head>

<body class="forum">
  <nav class="navbar navbar-inverse navbar-fixed-top" role="navigation">
    <div class="container">
  
      <div class="navbar-header">
        <button type="button" class="navbar-toggle collapsed" data-toggle="collapse" data-target="#navbar" aria-expanded="false" aria-controls="navbar">
          <span class="sr-only">Toggle navigation</span>
          <span class="icon-bar"></span>
          <span class="icon-bar"></span>
          <span class="icon-bar"></span>
        </button>
        <a class="navbar-brand home push-link" href="/"><strong>OpenReview</strong>.net</a>
      </div>
  
      <div id="navbar" class="navbar-collapse collapse">
        <form class="navbar-form navbar-left profile-search" role="search">
          <div class="form-group has-feedback">
            <input id="search_input" type="text" class="form-control ui-autocomplete-input" placeholder="Search ICLR 2019 Conference" autocomplete="off" />
            <span class="glyphicon glyphicon-search form-control-feedback" aria-hidden="true"></span>
          </div>
  
          <input id="search_group" type="hidden" value="ICLR.cc/2019/Conference" />
          <input id="search_content" type="hidden" value="all" />
        <ul id="ui-id-1" tabindex="0" class="ui-menu ui-widget ui-widget-content ui-autocomplete ui-front" style="display: none;"></ul></form>
  
        <ul class="nav navbar-nav navbar-right">
        
            <li id="user-menu"><a href="/login">Login</a></li>
        </ul>
      </div>
  
    </div>
  </nav>

  <div id="or-banner" class="banner" style="">
  <div class="container">
    <div class="row">
      <div class="col-xs-12"><a href="/group?id=ICLR.cc/2019/Conference" title="Venue Homepage"><img class="icon" src="/static/images/arrow_left.svg" /> Go to <strong>ICLR 2019 Conference</strong> homepage</a></div>
    </div>
  </div>
</div>
<div id="flash-message-container" class="alert alert-danger" role="alert" style="display: none;">
  <div class="container">
    <div class="row">
      <div class="col-xs-12">
        <div class="alert-content">
          <button type="button" class="close" aria-label="Close"><span aria-hidden="true">×</span></button>
        </div>
      </div>
    </div>
  </div>
</div>

<div class="container">
  <div class="row">
    <div class="col-xs-12">
      <main id="content" class="clearfix openbanner-visible legacy-styles"><div id="note_BJzVUj0qtQ" class="note panel"><div class="title_pdf_row clearfix"><h2 class="note_content_title"><a>Evading Defenses to Transferable Adversarial Examples by Mitigating Attention Shift</a> <a class="note_content_pdf" href="/pdf?id=BJzVUj0qtQ" title="Download PDF" target="_blank"><img src="/static/images/pdf_icon_blue.svg" /></a> </h2></div><div class="meta_row"><span class="signatures">Anonymous</span></div><div class="clearfix"><div class="meta_row pull-left"><span class="date item">28 Sep 2018 (modified: 11 Oct 2018)</span><span class="item">ICLR 2019 Conference Blind Submission</span><span class="item">Readers: <span class="readers-icon glyphicon glyphicon-globe"></span> Everyone</span><span class="item"><a class="action-bibtex-modal" data-bibtex="@inproceedings{    &#10;anonymous2019evading,    &#10;title={Evading Defenses to Transferable Adversarial Examples by Mitigating Attention Shift},    &#10;author={Anonymous},    &#10;booktitle={Submitted to International Conference on Learning Representations},    &#10;year={2019},    &#10;url={https://openreview.net/forum?id=BJzVUj0qtQ},    &#10;note={under review}    &#10;}">Show Bibtex</a></span></div><div class="meta_row meta_actions"></div></div><div class="note_contents"><span class="note_content_field">Abstract: </span><span class="note_content_value">Deep neural networks are vulnerable to adversarial examples, which can mislead classifiers by adding imperceptible perturbations. An intriguing property of adversarial examples is their good transferability, making black-box attacks feasible in real-world applications. Due to the threat of adversarial attacks, many methods have been proposed to improve the robustness, and several state-of-the-art defenses are shown to be robust against transferable adversarial examples. In this paper, we identify the attention shift phenomenon, which may hinder the transferability of adversarial examples to the defense models. It indicates that the defenses rely on different discriminative regions to make predictions compared with normally trained models. Therefore, we propose an attention-invariant attack method to generate more transferable adversarial examples. Extensive experiments on the ImageNet dataset validate the effectiveness of the proposed method. Our best attack fools eight state-of-the-art defenses at an 82% success rate on average based only on the transferability, demonstrating the insecurity of the defense techniques. </span></div><div class="note_contents"><span class="note_content_field">Keywords: </span><span class="note_content_value">adversarial examples, black-box attack, transferability</span></div><div class="note_contents"><span class="note_content_field">TL;DR: </span><span class="note_content_value">We propose an attention-invariant attack method to generate more transferable adversarial examples for black-box attacks, which can fool state-of-the-art defenses with a high success rate.</span></div><div class="reply_row clearfix"><div class="item" id="reply_count">8 Replies</div></div></div><hr class="small" /><div id="note_children"><div class="note_with_children"><div id="note_BkerUwDZAX" class="note panel"><div class="title_pdf_row clearfix"><h2 class="note_content_title"><a>Submission Withdrawn by the Authors</a>  <button class="btn btn-xs btn-default permalink-button" title="Link to this comment" data-permalink-url="https://openreview.net/forum?id=BJzVUj0qtQ&amp;noteId=BkerUwDZAX"><span class="glyphicon glyphicon-link" aria-hidden="true"></span></button></h2></div><div class="meta_row"><span class="signatures">ICLR 2019 Conference Paper166 Authors</span></div><div class="clearfix"><div class="meta_row pull-left"><span class="date item">20 Nov 2018</span><span class="item">ICLR 2019 Conference Paper166 Withdraw Submission</span><span class="item">Readers: <span class="readers-icon glyphicon glyphicon-globe"></span> Everyone</span></div><div class="meta_row meta_actions"></div></div><div class="note_contents"><span class="note_content_field">Withdrawal confirmation: </span><span class="note_content_value">I have read and agree with the withdrawal statement on behalf of myself and my co-authors.</span></div><div class="reply_row clearfix"></div></div><div class="children"></div></div><div class="note_with_children"><div id="note_Byl7O1Y1TQ" class="note panel"><div class="title_pdf_row clearfix"><h2 class="note_content_title"><a>Review</a>  <button class="btn btn-xs btn-default permalink-button" title="Link to this comment" data-permalink-url="https://openreview.net/forum?id=BJzVUj0qtQ&amp;noteId=Byl7O1Y1TQ"><span class="glyphicon glyphicon-link" aria-hidden="true"></span></button></h2></div><div class="meta_row"><span class="signatures">ICLR 2019 Conference Paper166 AnonReviewer1</span></div><div class="clearfix"><div class="meta_row pull-left"><span class="date item">07 Nov 2018</span><span class="item">ICLR 2019 Conference Paper166 Official Review</span><span class="item">Readers: <span class="readers-icon glyphicon glyphicon-globe"></span> Everyone</span></div><div class="meta_row meta_actions"></div></div><div class="note_contents"><span class="note_content_field">Review: </span><span class="note_content_value">This paper suggests that "attention shift" is a key property behind failure of adversarial attacks to transfer. The authors propose an attention-invariant attack method by using a Gaussian filter on the gradient step before applying it. This method is shown to circumvent several defenses.

Comments: The method is interesting and evidently effective. However, the reason behind so-called "attention shift" being behind these defenses is hard to see---more likely, I suspect that the attack is exploiting a property similar to those used in [1]. Furthermore, the paper uses neural networks' shift invariance heavily in the proofs, which has shown to be not true [2,3]. The paper also has many typos and some broken references, which affect the readability of the work.

My main comment is that this work requires (a) more substantiation of the claim that attention shift is the phenomenon at play when it comes to lack of transferability, (b) improvement to the writing, and (c) more motivation behind the choice of mitigation mechanism.

Smaller comments:
- page 2 paragraph 3 line 3 “composed of an legitimate”: a instead of an
- page 3 paragraph 1 line 3 “another line of works that perform attacks” work instead of works and performs instead of perform
- page 3 paragraph 1 line 6 missing reference
- page 3 paragraph 2 line 3,4 missing reference
- page 4 paragraph 2 in section 3.2, and last sentence of pg 4: meaning is very unclear, needs rewriting
- page 7 paragraph 3 in section 4.3 line 2 “It should by noted“: be rather than by

[1] <a href="https://arxiv.org/pdf/1707.07397.pdf" target="_blank" rel="nofollow">https://arxiv.org/pdf/1707.07397.pdf</a>
[2] https://arxiv.org/pdf/1711.09115.pdf
[3] https://arxiv.org/pdf/1712.02779.pdf</span></div><div class="note_contents"><span class="note_content_field">Rating: </span><span class="note_content_value">4: Ok but not good enough - rejection</span></div><div class="note_contents"><span class="note_content_field">Confidence: </span><span class="note_content_value">4: The reviewer is confident but not absolutely certain that the evaluation is correct</span></div><div class="reply_row clearfix"></div></div><div class="children"><div class="note_with_children"><div id="note_HJxUqUvW0Q" class="note panel"><div class="title_pdf_row clearfix"><h2 class="note_content_title"><a>Response to Reviewer 1</a>  <button class="btn btn-xs btn-default permalink-button" title="Link to this comment" data-permalink-url="https://openreview.net/forum?id=BJzVUj0qtQ&amp;noteId=HJxUqUvW0Q"><span class="glyphicon glyphicon-link" aria-hidden="true"></span></button></h2></div><div class="meta_row"><span class="signatures">ICLR 2019 Conference Paper166 Authors</span></div><div class="clearfix"><div class="meta_row pull-left"><span class="date item">20 Nov 2018</span><span class="item">ICLR 2019 Conference Paper166 Official Comment</span><span class="item">Readers: <span class="readers-icon glyphicon glyphicon-globe"></span> Everyone</span></div><div class="meta_row meta_actions"></div></div><div class="note_contents"><span class="note_content_field">Comment: </span><span class="note_content_value">We thank the reviewer for the constructive comments. We provide the feedbacks below:

- The attention-shift phenomenon.
We showed that the defense models have the attention-shift phenomenon, that the defenses use different discriminative regions for predictions compared with normally trained models. A similar phenomenon is also found in [1], that the gradients of the defense models in the input space are different from those of normally trained models. As previous attack methods (e.g., FGSM, MI-FGSM) generate an adversarial example for a single input image, the adversarial example may be highly sensitive to the discriminative region/gradient of the white-box model, and can be hardly transferred to the defense models. 
We agree with the reviewer that our method is similar to [2]. We also generated an adversarial example for an ensemble of images, to make the generated adversarial example less sensitive to the discriminative region/gradient of the white-box model. But our method differs from [2] in the following three aspects: 1) we aimed to evade the defenses, while they generated robust adversarial examples in the physical world; 2) we only used the shift operation, while they used a lot of transformations; 3) we developed an efficient algorithm for gradient calculation, while they performed SGD by sampling a batch of transformed images in each iteration.
We’ll make our motivations and contributions clearer in the next version.

- Shift-invariance of CNNs
Thanks for your comment. We further did an experiment that we examine the shift-invariance of CNNs with very small shifts (less than 10 pixels in each dimension). We empirically found that CNNs are generally robust to such small shifts.

- Typos
Thanks for pointing out. We’ll further improve our writing.

Considering the concerns raised by the reviewers, we decide to withdraw the submission to further improve it.

[1] D. Tsipras, S. Santurkar, L. Engstrom, A. Turner, and A. Madry. Robustness may be at odds with accuracy. arXiv preprint arXiv:1805.12152, 2018.
[2] A. Athalye, L. Engstrom, A. Ilyas, and K. Kwok. Synthesizing robust adversarial examples. In ICML, 2018.
</span></div><div class="reply_row clearfix"></div></div><div class="children"></div></div><div class="note_with_children"><div id="note_SkeJbVP-Am" class="note panel trashed"><div class="title_pdf_row clearfix"><h2 class="note_content_title">  <button class="btn btn-xs btn-default permalink-button" title="Link to this comment" data-permalink-url="https://openreview.net/forum?id=BJzVUj0qtQ&amp;noteId=SkeJbVP-Am"><span class="glyphicon glyphicon-link" aria-hidden="true"></span></button></h2></div><div class="meta_row"><span class="signatures">[Deleted]</span></div><div class="clearfix"><div class="meta_row pull-left"><span class="item">ICLR 2019 Conference Paper166 Public Comment</span><span class="item">Readers: <span class="readers-icon glyphicon glyphicon-globe"></span> Everyone</span></div><div class="meta_row meta_actions"></div></div><div class="reply_row clearfix"></div></div><div class="children"></div></div></div></div><div class="note_with_children"><div id="note_rylnQeaYh7" class="note panel"><div class="title_pdf_row clearfix"><h2 class="note_content_title"><a>The motivation of the method is unclear</a>  <button class="btn btn-xs btn-default permalink-button" title="Link to this comment" data-permalink-url="https://openreview.net/forum?id=BJzVUj0qtQ&amp;noteId=rylnQeaYh7"><span class="glyphicon glyphicon-link" aria-hidden="true"></span></button></h2></div><div class="meta_row"><span class="signatures">ICLR 2019 Conference Paper166 AnonReviewer3</span></div><div class="clearfix"><div class="meta_row pull-left"><span class="date item">02 Nov 2018 (modified: 07 Nov 2018)</span><span class="item">ICLR 2019 Conference Paper166 Official Review</span><span class="item">Readers: <span class="readers-icon glyphicon glyphicon-globe"></span> Everyone</span></div><div class="meta_row meta_actions"></div></div><div class="note_contents"><span class="note_content_field">Review: </span><span class="note_content_value">This paper argues the expansion of the attentive regions (attention shift) as a mechanism of several defense models, then proposes an attention-invariant attack method by convolving attack gradients with the Gaussian filter. The technique demonstrates its effectiveness against various defense models.

The main concerning to me is that the motivation of using shifting operation to solve the attention "shift" is entirely unclear.  As shown in Figure 1, the expansion of the attentive regions seem to be fairly large, so it is difficult to say if shifting the image by several pixels (or convolving the gradients by a 15x15 kernel) can mitigate the effects of attention "shift". The true mechanism of this attack could be very different in my opinion.

Secondly, as shown in Figure 2, the patterns of adversarial perturbations of the proposed method seem to be reasonably easier to be detected, or more perceptible to humans (at least to me). I wonder if the authors could training a detection network to distinguish if an image is attacked by the proposed method and report the results.</span></div><div class="note_contents"><span class="note_content_field">Rating: </span><span class="note_content_value">4: Ok but not good enough - rejection</span></div><div class="note_contents"><span class="note_content_field">Confidence: </span><span class="note_content_value">3: The reviewer is fairly confident that the evaluation is correct</span></div><div class="reply_row clearfix"></div></div><div class="children"><div class="note_with_children"><div id="note_rkenyDwZR7" class="note panel"><div class="title_pdf_row clearfix"><h2 class="note_content_title"><a>Response to Reviewer 3</a>  <button class="btn btn-xs btn-default permalink-button" title="Link to this comment" data-permalink-url="https://openreview.net/forum?id=BJzVUj0qtQ&amp;noteId=rkenyDwZR7"><span class="glyphicon glyphicon-link" aria-hidden="true"></span></button></h2></div><div class="meta_row"><span class="signatures">ICLR 2019 Conference Paper166 Authors</span></div><div class="clearfix"><div class="meta_row pull-left"><span class="date item">20 Nov 2018</span><span class="item">ICLR 2019 Conference Paper166 Official Comment</span><span class="item">Readers: <span class="readers-icon glyphicon glyphicon-globe"></span> Everyone</span></div><div class="meta_row meta_actions"></div></div><div class="note_contents"><span class="note_content_field">Comment: </span><span class="note_content_value">We thank the reviewer for the constructive comments. We provide the feedbacks below:

- The motivation of using the shift operation.
Since previous attack methods (e.g., FGSM, MI-FGSM) generate an adversarial example for a single input image, the adversarial example may be highly sensitive to the discriminative region/gradient of the white-box model, and can be hardly transferred to the defense models. So we proposed to generate an adversarial example for an ensemble of shifted images. In this way, the generated adversarial examples are less sensitive to the discriminative region/gradient of the white-box model. We used the shift operation rather than others (e.g., rotation, scaling) because we developed an efficient algorithm for calculating the gradients. We’ll make our motivations clearer in the next version.

- The adversarial examples can be detected.

We agree that the adversarial examples are more visually perceptible to humans. However, the L-infinity norm of the perturbations is still 16. Since our method convolves the gradients with a Gaussian kernel, the adversarial noises are smoother, which makes them perceptible. We’ll further conduct an experiment to examine whether they can be detected by detection networks.

Considering the concerns raised by the reviewers, we decide to withdraw the submission to further improve it.

[1] A. Athalye, L. Engstrom, A. Ilyas, and K. Kwok. Synthesizing robust adversarial examples. In ICML, 2018.
</span></div><div class="reply_row clearfix"></div></div><div class="children"></div></div><div class="note_with_children"><div id="note_BJg57oukpm" class="note panel"><div class="title_pdf_row clearfix"><h2 class="note_content_title"><a>On the shift</a>  <button class="btn btn-xs btn-default permalink-button" title="Link to this comment" data-permalink-url="https://openreview.net/forum?id=BJzVUj0qtQ&amp;noteId=BJg57oukpm"><span class="glyphicon glyphicon-link" aria-hidden="true"></span></button></h2></div><div class="meta_row"><span class="signatures">ICLR 2019 Conference Paper166 AnonReviewer2</span></div><div class="clearfix"><div class="meta_row pull-left"><span class="date item">07 Nov 2018</span><span class="item">ICLR 2019 Conference Paper166 Official Comment</span><span class="item">Readers: <span class="readers-icon glyphicon glyphicon-globe"></span> Everyone</span></div><div class="meta_row meta_actions"></div></div><div class="note_contents"><span class="note_content_field">Comment: </span><span class="note_content_value">I do agree that while the attack presented works, there's actually no evidence that it' s due to the attention "shift" (the maths presented being are only a sufficient condition). If there's no empirical (or maths) evidences (stronger than Fig. 1) identifying the "shift" mechanism as origin of the attack, I might reconsider my review to 4. as it would require rewriting some parts of the paper.</span></div><div class="reply_row clearfix"></div></div><div class="children"></div></div></div></div><div class="note_with_children"><div id="note_B1ehXYhwnX" class="note panel"><div class="title_pdf_row clearfix"><h2 class="note_content_title"><a>Simple and elegant CNN adversarial attack. Some typos</a>  <button class="btn btn-xs btn-default permalink-button" title="Link to this comment" data-permalink-url="https://openreview.net/forum?id=BJzVUj0qtQ&amp;noteId=B1ehXYhwnX"><span class="glyphicon glyphicon-link" aria-hidden="true"></span></button></h2></div><div class="meta_row"><span class="signatures">ICLR 2019 Conference Paper166 AnonReviewer2</span></div><div class="clearfix"><div class="meta_row pull-left"><span class="date item">01 Nov 2018 (modified: 07 Nov 2018)</span><span class="item">ICLR 2019 Conference Paper166 Official Review</span><span class="item">Readers: <span class="readers-icon glyphicon glyphicon-globe"></span> Everyone</span></div><div class="meta_row meta_actions"></div></div><div class="note_contents"><span class="note_content_field">Review: </span><span class="note_content_value">*Summary:* The present paper propose a new way of overcoming state of the art defences against adversarial attacks on CNN. This is done efficiently by noticing that many defences agains adversarial attacks have a an attention shift, and exploiting it efficiently via CNN invariance to translations. The authors shows that the proposed method can fool many state of the art defences.

*Clarity:* The paper is well written and easy to follow. To the best of my knowledge the maths are correct.

*Originality and Significance:*  to the best of my knowledge the proposed method is new and the paper looks complete enough to be fully reproducible. The experiments show that on a wide selection of dataset, many state of the art method are significantly sensitive to the proposed attack. If the paper is accepted ot would be really nice to provide a link to the code used. As the experiments showed that the proposed attacks fools many state of the art defences  and the method is new I believe this paper has a good significance/impact.

*Questions:*
   - Have you tried to fool the defences with other invariants that CNN are also known to be invariant too? eg rotations or diffeomorphism? Would it be possible to obtain the same computational 'trick' as in eq 9?
   - I would see the continuous version of the method as integrating over all the possible translation T_{uv} over a measure mu(u, v) (instead of T_{ij}, w_{ij}) is that correct?
   - Have you tried something else than the gaussian kernel for the convolution? Do you have a proof / explaination / experimental evidence that other kernels such as a uniform / laplacian are less performant than a Gaussian kernel? (point 1) page 5 is convincing but point 2) is irrelevant in my opinion).
   - Could you propose new defences agains your attack?

Typos:
some references (eg p 3) are broken (ie ??)
</span></div><div class="note_contents"><span class="note_content_field">Rating: </span><span class="note_content_value">7: Good paper, accept</span></div><div class="note_contents"><span class="note_content_field">Confidence: </span><span class="note_content_value">3: The reviewer is fairly confident that the evaluation is correct</span></div><div class="reply_row clearfix"></div></div><div class="children"><div class="note_with_children"><div id="note_HyxMGPP-Am" class="note panel"><div class="title_pdf_row clearfix"><h2 class="note_content_title"><a>Response to Reviewer 2</a>  <button class="btn btn-xs btn-default permalink-button" title="Link to this comment" data-permalink-url="https://openreview.net/forum?id=BJzVUj0qtQ&amp;noteId=HyxMGPP-Am"><span class="glyphicon glyphicon-link" aria-hidden="true"></span></button></h2></div><div class="meta_row"><span class="signatures">ICLR 2019 Conference Paper166 Authors</span></div><div class="clearfix"><div class="meta_row pull-left"><span class="date item">20 Nov 2018</span><span class="item">ICLR 2019 Conference Paper166 Official Comment</span><span class="item">Readers: <span class="readers-icon glyphicon glyphicon-globe"></span> Everyone</span></div><div class="meta_row meta_actions"></div></div><div class="note_contents"><span class="note_content_field">Comment: </span><span class="note_content_value">We thank the reviewer for the constructive comments. We provide the feedbacks below:

- Other transformations.
We found that for other transformations (e.g., rotation, scaling), it’s hard to develop an efficient algorithm as Eq.(9). That’s why we choose the shift operation in this paper.

- Other kernels.
Thanks for your comment. We tried several other kernels (e.g., uniform kernel, linear kernel). We also obtained good results.

- Defenses against our attacks.
We actually performed adversarial training with the adversarial examples generated by our method. We found that this defense is robust against our attacks. However, we did not find a robust model against our attacks if it has no knowledge of our method. So one of the contributions of our paper is that we provide a new attack method for adversarial training to build more robust models.

Considering the concerns raised by the reviewers, we decide to withdraw the submission to further improve it.
</span></div><div class="reply_row clearfix"></div></div><div class="children"></div></div></div></div></div></main></div>
  </div>
</div>


    <!-- Footer -->
    <footer class="sitemap">
      <div class="container">
        <div class="row hidden-xs">
          <div class="col-sm-4">
            <ul class="list-unstyled">
              <li><a href="/" class="home">Home</a></li>
              <li><a href="/venues">All Venues</a></li>
            </ul>
          </div>
    
          <div class="col-sm-4">
            <ul class="list-unstyled">
              <li><a href="/about">About OpenReview</a></li>
              <li><a href="/contact">Contact</a></li>
              <li><a href="#" data-toggle="modal" data-target="#feedback-modal">Feedback</a></li>
            </ul>
          </div>
    
          <div class="col-sm-4">
            <ul class="list-unstyled">
              <li><a href="/terms">Terms of Service</a></li>
              <li><a href="/privacy">Privacy Policy</a></li>
            </ul>
          </div>
        </div>
    
        <div class="row visible-xs-block">
          <div class="col-xs-6">
            <ul class="list-unstyled">
              <li><a href="/" class="home">Home</a></li>
              <li><a href="/about">About OpenReview</a></li>
              <li><a href="#" data-toggle="modal" data-target="#feedback-modal">Feedback</a></li>
            </ul>
          </div>
    
          <div class="col-xs-6">
            <ul class="list-unstyled">
              <li><a href="/contact">Contact</a></li>
              <li><a href="/terms">Terms of Service</a></li>
              <li><a href="/privacy">Privacy Policy</a></li>
            </ul>
          </div>
        </div>
      </div>
    </footer>
    
    <footer class="sponsor">
      <div class="container">
        <div class="row">
          <div class="col-sm-10 col-sm-offset-1">
            <p class="text-center">
              OpenReview is created by the <a href="http://www.iesl.cs.umass.edu/" target="_blank">Information Extraction and Synthesis Laboratory</a>, College of Information and Computer Science, University of Massachusetts Amherst. We gratefully acknowledge the support of the OpenReview sponsors:  Google,  Facebook, NSF, the University of Massachusetts Amherst Center for Data Science, and Center for Intelligent Information Retrieval, as well as the Google Cloud Platform for donating the computing and networking services on which OpenReview.net runs.
            </p>
          </div>
        </div>
      </div>
    </footer>

  <div id="feedback-modal" class="modal fade" tabindex="-1" role="dialog">
    <div class="modal-dialog">
      <div class="modal-content">
        <div class="modal-header">
          <button type="button" class="close" data-dismiss="modal" aria-label="Close"><span aria-hidden="true">×</span></button>
          <h3 class="modal-title">Send Feedback</h3>
        </div>
        <div class="modal-body">
          <p>Enter your feedback below and we'll get back to you as soon as possible.</p>
          <form action="/feedback" method="POST">
            <div class="form-group">
              <input type="email" name="from" class="form-control" placeholder="Email" />
            </div>
            <div class="form-group">
              <input type="text" name="subject" class="form-control" placeholder="Subject" />
            </div>
            <div class="form-group">
              <textarea name="message" class="form-control feedback-input" rows="5" placeholder="Message" required=""></textarea>
            </div>
          <ul id="ui-id-2" tabindex="0" class="ui-menu ui-widget ui-widget-content" style="display: none;"></ul></form>
        </div>
        <div class="modal-footer">
          <button type="button" class="btn btn-default" data-dismiss="modal">Cancel</button>
          <button type="button" class="btn btn-primary">Send</button>
        </div>
      </div>
    </div>
  </div>

  <script type="text/javascript" async="" src="https://www.google-analytics.com/analytics.js"></script><script src="//cdnjs.cloudflare.com/ajax/libs/jquery/2.2.4/jquery.min.js"></script>
  <script>window.jQuery || document.write('&lt;script src="/static/js/vendor/jquery-2.2.4.min.js"&gt;&lt;\/script&gt;')</script>

  <script src="/static/dist/vendor.min.js"></script>
  <script src="/static/dist/templates.min.js"></script>
  <script src="/static/dist/openreview.min.js"></script>

    <script>window.legacyScripts = true;</script>


    <script async="" src="https://www.googletagmanager.com/gtag/js?id=UA-108703919-1"></script>
    <script>
      window.dataLayer = window.dataLayer || [];
      function gtag() { dataLayer.push(arguments); }
      gtag('js', new Date());
      gtag('config', 'UA-108703919-1');
    </script>


<div role="status" aria-live="assertive" aria-relevant="additions" class="ui-helper-hidden-accessible"></div></body></html>