<!DOCTYPE html><html xmlns="http://www.w3.org/1999/xhtml" class="no-js"><head>
  <meta charset="utf-8" />
  <meta http-equiv="X-UA-Compatible" content="IE=edge,chrome=1" />
  <meta name="viewport" content="width=device-width, initial-scale=1" />

  <title>Modular Deep Probabilistic Programming | OpenReview</title>
  <meta name="description" content="" />

      <meta name="citation_title" content="Modular Deep Probabilistic Programming" />
        <meta name="citation_author" content="Anonymous" />
      <meta name="citation_publication_date" content="2018/09/27" />
      <meta name="citation_online_date" content="2018/09/27" />
      <meta name="citation_pdf_url" content="https://openreview.net/pdf?id=B1xnPsA5KX" />
      <meta name="twitter:card" content="summary" />
      <meta name="twitter:site" content="@openreviewnet" />
      <meta name="og:title" content="Modular Deep Probabilistic Programming" />
      <meta name="og:description" content="Modularity is a key feature of deep learning libraries but has not been fully exploited for probabilistic programming. We propose to improve modularity of probabilistic programming language by..." />
      <meta name="og:image" content="https://openreview.net/static/images/openreview_logo_512.png" />

  <link rel="stylesheet" href="https://fonts.googleapis.com/css?family=Noto+Sans:400,400i,700,700i" />

  <link rel="stylesheet" href="/static/css/bootstrap.min.css" />
  <link rel="stylesheet" href="/static/css/jquery-ui.min.css" />
  <link rel="stylesheet" href="/static/css/main.min.css" />
</head>

<body class="forum">
  <nav class="navbar navbar-inverse navbar-fixed-top" role="navigation">
    <div class="container">
  
      <div class="navbar-header">
        <button type="button" class="navbar-toggle collapsed" data-toggle="collapse" data-target="#navbar" aria-expanded="false" aria-controls="navbar">
          <span class="sr-only">Toggle navigation</span>
          <span class="icon-bar"></span>
          <span class="icon-bar"></span>
          <span class="icon-bar"></span>
        </button>
        <a class="navbar-brand home push-link" href="/"><strong>OpenReview</strong>.net</a>
      </div>
  
      <div id="navbar" class="navbar-collapse collapse">
        <form class="navbar-form navbar-left profile-search" role="search">
          <div class="form-group has-feedback">
            <input id="search_input" type="text" class="form-control ui-autocomplete-input" placeholder="Search ICLR 2019 Conference" autocomplete="off" />
            <span class="glyphicon glyphicon-search form-control-feedback" aria-hidden="true"></span>
          </div>
  
          <input id="search_group" type="hidden" value="ICLR.cc/2019/Conference" />
          <input id="search_content" type="hidden" value="all" />
        <ul id="ui-id-1" tabindex="0" class="ui-menu ui-widget ui-widget-content ui-autocomplete ui-front" style="display: none;"></ul></form>
  
        <ul class="nav navbar-nav navbar-right">
        
            <li id="user-menu"><a href="/login">Login</a></li>
        </ul>
      </div>
  
    </div>
  </nav>

  <div id="or-banner" class="banner" style="">
  <div class="container">
    <div class="row">
      <div class="col-xs-12"><a href="/group?id=ICLR.cc/2019/Conference" title="Venue Homepage"><img class="icon" src="/static/images/arrow_left.svg" /> Go to <strong>ICLR 2019 Conference</strong> homepage</a></div>
    </div>
  </div>
</div>
<div id="flash-message-container" class="alert alert-danger" role="alert" style="display: none;">
  <div class="container">
    <div class="row">
      <div class="col-xs-12">
        <div class="alert-content">
          <button type="button" class="close" aria-label="Close"><span aria-hidden="true">×</span></button>
        </div>
      </div>
    </div>
  </div>
</div>

<div class="container">
  <div class="row">
    <div class="col-xs-12">
      <main id="content" class="clearfix openbanner-visible legacy-styles"><div id="note_B1xnPsA5KX" class="note panel"><div class="title_pdf_row clearfix"><h2 class="note_content_title"><a>Modular Deep Probabilistic Programming</a> <a class="note_content_pdf" href="/pdf?id=B1xnPsA5KX" title="Download PDF" target="_blank"><img src="/static/images/pdf_icon_blue.svg" /></a> </h2></div><div class="meta_row"><span class="signatures">Anonymous</span></div><div class="clearfix"><div class="meta_row pull-left"><span class="date item">28 Sep 2018 (modified: 11 Oct 2018)</span><span class="item">ICLR 2019 Conference Blind Submission</span><span class="item">Readers: <span class="readers-icon glyphicon glyphicon-globe"></span> Everyone</span><span class="item"><a class="action-bibtex-modal" data-bibtex="@inproceedings{    &#10;anonymous2019modular,    &#10;title={Modular Deep Probabilistic Programming},    &#10;author={Anonymous},    &#10;booktitle={Submitted to International Conference on Learning Representations},    &#10;year={2019},    &#10;url={https://openreview.net/forum?id=B1xnPsA5KX},    &#10;note={under review}    &#10;}">Show Bibtex</a></span></div><div class="meta_row meta_actions"></div></div><div class="note_contents"><span class="note_content_field">Abstract: </span><span class="note_content_value">Modularity is a key feature of deep learning libraries but has not been fully exploited for probabilistic programming. We propose to improve modularity of probabilistic programming language by offering not only plain probabilistic distributions but also sophisticated probabilistic model such as Bayesian non-parametric models as fundamental building blocks. We demonstrate this idea by presenting a modular probabilistic programming language MXFusion, which includes a new type of re-usable building blocks, called probabilistic modules. A probabilistic module consists of a set of random variables with associated probabilistic distributions and dedicated inference methods. Under the framework of variational inference, the pre-specified inference methods of individual probabilistic modules can be transparently used for inference of the whole probabilistic model. We demonstrate the power and convenience of probabilistic modules in MXFusion with various examples of Gaussian process models, which are evaluated with experiments on real data.</span></div><div class="reply_row clearfix"><div class="item" id="reply_count">4 Replies</div></div></div><hr class="small" /><div id="note_children"><div class="note_with_children"><div id="note_BJlhDyz5nX" class="note panel"><div class="title_pdf_row clearfix"><h2 class="note_content_title"><a>The paper proposes a new probabilistic programming language, but has a lack of scientific novelty</a>  <button class="btn btn-xs btn-default permalink-button" title="Link to this comment" data-permalink-url="https://openreview.net/forum?id=B1xnPsA5KX&amp;noteId=BJlhDyz5nX"><span class="glyphicon glyphicon-link" aria-hidden="true"></span></button></h2></div><div class="meta_row"><span class="signatures">ICLR 2019 Conference Paper299 AnonReviewer1</span></div><div class="clearfix"><div class="meta_row pull-left"><span class="date item">03 Nov 2018 (modified: 07 Nov 2018)</span><span class="item">ICLR 2019 Conference Paper299 Official Review</span><span class="item">Readers: <span class="readers-icon glyphicon glyphicon-globe"></span> Everyone</span></div><div class="meta_row meta_actions"></div></div><div class="note_contents"><span class="note_content_field">Review: </span><span class="note_content_value">In this paper authors present a new Probabilistic Programming Language (PPL) MXFusion. Similarly to the languages for the deep learning (TensorFlow, PyTorch, etc.), this language introduce probabilistic modules that are used as building blocks for complex probabilistic models. Introducing modularity to the probabilistic programming, raises the problem of inference for probabilistic models. Since, we cannot obtain the exact solution on practice we have to resort to approximate inference methods. The approximate inference methods can be either generic, thus, being suitable for many probabilistic models but resulting in poor approximation, or specific, thus, having good approximation quality, but only for specific probabilistic models. Authors propose to address this problem by encapsulating specific inference methods in corresponding probabilistic modules. Doing so, one can perform approximate inference for every module with the best suitable inference technique. Authors demonstrate interface of MXFusion for three well known probabilistic models: Bayesian linear regression, deep kernel learning, Bayesian Gaussian process latent variable model.

Approaching the problem of building complex probabilistic models by introducing modular PPL is an important direction of study. But, regarding this paper I have the following concerns.
- In my opinion, the structure of the paper can be greatly improved. From general words about modularity and approximate inference authors dive to the very specific cases of probabilistic models. Following such structure, authors don’t give a clear answer to the following questions. Why the paradigm of encapsulating inference methods in probabilistic modules is legitimate for constructing complex probabilistic models? What inference methods and probabilistic models can we use as building blocks? Do we need to be aware of specific inference methods that are encapsulated or we can use any blocks in any order as we do in deep learning frameworks?
- Novelty of that paper is the new design of PPL. That is an interesting and important question for the community, but maybe ICLR paper is not the best format to present such kind of novelty. 
- From the specific examples in the paper, legitimacy of such modular structure is clear only for variational inference (that seems to be a common knowledge) and variational approximation of gaussian processes. But the application area of MXFusion remains unclear. Verbatim examples of code for the specific examples doesn’t make the difference between MXFusion and other PPLs clear, because it can be treated as encapsulation of the code into some classes, that can be implemented in other languages as well.
- Comparison with other frameworks can be improved. In experimental section authors provide comparison with GPy framework in terms of RMSE and log-likelihood for gaussian process with 50 inducing points. As I understood both frameworks use the same inference methods and achieve the same performance, so the experiment can be considered as sanity check for MXFusion. The paper could benefit from comparison between different inference methods and providing benchmarks for inference time.

Overall, the paper proposes a new PPL that is an important direction of study, but have several drawbacks and conference paper format is not the best way to present such kind of novelty.

Typos:
- Page 1, “despite the different of DNNs…” -&gt; “despite the difference of DNNs…”?
- Page 2, missing reference of the section
- Page 2, section 3, “... sightly different form.” -&gt; “... slightly different form”?</span></div><div class="note_contents"><span class="note_content_field">Rating: </span><span class="note_content_value">3: Clear rejection</span></div><div class="note_contents"><span class="note_content_field">Confidence: </span><span class="note_content_value">3: The reviewer is fairly confident that the evaluation is correct</span></div><div class="reply_row clearfix"></div></div><div class="children"></div></div><div class="note_with_children"><div id="note_HklYg8l93X" class="note panel"><div class="title_pdf_row clearfix"><h2 class="note_content_title"><a>The main idea is the introduction of a new building block-probabilistic modules-into probabilistic programming with the aspiration to improve the modularity of the language.</a>  <button class="btn btn-xs btn-default permalink-button" title="Link to this comment" data-permalink-url="https://openreview.net/forum?id=B1xnPsA5KX&amp;noteId=HklYg8l93X"><span class="glyphicon glyphicon-link" aria-hidden="true"></span></button></h2></div><div class="meta_row"><span class="signatures">ICLR 2019 Conference Paper299 AnonReviewer3</span></div><div class="clearfix"><div class="meta_row pull-left"><span class="date item">03 Nov 2018 (modified: 07 Nov 2018)</span><span class="item">ICLR 2019 Conference Paper299 Official Review</span><span class="item">Readers: <span class="readers-icon glyphicon glyphicon-globe"></span> Everyone</span></div><div class="meta_row meta_actions"></div></div><div class="note_contents"><span class="note_content_field">Review: </span><span class="note_content_value">The paper works with the modularization of PPLs with natural inspiration for the successful modularization recently introduced in all deep learning softwares. 

* Within your so-called probabilistic modules you package dedicated inference methods that are tailored for this particular class of problems and argues that this will perform better than using a general purpose solver. For each specific case this does of course make a lot of sense. However, when it comes to the relevant case (especially within probabilistic programming) when we have a (often complex) combination of several probabilistic modules, how do you then leverage the tailored solvers? What is it that guarantees that these are relevant in the new combined construction? 

* Related to the above you write in your conclusion that "Once an inference algorithm is chosen, it remains the same across a probabilistic model. However, given a specific probabilistic model, e.g., a conjugate model, a specialized inference algorithm that exploits the mathematical properties of that particular model will always produce inference results that are as good or better than the generic inference in terms of both accuracy and efficiency." This is of course true and it is also part of some existing PPLs, for example Birch via their so-called "delayed sampling": 
<a href="http://proceedings.mlr.press/v84/murray18a/murray18a.pdf" target="_blank" rel="nofollow">http://proceedings.mlr.press/v84/murray18a/murray18a.pdf</a>
The implementation there is very different from what you propose. As far as I can understand you require hard-coding of each specific model, whereas in the paper mentioned above they seem to automate att conjugate gradient calculations to a much greater extent. Why is it better to insist on hard-coding this for each probabilistic module? and how can you guarantee smooth functioning when several probabilistic modules are combined in complex ways?

* In the inference method that you briefly sketch in Section 3 you make use of VI and the intractable integrals that results are then handled using Monte Carlo. What is the gain of using VI + Monte Carlo compared to direct use of Monte Carlo? Via direct use of some kind of Monte Carlo method you would be able to guarantee performance and do proper analysis, whereas with VI you loose that capability. However, VI does of course have other pros, but my question arises due to the fact that you end up using Monte Carlo anyway.

* You write that "In PPLs, a probabilistic model is often presented as a graph of random variables...". This is certainly true and the word "often" is very important in this sentence. At the same time, is not one of the key reasons for using PPLs compared to probabilistic graphical models that it offers a richer model class compared to probabilistic graphical models? While I perfectly respect you choice to specifying models in MXFusion using using probabilistic graphical models I do find this quite restrictive and it seems to miss some of the key possibilities with PPLs.

* In your BLR example (which is very instructive by the way) you compute the solution via MAP. This is also find rather puzzling since that removes another great feature of PPLs, namely to work with probability distributions throughout the entire inference stage. The user can then of course choose to extract whatever point estimate might be needed in the end. Why do you remove this possibility by insisting on a specific point estimate? or is this just a particular choice of this example and not a general design choice?


The paper contains a lot of issues related to the use of the English language and would benefit from proper proofreading.
</span></div><div class="note_contents"><span class="note_content_field">Rating: </span><span class="note_content_value">4: Ok but not good enough - rejection</span></div><div class="note_contents"><span class="note_content_field">Confidence: </span><span class="note_content_value">4: The reviewer is confident but not absolutely certain that the evaluation is correct</span></div><div class="reply_row clearfix"></div></div><div class="children"></div></div><div class="note_with_children"><div id="note_rkegcHEfsm" class="note panel"><div class="title_pdf_row clearfix"><h2 class="note_content_title"><a>A good work on modularisation of probabilistic programming languages</a>  <button class="btn btn-xs btn-default permalink-button" title="Link to this comment" data-permalink-url="https://openreview.net/forum?id=B1xnPsA5KX&amp;noteId=rkegcHEfsm"><span class="glyphicon glyphicon-link" aria-hidden="true"></span></button></h2></div><div class="meta_row"><span class="signatures">ICLR 2019 Conference Paper299 AnonReviewer2</span></div><div class="clearfix"><div class="meta_row pull-left"><span class="date item">15 Oct 2018 (modified: 16 Nov 2018)</span><span class="item">ICLR 2019 Conference Paper299 Official Review</span><span class="item">Readers: <span class="readers-icon glyphicon glyphicon-globe"></span> Everyone</span><a class="note_content_pdf item" href="/revisions?id=rkegcHEfsm" target="_blank">Show Revisions</a></div><div class="meta_row meta_actions"></div></div><div class="note_contents"><span class="note_content_field">Review: </span><span class="note_content_value">The paper presents an extension of the MXFusion language that allows the use of probabilistic modules. These modules are defined as a set of random variables and a specific probabilistic distribution. The modules also contain dedicated inference methods. Using these modules, one can use probabilistic distributions with inference methods tailored to the distribution, which are usually more efficient than generic inference systems.
The paper presents several examples using Gaussian process models, evaluated by comparison with GPy and the standard spare gaussian process method implemented in MXFusion.

Overall, the paper is well written and clear, and all claims are justified. The idea of modularization is not really new (as other systems implement something similar) but this approach tries to be general, in order not to pose constraints on the specification of modules. The related work section provides a good positioning of the approach.
I have not found any specific problems in the paper, the quality is rather high. However, the actual content of the paper describes an extension of an existing system. Such an extension is certainly important, but the paper does not provide much more information.
Moreover, the results of the experimental test do not seem to me to be able to support the main objective of the extension, which is to give the possibility to exploit more specific probabilistic model and inference methods to achieve better results than an approach using general methods.
As far as the execution of the system is concerned, is this extension able to improve the scalability or reduce the walltime? Is this visible in the presented test (at least in terms of speed up)? Or is the convenience of this approach the simpler way to define distributions?

As for minor issues that I can point out, one concerns the definition of shape in the Variable of m.sigma2 (figures 1, 2, 3). I do not know the used in MXFusion, thus this might not be an error, but it seems that in the shape definition something is missing. It is written that shape=(1,), is it correct or is there an error? In case of absence of error, what does the empty argument mean?

The power benchmark is not described.

In references, Thomas V. Wiecki is mentioned with and without the first letter of middle name. I suggest to uniform the references.

Typos:
- Abstract: "... but also sophisticated probabilistic model*s* such as ..."
- Sec. 1, first row of page 2: The sentence "this would bring the a lot of benefits ...". The "the" word should be deleted.
- Sec. 1 refers to a section after 4 which does not exist in the paper.
- Page 5: remove the full stop before the colon in the 4th row.
- Page 5: "The log_pdf method of the SGPR module compute*s* the above variational lower bound"
- Sec. 6: the sentence "MXFusion aims at closing the gap between having specialized, highly performant algorithms and generic, easily maintained generic algorithms by introducing probabilistic modules." should be corrected.


Pros
- The extension allows the use of modules that define specific probabilistic distribution/inference methods
- It seems easy to extend the system with other modules
- Its a really useful extension...

Cons
- The performance presented in the paper is not entirely convincing
- ... but it is just an extension of an existing system</span></div><div class="note_contents"><span class="note_content_field">Rating: </span><span class="note_content_value">5: Marginally below acceptance threshold</span></div><div class="note_contents"><span class="note_content_field">Confidence: </span><span class="note_content_value">3: The reviewer is fairly confident that the evaluation is correct</span></div><div class="reply_row clearfix"></div></div><div class="children"></div></div><div class="note_with_children"><div id="note_HJeHr0F0K7" class="note panel"><div class="title_pdf_row clearfix"><h2 class="note_content_title"><a>Somewhat important direction of research, but not entirely novel and lacking discussion</a>  <button class="btn btn-xs btn-default permalink-button" title="Link to this comment" data-permalink-url="https://openreview.net/forum?id=B1xnPsA5KX&amp;noteId=HJeHr0F0K7"><span class="glyphicon glyphicon-link" aria-hidden="true"></span></button></h2></div><div class="meta_row"><span class="signatures">(anonymous)</span></div><div class="clearfix"><div class="meta_row pull-left"><span class="date item">01 Oct 2018</span><span class="item">ICLR 2019 Conference Paper299 Public Comment</span><span class="item">Readers: <span class="readers-icon glyphicon glyphicon-globe"></span> Everyone</span></div><div class="meta_row meta_actions"></div></div><div class="note_contents"><span class="note_content_field">Comment: </span><span class="note_content_value">The paper proposes a probabilistic programming (PP) framework with modular building blocks for deep learning (DL) models. Although suggested as novel, this is not new in the PP community. Model modularity is a fundamental to essentially all PP systems, and modular inference has been presented in a number of works [1-4]. Moreover, several frameworks such as Edward [1] and Pyro [5] are built specifically for DL models.

The authors focus on variational inference tactics for PP, yet this approach to inference in PP is well-demonstrated in Edward and others. A lesser focus is on Gaussian process (GP) models, which is an important direction of research in PP. The presented analysis on real world problems is useful and interesting. Yet the GP parts of this paper are lacking:
- Details of how a GP fits within the probabilistic programming framework is missing
- Should be more discussion of results

A couple important points regarding the submission:
- Authors should cite recent works in PP. For example, [3, 4] propose PP languages designed for modular inference, and [4] presents a probabilistic DSL designed for GPs. See <a href="https://probprog.cc/" target="_blank" rel="nofollow">https://probprog.cc/</a> for most recent work in the field.
- By naming the framework "MXFusion" in the paper the submission is no longer anonymized (see https://github.com/amzn/MXFusion#contributing)

[1] Tran et al. Edward: A library for probabilistic modeling, inference, and criticism. 2016.
[2] Mansinghka et al. Venture: a higher-order probabilistic
programming platform with programmable inference. 2014.
[3] Ge et al. Turing: a language for flexible probabilistic inference. 2018.
[4] Lavin &amp; Mansinghka. Probabilistic programming for data-efficient robotics. 2018.
[5] Noah Goodman. Uber AI Labs open sources Pyro, a deep probabilistic programming language. 2017.</span></div><div class="reply_row clearfix"></div></div><div class="children"></div></div></div></main></div>
  </div>
</div>


    <!-- Footer -->
    <footer class="sitemap">
      <div class="container">
        <div class="row hidden-xs">
          <div class="col-sm-4">
            <ul class="list-unstyled">
              <li><a href="/" class="home">Home</a></li>
              <li><a href="/venues">All Venues</a></li>
            </ul>
          </div>
    
          <div class="col-sm-4">
            <ul class="list-unstyled">
              <li><a href="/about">About OpenReview</a></li>
              <li><a href="/contact">Contact</a></li>
              <li><a href="#" data-toggle="modal" data-target="#feedback-modal">Feedback</a></li>
            </ul>
          </div>
    
          <div class="col-sm-4">
            <ul class="list-unstyled">
              <li><a href="/terms">Terms of Service</a></li>
              <li><a href="/privacy">Privacy Policy</a></li>
            </ul>
          </div>
        </div>
    
        <div class="row visible-xs-block">
          <div class="col-xs-6">
            <ul class="list-unstyled">
              <li><a href="/" class="home">Home</a></li>
              <li><a href="/about">About OpenReview</a></li>
              <li><a href="#" data-toggle="modal" data-target="#feedback-modal">Feedback</a></li>
            </ul>
          </div>
    
          <div class="col-xs-6">
            <ul class="list-unstyled">
              <li><a href="/contact">Contact</a></li>
              <li><a href="/terms">Terms of Service</a></li>
              <li><a href="/privacy">Privacy Policy</a></li>
            </ul>
          </div>
        </div>
      </div>
    </footer>
    
    <footer class="sponsor">
      <div class="container">
        <div class="row">
          <div class="col-sm-10 col-sm-offset-1">
            <p class="text-center">
              OpenReview is created by the <a href="http://www.iesl.cs.umass.edu/" target="_blank">Information Extraction and Synthesis Laboratory</a>, College of Information and Computer Science, University of Massachusetts Amherst. We gratefully acknowledge the support of the OpenReview sponsors:  Google,  Facebook, NSF, the University of Massachusetts Amherst Center for Data Science, and Center for Intelligent Information Retrieval, as well as the Google Cloud Platform for donating the computing and networking services on which OpenReview.net runs.
            </p>
          </div>
        </div>
      </div>
    </footer>

  <div id="feedback-modal" class="modal fade" tabindex="-1" role="dialog">
    <div class="modal-dialog">
      <div class="modal-content">
        <div class="modal-header">
          <button type="button" class="close" data-dismiss="modal" aria-label="Close"><span aria-hidden="true">×</span></button>
          <h3 class="modal-title">Send Feedback</h3>
        </div>
        <div class="modal-body">
          <p>Enter your feedback below and we'll get back to you as soon as possible.</p>
          <form action="/feedback" method="POST">
            <div class="form-group">
              <input type="email" name="from" class="form-control" placeholder="Email" />
            </div>
            <div class="form-group">
              <input type="text" name="subject" class="form-control" placeholder="Subject" />
            </div>
            <div class="form-group">
              <textarea name="message" class="form-control feedback-input" rows="5" placeholder="Message" required=""></textarea>
            </div>
          <ul id="ui-id-2" tabindex="0" class="ui-menu ui-widget ui-widget-content" style="display: none;"></ul></form>
        </div>
        <div class="modal-footer">
          <button type="button" class="btn btn-default" data-dismiss="modal">Cancel</button>
          <button type="button" class="btn btn-primary">Send</button>
        </div>
      </div>
    </div>
  </div>

  <script type="text/javascript" async="" src="https://www.google-analytics.com/analytics.js"></script><script src="//cdnjs.cloudflare.com/ajax/libs/jquery/2.2.4/jquery.min.js"></script>
  <script>window.jQuery || document.write('&lt;script src="/static/js/vendor/jquery-2.2.4.min.js"&gt;&lt;\/script&gt;')</script>

  <script src="/static/dist/vendor.min.js"></script>
  <script src="/static/dist/templates.min.js"></script>
  <script src="/static/dist/openreview.min.js"></script>

    <script>window.legacyScripts = true;</script>


    <script async="" src="https://www.googletagmanager.com/gtag/js?id=UA-108703919-1"></script>
    <script>
      window.dataLayer = window.dataLayer || [];
      function gtag() { dataLayer.push(arguments); }
      gtag('js', new Date());
      gtag('config', 'UA-108703919-1');
    </script>


<div role="status" aria-live="assertive" aria-relevant="additions" class="ui-helper-hidden-accessible"></div></body></html>