<!DOCTYPE html><html xmlns="http://www.w3.org/1999/xhtml" class="no-js"><head>
  <meta charset="utf-8" />
  <meta http-equiv="X-UA-Compatible" content="IE=edge,chrome=1" />
  <meta name="viewport" content="width=device-width, initial-scale=1" />

  <title>Learning Spatio-Temporal Representations Using Spike-Based Backpropagation | OpenReview</title>
  <meta name="description" content="" />

      <meta name="citation_title" content="Learning Spatio-Temporal Representations Using Spike-Based Backpropagation" />
        <meta name="citation_author" content="Deboleena Roy" />
        <meta name="citation_author" content="Priyadarshini Panda" />
        <meta name="citation_author" content="Kaushik Roy" />
      <meta name="citation_publication_date" content="2018/09/27" />
      <meta name="citation_online_date" content="2018/09/27" />
      <meta name="citation_pdf_url" content="https://openreview.net/pdf?id=BJxfJnC9YX" />
      <meta name="twitter:card" content="summary" />
      <meta name="twitter:site" content="@openreviewnet" />
      <meta name="og:title" content="Learning Spatio-Temporal Representations Using Spike-Based..." />
      <meta name="og:description" content="Spiking neural networks (SNNs) offer a promising alternative to current artificial neural networks to enable low-power event-driven neuromorphic hardware. However, training SNNs remains a challenge..." />
      <meta name="og:image" content="https://openreview.net/static/images/openreview_logo_512.png" />

  <link rel="stylesheet" href="https://fonts.googleapis.com/css?family=Noto+Sans:400,400i,700,700i" />

  <link rel="stylesheet" href="/static/css/bootstrap.min.css" />
  <link rel="stylesheet" href="/static/css/jquery-ui.min.css" />
  <link rel="stylesheet" href="/static/css/main.min.css" />
</head>

<body class="forum">
  <nav class="navbar navbar-inverse navbar-fixed-top" role="navigation">
    <div class="container">
  
      <div class="navbar-header">
        <button type="button" class="navbar-toggle collapsed" data-toggle="collapse" data-target="#navbar" aria-expanded="false" aria-controls="navbar">
          <span class="sr-only">Toggle navigation</span>
          <span class="icon-bar"></span>
          <span class="icon-bar"></span>
          <span class="icon-bar"></span>
        </button>
        <a class="navbar-brand home push-link" href="/"><strong>OpenReview</strong>.net</a>
      </div>
  
      <div id="navbar" class="navbar-collapse collapse">
        <form class="navbar-form navbar-left profile-search" role="search">
          <div class="form-group has-feedback">
            <input id="search_input" type="text" class="form-control ui-autocomplete-input" placeholder="Search ICLR 2019 Conference" autocomplete="off" />
            <span class="glyphicon glyphicon-search form-control-feedback" aria-hidden="true"></span>
          </div>
  
          <input id="search_group" type="hidden" value="ICLR.cc/2019/Conference" />
          <input id="search_content" type="hidden" value="all" />
        <ul id="ui-id-1" tabindex="0" class="ui-menu ui-widget ui-widget-content ui-autocomplete ui-front" style="display: none;"></ul></form>
  
        <ul class="nav navbar-nav navbar-right">
        
            <li id="user-menu"><a href="/login">Login</a></li>
        </ul>
      </div>
  
    </div>
  </nav>

  <div id="or-banner" class="banner" style="">
  <div class="container">
    <div class="row">
      <div class="col-xs-12"><a href="/group?id=ICLR.cc/2019/Conference" title="Venue Homepage"><img class="icon" src="/static/images/arrow_left.svg" /> Go to <strong>ICLR 2019 Conference</strong> homepage</a></div>
    </div>
  </div>
</div>
<div id="flash-message-container" class="alert alert-danger" role="alert" style="display: none;">
  <div class="container">
    <div class="row">
      <div class="col-xs-12">
        <div class="alert-content">
          <button type="button" class="close" aria-label="Close"><span aria-hidden="true">Ã—</span></button>
        </div>
      </div>
    </div>
  </div>
</div>

<div class="container">
  <div class="row">
    <div class="col-xs-12">
      <main id="content" class="clearfix openbanner-visible legacy-styles"><div id="note_BJxfJnC9YX" class="note panel"><div class="title_pdf_row clearfix"><h2 class="note_content_title"><a>Learning Spatio-Temporal Representations Using Spike-Based Backpropagation</a> <a class="note_content_pdf" href="/pdf?id=BJxfJnC9YX" title="Download PDF" target="_blank"><img src="/static/images/pdf_icon_blue.svg" /></a> </h2></div><div class="meta_row"><span class="signatures"><a href="/profile?email=roy77%40purdue.edu" class="profile-link" data-toggle="tooltip" data-placement="bottom" title="" data-original-title="roy77@purdue.edu">Deboleena Roy</a>, <a href="/profile?email=pandap%40purdue.edu" class="profile-link" data-toggle="tooltip" data-placement="bottom" title="" data-original-title="pandap@purdue.edu">Priyadarshini Panda</a>, <a href="/profile?email=kaushik%40purdue.edu" class="profile-link" data-toggle="tooltip" data-placement="bottom" title="" data-original-title="kaushik@purdue.edu">Kaushik Roy</a></span></div><div class="clearfix"><div class="meta_row pull-left"><span class="date item">28 Sep 2018 (modified: 20 Nov 2018)</span><span class="item">ICLR 2019 Conference Withdrawn Submission</span><span class="item">Readers: <span class="readers-icon glyphicon glyphicon-globe"></span> Everyone</span></div><div class="meta_row meta_actions"></div></div><div class="note_contents"><span class="note_content_field">Abstract: </span><span class="note_content_value">Spiking neural networks (SNNs) offer a promising alternative to current artificial neural networks to enable low-power event-driven neuromorphic hardware. However, training SNNs remains a challenge primarily because of the complex non-differentiable neuronal behavior arising from their spike-based computation. In this paper, we propose an algorithm to train spiking autoencoders on regenerative learning tasks. A sigmoid approximation is used in place of the Leaky Integrate-and-Fire neuron's threshold based activation during backpropagation to enable differentiability. The loss is computed on the membrane potential of the output layer, which is then backpropagated through the network at each time step. These spiking autoencoders learn meaningful spatio-temporal representations of the data, across two modalities - audio and visual. We demonstrate audio to image synthesis in a spike-based environment by sharing these spatio-temporal representations between the two modalities. These models achieve very low reconstruction loss, comparable to ANNs, on MNIST and Fashion-MNIST datasets, and while converting TI-46 digits audio samples to MNIST images. </span></div><div class="note_contents"><span class="note_content_field">Keywords: </span><span class="note_content_value">spiking neural networks, autoencoders, representation learning, backpropagation, multimodal</span></div><div class="reply_row clearfix"><div class="item" id="reply_count">4 Replies</div></div></div><hr class="small" /><div id="note_children"><div class="note_with_children"><div id="note_SJgmzVI107" class="note panel"><div class="title_pdf_row clearfix"><h2 class="note_content_title"><a>Submission Withdrawn by the Authors</a>  <button class="btn btn-xs btn-default permalink-button" title="Link to this comment" data-permalink-url="https://openreview.net/forum?id=BJxfJnC9YX&amp;noteId=SJgmzVI107"><span class="glyphicon glyphicon-link" aria-hidden="true"></span></button></h2></div><div class="meta_row"><span class="signatures">ICLR 2019 Conference Paper969 Authors</span></div><div class="clearfix"><div class="meta_row pull-left"><span class="date item">19 Nov 2018</span><span class="item">ICLR 2019 Conference Paper969 Withdraw Submission</span><span class="item">Readers: <span class="readers-icon glyphicon glyphicon-globe"></span> Everyone</span></div><div class="meta_row meta_actions"></div></div><div class="note_contents"><span class="note_content_field">Withdrawal confirmation: </span><span class="note_content_value">I have read and agree with the withdrawal statement on behalf of myself and my co-authors.</span></div><div class="reply_row clearfix"></div></div><div class="children"></div></div><div class="note_with_children"><div id="note_r1egElx5hX" class="note panel"><div class="title_pdf_row clearfix"><h2 class="note_content_title"><a>Gradient based optimization of spiking networks via differentiable approximations during backpropagation</a>  <button class="btn btn-xs btn-default permalink-button" title="Link to this comment" data-permalink-url="https://openreview.net/forum?id=BJxfJnC9YX&amp;noteId=r1egElx5hX"><span class="glyphicon glyphicon-link" aria-hidden="true"></span></button></h2></div><div class="meta_row"><span class="signatures">ICLR 2019 Conference Paper969 AnonReviewer2</span></div><div class="clearfix"><div class="meta_row pull-left"><span class="date item">02 Nov 2018 (modified: 07 Nov 2018)</span><span class="item">ICLR 2019 Conference Paper969 Official Review</span><span class="item">Readers: <span class="readers-icon glyphicon glyphicon-globe"></span> Everyone</span></div><div class="meta_row meta_actions"></div></div><div class="note_contents"><span class="note_content_field">Review: </span><span class="note_content_value">In this work, the authors investigate the training of spiking (leaky integrate and fire) neural networks, for which the normally used threshold function is replaced with the sigmoid function for backpropagation. They demonstrate the proposed method for an autoencoder network on MNIST and Fashion-MNIST, as well as an audio to image synthesis task. The reconstruction loss of the ANN and SNN models is compared, typically with an advantage for the ANN model, but on Fashion-MNIST the SNN results look better.

The paper addresses learning of deep SNNs, which is a difficult topic with relevance to both the machine learning and computational neuroscience communities. Since the paper does not aim for biological plausibility, the target audience is ML and neuromorphic engineering. For ICLR this is a paper of medium relevance.

The originality of the work is low, because approaches that propose to replace the non-differentiable threshold function with a differentiable proxy, in this case membrane potentials, have been known for several years, e.g. "Spiking Deep Networks with LIF Neurons", Eric Hunsberger and Chris Eliasmith, 2015, and "Training Deep Spiking Neural Networks Using Backpropagation", Jun-Haeng Lee et al. 2016. The main novelty is therefore the application to autoencoders, but overall this reduces originality and hence relevance for ICLR. A thorough discussion and differentiation to previous work is missing.

Another criticism is that the relevance of the multi-modal experiments for the idea of replacing the threshold function with a sigmoid function is not clear. Instead, the authors could provide additional experiments on classification tasks or investigate the scalability of the idea for deep (more layers) networks (e.g., on CIFAR10 classification). In general, the experimental section could be more tailored towards the central idea of the paper.

While some of the parameters are explored in detail, the choice of others is not motivated. For example, the network sizes are just given without further justification, similarly the parameters for ANN training, which might have required more fine-tuning (e.g. learning rates). Figure 3(b) remains strange, because at some batch number the unmasked version simply ceases to work, but this could have been explained without a graph. Especially the comparison to the ANN without ANN would have required more fine tuning of learning rates, thus I do not see much value in the AE-SNN outperforming this method. The resulting reconstructions in Figures 4c look like they have some completely dark and some very strongly firing pixels, which indicates a far-from-perfect reconstruction. Actually the final reconstruction MSE of 0.2 on the binary MNIST images looks rather high.
I find it surprising that the SNN outperforms the ANN on Fashion-MNIST, and I don't really see a reason why this should happen, and the argument in the article is not convincing, neither are the differences explored in detail. I therefore assume this is a bad parameter setting for the ANNs, and would encourage the authors to test and evaluate this in more detail.


Pros:
+ Overall the paper is well written and has a transparent and meaningful structure. The central ideas are laid out in a comprehensive manner.
+ In general, the training of spiking neural networks remains a challenging task and investigation of potential solutions is relevant to ICLR.

Cons:
- Low originality and missing comparison to related work
- Unconvincing experimental section


Minor remarks:
* The title is misleading in the sense, that the spatial part of the spatio-temporal representation is not related to actual space, and further, that the backpropagation is not spike-based
* The figures appear blurry, and in the case of Fig. 2 is very hard to read.
* In Figure 3c the unit of the spike train duration is not given in figure or caption.
* There are a number of grammatical errors.</span></div><div class="note_contents"><span class="note_content_field">Rating: </span><span class="note_content_value">3: Clear rejection</span></div><div class="note_contents"><span class="note_content_field">Confidence: </span><span class="note_content_value">5: The reviewer is absolutely certain that the evaluation is correct and very familiar with the relevant literature</span></div><div class="reply_row clearfix"></div></div><div class="children"></div></div><div class="note_with_children"><div id="note_H1eq573_2m" class="note panel"><div class="title_pdf_row clearfix"><h2 class="note_content_title"><a>hard to get a handle on due to very limited clarity</a>  <button class="btn btn-xs btn-default permalink-button" title="Link to this comment" data-permalink-url="https://openreview.net/forum?id=BJxfJnC9YX&amp;noteId=H1eq573_2m"><span class="glyphicon glyphicon-link" aria-hidden="true"></span></button></h2></div><div class="meta_row"><span class="signatures">ICLR 2019 Conference Paper969 AnonReviewer3</span></div><div class="clearfix"><div class="meta_row pull-left"><span class="date item">02 Nov 2018 (modified: 07 Nov 2018)</span><span class="item">ICLR 2019 Conference Paper969 Official Review</span><span class="item">Readers: <span class="readers-icon glyphicon glyphicon-globe"></span> Everyone</span></div><div class="meta_row meta_actions"></div></div><div class="note_contents"><span class="note_content_field">Review: </span><span class="note_content_value">The paper is hard to read, and hard to get a handle on the significance of the result.

The work describes an error-backpropagation rule based on an approximation of the activation of each neuron, where an error measure is taken from the membrane potential of the output neuron and then backpropagated in auto encoders with a single hidden layer (which severely limits the complexity of backprop). The backprop as proposed is essentially standard backprop, where a continuous approximation of the forward activation is made such that the gradients can be computed. There is mention of applying effectively BPTT, but I do not see that in the model (sec 2.3). 

I am also concerned as to whether the comparison with ANNs is correct in section 3.1. Why only one epoch with specific parameters? Do the parameters matter? 

The quality of the work is unclear in places, like whether the comparisons are valid. The clarity is problematic, also because the grammar is off in many places. 

I am also highly dubious about the claim that auto encoders can only learn the hidden representations  of only one modality. Citation? Or a detailed explanation. 

The performance shown in Figure 7 for the SNN seems rather poor compared to an ANN, if the comparison is valid. The reported results in Table 1 are confusing, as some are and some are not following from the graphs. </span></div><div class="note_contents"><span class="note_content_field">Rating: </span><span class="note_content_value">4: Ok but not good enough - rejection</span></div><div class="note_contents"><span class="note_content_field">Confidence: </span><span class="note_content_value">5: The reviewer is absolutely certain that the evaluation is correct and very familiar with the relevant literature</span></div><div class="reply_row clearfix"></div></div><div class="children"></div></div><div class="note_with_children"><div id="note_BJlgIDy_nX" class="note panel"><div class="title_pdf_row clearfix"><h2 class="note_content_title"><a>Neuromorphic learning rule that needs comparisons and a temporally more interesting task</a>  <button class="btn btn-xs btn-default permalink-button" title="Link to this comment" data-permalink-url="https://openreview.net/forum?id=BJxfJnC9YX&amp;noteId=BJlgIDy_nX"><span class="glyphicon glyphicon-link" aria-hidden="true"></span></button></h2></div><div class="meta_row"><span class="signatures">ICLR 2019 Conference Paper969 AnonReviewer1</span></div><div class="clearfix"><div class="meta_row pull-left"><span class="date item">01 Nov 2018 (modified: 07 Nov 2018)</span><span class="item">ICLR 2019 Conference Paper969 Official Review</span><span class="item">Readers: <span class="readers-icon glyphicon glyphicon-globe"></span> Everyone</span></div><div class="meta_row meta_actions"></div></div><div class="note_contents"><span class="note_content_field">Review: </span><span class="note_content_value">"Learning spatio-temporal representations using spike-based backpropagation" submitted to ICLR 2019 describes a gradient based learning algorithm for discrete-time feed-forward leaky-integrate-and-fire network. Based on (1) the mean square error of the membrane potential for misclassified time points, and (2) smooth approximation of the firing threshold with a sigmoid funciton, the authors obtain a chain rule through the network. The resulting algorithm is very far from being biologically plasible. They train an autoencoder for images and audio-to-image translator. Although this reviewer is happy to see spiking neural network in ICLR, there are several major flaws.

1. Comparison with previous methods: the authors cite (Bohte 2002) and (Lee 2016) as alternate backpropagation schemes, however their performance is not compared. In addition, a large portion of the literature on training spiking neural networks is missing: Spikeprop, ReSuMe, Tempotron (and its extensions), SuperSpike, Huh &amp; Sejnowski 2017, to name a few. I suggest comparing with these existing methods.

2. Effect of leak coefficient (Fig 3a) indicates that alpha = 0.1 performs significantly better than less leak. Why didn't you investigate even larger alpha (shorter time constant)? As indicated by Fig 2b, your time constants seems to be very long relative to the time steps.

3. (Comment) Bernoulli encoding of intensity (Poisson process discretized over 15 or 60 time steps) in the image is similar to dropout regularization, but not equivalent because (1) you only apply it to the encoding layer, and (2) probability depends on the strength of the image. Despite this difference, this encoding scheme seems to be providing some robustness to the encoding.

4. (Minor) Spike-MSE is just number of spikes that were precisely matched in discrete time, and normalized MSE is proportional to the correlation coefficient, right? The names of the metric you use are somewhat misleading.

5. (Wishlist) I'd love to see the features learned by the auto/trans-encoder. Does it extract receptive fields that are gabor-like? For MNIST, does it obtain common line segments?

6. The sigmoid activation function assumption is unprincipled. How is the width of the sigmoid chosen? Have you tried to optimize the sigmoid for training performances? Is this related to the escape rate approximation?

7. In general, training SNN with gradient descent is difficult because of the non-smooth threshold. Small changes can bring very large changes to the entire future outputs. The proposed method is no exception. The reason it doesn't seem destructive due to the static nature of the task. In a realistic setting where spike times are sparse and relative precisions carries information, I do not believe this algorithm can solve the temporal credit assignment problem. I suggest including a temporal task where the input is changing over time (not just the noise).
</span></div><div class="note_contents"><span class="note_content_field">Rating: </span><span class="note_content_value">3: Clear rejection</span></div><div class="note_contents"><span class="note_content_field">Confidence: </span><span class="note_content_value">4: The reviewer is confident but not absolutely certain that the evaluation is correct</span></div><div class="reply_row clearfix"></div></div><div class="children"></div></div></div></main></div>
  </div>
</div>


    <!-- Footer -->
    <footer class="sitemap">
      <div class="container">
        <div class="row hidden-xs">
          <div class="col-sm-4">
            <ul class="list-unstyled">
              <li><a href="/" class="home">Home</a></li>
              <li><a href="/venues">All Venues</a></li>
            </ul>
          </div>
    
          <div class="col-sm-4">
            <ul class="list-unstyled">
              <li><a href="/about">About OpenReview</a></li>
              <li><a href="/contact">Contact</a></li>
              <li><a href="#" data-toggle="modal" data-target="#feedback-modal">Feedback</a></li>
            </ul>
          </div>
    
          <div class="col-sm-4">
            <ul class="list-unstyled">
              <li><a href="/terms">Terms of Service</a></li>
              <li><a href="/privacy">Privacy Policy</a></li>
            </ul>
          </div>
        </div>
    
        <div class="row visible-xs-block">
          <div class="col-xs-6">
            <ul class="list-unstyled">
              <li><a href="/" class="home">Home</a></li>
              <li><a href="/about">About OpenReview</a></li>
              <li><a href="#" data-toggle="modal" data-target="#feedback-modal">Feedback</a></li>
            </ul>
          </div>
    
          <div class="col-xs-6">
            <ul class="list-unstyled">
              <li><a href="/contact">Contact</a></li>
              <li><a href="/terms">Terms of Service</a></li>
              <li><a href="/privacy">Privacy Policy</a></li>
            </ul>
          </div>
        </div>
      </div>
    </footer>
    
    <footer class="sponsor">
      <div class="container">
        <div class="row">
          <div class="col-sm-10 col-sm-offset-1">
            <p class="text-center">
              OpenReview is created by the <a href="http://www.iesl.cs.umass.edu/" target="_blank">Information Extraction and Synthesis Laboratory</a>, College of Information and Computer Science, University of Massachusetts Amherst. We gratefully acknowledge the support of the OpenReview sponsors:  Google,  Facebook, NSF, the University of Massachusetts Amherst Center for Data Science, and Center for Intelligent Information Retrieval, as well as the Google Cloud Platform for donating the computing and networking services on which OpenReview.net runs.
            </p>
          </div>
        </div>
      </div>
    </footer>

  <div id="feedback-modal" class="modal fade" tabindex="-1" role="dialog">
    <div class="modal-dialog">
      <div class="modal-content">
        <div class="modal-header">
          <button type="button" class="close" data-dismiss="modal" aria-label="Close"><span aria-hidden="true">Ã—</span></button>
          <h3 class="modal-title">Send Feedback</h3>
        </div>
        <div class="modal-body">
          <p>Enter your feedback below and we'll get back to you as soon as possible.</p>
          <form action="/feedback" method="POST">
            <div class="form-group">
              <input type="email" name="from" class="form-control" placeholder="Email" />
            </div>
            <div class="form-group">
              <input type="text" name="subject" class="form-control" placeholder="Subject" />
            </div>
            <div class="form-group">
              <textarea name="message" class="form-control feedback-input" rows="5" placeholder="Message" required=""></textarea>
            </div>
          <ul id="ui-id-2" tabindex="0" class="ui-menu ui-widget ui-widget-content" style="display: none;"></ul></form>
        </div>
        <div class="modal-footer">
          <button type="button" class="btn btn-default" data-dismiss="modal">Cancel</button>
          <button type="button" class="btn btn-primary">Send</button>
        </div>
      </div>
    </div>
  </div>

  <script type="text/javascript" async="" src="https://www.google-analytics.com/analytics.js"></script><script src="//cdnjs.cloudflare.com/ajax/libs/jquery/2.2.4/jquery.min.js"></script>
  <script>window.jQuery || document.write('&lt;script src="/static/js/vendor/jquery-2.2.4.min.js"&gt;&lt;\/script&gt;')</script>

  <script src="/static/dist/vendor.min.js"></script>
  <script src="/static/dist/templates.min.js"></script>
  <script src="/static/dist/openreview.min.js"></script>

    <script>window.legacyScripts = true;</script>


    <script async="" src="https://www.googletagmanager.com/gtag/js?id=UA-108703919-1"></script>
    <script>
      window.dataLayer = window.dataLayer || [];
      function gtag() { dataLayer.push(arguments); }
      gtag('js', new Date());
      gtag('config', 'UA-108703919-1');
    </script>


<div role="status" aria-live="assertive" aria-relevant="additions" class="ui-helper-hidden-accessible"></div></body></html>