<!DOCTYPE html><html xmlns="http://www.w3.org/1999/xhtml" class="no-js"><head>
  <meta charset="utf-8" />
  <meta http-equiv="X-UA-Compatible" content="IE=edge,chrome=1" />
  <meta name="viewport" content="width=device-width, initial-scale=1" />

  <title>Discriminative out-of-distribution detection for semantic segmentation | OpenReview</title>
  <meta name="description" content="" />

      <meta name="citation_title" content="Discriminative out-of-distribution detection for semantic segmentation" />
        <meta name="citation_author" content="Anonymous" />
      <meta name="citation_publication_date" content="2018/09/27" />
      <meta name="citation_online_date" content="2018/09/27" />
      <meta name="citation_pdf_url" content="https://openreview.net/pdf?id=H1x1noAqKX" />
      <meta name="twitter:card" content="summary" />
      <meta name="twitter:site" content="@openreviewnet" />
      <meta name="og:title" content="Discriminative out-of-distribution detection for semantic segmentation" />
      <meta name="og:description" content="Most classification and segmentation datasets assume a closed-world scenario in which predictions are expressed as distribution over a predetermined set of visual classes. However, such assumption..." />
      <meta name="og:image" content="https://openreview.net/static/images/openreview_logo_512.png" />

  <link rel="stylesheet" href="https://fonts.googleapis.com/css?family=Noto+Sans:400,400i,700,700i" />

  <link rel="stylesheet" href="/static/css/bootstrap.min.css" />
  <link rel="stylesheet" href="/static/css/jquery-ui.min.css" />
  <link rel="stylesheet" href="/static/css/main.min.css" />
</head>

<body class="forum">
  <nav class="navbar navbar-inverse navbar-fixed-top" role="navigation">
    <div class="container">
  
      <div class="navbar-header">
        <button type="button" class="navbar-toggle collapsed" data-toggle="collapse" data-target="#navbar" aria-expanded="false" aria-controls="navbar">
          <span class="sr-only">Toggle navigation</span>
          <span class="icon-bar"></span>
          <span class="icon-bar"></span>
          <span class="icon-bar"></span>
        </button>
        <a class="navbar-brand home push-link" href="/"><strong>OpenReview</strong>.net</a>
      </div>
  
      <div id="navbar" class="navbar-collapse collapse">
        <form class="navbar-form navbar-left profile-search" role="search">
          <div class="form-group has-feedback">
            <input id="search_input" type="text" class="form-control ui-autocomplete-input" placeholder="Search ICLR 2019 Conference" autocomplete="off" />
            <span class="glyphicon glyphicon-search form-control-feedback" aria-hidden="true"></span>
          </div>
  
          <input id="search_group" type="hidden" value="ICLR.cc/2019/Conference" />
          <input id="search_content" type="hidden" value="all" />
        <ul id="ui-id-1" tabindex="0" class="ui-menu ui-widget ui-widget-content ui-autocomplete ui-front" style="display: none;"></ul></form>
  
        <ul class="nav navbar-nav navbar-right">
        
            <li id="user-menu"><a href="/login">Login</a></li>
        </ul>
      </div>
  
    </div>
  </nav>

  <div id="or-banner" class="banner" style="">
  <div class="container">
    <div class="row">
      <div class="col-xs-12"><a href="/group?id=ICLR.cc/2019/Conference" title="Venue Homepage"><img class="icon" src="/static/images/arrow_left.svg" /> Go to <strong>ICLR 2019 Conference</strong> homepage</a></div>
    </div>
  </div>
</div>
<div id="flash-message-container" class="alert alert-danger" role="alert" style="display: none;">
  <div class="container">
    <div class="row">
      <div class="col-xs-12">
        <div class="alert-content">
          <button type="button" class="close" aria-label="Close"><span aria-hidden="true">×</span></button>
        </div>
      </div>
    </div>
  </div>
</div>

<div class="container">
  <div class="row">
    <div class="col-xs-12">
      <main id="content" class="clearfix openbanner-visible legacy-styles"><div id="note_H1x1noAqKX" class="note panel"><div class="title_pdf_row clearfix"><h2 class="note_content_title"><a>Discriminative out-of-distribution detection for semantic segmentation</a> <a class="note_content_pdf" href="/pdf?id=H1x1noAqKX" title="Download PDF" target="_blank"><img src="/static/images/pdf_icon_blue.svg" /></a> </h2></div><div class="meta_row"><span class="signatures">Anonymous</span></div><div class="clearfix"><div class="meta_row pull-left"><span class="date item">28 Sep 2018 (modified: 11 Oct 2018)</span><span class="item">ICLR 2019 Conference Blind Submission</span><span class="item">Readers: <span class="readers-icon glyphicon glyphicon-globe"></span> Everyone</span><span class="item"><a class="action-bibtex-modal" data-bibtex="@inproceedings{    &#10;anonymous2019discriminative,    &#10;title={Discriminative out-of-distribution detection for semantic segmentation},    &#10;author={Anonymous},    &#10;booktitle={Submitted to International Conference on Learning Representations},    &#10;year={2019},    &#10;url={https://openreview.net/forum?id=H1x1noAqKX},    &#10;note={under review}    &#10;}">Show Bibtex</a></span></div><div class="meta_row meta_actions"></div></div><div class="note_contents"><span class="note_content_field">Abstract: </span><span class="note_content_value">Most classification and segmentation datasets assume a closed-world scenario in which predictions are expressed as distribution over a predetermined set of visual classes. However, such assumption implies unavoidable and often unnoticeable failures in presence of out-of-distribution (OOD) input. These failures are bound to happen in most real-life applications since current visual ontologies are far from being comprehensive. We propose to address this issue by discriminative detection 
of OOD pixels in input data. Different from recent approaches, we avoid to bring any decisions by only observing the training dataset of the primary model trained to solve the desired computer vision task. Instead, we train a dedicated OOD model
which discriminates the primary training set from a much larger "background" dataset which approximates the variety of the visual world. We perform our experiments on high resolution natural images in a dense prediction setup. We use several road driving datasets as our training distribution, while we approximate the background distribution with the ILSVRC dataset. We evaluate our approach on WildDash test, which is currently the only public test dataset with out-of-distribution images.
The obtained results show that the proposed approach succeeds to identify out-of-distribution pixels while outperforming previous work by a wide margin.</span></div><div class="note_contents"><span class="note_content_field">Keywords: </span><span class="note_content_value">out-of-distribution detection, semantic segmentation</span></div><div class="note_contents"><span class="note_content_field">TL;DR: </span><span class="note_content_value">We present a novel approach for detecting out-of-distribution pixels in semantic segmentation.</span></div><div class="reply_row clearfix"><div class="item" id="reply_count">7 Replies</div></div></div><hr class="small" /><div id="note_children"><div class="note_with_children"><div id="note_S1lH3uCypm" class="note panel"><div class="title_pdf_row clearfix"><h2 class="note_content_title"><a>Discriminative out-of-distribution detection for semantic segmentation</a>  <button class="btn btn-xs btn-default permalink-button" title="Link to this comment" data-permalink-url="https://openreview.net/forum?id=H1x1noAqKX&amp;noteId=S1lH3uCypm"><span class="glyphicon glyphicon-link" aria-hidden="true"></span></button></h2></div><div class="meta_row"><span class="signatures">ICLR 2019 Conference Paper675 AnonReviewer2</span></div><div class="clearfix"><div class="meta_row pull-left"><span class="date item">07 Nov 2018</span><span class="item">ICLR 2019 Conference Paper675 Official Review</span><span class="item">Readers: <span class="readers-icon glyphicon glyphicon-globe"></span> Everyone</span></div><div class="meta_row meta_actions"></div></div><div class="note_contents"><span class="note_content_field">Review: </span><span class="note_content_value">Summary:
This paper addresses the problem of out-of-distribution detection for helping the segmentation process. Therefore, the detection is performed on a pixel basis. The application of the approach is to datasets used for autonomous driving, where semantic segmentation of the view of the road is a typical application. Since in a road view there will be pixels that are projections of objects that are likely not in the set of classes known by the semantic segmentation algorithm, it makes sense to flag them as being out of distribution (OOD), or not known, or to assign to them a low confidence level. The proposed approach is trivial: train a binary classifier that distinguishes image patches from a known set of classes from image patches coming from an unknown (background class). The classifier output applied at every pixel will give the confidence value. While there are different dataset options to represent the known classes, the background class is represented by images from ILSVRC. The results show that for the segmentation application the approach works better than using an adaptation of more elaborate out-of-distribution methods.

Quality and clarity:
The paper is well organized and is described very clearly and provides an ok set of results, despite the simplicity of the approach.

Originality and significance:
Unfortunately, I do not see any relevant technical novelty, and this is a major issue. Perhaps the only significant conclusion about this paper is that before designing a new OOD detector, if representing the set of “unknown” classes with ILVRC is reasonable, then it makes sense to simply train a binary classifier and see how it works.

Besides the novelty, I disagree with the way the paper has been positioned and motivated. It brings into play epistemic and aleatoric uncertainty concepts to justify (the simplicity of) the approach, and it overlooks a large body of machine learning (novelty detection, one-class classification, …). This is also a major issue.


Additional comments:

One of the biggest motivations for this work is that other approaches do not distinguish between epistemic and aleatoric uncertainty and this is why they do not work. This is regarded as a distinctive advantage of the proposed approach. It is claimed that the proposed formulation is insensitive to any aleatoric uncertainty. On the other hand, the paper is written in a way that ignores a large body of literature that goes under the name of “novelty detection”, “anomaly detection”, “one-class classification”, and related names. So, I am wondering how the approaches just mentioned compare with the proposed method, when epistemic and aleatoric uncertainty become part of the discussion. Isn’t every novelty detector insensitive to aleatoric uncertainty as well? Could the Authors clarify what they claim with that statement, while considering a broader view? 

The paper should relate to the literature mentioned above. In particular, I would point the Authors to a couple of recent works that seem to precisely contradict the premises of the proposed approach, which are given at the beginning of section 3:

- Adversarially Learned One-Class Classifier for Novelty Detection, CVPR 2018
- Generative Probabilistic Novelty Detection with Adversarial Autoencoders, arXiv, July 2018.


Again, related to novelty detection, it looks like the proposed approach still requires tuning one or more thresholds. Therefore, it would not be that different from tuning the threshold of a novelty detector, or a one-class classifier. It would have strengthened the paper if the approach was compared also to a novelty detector.

It is not clear if the fully convolutional OOD detector is working on a patch or on the entire image. If it is a patch, of what size?

Page 4, define the “ID” acronym. 
</span></div><div class="note_contents"><span class="note_content_field">Rating: </span><span class="note_content_value">4: Ok but not good enough - rejection</span></div><div class="note_contents"><span class="note_content_field">Confidence: </span><span class="note_content_value">4: The reviewer is confident but not absolutely certain that the evaluation is correct</span></div><div class="reply_row clearfix"></div></div><div class="children"><div class="note_with_children"><div id="note_HygePKfMTX" class="note panel"><div class="title_pdf_row clearfix"><h2 class="note_content_title"><a>Reply to AnonReviewer2, Part 2</a>  <button class="btn btn-xs btn-default permalink-button" title="Link to this comment" data-permalink-url="https://openreview.net/forum?id=H1x1noAqKX&amp;noteId=HygePKfMTX"><span class="glyphicon glyphicon-link" aria-hidden="true"></span></button></h2></div><div class="meta_row"><span class="signatures">ICLR 2019 Conference Paper675 Authors</span></div><div class="clearfix"><div class="meta_row pull-left"><span class="date item">09 Nov 2018</span><span class="item">ICLR 2019 Conference Paper675 Official Comment</span><span class="item">Readers: <span class="readers-icon glyphicon glyphicon-globe"></span> Everyone</span></div><div class="meta_row meta_actions"></div></div><div class="note_contents"><span class="note_content_field">Comment: </span><span class="note_content_value">
&gt; Adversarially Learned One-Class Classifier for Novelty Detection, CVPR 2018

We have located and inspected the code at <a href="https://github.com/khalooei/ALOCC-CVPR2018," target="_blank" rel="nofollow">https://github.com/khalooei/ALOCC-CVPR2018,</a> however it appears to be out of sync with the paper: the refinement loss in the code (grep g_r_loss) does not seem to match the equation (4) in the paper. The code does not include information to reproduce the numbers from the tables. Straight-forward evaluation of the provided trained model on few UCSD images does not appear to separate inliers from outliers.

&gt; It would have strengthened the paper if the approach was compared also to a novelty detector.

Thank you for your suggestion! We agree, we shall present such discussion in the revised paper.

&gt; It is not clear if the fully convolutional OOD detector is working 
&gt; on a patch or on the entire image. If it is a patch, of what size?

Our fully convolutional OOD detector operates on entire images. It outputs a dense prediction in the form of a H/32xW/32 matrix, where HxW are dimensions of the original image (cf. [long15cvpr]). One could say that our detector operates as if it were applied to RxR patches situated 32 pixels apart where R is effective receptive field of the discriminator (for DenseNet 121 finetuned on full Cityscapes images, R is around 600 pixels).

&gt; Page 4, define the “ID” acronym. 

ID stands for "in-distribution". We shall clarify that, thanks! </span></div><div class="reply_row clearfix"></div></div><div class="children"></div></div><div class="note_with_children"><div id="note_rJg8XtMMa7" class="note panel"><div class="title_pdf_row clearfix"><h2 class="note_content_title"><a>Reply to AnonReviewer2, Part 1</a>  <button class="btn btn-xs btn-default permalink-button" title="Link to this comment" data-permalink-url="https://openreview.net/forum?id=H1x1noAqKX&amp;noteId=rJg8XtMMa7"><span class="glyphicon glyphicon-link" aria-hidden="true"></span></button></h2></div><div class="meta_row"><span class="signatures">ICLR 2019 Conference Paper675 Authors</span></div><div class="clearfix"><div class="meta_row pull-left"><span class="date item">09 Nov 2018</span><span class="item">ICLR 2019 Conference Paper675 Official Comment</span><span class="item">Readers: <span class="readers-icon glyphicon glyphicon-globe"></span> Everyone</span></div><div class="meta_row meta_actions"></div></div><div class="note_contents"><span class="note_content_field">Comment: </span><span class="note_content_value">Thank you for your review! We answer your questions as follows.

&gt; Unfortunately, I do not see any relevant technical novelty, and this is a major issue. 

In our opinion, a simple solution to a difficult problem is preferred to a complex one.

&gt; Perhaps the only significant conclusion about this paper is that 
&gt; before designing a new OOD detector, if representing 
&gt; the set of “unknown” classes with ILVRC is reasonable, 
&gt; then it makes sense to simply train a binary classifier and see how it works.

Our experiments suggest that representing outliers with ILSVRC might be reasonable more often than not, since our method correctly classified negative WildDash images such as a white wall, two kinds of noise, anthill closeup, aquarium, etc.

A concurrent submission to ICLR 2019 shows that representing outliers with ImageNet and 80 million images is a reasonable choice for a wide selection of datasets. We feel that our submission nicely complements their work by presenting a similar idea in the dense prediction context.

<a href="https://openreview.net/forum?id=HyxCxhRcY7" target="_blank" rel="nofollow">https://openreview.net/forum?id=HyxCxhRcY7</a>

Yes, our main conclusion is that ImageNet seems to be diverse enough to support discriminative OOD detection in diverse traffic images. The other important finding is that entropy-based OOD detection approaches (e.g. max-softmax) are not appropriate for dense prediction due to inherent aleatoric uncertainty involved.

We thank for the comment, we shall revise the conclusion accordingly.

&gt; Besides the novelty, I disagree with the way the paper has been positioned and motivated. 
&gt; It brings into play epistemic and aleatoric uncertainty concepts to justify (the simplicity of) the approach, 
&gt; and it overlooks a large body of machine learning (novelty detection, one-class classification, …). 
&gt; This is also a major issue.

We avoided several methods based on complex GAN designs due to large space and time complexities which would cause GPU memory exhaustion and long training times. One of our in-distribution datasets contains 18000 complex road driving images (Vistas, we reduce resolution to 512x1024 pixels). This means 150 times more pixels than in CIFAR and more than 50 times more pixels than in UCSD ped2, even if we neglect diversity of scenes (different continents, night, rain, snow, ...) and cameras. We do not say it is impossible to adapt some GAN approach to OOD detection on Vistas, however that is certainly not straightforward (as we try to describe below) and therefore out of our scope.

The diversity of our in-distribution dataset restricts our options since now, when we train only the discriminator, our training procedure takes an entire day. We reckon that training a GAN variant with suitable capacity would require much more time since the generator would initially produce useless counterexamples.

Our memory requirements are even more restrictive since our ambition is to attach the discriminator as the second head to a regular semantic segmentation pipeline (we tried that already, it works). State of the art semantic segmentation pipelines are designed to use the whole GPU memory so that joint approaches with large generators would not be feasible on current state of the art GPUs.

We thank for the comment and apologize for not discussing these issues in our original submission! We shall include these considerations and cite the corresponding previous work in the revised paper.

&gt; Isn’t every novelty detector insensitive to aleatoric uncertainty as well? 
&gt; Could the Authors clarify what they claim with that statement, while considering a broader view? 

It is true that novelty detectors such as the one which you propose below would be insensitive to aleatoric uncertainty as well. However, those papers appear to address much simpler problems. For instance, the UCSD dataset involves a fixed camera and very simple scenery. Their method does not appear suitable for the diversity of the Vistas dataset.

We thank for the comment! We shall include a more accurate discussion in the revised paper.

&gt; I would point the Authors to a couple of recent works that seem to precisely contradict 
&gt; the premises of the proposed approach, which are given at the beginning of section 3

An interesting concurrent submission shows experiments in which a state of the art generative model trained on CIFAR assigns larger probabilities to SVHN images than to CIFAR images. That supports premises at the beginning of section 3, that learning the probability density of real datasets is a hard and not completely solved problem (even for datasets which are much less diverse than Vistas).

https://openreview.net/forum?id=H1xwNhCcYm </span></div><div class="reply_row clearfix"></div></div><div class="children"></div></div></div></div><div class="note_with_children"><div id="note_SJewfWvLhm" class="note panel"><div class="title_pdf_row clearfix"><h2 class="note_content_title"><a>Interesting results, good direction to follow </a>  <button class="btn btn-xs btn-default permalink-button" title="Link to this comment" data-permalink-url="https://openreview.net/forum?id=H1x1noAqKX&amp;noteId=SJewfWvLhm"><span class="glyphicon glyphicon-link" aria-hidden="true"></span></button></h2></div><div class="meta_row"><span class="signatures">ICLR 2019 Conference Paper675 AnonReviewer3</span></div><div class="clearfix"><div class="meta_row pull-left"><span class="date item">31 Oct 2018 (modified: 07 Nov 2018)</span><span class="item">ICLR 2019 Conference Paper675 Official Review</span><span class="item">Readers: <span class="readers-icon glyphicon glyphicon-globe"></span> Everyone</span></div><div class="meta_row meta_actions"></div></div><div class="note_contents"><span class="note_content_field">Review: </span><span class="note_content_value">This paper aims to detect out-of-distribution pixels for semantic segmentation, which is a good direction for researchers in this field to explore. As the authors point out, recent semantic segmentation systems surpass 80% mIoU on Pascal VOC 2012  and  Cityscapes, which is a good achievement. Unfortunately, most existing semantic segmentation datasets assume closed-world evaluation which means that they require predictions over a predetermined set of visual classes. This work utilize data from other domain to detect undetermined classes, thus can model uncertainty better in an explicit way. I just have minor comments. 

1. When you perform training, do you train from scratch or from a pre-trained model? If using pre-trained model, then ILSVRC is not actually pure OOD pixels. 

2. How to interpret the results in Table 5? 

</span></div><div class="note_contents"><span class="note_content_field">Rating: </span><span class="note_content_value">7: Good paper, accept</span></div><div class="note_contents"><span class="note_content_field">Confidence: </span><span class="note_content_value">5: The reviewer is absolutely certain that the evaluation is correct and very familiar with the relevant literature</span></div><div class="reply_row clearfix"></div></div><div class="children"><div class="note_with_children"><div id="note_SyeibDGGpQ" class="note panel"><div class="title_pdf_row clearfix"><h2 class="note_content_title"><a>Reply to AnonReviewer3</a>  <button class="btn btn-xs btn-default permalink-button" title="Link to this comment" data-permalink-url="https://openreview.net/forum?id=H1x1noAqKX&amp;noteId=SyeibDGGpQ"><span class="glyphicon glyphicon-link" aria-hidden="true"></span></button></h2></div><div class="meta_row"><span class="signatures">ICLR 2019 Conference Paper675 Authors</span></div><div class="clearfix"><div class="meta_row pull-left"><span class="date item">09 Nov 2018</span><span class="item">ICLR 2019 Conference Paper675 Official Comment</span><span class="item">Readers: <span class="readers-icon glyphicon glyphicon-globe"></span> Everyone</span></div><div class="meta_row meta_actions"></div></div><div class="note_contents"><span class="note_content_field">Comment: </span><span class="note_content_value">Thank you for your review! We answer your questions as follows.

&gt; When you perform training, do you train from scratch or from a pre-trained model?

Parameters of the feature extractor were initialized from ImageNet pre-trained models. All heads are trained from scratch.

&gt; If using pre-trained model, then ILSVRC is not actually pure OOD pixels.

We do not perceive that as a problem neither in discriminative OOD detection (where we train on road driving vs ILSVRC) nor in single-class OOD detection (where we train on road driving images and rely on max-softmax). In discriminative OOD, we cast the problem as binary classification where pre-training can only help. In single-class OOD (max-softmax), the classifier is fine-tuned through 40 epochs on a road driving dataset. Previous work on catastrophic forgetting suggests this likely results in a complete oblivion of features for ILSVRC classes which are not seen in road driving datasets.

Please note that we also successfully detect OOD pixels in WildDash negative images that (at least nominally) do not have anything to do with ILSVRC (white wall, two kinds of noise, anthill closeup, aquarium, etc).

Maybe we do not understand your concerns. Could you please clarify?

&gt; How to interpret the results in Table 5?

Table 5 shows how well the proposed OOD-detection models generalize to datasets which were not seen during training.

Rows 1 and 3 show the difference between using Vistas and Cityscapes as ID dataset. When using Vistas as ID, almost no OOD pixels are detected in Cityscapes. On the other hand, when using Cityscapes as ID, most Vistas pixels are classified as OOD. This suggests that Cityscapes poorly represents the variety of traffic scenes.

Row 2 shows that almost all Pascal VOC 2007 pixels are classified as OOD. This finding complements the results from figure 4, and suggests that training OOD detection on ILSVRC is able to generalize to other datasets. </span></div><div class="reply_row clearfix"></div></div><div class="children"></div></div></div></div><div class="note_with_children"><div id="note_rygvK39V2m" class="note panel"><div class="title_pdf_row clearfix"><h2 class="note_content_title"><a>Difficult to understand</a>  <button class="btn btn-xs btn-default permalink-button" title="Link to this comment" data-permalink-url="https://openreview.net/forum?id=H1x1noAqKX&amp;noteId=rygvK39V2m"><span class="glyphicon glyphicon-link" aria-hidden="true"></span></button></h2></div><div class="meta_row"><span class="signatures">ICLR 2019 Conference Paper675 AnonReviewer1</span></div><div class="clearfix"><div class="meta_row pull-left"><span class="date item">29 Oct 2018 (modified: 07 Nov 2018)</span><span class="item">ICLR 2019 Conference Paper675 Official Review</span><span class="item">Readers: <span class="readers-icon glyphicon glyphicon-globe"></span> Everyone</span></div><div class="meta_row meta_actions"></div></div><div class="note_contents"><span class="note_content_field">Review: </span><span class="note_content_value">ML models are trained on a predefined dataset formed by a set of classes. Those classes use to be the same ones for training and testing. However, what happen when during testing time images with classes unseen during training are shown to the model? This article focus in this problem which is not currently taking much attention by the mainstream research community and is of great importance for the real world applications.

This article tries to detect areas of the image where those out-of-distribution situations appear in semantic segmentation applications. The approach used is by training a classifier that detects which pixels are out of distribution. For training two datasets are used: the dataset of interest and another different one. The classifier learns to detect if a pixel is from the dataset of interest or from another distribution.

The main problem I found with this article is that I couldn't fully understand it. Maybe because the text needs a bit more of review and improvement or maybe because Im not very familiar with the topic. Moreover the article is 10 pages while it is encouraged to be 8. I find that the method of the paper is quite simple and can be explained more straight forward and in less pages. The related work section overlaps a lot with the intro, I suggest to combine both. First two paragraphs of the method seam that should be in the intro. Model details from the experiments I consider that should be explained in the method. I miss a figure explaining the architecture of the model. Why using the semantic segmentation model proposed and no something standard? For instance Tiramisu (That is also based on dense layers). Note that the method used for semantic segmentation is 10 points lower than the SOTA in Cityscapes. Figure 1 is impossible to read as the captions are too small. The representations of figures 2-5 are difficult to interpret. There is no comparison to SOTA

</span></div><div class="note_contents"><span class="note_content_field">Rating: </span><span class="note_content_value">3: Clear rejection</span></div><div class="note_contents"><span class="note_content_field">Confidence: </span><span class="note_content_value">3: The reviewer is fairly confident that the evaluation is correct</span></div><div class="reply_row clearfix"></div></div><div class="children"><div class="note_with_children"><div id="note_S1g1JuzM6m" class="note panel"><div class="title_pdf_row clearfix"><h2 class="note_content_title"><a>Reply to AnonReviewer1</a>  <button class="btn btn-xs btn-default permalink-button" title="Link to this comment" data-permalink-url="https://openreview.net/forum?id=H1x1noAqKX&amp;noteId=S1g1JuzM6m"><span class="glyphicon glyphicon-link" aria-hidden="true"></span></button></h2></div><div class="meta_row"><span class="signatures">ICLR 2019 Conference Paper675 Authors</span></div><div class="clearfix"><div class="meta_row pull-left"><span class="date item">09 Nov 2018</span><span class="item">ICLR 2019 Conference Paper675 Official Comment</span><span class="item">Readers: <span class="readers-icon glyphicon glyphicon-globe"></span> Everyone</span></div><div class="meta_row meta_actions"></div></div><div class="note_contents"><span class="note_content_field">Comment: </span><span class="note_content_value">Thank you for your review. We answer your concerns as follows.

&gt; The main problem I found with this article is that I couldn't fully understand it. 
&gt; Maybe because the text needs a bit more of review and improvement 
&gt; or maybe because Im not very familiar with the topic.

We are sorry the paper was not clear to you. Please provide more specific information regarding which parts of the paper could be clarified.

&gt; Moreover the article is 10 pages while it is encouraged to be 8. 
&gt; I find that the method of the paper is quite simple and can be explained 
&gt; more straight forward and in less pages.

Current method section (section 3) is less than one page long. It appears to us that shortening that section would not decrease the number of pages. We could move figures 2-5 to the appendix although we feel that leaving them as they are would result in a better flow.

&gt; The related work section overlaps a lot with the intro, I suggest to combine both.

We shall resolve some redundancies which we introduced for the convenience of the reader.

    remove the second-to-last paragraph and shorten the last paragraph in the introduction
    remove the last paragraph in the related work

&gt; First two paragraphs of the method seam that should be in the intro.
&gt; Model details from the experiments I consider that should be explained in the method.
&gt; I miss a figure explaining the architecture of the model.

We shall refactor and shorten the first two paragraphs of section 3 and add the figure.

&gt; Why using the semantic segmentation model proposed and no something standard?
&gt; For instance Tiramisu (That is also based on dense layers).
&gt; Note that the method used for semantic segmentation is 10 points lower than the SOTA in Cityscapes.

As you noted in your review, OOD detection on pixel level has not been previously investigated. We prefer to focus on baseline models at this stage in order to simplify conclusions as well as speed-up the training.

SOTA on cityscapes achieve high benchmark results by recovering fine spatial detail lost due to downsampling. Our segmentation models are not as accurate on object borders or small objects like poles, but work reasonably well. When it comes to OOD detection, we are more interested in existence of OOD regions, rather than their exact outlines. Furthermore, as table 5 shows, cityscapes is in many ways a very specific dataset (single camera, nice weather conditions, German cities). Consequently, chasing SOTA on cityscapes is likely to poorly affect max-softmax OOD detection due to overfitting. Using simpler models is also a way of regularization.

Table 3 in our original submission includes OOD-detection results obtained with the model which achieves 77.1 mIoU on Cityscapes. The table shows only results of OOD-detection by classification into foreign classes since this approach worked much better than max-softmax. In the revised paper we show the max-softmax results as well.

We agree that densely connected layers are a very good choice for semantic segmentation. However, tiramisu would not be a suitable choice for our experiments due to following reasons:

    it has a thick up-sampling path which complicates training on large images due to large memory requirements
    the Tiramisu paper proposes exotic downsampling paths for which there are no ImageNet-pretrained parameters available; consequently this would require training from scratch and lead to at least a 10-fold increase in training time and loss of accuracy due to overfitting.

&gt; Figure 1 is impossible to read as the captions are too small.

We shall improve the captions in the revised paper.

&gt; The representations of figures 2-5 are difficult to interpret.

Could you please be more specific about what could be done to clarify these figures?

&gt; There is no comparison to SOTA

To the best of our knowledge, there is almost no previous work in OOD detection on pixel level. Previous work in OOD detection focuses on classification tasks on entire images. We adapt these approaches for dense OOD-detection and show that our approach performs better. Kendall and Gal (2017) model epistemic uncertainty on pixel level, although they do not use it for OOD detection. Our early experiments with this approach resulted in poor OOD detection performance. WildDash is the first semantic segmentation benchmark that introduces OOD images. Most of existing submissions on WildDash come without an accompanying paper, so it is not clear what, if anything, was used for OOD detection. </span></div><div class="reply_row clearfix"></div></div><div class="children"></div></div></div></div></div></main></div>
  </div>
</div>


    <!-- Footer -->
    <footer class="sitemap">
      <div class="container">
        <div class="row hidden-xs">
          <div class="col-sm-4">
            <ul class="list-unstyled">
              <li><a href="/" class="home">Home</a></li>
              <li><a href="/venues">All Venues</a></li>
            </ul>
          </div>
    
          <div class="col-sm-4">
            <ul class="list-unstyled">
              <li><a href="/about">About OpenReview</a></li>
              <li><a href="/contact">Contact</a></li>
              <li><a href="#" data-toggle="modal" data-target="#feedback-modal">Feedback</a></li>
            </ul>
          </div>
    
          <div class="col-sm-4">
            <ul class="list-unstyled">
              <li><a href="/terms">Terms of Service</a></li>
              <li><a href="/privacy">Privacy Policy</a></li>
            </ul>
          </div>
        </div>
    
        <div class="row visible-xs-block">
          <div class="col-xs-6">
            <ul class="list-unstyled">
              <li><a href="/" class="home">Home</a></li>
              <li><a href="/about">About OpenReview</a></li>
              <li><a href="#" data-toggle="modal" data-target="#feedback-modal">Feedback</a></li>
            </ul>
          </div>
    
          <div class="col-xs-6">
            <ul class="list-unstyled">
              <li><a href="/contact">Contact</a></li>
              <li><a href="/terms">Terms of Service</a></li>
              <li><a href="/privacy">Privacy Policy</a></li>
            </ul>
          </div>
        </div>
      </div>
    </footer>
    
    <footer class="sponsor">
      <div class="container">
        <div class="row">
          <div class="col-sm-10 col-sm-offset-1">
            <p class="text-center">
              OpenReview is created by the <a href="http://www.iesl.cs.umass.edu/" target="_blank">Information Extraction and Synthesis Laboratory</a>, College of Information and Computer Science, University of Massachusetts Amherst. We gratefully acknowledge the support of the OpenReview sponsors:  Google,  Facebook, NSF, the University of Massachusetts Amherst Center for Data Science, and Center for Intelligent Information Retrieval, as well as the Google Cloud Platform for donating the computing and networking services on which OpenReview.net runs.
            </p>
          </div>
        </div>
      </div>
    </footer>

  <div id="feedback-modal" class="modal fade" tabindex="-1" role="dialog">
    <div class="modal-dialog">
      <div class="modal-content">
        <div class="modal-header">
          <button type="button" class="close" data-dismiss="modal" aria-label="Close"><span aria-hidden="true">×</span></button>
          <h3 class="modal-title">Send Feedback</h3>
        </div>
        <div class="modal-body">
          <p>Enter your feedback below and we'll get back to you as soon as possible.</p>
          <form action="/feedback" method="POST">
            <div class="form-group">
              <input type="email" name="from" class="form-control" placeholder="Email" />
            </div>
            <div class="form-group">
              <input type="text" name="subject" class="form-control" placeholder="Subject" />
            </div>
            <div class="form-group">
              <textarea name="message" class="form-control feedback-input" rows="5" placeholder="Message" required=""></textarea>
            </div>
          <ul id="ui-id-2" tabindex="0" class="ui-menu ui-widget ui-widget-content" style="display: none;"></ul></form>
        </div>
        <div class="modal-footer">
          <button type="button" class="btn btn-default" data-dismiss="modal">Cancel</button>
          <button type="button" class="btn btn-primary">Send</button>
        </div>
      </div>
    </div>
  </div>

  <script type="text/javascript" async="" src="https://www.google-analytics.com/analytics.js"></script><script src="//cdnjs.cloudflare.com/ajax/libs/jquery/2.2.4/jquery.min.js"></script>
  <script>window.jQuery || document.write('&lt;script src="/static/js/vendor/jquery-2.2.4.min.js"&gt;&lt;\/script&gt;')</script>

  <script src="/static/dist/vendor.min.js"></script>
  <script src="/static/dist/templates.min.js"></script>
  <script src="/static/dist/openreview.min.js"></script>

    <script>window.legacyScripts = true;</script>


    <script async="" src="https://www.googletagmanager.com/gtag/js?id=UA-108703919-1"></script>
    <script>
      window.dataLayer = window.dataLayer || [];
      function gtag() { dataLayer.push(arguments); }
      gtag('js', new Date());
      gtag('config', 'UA-108703919-1');
    </script>


<div role="status" aria-live="assertive" aria-relevant="additions" class="ui-helper-hidden-accessible"></div></body></html>