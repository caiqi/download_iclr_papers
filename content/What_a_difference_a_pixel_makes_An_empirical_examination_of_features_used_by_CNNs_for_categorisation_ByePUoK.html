<!DOCTYPE html><html xmlns="http://www.w3.org/1999/xhtml" class="no-js"><head>
  <meta charset="utf-8" />
  <meta http-equiv="X-UA-Compatible" content="IE=edge,chrome=1" />
  <meta name="viewport" content="width=device-width, initial-scale=1" />

  <title>What a difference a pixel makes: An empirical examination of features used by CNNs for categorisation | OpenReview</title>
  <meta name="description" content="" />

      <meta name="citation_title" content="What a difference a pixel makes: An empirical examination of features used by CNNs for categorisation" />
        <meta name="citation_author" content="Anonymous" />
      <meta name="citation_publication_date" content="2018/09/27" />
      <meta name="citation_online_date" content="2018/09/27" />
      <meta name="citation_pdf_url" content="https://openreview.net/pdf?id=ByePUo05K7" />
      <meta name="twitter:card" content="summary" />
      <meta name="twitter:site" content="@openreviewnet" />
      <meta name="og:title" content="What a difference a pixel makes: An empirical examination of..." />
      <meta name="og:description" content="Convolutional neural networks (CNNs) were inspired by human vision and, in some settings, achieve a performance comparable to human object recognition. This has lead to the speculation that both..." />
      <meta name="og:image" content="https://openreview.net/static/images/openreview_logo_512.png" />

  <link rel="stylesheet" href="https://fonts.googleapis.com/css?family=Noto+Sans:400,400i,700,700i" />

  <link rel="stylesheet" href="/static/css/bootstrap.min.css" />
  <link rel="stylesheet" href="/static/css/jquery-ui.min.css" />
  <link rel="stylesheet" href="/static/css/main.min.css" />
</head>

<body class="forum">
  <nav class="navbar navbar-inverse navbar-fixed-top" role="navigation">
    <div class="container">
  
      <div class="navbar-header">
        <button type="button" class="navbar-toggle collapsed" data-toggle="collapse" data-target="#navbar" aria-expanded="false" aria-controls="navbar">
          <span class="sr-only">Toggle navigation</span>
          <span class="icon-bar"></span>
          <span class="icon-bar"></span>
          <span class="icon-bar"></span>
        </button>
        <a class="navbar-brand home push-link" href="/"><strong>OpenReview</strong>.net</a>
      </div>
  
      <div id="navbar" class="navbar-collapse collapse">
        <form class="navbar-form navbar-left profile-search" role="search">
          <div class="form-group has-feedback">
            <input id="search_input" type="text" class="form-control ui-autocomplete-input" placeholder="Search ICLR 2019 Conference" autocomplete="off" />
            <span class="glyphicon glyphicon-search form-control-feedback" aria-hidden="true"></span>
          </div>
  
          <input id="search_group" type="hidden" value="ICLR.cc/2019/Conference" />
          <input id="search_content" type="hidden" value="all" />
        <ul id="ui-id-1" tabindex="0" class="ui-menu ui-widget ui-widget-content ui-autocomplete ui-front" style="display: none;"></ul></form>
  
        <ul class="nav navbar-nav navbar-right">
        
            <li id="user-menu"><a href="/login">Login</a></li>
        </ul>
      </div>
  
    </div>
  </nav>

  <div id="or-banner" class="banner" style="">
  <div class="container">
    <div class="row">
      <div class="col-xs-12"><a href="/group?id=ICLR.cc/2019/Conference" title="Venue Homepage"><img class="icon" src="/static/images/arrow_left.svg" /> Go to <strong>ICLR 2019 Conference</strong> homepage</a></div>
    </div>
  </div>
</div>
<div id="flash-message-container" class="alert alert-danger" role="alert" style="display: none;">
  <div class="container">
    <div class="row">
      <div class="col-xs-12">
        <div class="alert-content">
          <button type="button" class="close" aria-label="Close"><span aria-hidden="true">×</span></button>
        </div>
      </div>
    </div>
  </div>
</div>

<div class="container">
  <div class="row">
    <div class="col-xs-12">
      <main id="content" class="clearfix openbanner-visible legacy-styles"><div id="note_ByePUo05K7" class="note panel"><div class="title_pdf_row clearfix"><h2 class="note_content_title"><a>What a difference a pixel makes: An empirical examination of features used by CNNs for categorisation</a> <a class="note_content_pdf" href="/pdf?id=ByePUo05K7" title="Download PDF" target="_blank"><img src="/static/images/pdf_icon_blue.svg" /></a> </h2></div><div class="meta_row"><span class="signatures">Anonymous</span></div><div class="clearfix"><div class="meta_row pull-left"><span class="date item">28 Sep 2018 (modified: 19 Nov 2018)</span><span class="item">ICLR 2019 Conference Blind Submission</span><span class="item">Readers: <span class="readers-icon glyphicon glyphicon-globe"></span> Everyone</span><span class="item"><a class="action-bibtex-modal" data-bibtex="@inproceedings{    &#10;anonymous2019what,    &#10;title={What a difference a pixel makes: An empirical examination of features used by CNNs for categorisation},    &#10;author={Anonymous},    &#10;booktitle={Submitted to International Conference on Learning Representations},    &#10;year={2019},    &#10;url={https://openreview.net/forum?id=ByePUo05K7},    &#10;note={under review}    &#10;}">Show Bibtex</a></span><a class="note_content_pdf item" href="/revisions?id=ByePUo05K7" target="_blank">Show Revisions</a></div><div class="meta_row meta_actions"></div></div><div class="note_contents"><span class="note_content_field">Abstract: </span><span class="note_content_value">Convolutional neural networks (CNNs) were inspired by human vision and, in some settings, achieve a performance comparable to human object recognition. This has lead to the speculation that both systems use similar mechanisms to perform recognition. In this study, we conducted a series of simulations that indicate that there is a fundamental difference between human vision and CNNs: while object recognition in humans relies on analysing shape, CNNs do not have such a shape-bias. We teased apart the type of features selected by the model by modifying the CIFAR-10 dataset so that, in addition to containing objects with shape, the images concurrently contained non-shape features, such as a noise-like mask. When trained on these modified set of images, the model did not show any bias towards selecting shapes as features. Instead it relied on whichever feature allowed it to perform the best prediction -- even when this feature was a noise-like mask or a single predictive pixel amongst 50176 pixels. We also found that regularisation methods, such as batch normalisation or Dropout, did not change this behaviour and neither did past or concurrent experience with images from other datasets.</span></div><div class="note_contents"><span class="note_content_field">Keywords: </span><span class="note_content_value">deep learning, shape bias, vision, feature selection</span></div><div class="note_contents"><span class="note_content_field">TL;DR: </span><span class="note_content_value">This study highlights a key difference between human vision and CNNs: while object recognition in humans relies on analysing shape, CNNs do not have such a shape-bias.</span></div><div class="reply_row clearfix"><div class="item" id="reply_count">12 Replies</div></div></div><hr class="small" /><div id="note_children"><div class="note_with_children"><div id="note_HkebvSNxAQ" class="note panel"><div class="title_pdf_row clearfix"><h2 class="note_content_title"><a>Response to All Reviewers (and the anonymous commenter)</a>  <button class="btn btn-xs btn-default permalink-button" title="Link to this comment" data-permalink-url="https://openreview.net/forum?id=ByePUo05K7&amp;noteId=HkebvSNxAQ"><span class="glyphicon glyphicon-link" aria-hidden="true"></span></button></h2></div><div class="meta_row"><span class="signatures">ICLR 2019 Conference Paper182 Authors</span></div><div class="clearfix"><div class="meta_row pull-left"><span class="date item">19 Nov 2018</span><span class="item">ICLR 2019 Conference Paper182 Official Comment</span><span class="item">Readers: <span class="readers-icon glyphicon glyphicon-globe"></span> Everyone</span></div><div class="meta_row meta_actions"></div></div><div class="note_contents"><span class="note_content_field">Comment: </span><span class="note_content_value">We would like to thank the reviewers for their positive comments and constructive feedback.  We have uploaded a revision in response to these points.  Here we respond to the most important concern shared by the reviewers, and then respond to the more specific comments under each review. 

The main concern is that we have altered the CIFAR images by introducing noise strongly correlated with the output categories. Accordingly, we have only shown that CNNs rely on non-shape cues under (unusual) conditions in which non-shape information is highly diagnostic of the categories. On this view, there is no reason to assume that CNNs trained on unperturbed images will rely on non-shape information, and thus no reason to conclude that CNNs ignore shape, nor claim that CNNs fail to show a shape bias. 

This concern reflects a misunderstanding.  When we conclude that CNNs do not have a shape bias we do not mean to imply that CNNs ignore shape.  Rather, we are claiming that CNNs do not have an inbuilt inductive bias to rely on shape and that they will use any sort of statistical regularities that can be exploited (shape or non-shape).  In our simulations we made non-shape features more diagnostic than shape, and accordingly, the CNNs used non-shape.   By contrast, there is a large literature in psychology that shows that humans do have an inductive bias to rely on shape when identifying objects, and accordingly, psychological theory predicts that humans would continue to rely heavily on shape even when non-shape features are more diagnostic of object category.  We are making a psychological claim that CNNs are importantly different than humans rather than claiming that CNNs ignore shape in general. 

Looking over the paper again we see that we were not very clear on this point.  Our revision now includes a modified introduction with a more detailed discussion of the shape bias in human object recognition.  We now emphasize that our main question is whether CNNs have a preference to categorize images according to shape (a shape bias, consistent with psychological theory), or whether CNNs are equally happy to rely on non-shape information for the sake of object identification (and use whatever is more diagnostic in a given dataset).  We think this is an interesting question given that CNNs were inspired by the visual cortex and given that some researchers claim that CNNs capture important properties of human vision (e.g., Rajalingham et al., 2018). In order to accommodate all of these changes and still stay within the page-limit, we have moved one of the experiments that was not central to the main story to Appendix C.  

Rajalingham, R., Issa, E. B., Bashivan, P., Kar, K., Schmidt, K., &amp; DiCarlo, J. J. (2018). Large-scale, high-resolution comparison of the core visual object recognition behavior of humans, monkeys, and state-of-the-art deep artificial neural networks. Journal of Neuroscience, 38(33), 7255-7269. </span></div><div class="reply_row clearfix"></div></div><div class="children"></div></div><div class="note_with_children"><div id="note_H1xUCxgEpm" class="note panel"><div class="title_pdf_row clearfix"><h2 class="note_content_title"><a>This paper addresses an important issue but fails to propose a solution</a>  <button class="btn btn-xs btn-default permalink-button" title="Link to this comment" data-permalink-url="https://openreview.net/forum?id=ByePUo05K7&amp;noteId=H1xUCxgEpm"><span class="glyphicon glyphicon-link" aria-hidden="true"></span></button></h2></div><div class="meta_row"><span class="signatures">ICLR 2019 Conference Paper182 AnonReviewer2</span></div><div class="clearfix"><div class="meta_row pull-left"><span class="date item">10 Nov 2018</span><span class="item">ICLR 2019 Conference Paper182 Official Review</span><span class="item">Readers: <span class="readers-icon glyphicon glyphicon-globe"></span> Everyone</span></div><div class="meta_row meta_actions"></div></div><div class="note_contents"><span class="note_content_field">Review: </span><span class="note_content_value">Humans leverage shape information to recognize objects. Shape prior information helps human object recognition ability to generalize well to different scenarios. This paper aims to highlight the fact that CNNs will not necessarily learn to recognize objects based on their shape. Authors modified training images by changing a value of a pixel where its location is correlated with object category or by adding noise-like (additive or Salt-and-pepper) masks to training images. Parameters of such noise-like masks are correlated with object category. In other words if one learns noise parameters or location of altered pixel for each object category, they can categorize all images in the training set. This paper shows that CNNs will overfeat to these noise based features and fail to correctly classify images at test time when these noise based features are changed or not added to the test images.  

Dataset bias is a very important factor in designing a dataset (Torralba et al,. 2011). Consider the case where we have a dataset of birds and cats. The task is image classification. All birds' images have the same background which is different than cats' background. As a result the network that is trained on these images will learn to categorize training images based on their background. Because extracting object based features such as shape of a bird and bird's texture is more difficult than extracting background features which is the same for all training images. 

Authors have carefully designed a set of experiments which shows CNNs will overfeat to non-shape features that they added to training images. However, this outcome is not surprising. Similar to dataset design example, if you add a noise pattern correlated with object categories to training images, you are adding a significant bias to your dataset. As a result networks that are trained on this dataset will overfeat to these noise patterns. Because it is easier to extract these noise parameters  than to extract object based features which are different for each image due to different viewpoints or illumination and so on. 

This paper would have been a stronger paper if authors had suggested mechanisms or solutions which could have reduced dataset bias or geared CNNs towards extracting shape like features.  

Antonio Torralba and Alexei A. Efros. Unbiased look at dataset bias. In Proceedings of the 2011 IEEE Conference on Computer Vision and Pattern Recognition (CVPR '11).</span></div><div class="note_contents"><span class="note_content_field">Rating: </span><span class="note_content_value">4: Ok but not good enough - rejection</span></div><div class="note_contents"><span class="note_content_field">Confidence: </span><span class="note_content_value">4: The reviewer is confident but not absolutely certain that the evaluation is correct</span></div><div class="reply_row clearfix"></div></div><div class="children"><div class="note_with_children"><div id="note_S1gzNLVxRm" class="note panel"><div class="title_pdf_row clearfix"><h2 class="note_content_title"><a>Response to review: clarification of contribution</a>  <button class="btn btn-xs btn-default permalink-button" title="Link to this comment" data-permalink-url="https://openreview.net/forum?id=ByePUo05K7&amp;noteId=S1gzNLVxRm"><span class="glyphicon glyphicon-link" aria-hidden="true"></span></button></h2></div><div class="meta_row"><span class="signatures">ICLR 2019 Conference Paper182 Authors</span></div><div class="clearfix"><div class="meta_row pull-left"><span class="date item">19 Nov 2018</span><span class="item">ICLR 2019 Conference Paper182 Official Comment</span><span class="item">Readers: <span class="readers-icon glyphicon glyphicon-globe"></span> Everyone</span></div><div class="meta_row meta_actions"></div></div><div class="note_contents"><span class="note_content_field">Comment: </span><span class="note_content_value">We thank the reviewer for taking time to read the paper and for their feedback. The reviewer’s key concern was that he/she did not find our results sufficiently novel or surprising as it is already well established that bias in datasets can lead to incorrect generalisation, as shown by Torralba &amp; Efros (2011). 
 
We agree that we are not the first to make this point.  Our contribution is to highlight the surprising extent to which non-shape features can drive performance, and indeed, even the slightest bias (even a single pixel) is enough for CNNs to ignore shape and rely on the diagnostic signal.  Also, unlike previous studies, we systematically manipulated the conditions in which non-shape information impacted performance:  We varied the type of non-shape noise, we varied the timing at which the noise was introduced (and found that the non-shape information overwrote shape information through catastrophic interference), manipulated the percentage of images in which the noise was embedded, and varied the form of regularization in order to see whether the effects are related to overfitting (we found no effect).   Most importantly, we manipulated the degree to which the noise biased the training set (e.g., we manipulate the fraction of images containing the noise mask and the variability in the mask from one image to another), and showed that CNNs are strongly impacted across all levels of noise (and types of noise).  This is important given that all image datasets undoubtedly included uncontrolled noise that is correlated with the output categories (as pointed by Torralba &amp; Efros).  Our findings highlight that this may have a larger impact on performance than previously assumed (and indeed, may help explain how single-pixel attacks can be successful).  
 
Furthermore, our findings with CNNs contrast with human visual perception where the extraction of shape occurs quickly and automatically and shape holds a privileged status compared to other diagnostic features, such as size, colour or texture. These features may allow humans to overcome biases present in their own environments. As Torralba &amp; Efros point out: “a human learns about vision by living in a reduced environment with many potential local biases and yet the visual system is robust enough to overcome this.” Being biased towards finding shape may be a way in which the visual system overcomes one type of dataset biases (ones due to non-shape features present within the environment), and our results show that this shape-bias is missing from CNNs. 
 
We disagree with Reviewer 1 that our findings are due to overfitting.  Rather than overfitting, we have shown that CNNs are happy to fit to non-shape data. This is why regularisation methods such as weight-decay, batch normalisation and dropout have no impact on the results (Figure 3). 
 
In light of the reviewer’s comments, we have revised the Introduction and Discussion to make these issues clear.   </span></div><div class="reply_row clearfix"></div></div><div class="children"></div></div></div></div><div class="note_with_children"><div id="note_H1ebFhuRnQ" class="note panel"><div class="title_pdf_row clearfix"><h2 class="note_content_title"><a>Shape-bias or shortcut-bias and catastrophic forgetting?</a>  <button class="btn btn-xs btn-default permalink-button" title="Link to this comment" data-permalink-url="https://openreview.net/forum?id=ByePUo05K7&amp;noteId=H1ebFhuRnQ"><span class="glyphicon glyphicon-link" aria-hidden="true"></span></button></h2></div><div class="meta_row"><span class="signatures">ICLR 2019 Conference Paper182 AnonReviewer1</span></div><div class="clearfix"><div class="meta_row pull-left"><span class="date item">06 Nov 2018 (modified: 07 Nov 2018)</span><span class="item">ICLR 2019 Conference Paper182 Official Review</span><span class="item">Readers: <span class="readers-icon glyphicon glyphicon-globe"></span> Everyone</span></div><div class="meta_row meta_actions"></div></div><div class="note_contents"><span class="note_content_field">Review: </span><span class="note_content_value">The paper seeks to establish via a series of well-designed experiments that CNNs trained for image classification differ in a fundamental way from human vision – they don’t encode shape-bias like human vision. Towards this goal, the authors modified the training data with ‘shortcut’ features to be functions of the category label using single diagnostic pixels and their placements, noise masks (salt and pepper, additive) and their parameters and demonstrate that image categorization CNNs learn whatever statistical features are there in the data most relevant to the learning task.

Investigation of the properties of neural architectures like CNNs and using the understanding thus developed to create better neural architectures, learning algorithms and training paradigms are good directions for the community and from that perspective, the direction explored in the paper is of great relevance and interest to the community. 

The paper presents careful experimentation to establish that image categorization CNNs learn the statistical features most relevant to the learning task. And, it seems to satisfactorily demonstrate this. It shows that such features could be single pixels, noise masks and even parameters of stochastic distributions which randomly produce these features, as long as the parameters are predictive of the image category. The experiments are well designed and they demonstrate this point quite well. They also demonstrate the well-known problem of catastrophic forgetting.

Nonetheless, there are significant drawbacks in the presented work:

1.	The experiments don't seem to effectively demonstrate the main claim of the paper that categorization CNNs do not have inductive shape bias (encode shape information). (Let’s make this claim more concrete: categorization CNNs when trained via supervised learning with paired training data of {(image, category_label)} do not have inductive shape bias.)

The best way to demonstrate this would have been to subject a trained image-categorization CNN to test data with object shapes in a way that the appearance information couldn’t be used to predict the object label. The paper doesn’t do this. None of the experiments logically imply that with an unaltered training regime, a trained network would not be predictive of the category label if shapes corresponding to that category are presented. 

2.	Due to the surprising results (especially the intensity of observed effects), we tried to reproduce some results from the paper in our lab and faced difficulties in doing so:

a.	We tried to replicate Figure 4(a) 'nopix' and 'same' cases on a standard setting (VGG-12-BN on CIFAR-10). The results deviated significantly (33%-72% margin) on ‘nopix’ case from the results reported in the paper on a much stronger setting (1/3072 pixels vs 1/50176 as in the paper). Please let me know any crucial settings (see below) that we might have missed.

Details: We used the vgg-cifar10 repository by chengyangfu. The only additions was fixing the pixel values while sending in the data. The code is anonymized and hosted here: <a href="https://file.io/qiziAK." target="_blank" rel="nofollow">https://file.io/qiziAK.</a> The pixel values in CIFAR-10 using the pytorch dataloader are between [-0.45, 0.45] theoretically, typically much smaller. We set the (0,0) RGB pixels categorically spacing it uniformly from [-0.25, 0.25), [-0.025, 0.025), [-0.0025, 0.0025) as a simple experiment. The third case did not suffer any decrease in the nopix case or any increase in the pix at all. The first case showed significant deviations from the claimed results with the no-pix resulting in ~43% accuracy which is 33% off vis-à-vis the results in the paper. The ‘same’ setting didn’t achieve 100% either though it got close - achieving 98.4%. 

Summary: The paper presents an important line of investigation to understand the properties of CNNs. However, it fails to effectively demonstrate its main claim. Further, we had difficulties in reproducing the results. As it stands, the submission is not of publishable quality.

I encourage the authors to do more careful experimentation to demonstrate their main claim and perhaps work on strategies to encourage CNNs to learn more meaningful features, including ‘shape’-features and submit to a future conference.
</span></div><div class="note_contents"><span class="note_content_field">Rating: </span><span class="note_content_value">3: Clear rejection</span></div><div class="note_contents"><span class="note_content_field">Confidence: </span><span class="note_content_value">4: The reviewer is confident but not absolutely certain that the evaluation is correct</span></div><div class="reply_row clearfix"></div></div><div class="children"><div class="note_with_children"><div id="note_Byeoli4gCQ" class="note panel"><div class="title_pdf_row clearfix"><h2 class="note_content_title"><a>Response to review: Replication</a>  <button class="btn btn-xs btn-default permalink-button" title="Link to this comment" data-permalink-url="https://openreview.net/forum?id=ByePUo05K7&amp;noteId=Byeoli4gCQ"><span class="glyphicon glyphicon-link" aria-hidden="true"></span></button></h2></div><div class="meta_row"><span class="signatures">ICLR 2019 Conference Paper182 Authors</span></div><div class="clearfix"><div class="meta_row pull-left"><span class="date item">19 Nov 2018</span><span class="item">ICLR 2019 Conference Paper182 Official Comment</span><span class="item">Readers: <span class="readers-icon glyphicon glyphicon-globe"></span> Everyone</span></div><div class="meta_row meta_actions"></div></div><div class="note_contents"><span class="note_content_field">Comment: </span><span class="note_content_value">We thank the reviewer for some really useful feedback and especially the effort put in to replicating our study. We are pleased that this reviewer found our findings sufficiently surprising that he/she ran a replication study based on Figure 4(a) and provided us a link to determine exactly what was done.  We were of course concerned to hear that our findings did not replicate.  However, it turns out that this is due to the way in which the dataset has been generated. When generated in the correct manner, our results indeed replicate using the code provided by the reviewer. The other major concern the reviewer had was whether our experiments correctly examine our main claim. We address both these concerns in detail below. 
 
[Replication] There are two crucial differences between our code and the reviewer’s in the way the pixel was inserted into any image. Firstly, we inserted the pixel at a different (x,y) location for each category. This made the pixel location diagnostic of the category of each image. In contrast, the reviewer inserted the pixel at the same location (0,0) for all categories, making the location of the pixel non-diagnostic. Secondly, the reviewer assumes that the pixel values in CIFAR-10 using the pytorch dataloader are between [-0.45, 0.45]. This is not the case in the code provided by the reviewer. Due to normalisation, these values are, in fact, approximately between [-2.5, 2.5]. Therefore, pixel values used by the reviewer to test the results ([-0.25, 0.25] / [-0.025, 0.025] / [-0.0025, 0.0025]) provide a very weak diagnostic signal to the network. 
 
To check whether these settings make a difference, we modified the code provided by the reviewer in two ways: (i) we picked the (x,y) location of the pixel for each category randomly from a uniform distribution [0, 32) (but kept it constant for all images within a category), and (ii) we picked each of the RGB values for the inserted pixel from a uniform distribution in the range [-2.0, 2.0), [-1.0, 1.0) or [-0.01, 0.01). Other than these changes the code provided by the reviewer remains the same (only changes are around lines 60-70 and then 175-180). When the pixel values were in the interval [-2.0, 2.0), the accuracy is ~100% in the Same condition and between 10-20% in the NoPix condition, which is very close to what we find. The small remaining difference could be due to a difference in learning algorithm used (we used RMSProp while the reviewer used SGD) or due to pretraining (we used a VGG-16 network pretrained on ImageNet while the reviewer used a VGG-11 network that was trained from scratch on the modified dataset). When pixel values are in the range [-1.0, 1.0) we again get an accuracy of ~100% in the Same condition, which drops to ~20% in the NoPix condition. Even when the inserted pixel provides a very weak diagnostic signal, with pixel values nearly at the mean [-0.01, 0.01), the network shows a large drop in performance from ~100% in the Same condition to ~50% in the NoPix condition, clearly demonstrating the reliance on this diagnostic pixel. Furthermore, it should also be noted that even under the conditions used by the reviewer, where all the categories had the pixel inserted at the same location and pixel values were grayscale and in the small range [-0.25, 0.25), performance dropped from 98% to 42% when the pixel was removed. It is inconceivable that this will happen for human participants and provides additional support for our observation that the model simply picks up whatever statistical structure is most relevant to learning the training set, with shape playing no special role.  
 
We have uploaded the modified code and log files here: <a href="http://s000.tinyupload.com/?file_id=24861367338244091333." target="_blank" rel="nofollow">http://s000.tinyupload.com/?file_id=24861367338244091333.</a> However, we do understand that some of these settings may not have been completely obvious in the previous version of the manuscript. To facilitate future replication, we have moved the description of some of these settings from the Appendix to the main text. Furthermore, in order to check if the results replicated on other networks, we have now run the key simulation using ResNet-101 and found that we get a similar pattern of results. We have updated the key figures (Figure 2 &amp; Figure 6) to show these results.</span></div><div class="reply_row clearfix"></div></div><div class="children"><div class="note_with_children"><div id="note_SJe3jCqg0m" class="note panel"><div class="title_pdf_row clearfix"><h2 class="note_content_title"><a>Replication verification</a>  <button class="btn btn-xs btn-default permalink-button" title="Link to this comment" data-permalink-url="https://openreview.net/forum?id=ByePUo05K7&amp;noteId=SJe3jCqg0m"><span class="glyphicon glyphicon-link" aria-hidden="true"></span></button></h2></div><div class="meta_row"><span class="signatures">ICLR 2019 Conference Paper182 AnonReviewer1</span></div><div class="clearfix"><div class="meta_row pull-left"><span class="date item">20 Nov 2018</span><span class="item">ICLR 2019 Conference Paper182 Official Comment</span><span class="item">Readers: <span class="readers-icon glyphicon glyphicon-globe"></span> Everyone</span></div><div class="meta_row meta_actions"></div></div><div class="note_contents"><span class="note_content_field">Comment: </span><span class="note_content_value">Unfortunately, we are not able to download the file -- we just get a download.php with 0 bytes. However, we took the values provided in the response above and are indeed able to verify that we are now able to reproduce the results in the paper (we did not test all experiments). 

Based on the above, our concerns regarding the replicability are met.  We thank the authors for the clarifications, and are satisfied with the corresponding changes made in the manuscript. </span></div><div class="reply_row clearfix"></div></div><div class="children"><div class="note_with_children"><div id="note_BJglwmOZA7" class="note panel"><div class="title_pdf_row clearfix"><h2 class="note_content_title"><a>New link</a>  <button class="btn btn-xs btn-default permalink-button" title="Link to this comment" data-permalink-url="https://openreview.net/forum?id=ByePUo05K7&amp;noteId=BJglwmOZA7"><span class="glyphicon glyphicon-link" aria-hidden="true"></span></button></h2></div><div class="meta_row"><span class="signatures">ICLR 2019 Conference Paper182 Authors</span></div><div class="clearfix"><div class="meta_row pull-left"><span class="date item">20 Nov 2018</span><span class="item">ICLR 2019 Conference Paper182 Official Comment</span><span class="item">Readers: <span class="readers-icon glyphicon glyphicon-globe"></span> Everyone</span></div><div class="meta_row meta_actions"></div></div><div class="note_contents"><span class="note_content_field">Comment: </span><span class="note_content_value">Thank you for the quick response. We are pleased to hear that you have been able to replicate the findings. Apologies about the problem with the link. In case you're curious, here's a new link that should work: <a href="https://ufile.io/9iw95" target="_blank" rel="nofollow">https://ufile.io/9iw95</a></span></div><div class="reply_row clearfix"></div></div><div class="children"></div></div></div></div><div class="note_with_children"><div id="note_BklELiNxCX" class="note panel"><div class="title_pdf_row clearfix"><h2 class="note_content_title"><a>Response to review: Claim</a>  <button class="btn btn-xs btn-default permalink-button" title="Link to this comment" data-permalink-url="https://openreview.net/forum?id=ByePUo05K7&amp;noteId=BklELiNxCX"><span class="glyphicon glyphicon-link" aria-hidden="true"></span></button></h2></div><div class="meta_row"><span class="signatures">ICLR 2019 Conference Paper182 Authors</span></div><div class="clearfix"><div class="meta_row pull-left"><span class="date item">19 Nov 2018</span><span class="item">ICLR 2019 Conference Paper182 Official Comment</span><span class="item">Readers: <span class="readers-icon glyphicon glyphicon-globe"></span> Everyone</span></div><div class="meta_row meta_actions"></div></div><div class="note_contents"><span class="note_content_field">Comment: </span><span class="note_content_value">[Claim] The reviewer was also concerned that we do not effectively demonstrate the main claim of the paper “that categorization CNNs do not have an inductive shape bias (encode shape information)”. In fact, this is not our claim. We agree that, given the right dataset, CNNs may indeed encode shape information. The reviewer is right in saying that our study does not demonstrate that CNNs do not encode shape information when trained on well-known datasets such as CIFAR-10 and ImageNet. If we wanted to test this, the experiment suggested by the reviewer would indeed make sense. However, our intention was to test a different hypothesis: that shape has a privileged status amongst features used to perform recognition. It is this privileged status of shape that is frequently referred to as "shape-bias" in psychological literature. There is a lot of evidence from psychological experiments showing that humans show a preference for using shape over other features such as size, colour or texture when any of these features are diagnostic of an object’s category. (We have added examples of these studies to the revised manuscript). We suspected that CNNs do not have such a shape bias, and in order to test how far this lack of shape bias could be pushed, we created a dataset where, in addition to shape, an almost imperceptible feature (a single pixel / an additive noise mask) was also diagnostic of an image’s category. Our experiments show that even for these nearly imperceptible features, the model did not show any preference for using shape. While this does not mean that CNNs do not use shape when trained on any given dataset, they do show that they tend to pick on whatever feature is more predictive, a behaviour that contrasts with human cognition. 
 
Given the reviewer’s comment, we can see that this aspect of the paper was not clearly laid out. Therefore, we have revised the manuscript, clarifying our claim and laying out the psychological literature that motivated our study at the outset. </span></div><div class="reply_row clearfix"></div></div><div class="children"></div></div></div></div></div></div><div class="note_with_children"><div id="note_BygClzecn7" class="note panel"><div class="title_pdf_row clearfix"><h2 class="note_content_title"><a>clever experiments with interesting implications</a>  <button class="btn btn-xs btn-default permalink-button" title="Link to this comment" data-permalink-url="https://openreview.net/forum?id=ByePUo05K7&amp;noteId=BygClzecn7"><span class="glyphicon glyphicon-link" aria-hidden="true"></span></button></h2></div><div class="meta_row"><span class="signatures">ICLR 2019 Conference Paper182 AnonReviewer3</span></div><div class="clearfix"><div class="meta_row pull-left"><span class="date item">02 Nov 2018 (modified: 07 Nov 2018)</span><span class="item">ICLR 2019 Conference Paper182 Official Review</span><span class="item">Readers: <span class="readers-icon glyphicon glyphicon-globe"></span> Everyone</span></div><div class="meta_row meta_actions"></div></div><div class="note_contents"><span class="note_content_field">Review: </span><span class="note_content_value">This paper adds to a growing body of literature which suggests that modern CNNs use qualitatively different visual strategies for object recognition compared to human observers. More specifically, the authors create shapeless object features (by adding noise masks in various forms or single pixels that are predictive of categorization to object images) to study how much CNNs rely on shape information (as humans would) as opposed to shapeless arbitrary statistical dependencies between pixels. 

The hypotheses tested are straightforward and the experiments cleverly answer these questions. On the negative side, there is nothing groundbreaking in this study. As acknowledged by the authors, the results are not all that novel in light of recent work that has already shown that one could conduct adversarial attacks by corrupting a single pixel as well as work that has shown that CNNs do not generalize to noise degradations they have not seen. Still, there is value in the work presented as the empirical tests described address the role of shape in object recognition with CNNs.

In a sense, the present study offers a null result and obviously, the work would have been much more significant had the authors offered a mechanism to get CNNs to learn to prioritize "shape" features (then verifying that such network would work on CIFAR, but performed poorly on the shapeless images).

Additional analysis involving visualization methods to further explain why shape features were ignored would have been a plus– with bonus points for providing a heuristic to determine the "shapelessness" of a convolution kernel.</span></div><div class="note_contents"><span class="note_content_field">Rating: </span><span class="note_content_value">7: Good paper, accept</span></div><div class="note_contents"><span class="note_content_field">Confidence: </span><span class="note_content_value">5: The reviewer is absolutely certain that the evaluation is correct and very familiar with the relevant literature</span></div><div class="reply_row clearfix"></div></div><div class="children"><div class="note_with_children"><div id="note_BkgGpsVlCX" class="note panel"><div class="title_pdf_row clearfix"><h2 class="note_content_title"><a>Response to review: novelty</a>  <button class="btn btn-xs btn-default permalink-button" title="Link to this comment" data-permalink-url="https://openreview.net/forum?id=ByePUo05K7&amp;noteId=BkgGpsVlCX"><span class="glyphicon glyphicon-link" aria-hidden="true"></span></button></h2></div><div class="meta_row"><span class="signatures">ICLR 2019 Conference Paper182 Authors</span></div><div class="clearfix"><div class="meta_row pull-left"><span class="date item">19 Nov 2018</span><span class="item">ICLR 2019 Conference Paper182 Official Comment</span><span class="item">Readers: <span class="readers-icon glyphicon glyphicon-globe"></span> Everyone</span></div><div class="meta_row meta_actions"></div></div><div class="note_contents"><span class="note_content_field">Comment: </span><span class="note_content_value">We thank the reviewer for his/her positive comments.  We agree with the reviewer that the results may not be ground-breaking as a demonstration of the limitations of CNNs, but as the reviewer correctly identifies, the key novelty of our work lies in contrasting how shape is treated very differently by humans and CNNs. In this sense, our study is more constructive than a typical adversarial attack, as it points in a direction where deep learning research could benefit from understanding the representations and processes underlying human vision. (Please see the revised Introduction, which highlights this contrast). 

Another contrast with adversarial studies (such as single pixel attacks) is that we manually insert non-shape features in the training set, rather than exploring susceptibilities of CNNs trained on well-known datasets. By creating these biases in the dataset, we were able to understand how such biases affect performance and the surprising extent to which non-shape features can drive performance (even in cases when such non-shape features would be nearly imperceptible to a human being). Also, our study is the first to systematically manipulate the conditions in which non-shape information impacted performance: We varied the type of non-shape noise, we varied the timing at which the noise was introduced (and found that the non-shape information overwrote shape information through catastrophic interference), manipulated the percentage of images in which the noise was embedded, and varied the form of regularization in order to see whether the effects are related to overfitting (we found no effect).  This is a very different approach than past papers that search for adversarial images, but our findings may help explain why some adversarial images are effective. 

We completely agree that it is an important question about how to make CNNs focus more on shape (that is, how to induce a shape bias in CNNs).  This might help make these models less susceptible to adversarial images, and, make the models more informative about human vision. </span></div><div class="reply_row clearfix"></div></div><div class="children"></div></div></div></div><div class="note_with_children"><div id="note_rkex459YhQ" class="note panel"><div class="title_pdf_row clearfix"><h2 class="note_content_title"><a>Question regarding interpretation</a>  <button class="btn btn-xs btn-default permalink-button" title="Link to this comment" data-permalink-url="https://openreview.net/forum?id=ByePUo05K7&amp;noteId=rkex459YhQ"><span class="glyphicon glyphicon-link" aria-hidden="true"></span></button></h2></div><div class="meta_row"><span class="signatures">(anonymous)</span></div><div class="clearfix"><div class="meta_row pull-left"><span class="date item">02 Nov 2018</span><span class="item">ICLR 2019 Conference Paper182 Public Comment</span><span class="item">Readers: <span class="readers-icon glyphicon glyphicon-globe"></span> Everyone</span></div><div class="meta_row meta_actions"></div></div><div class="note_contents"><span class="note_content_field">Comment: </span><span class="note_content_value">Thank you for your work, I have read it with great interest!

I have a question concerning the interpretation of your results. When CNNs are given the choice to learn shape features or a category-specific noise mask / pixel, you show that CNNs take the easy route and rely on the predictive noise pattern. You interpret this as a "fundamental difference" to human vision (which relies on shape); and contrast it to claims about human-like CNNs in the literature. These claims are usually being made about CNNs trained on normal, uncorrupted images (i.e., images without predictive noise patterns).

If you say that "CNNs do not have a shape bias", do you refer to CNNs in general (including CNNs trained under normal circumstances, i.e. on noise-free images) or only to CNNs trained with category-specific noise (as investigated in your work)? </span></div><div class="reply_row clearfix"></div></div><div class="children"><div class="note_with_children"><div id="note_Bked7pEg07" class="note panel"><div class="title_pdf_row clearfix"><h2 class="note_content_title"><a>Thank you for the important question</a>  <button class="btn btn-xs btn-default permalink-button" title="Link to this comment" data-permalink-url="https://openreview.net/forum?id=ByePUo05K7&amp;noteId=Bked7pEg07"><span class="glyphicon glyphicon-link" aria-hidden="true"></span></button></h2></div><div class="meta_row"><span class="signatures">ICLR 2019 Conference Paper182 Authors</span></div><div class="clearfix"><div class="meta_row pull-left"><span class="date item">19 Nov 2018</span><span class="item">ICLR 2019 Conference Paper182 Official Comment</span><span class="item">Readers: <span class="readers-icon glyphicon glyphicon-globe"></span> Everyone</span></div><div class="meta_row meta_actions"></div></div><div class="note_contents"><span class="note_content_field">Comment: </span><span class="note_content_value">Thank you for reading the manuscript and asking the question. This is indeed an important point and related to a common misunderstanding about our work. We do not make the claim that CNNs do not learn about shape under any circumstance. Rather, we claim that CNNs do not show a preference towards learning shape, given other diagnostic features, even when these features are noise-like masks or even single diagnostic features. This behaviour contrasts with human vision. We have clarified this point in a revision of the manuscript, where we have discussed the psychological literature related to this shape bias. Please see our common response to above for further details.</span></div><div class="reply_row clearfix"></div></div><div class="children"></div></div></div></div></div></main></div>
  </div>
</div>


    <!-- Footer -->
    <footer class="sitemap">
      <div class="container">
        <div class="row hidden-xs">
          <div class="col-sm-4">
            <ul class="list-unstyled">
              <li><a href="/" class="home">Home</a></li>
              <li><a href="/venues">All Venues</a></li>
            </ul>
          </div>
    
          <div class="col-sm-4">
            <ul class="list-unstyled">
              <li><a href="/about">About OpenReview</a></li>
              <li><a href="/contact">Contact</a></li>
              <li><a href="#" data-toggle="modal" data-target="#feedback-modal">Feedback</a></li>
            </ul>
          </div>
    
          <div class="col-sm-4">
            <ul class="list-unstyled">
              <li><a href="/terms">Terms of Service</a></li>
              <li><a href="/privacy">Privacy Policy</a></li>
            </ul>
          </div>
        </div>
    
        <div class="row visible-xs-block">
          <div class="col-xs-6">
            <ul class="list-unstyled">
              <li><a href="/" class="home">Home</a></li>
              <li><a href="/about">About OpenReview</a></li>
              <li><a href="#" data-toggle="modal" data-target="#feedback-modal">Feedback</a></li>
            </ul>
          </div>
    
          <div class="col-xs-6">
            <ul class="list-unstyled">
              <li><a href="/contact">Contact</a></li>
              <li><a href="/terms">Terms of Service</a></li>
              <li><a href="/privacy">Privacy Policy</a></li>
            </ul>
          </div>
        </div>
      </div>
    </footer>
    
    <footer class="sponsor">
      <div class="container">
        <div class="row">
          <div class="col-sm-10 col-sm-offset-1">
            <p class="text-center">
              OpenReview is created by the <a href="http://www.iesl.cs.umass.edu/" target="_blank">Information Extraction and Synthesis Laboratory</a>, College of Information and Computer Science, University of Massachusetts Amherst. We gratefully acknowledge the support of the OpenReview sponsors:  Google,  Facebook, NSF, the University of Massachusetts Amherst Center for Data Science, and Center for Intelligent Information Retrieval, as well as the Google Cloud Platform for donating the computing and networking services on which OpenReview.net runs.
            </p>
          </div>
        </div>
      </div>
    </footer>

  <div id="feedback-modal" class="modal fade" tabindex="-1" role="dialog">
    <div class="modal-dialog">
      <div class="modal-content">
        <div class="modal-header">
          <button type="button" class="close" data-dismiss="modal" aria-label="Close"><span aria-hidden="true">×</span></button>
          <h3 class="modal-title">Send Feedback</h3>
        </div>
        <div class="modal-body">
          <p>Enter your feedback below and we'll get back to you as soon as possible.</p>
          <form action="/feedback" method="POST">
            <div class="form-group">
              <input type="email" name="from" class="form-control" placeholder="Email" />
            </div>
            <div class="form-group">
              <input type="text" name="subject" class="form-control" placeholder="Subject" />
            </div>
            <div class="form-group">
              <textarea name="message" class="form-control feedback-input" rows="5" placeholder="Message" required=""></textarea>
            </div>
          <ul id="ui-id-2" tabindex="0" class="ui-menu ui-widget ui-widget-content" style="display: none;"></ul></form>
        </div>
        <div class="modal-footer">
          <button type="button" class="btn btn-default" data-dismiss="modal">Cancel</button>
          <button type="button" class="btn btn-primary">Send</button>
        </div>
      </div>
    </div>
  </div>

  <script type="text/javascript" async="" src="https://www.google-analytics.com/analytics.js"></script><script src="//cdnjs.cloudflare.com/ajax/libs/jquery/2.2.4/jquery.min.js"></script>
  <script>window.jQuery || document.write('&lt;script src="/static/js/vendor/jquery-2.2.4.min.js"&gt;&lt;\/script&gt;')</script>

  <script src="/static/dist/vendor.min.js"></script>
  <script src="/static/dist/templates.min.js"></script>
  <script src="/static/dist/openreview.min.js"></script>

    <script>window.legacyScripts = true;</script>


    <script async="" src="https://www.googletagmanager.com/gtag/js?id=UA-108703919-1"></script>
    <script>
      window.dataLayer = window.dataLayer || [];
      function gtag() { dataLayer.push(arguments); }
      gtag('js', new Date());
      gtag('config', 'UA-108703919-1');
    </script>


<div role="status" aria-live="assertive" aria-relevant="additions" class="ui-helper-hidden-accessible"></div></body></html>