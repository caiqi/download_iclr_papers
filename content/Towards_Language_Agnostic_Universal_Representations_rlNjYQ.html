<!DOCTYPE html><html xmlns="http://www.w3.org/1999/xhtml" class="no-js"><head>
  <meta charset="utf-8" />
  <meta http-equiv="X-UA-Compatible" content="IE=edge,chrome=1" />
  <meta name="viewport" content="width=device-width, initial-scale=1" />

  <title>Towards Language Agnostic Universal Representations | OpenReview</title>
  <meta name="description" content="" />

      <meta name="citation_title" content="Towards Language Agnostic Universal Representations" />
        <meta name="citation_author" content="Anonymous" />
      <meta name="citation_publication_date" content="2018/09/27" />
      <meta name="citation_online_date" content="2018/09/27" />
      <meta name="citation_pdf_url" content="https://openreview.net/pdf?id=r1l9Nj09YQ" />
      <meta name="twitter:card" content="summary" />
      <meta name="twitter:site" content="@openreviewnet" />
      <meta name="og:title" content="Towards Language Agnostic Universal Representations" />
      <meta name="og:description" content="When a bilingual student learns to solve word problems in math, we expect the student to be able to solve these problem in both languages the student is fluent in, even if the math lessons were..." />
      <meta name="og:image" content="https://openreview.net/static/images/openreview_logo_512.png" />

  <link rel="stylesheet" href="https://fonts.googleapis.com/css?family=Noto+Sans:400,400i,700,700i" />

  <link rel="stylesheet" href="/static/css/bootstrap.min.css" />
  <link rel="stylesheet" href="/static/css/jquery-ui.min.css" />
  <link rel="stylesheet" href="/static/css/main.min.css" />
</head>

<body class="forum">
  <nav class="navbar navbar-inverse navbar-fixed-top" role="navigation">
    <div class="container">
  
      <div class="navbar-header">
        <button type="button" class="navbar-toggle collapsed" data-toggle="collapse" data-target="#navbar" aria-expanded="false" aria-controls="navbar">
          <span class="sr-only">Toggle navigation</span>
          <span class="icon-bar"></span>
          <span class="icon-bar"></span>
          <span class="icon-bar"></span>
        </button>
        <a class="navbar-brand home push-link" href="/"><strong>OpenReview</strong>.net</a>
      </div>
  
      <div id="navbar" class="navbar-collapse collapse">
        <form class="navbar-form navbar-left profile-search" role="search">
          <div class="form-group has-feedback">
            <input id="search_input" type="text" class="form-control ui-autocomplete-input" placeholder="Search ICLR 2019 Conference" autocomplete="off" />
            <span class="glyphicon glyphicon-search form-control-feedback" aria-hidden="true"></span>
          </div>
  
          <input id="search_group" type="hidden" value="ICLR.cc/2019/Conference" />
          <input id="search_content" type="hidden" value="all" />
        <ul id="ui-id-1" tabindex="0" class="ui-menu ui-widget ui-widget-content ui-autocomplete ui-front" style="display: none;"></ul></form>
  
        <ul class="nav navbar-nav navbar-right">
        
            <li id="user-menu"><a href="/login">Login</a></li>
        </ul>
      </div>
  
    </div>
  </nav>

  <div id="or-banner" class="banner" style="">
  <div class="container">
    <div class="row">
      <div class="col-xs-12"><a href="/group?id=ICLR.cc/2019/Conference" title="Venue Homepage"><img class="icon" src="/static/images/arrow_left.svg" /> Go to <strong>ICLR 2019 Conference</strong> homepage</a></div>
    </div>
  </div>
</div>
<div id="flash-message-container" class="alert alert-danger" role="alert" style="display: none;">
  <div class="container">
    <div class="row">
      <div class="col-xs-12">
        <div class="alert-content">
          <button type="button" class="close" aria-label="Close"><span aria-hidden="true">×</span></button>
        </div>
      </div>
    </div>
  </div>
</div>

<div class="container">
  <div class="row">
    <div class="col-xs-12">
      <main id="content" class="clearfix openbanner-visible legacy-styles"><div id="note_r1l9Nj09YQ" class="note panel"><div class="title_pdf_row clearfix"><h2 class="note_content_title"><a>Towards Language Agnostic Universal Representations</a> <a class="note_content_pdf" href="/pdf?id=r1l9Nj09YQ" title="Download PDF" target="_blank"><img src="/static/images/pdf_icon_blue.svg" /></a> </h2></div><div class="meta_row"><span class="signatures">Anonymous</span></div><div class="clearfix"><div class="meta_row pull-left"><span class="date item">28 Sep 2018 (modified: 07 Nov 2018)</span><span class="item">ICLR 2019 Conference Blind Submission</span><span class="item">Readers: <span class="readers-icon glyphicon glyphicon-globe"></span> Everyone</span><span class="item"><a class="action-bibtex-modal" data-bibtex="@inproceedings{    &#10;anonymous2019towards,    &#10;title={Towards Language Agnostic Universal Representations},    &#10;author={Anonymous},    &#10;booktitle={Submitted to International Conference on Learning Representations},    &#10;year={2019},    &#10;url={https://openreview.net/forum?id=r1l9Nj09YQ},    &#10;note={under review}    &#10;}">Show Bibtex</a></span><a class="note_content_pdf item" href="/revisions?id=r1l9Nj09YQ" target="_blank">Show Revisions</a></div><div class="meta_row meta_actions"></div></div><div class="note_contents"><span class="note_content_field">Abstract: </span><span class="note_content_value">When a bilingual student learns to solve word problems in math, we expect the student to be able to solve these problem in both languages the student is fluent in, even if the math lessons were only taught in one language. However, current representations in machine learning are language dependent. In this work, we present a method to decouple the language from the problem by learning language agnostic representations and therefore allowing training a model in one language and applying to a different one in a zero shot fashion. We learn these representations by taking inspiration from linguistics and formalizing Universal Grammar as an optimization process (Chomsky, 2014; Montague, 1970). We demonstrate the capabilities of these representations by showing that the models trained on a single language using language agnostic representations achieve very similar accuracies in other languages.</span></div><div class="note_contents"><span class="note_content_field">Keywords: </span><span class="note_content_value">universal representations, language agnostic representations, NLP, GAN</span></div><div class="note_contents"><span class="note_content_field">TL;DR: </span><span class="note_content_value">By formalizing universal grammar as an optimization problem we learn language agnostic universal representations which we can utilize to do zero-shot learning across languages.</span></div><div class="reply_row clearfix"><div class="item" id="reply_count">8 Replies</div></div></div><hr class="small" /><div id="note_children"><div class="note_with_children"><div id="note_SkeVkPm5nm" class="note panel"><div class="title_pdf_row clearfix"><h2 class="note_content_title"><a>Very good work on learning language-agnostic embeddings but makes bold claims that are not verified</a>  <button class="btn btn-xs btn-default permalink-button" title="Link to this comment" data-permalink-url="https://openreview.net/forum?id=r1l9Nj09YQ&amp;noteId=SkeVkPm5nm"><span class="glyphicon glyphicon-link" aria-hidden="true"></span></button></h2></div><div class="meta_row"><span class="signatures">ICLR 2019 Conference Paper24 AnonReviewer1</span></div><div class="clearfix"><div class="meta_row pull-left"><span class="date item">03 Nov 2018 (modified: 07 Nov 2018)</span><span class="item">ICLR 2019 Conference Paper24 Official Review</span><span class="item">Readers: <span class="readers-icon glyphicon glyphicon-globe"></span> Everyone</span></div><div class="meta_row meta_actions"></div></div><div class="note_contents"><span class="note_content_field">Review: </span><span class="note_content_value">This paper starts with the bold aim of extracting Montague's universal grammar from multiple languages. In order to do so, the authors train multiple language models where each LM is explicitly factorized into language-specific and language-independent representations. The authors then apply the GAN framework to the language-independent parts to enforce all languages to share the same latent space. The claim here is that the language independent parameters capture the essence of universal grammar. The authors show that their framework enables effective zero-shot learning of tasks over new languages (for example sentiment classifier learned on top of English data generalize to Chinese when trained on the universal grammar embeddings). 

The paper is overall well written and the experimental results are convincing.

The gripe, however, I have is that this paper makes the claims that go too far without evaluating them. It is entirely sufficient to claim that you're trying to learn language agnostic parameters/embeddings --  I'd be happy with that. But the paper goes further and claims to be learning a form of universal grammar. To justify this claims, it is not sufficient to show in the experiments that the new representations do better at sentiment and NLI. The authors must show that this captures the "innate" language learning abilities akin to human babies. While the paper aims to do some analysis in the discussion section, it is not unsatisfactory. As the paper says in the discussion section "From a machine learning perspective, we’re interested in extracting informative features and not necessarily a completely grammatical language model. That being said it is of interest to what extent language models capture grammar and furthermore the extent to which models trained toward the universal grammar objective learn grammar."

The problem is that simply comparing LM perplexities is not a solid test of whether this model has learned some form of universal grammar. First, this paper does not define a clear falsifiable hypothesis on the proof of learning universal grammar. One example of testing for learning grammar can be: does this model learn basic syntactic rules of a new language (e.g. as the authors suggested -- head-first or head-final syntax, or rules of conjugation)  with a small amount of data after being training a universal representation with n languages? There have been a series of recent papers on checking if Language Models have appropriately learned syntax. See e.g. Tal Linzen's work <a href="https://arxiv.org/pdf/1809.04179.pdf." target="_blank" rel="nofollow">https://arxiv.org/pdf/1809.04179.pdf.</a> Just to be clear I am not suggesting citing works in unpublished places but potentially using some of the tests suggested in these papers.

In conclusion, I think this work is useful but I also think it makes really grandiloquent claims without verifying them. That to me is a dangerous precedent.</span></div><div class="note_contents"><span class="note_content_field">Rating: </span><span class="note_content_value">5: Marginally below acceptance threshold</span></div><div class="note_contents"><span class="note_content_field">Confidence: </span><span class="note_content_value">4: The reviewer is confident but not absolutely certain that the evaluation is correct</span></div><div class="reply_row clearfix"></div></div><div class="children"><div class="note_with_children"><div id="note_rJgRGOtk6X" class="note panel"><div class="title_pdf_row clearfix"><h2 class="note_content_title"><a>RE: ICLR 2019 Conference Paper24 AnonReviewer1</a>  <button class="btn btn-xs btn-default permalink-button" title="Link to this comment" data-permalink-url="https://openreview.net/forum?id=r1l9Nj09YQ&amp;noteId=rJgRGOtk6X"><span class="glyphicon glyphicon-link" aria-hidden="true"></span></button></h2></div><div class="meta_row"><span class="signatures">ICLR 2019 Conference Paper24 Authors</span></div><div class="clearfix"><div class="meta_row pull-left"><span class="date item">07 Nov 2018</span><span class="item">ICLR 2019 Conference Paper24 Official Comment</span><span class="item">Readers: <span class="readers-icon glyphicon glyphicon-globe"></span> Everyone</span></div><div class="meta_row meta_actions"></div></div><div class="note_contents"><span class="note_content_field">Comment: </span><span class="note_content_value">Thank you for your review. I'd like to clear up that the claim of our paper was only to learn language agnostic representations, our goal was not to comment on the Universal Grammar hypothesis, but simply to take inspiration from it. 

In our introduction we explicitly state: "Our attempt to learn these [language agnostic universal] representations begins by taking inspiration from linguistics and formalizing UG as an optimization problem". In our discussion we comment on the learnability of grammar (POS argument) and state that we are simply "interested in extracting informative features and not necessarily a completely grammatical language model". Furthermore in our abstract we state the novelty of our paper is "present[ing] a method to decouple the language from the problem by learning language agnostic representations and therefore allowing training a model in one language and applying to a different one in a zero shot fashion." I don't believe we explictly make any claims about the Universal Grammar hypothesis.

That being said, we would like to explicitly state in our paper that we are not making any claims about UG. Because of this we've included two statements, one in our introduction and the other in our discussion section explicitly stating that we are only taking inspiration from UG and not making any claims. 

The new addition in the last paragraph of our introduction states: "We do not make any claims about the Universal Grammar hypothesis, but simply take inspiration from it". The new addition in the first paragraph of the Discussion section states: "The goal of our paper is not to make a statement on the Universal Grammar hypothesis."

Thank you once again for you review. Please let me know if you have further concerns.
</span></div><div class="reply_row clearfix"></div></div><div class="children"><div class="note_with_children"><div id="note_Bkl4hCAhT7" class="note panel"><div class="title_pdf_row clearfix"><h2 class="note_content_title"><a>Thank you; a few more corrections</a>  <button class="btn btn-xs btn-default permalink-button" title="Link to this comment" data-permalink-url="https://openreview.net/forum?id=r1l9Nj09YQ&amp;noteId=Bkl4hCAhT7"><span class="glyphicon glyphicon-link" aria-hidden="true"></span></button></h2></div><div class="meta_row"><span class="signatures">ICLR 2019 Conference Paper24 AnonReviewer1</span></div><div class="clearfix"><div class="meta_row pull-left"><span class="date item">17 Nov 2018</span><span class="item">ICLR 2019 Conference Paper24 Official Comment</span><span class="item">Readers: <span class="readers-icon glyphicon glyphicon-globe"></span> Everyone</span></div><div class="meta_row meta_actions"></div></div><div class="note_contents"><span class="note_content_field">Comment: </span><span class="note_content_value">When you claim that you are formulating UG as an optimization problem, you're implicitly claiming you know how to convert UG into a mathematical form (which can then be solved.) If you claim you have formulated X as an optimization problem then it means two things:
1. You know enough about X to define it's objective and constraints.
2. If solve the optimization problem, the solution has the properties that you expect from X.

Since you do not do so in this paper where X=Universal Grammar, you should rephrase all the places where you claim you have formulated UG as an optimization problem.

E.g I'd rephrase the following lines in intro as 
"Our attempt to learn these representations begins by taking inspiration from linguistics and formalizing UG as an optimization problem. We do not make any claims about the Universal Grammar hypothesis, but simply take inspiration from it."
as 
"We take inspiration from the UG hypothesis and learn latent representations that are language agnostic. These representations allow us to solve NLP problems in new languages without any language-specific training data."

Similarly, you need to revise the rest of sections.
</span></div><div class="reply_row clearfix"></div></div><div class="children"></div></div></div></div></div></div><div class="note_with_children"><div id="note_ryetcXFYnX" class="note panel"><div class="title_pdf_row clearfix"><h2 class="note_content_title"><a>The writing is unclear...</a>  <button class="btn btn-xs btn-default permalink-button" title="Link to this comment" data-permalink-url="https://openreview.net/forum?id=r1l9Nj09YQ&amp;noteId=ryetcXFYnX"><span class="glyphicon glyphicon-link" aria-hidden="true"></span></button></h2></div><div class="meta_row"><span class="signatures">ICLR 2019 Conference Paper24 AnonReviewer2</span></div><div class="clearfix"><div class="meta_row pull-left"><span class="date item">02 Nov 2018 (modified: 07 Nov 2018)</span><span class="item">ICLR 2019 Conference Paper24 Official Review</span><span class="item">Readers: <span class="readers-icon glyphicon glyphicon-globe"></span> Everyone</span></div><div class="meta_row meta_actions"></div></div><div class="note_contents"><span class="note_content_field">Review: </span><span class="note_content_value">This paper proposes the idea of language agnostic representation which could potentially provide zero shot solution if the downstream task is trained using another language. The solution uses linguistic features from every sentence, trains language model for multiple languages simultaneously, and matches distribution by using Wasserstein distance measure. 

pros:
The motivation of this paper is clear. 
The method proposed looks reasonable. 
The experimental results also make sense. 

cons:

Key technical parts are not clear. The description of the training method is vague, e.g., the author(s) mentioned 'we utilized dropout and locked dropout where appropriate'. What does 'appropriate' mean? The training procedure was described in only few sentences. For example, it is not clear to me if a batch is fully random, or a batch consists of same number of sentences from each language, or a batch consists of same number of sentences from two languages, and how you train the WGAN. It is a bit surprising to me that different lambda gives similar performance. 

The writing of the paper is not clear. Here are some of the reasons:
1. The last paragraph in Section 2 does not fit into 'related work' section at all, instead, it is almost a repetition of the last paragraph in Section 1. 
2. The notations in Section 3 are very inconsistent. Just to name a few: the input dimension of function $e_j$ defined in the last paragraph in page 2 is not consistent with (1); the '$\circle$' operation in (1) is not explained (although I can guess what it means); the $j_alpha, j_beta$ are not consistent with the $j^{th}$ language; in the last equation in page 3, the summation should be from 1 to m (instead of 0 to m) if there are m languages, and the superscript in $w$ is not defined. 
3. Key references missing, for example: there is no reference when deriving (4) using the 'Kantarovich-Rubenstein' duality. 
4. The organization for section 4 is not clear. The first sentence is quite confusing, and the content is a mixture of architecture design, training details, and experimental settings. Instead, one should separate these contents and address each of them. 
5. At the beginning of section 5.1, the hypothesis in the sentence 'to test this hypothesis' actually refers to the last paragraph in section 4. Figure 4 should be referred to in the last paragraph in section 5. 'english', 'german', 'chinese' should be 'English', 'German', 'Chinese'. 
</span></div><div class="note_contents"><span class="note_content_field">Rating: </span><span class="note_content_value">4: Ok but not good enough - rejection</span></div><div class="note_contents"><span class="note_content_field">Confidence: </span><span class="note_content_value">4: The reviewer is confident but not absolutely certain that the evaluation is correct</span></div><div class="reply_row clearfix"></div></div><div class="children"><div class="note_with_children"><div id="note_ByeaDdt1TX" class="note panel"><div class="title_pdf_row clearfix"><h2 class="note_content_title"><a>RE: ICLR 2019 Conference Paper24 AnonReviewer2</a>  <button class="btn btn-xs btn-default permalink-button" title="Link to this comment" data-permalink-url="https://openreview.net/forum?id=r1l9Nj09YQ&amp;noteId=ByeaDdt1TX"><span class="glyphicon glyphicon-link" aria-hidden="true"></span></button></h2></div><div class="meta_row"><span class="signatures">ICLR 2019 Conference Paper24 Authors</span></div><div class="clearfix"><div class="meta_row pull-left"><span class="date item">07 Nov 2018</span><span class="item">ICLR 2019 Conference Paper24 Official Comment</span><span class="item">Readers: <span class="readers-icon glyphicon glyphicon-globe"></span> Everyone</span></div><div class="meta_row meta_actions"></div></div><div class="note_contents"><span class="note_content_field">Comment: </span><span class="note_content_value">Thank you for taking the time to read our paper. 

We've taken your comments on our writing to heart and have updated our draft taking all your points into account. Please let us know if you have further comments on our writing.

Thank you once again for you review.
</span></div><div class="reply_row clearfix"></div></div><div class="children"></div></div></div></div><div class="note_with_children"><div id="note_Bkx6E1yHnm" class="note panel"><div class="title_pdf_row clearfix"><h2 class="note_content_title"><a>Good method and results overall, with a few questions on analysis</a>  <button class="btn btn-xs btn-default permalink-button" title="Link to this comment" data-permalink-url="https://openreview.net/forum?id=r1l9Nj09YQ&amp;noteId=Bkx6E1yHnm"><span class="glyphicon glyphicon-link" aria-hidden="true"></span></button></h2></div><div class="meta_row"><span class="signatures">ICLR 2019 Conference Paper24 AnonReviewer3</span></div><div class="clearfix"><div class="meta_row pull-left"><span class="date item">30 Oct 2018 (modified: 07 Nov 2018)</span><span class="item">ICLR 2019 Conference Paper24 Official Review</span><span class="item">Readers: <span class="readers-icon glyphicon glyphicon-globe"></span> Everyone</span></div><div class="meta_row meta_actions"></div></div><div class="note_contents"><span class="note_content_field">Review: </span><span class="note_content_value">This paper introduced a GAN-based method to learn language universal representations without parallel data. The model architecture is analogous to an autoencoder. The encoder is a compound of language-universal mapper plus a language-specific LSTM. For decoding, another language-universal module first map language-universal representation back to language-specific embedding space, then another LSTM decoder generates the original sentence. The authors used GAN to encourage intermediate representation to be language-universal. The authors tested the proposed method on zero-shot semantic analysis and NLI tasks and showed nice results.

Overall the proposed method is novel and nice, and experiment results are good. On both tasks the proposed method performs better than NMT methods on target languages while still achieving competitive performance on source languages. The paper is also clearly written and could be useful for future research on multilingual transfer.

My main complaint is around Figure 5, Table 3, and the corresponding analysis.
1. In Figure 5, does it make more sense to show the perplexity of a standard LM. That is, train 7 independent LMs and report averaged perplexity. My concern is that, even with \lambda=0.0, the model still have modules u and h that are shared across languages, and therefore I'm not sure if it implies "representative power of UG-WGAN grows as we increase the number of languages". It could be that the language-universal impose more constraints to model all languages, so the two variation (\lambda=0.0 or 0.1) come closer to each other.

2. In Figure 3, the perplexity difference is huge when number of languages is 2. In Table 3, however, the authors show no fundamental differences between the English and Spanish language models. I feel the two arguments contradict to each other. Is it because of the language pairs are different? The authors should provide more explanation on that.

Minor:
1. Equation 1 and 2 in page 2. Are they both compound functions? Why the first one use \circ and the second one use parenthesis?</span></div><div class="note_contents"><span class="note_content_field">Rating: </span><span class="note_content_value">6: Marginally above acceptance threshold</span></div><div class="note_contents"><span class="note_content_field">Confidence: </span><span class="note_content_value">3: The reviewer is fairly confident that the evaluation is correct</span></div><div class="reply_row clearfix"></div></div><div class="children"><div class="note_with_children"><div id="note_Bkl9lYFypm" class="note panel"><div class="title_pdf_row clearfix"><h2 class="note_content_title"><a>RE: ICLR 2019 Conference Paper24 AnonReviewer3</a>  <button class="btn btn-xs btn-default permalink-button" title="Link to this comment" data-permalink-url="https://openreview.net/forum?id=r1l9Nj09YQ&amp;noteId=Bkl9lYFypm"><span class="glyphicon glyphicon-link" aria-hidden="true"></span></button></h2></div><div class="meta_row"><span class="signatures">ICLR 2019 Conference Paper24 Authors</span></div><div class="clearfix"><div class="meta_row pull-left"><span class="date item">07 Nov 2018</span><span class="item">ICLR 2019 Conference Paper24 Official Comment</span><span class="item">Readers: <span class="readers-icon glyphicon glyphicon-globe"></span> Everyone</span></div><div class="meta_row meta_actions"></div></div><div class="note_contents"><span class="note_content_field">Comment: </span><span class="note_content_value">Thank you for your detailed review. We'd like to comment on both of the points you've made.
              1. We've internally debated prior to posting the paper whether or not to show perplexities of a standard LM vs the jointly trained language model. We decided to display the jointly trained language model with \lambda=0.0 due to the fact that we wanted to control over representative capacity of our language models. If we had 7 independent LM's, we would have to choose hyper-parameters in such a way that representative capacity of all 7 summed up to one UG-WGAN (in order to make the comparison informative). This is a non-trivial task. Furthermore in ~Figure 3, in the t-SNE plot of \lambda=0.0 we see that even the universal portion of UG-WGAN becomes language specific without a distribution constraint, therefore we can think of UG-WGAN \lambda=0.0 with $n$ languages as $n$ different LM's with a shared constraint of representative power. We believe our statement "representative power of UG-WGAN grows as we increase the number of languages" is rather ambiguous. What we were trying to communicate is as we increase the number of languages the perplexity gap between constrained and unconstrained UG-WGAN (\lambda=0.0) decreases which implies while controlling capacity, our constrained (universal \lambda=0.1) language model, models language (almost) as well as jointly trained language models with no universal constraints (\lambda=0.0). We have updated our paper clarifying our ambiguity.
              2. This is a great point. One of the reasons that the perplexity difference is large in Figure 2 (which is the figure I think you're referring to, not 3) is because of the differences in language pairs, as you mentioned. But even so it's of interest to us that we see no fundamental difference between the English and Spanish language models. Our explanation of this follows as such. Language modeling attempts to first model syntax and then semantics. From a syntax perspective both the universal and standard models more or less learn the syntax, and can construct sentences which grammatically look and sound correct without having much semantic meaning. Semantic meaning is not necessary in order to construct syntactically correct sentences (i.e. autonomy of syntax hypothesis). We believe although there is no discernible difference between the sentences generated syntactically, the models with a smaller perplexity generate sentences which contain more of a semantic meaning. We have updated our paper to include this hypothesis.
              
We've also fixed the \circ issue in the recent draft.

Thank you once again for reading our paper and giving us a detailed reply. 
Looking forward to your reply
</span></div><div class="reply_row clearfix"></div></div><div class="children"><div class="note_with_children"><div id="note_rygaeXg6aQ" class="note panel"><div class="title_pdf_row clearfix"><h2 class="note_content_title"><a>Thank you; a suggestion on point 2, and I agreed with Reviewer1</a>  <button class="btn btn-xs btn-default permalink-button" title="Link to this comment" data-permalink-url="https://openreview.net/forum?id=r1l9Nj09YQ&amp;noteId=rygaeXg6aQ"><span class="glyphicon glyphicon-link" aria-hidden="true"></span></button></h2></div><div class="meta_row"><span class="signatures">ICLR 2019 Conference Paper24 AnonReviewer3</span></div><div class="clearfix"><div class="meta_row pull-left"><span class="date item">17 Nov 2018</span><span class="item">ICLR 2019 Conference Paper24 Official Comment</span><span class="item">Readers: <span class="readers-icon glyphicon glyphicon-globe"></span> Everyone</span></div><div class="meta_row meta_actions"></div></div><div class="note_contents"><span class="note_content_field">Comment: </span><span class="note_content_value">On point 2, I actually meant Figure 5 where you showed perplexity for 2-7 pairs. If I understand correctly, you picked English and Russian for x=2, and the perplexity difference was big -- sorry for the confusion.

Thanks for giving explanation, but I would like to see perplexity difference between Spanish and English as well, and I would expect they are not as big as that between Russian and English. It would be nice to have some more discussion on that as well.

Regarding Reviewer1's complaint, I agree that some sentences need to be rephrased to make the claim better calibrated with your experiments. As Reviewer1 mentioned, the claim of learning universal "grammar" may confuse the reader that the model is able to learn "basic syntactic rules", but actually the model is just learning "hidden representations" of languages. I'd like the authors to follow Reviewer1's suggestions and make changes.</span></div><div class="reply_row clearfix"></div></div><div class="children"></div></div></div></div></div></div></div></main></div>
  </div>
</div>


    <!-- Footer -->
    <footer class="sitemap">
      <div class="container">
        <div class="row hidden-xs">
          <div class="col-sm-4">
            <ul class="list-unstyled">
              <li><a href="/" class="home">Home</a></li>
              <li><a href="/venues">All Venues</a></li>
            </ul>
          </div>
    
          <div class="col-sm-4">
            <ul class="list-unstyled">
              <li><a href="/about">About OpenReview</a></li>
              <li><a href="/contact">Contact</a></li>
              <li><a href="#" data-toggle="modal" data-target="#feedback-modal">Feedback</a></li>
            </ul>
          </div>
    
          <div class="col-sm-4">
            <ul class="list-unstyled">
              <li><a href="/terms">Terms of Service</a></li>
              <li><a href="/privacy">Privacy Policy</a></li>
            </ul>
          </div>
        </div>
    
        <div class="row visible-xs-block">
          <div class="col-xs-6">
            <ul class="list-unstyled">
              <li><a href="/" class="home">Home</a></li>
              <li><a href="/about">About OpenReview</a></li>
              <li><a href="#" data-toggle="modal" data-target="#feedback-modal">Feedback</a></li>
            </ul>
          </div>
    
          <div class="col-xs-6">
            <ul class="list-unstyled">
              <li><a href="/contact">Contact</a></li>
              <li><a href="/terms">Terms of Service</a></li>
              <li><a href="/privacy">Privacy Policy</a></li>
            </ul>
          </div>
        </div>
      </div>
    </footer>
    
    <footer class="sponsor">
      <div class="container">
        <div class="row">
          <div class="col-sm-10 col-sm-offset-1">
            <p class="text-center">
              OpenReview is created by the <a href="http://www.iesl.cs.umass.edu/" target="_blank">Information Extraction and Synthesis Laboratory</a>, College of Information and Computer Science, University of Massachusetts Amherst. We gratefully acknowledge the support of the OpenReview sponsors:  Google,  Facebook, NSF, the University of Massachusetts Amherst Center for Data Science, and Center for Intelligent Information Retrieval, as well as the Google Cloud Platform for donating the computing and networking services on which OpenReview.net runs.
            </p>
          </div>
        </div>
      </div>
    </footer>

  <div id="feedback-modal" class="modal fade" tabindex="-1" role="dialog">
    <div class="modal-dialog">
      <div class="modal-content">
        <div class="modal-header">
          <button type="button" class="close" data-dismiss="modal" aria-label="Close"><span aria-hidden="true">×</span></button>
          <h3 class="modal-title">Send Feedback</h3>
        </div>
        <div class="modal-body">
          <p>Enter your feedback below and we'll get back to you as soon as possible.</p>
          <form action="/feedback" method="POST">
            <div class="form-group">
              <input type="email" name="from" class="form-control" placeholder="Email" />
            </div>
            <div class="form-group">
              <input type="text" name="subject" class="form-control" placeholder="Subject" />
            </div>
            <div class="form-group">
              <textarea name="message" class="form-control feedback-input" rows="5" placeholder="Message" required=""></textarea>
            </div>
          <ul id="ui-id-2" tabindex="0" class="ui-menu ui-widget ui-widget-content" style="display: none;"></ul></form>
        </div>
        <div class="modal-footer">
          <button type="button" class="btn btn-default" data-dismiss="modal">Cancel</button>
          <button type="button" class="btn btn-primary">Send</button>
        </div>
      </div>
    </div>
  </div>

  <script type="text/javascript" async="" src="https://www.google-analytics.com/analytics.js"></script><script src="//cdnjs.cloudflare.com/ajax/libs/jquery/2.2.4/jquery.min.js"></script>
  <script>window.jQuery || document.write('&lt;script src="/static/js/vendor/jquery-2.2.4.min.js"&gt;&lt;\/script&gt;')</script>

  <script src="/static/dist/vendor.min.js"></script>
  <script src="/static/dist/templates.min.js"></script>
  <script src="/static/dist/openreview.min.js"></script>

    <script>window.legacyScripts = true;</script>


    <script async="" src="https://www.googletagmanager.com/gtag/js?id=UA-108703919-1"></script>
    <script>
      window.dataLayer = window.dataLayer || [];
      function gtag() { dataLayer.push(arguments); }
      gtag('js', new Date());
      gtag('config', 'UA-108703919-1');
    </script>


<div role="status" aria-live="assertive" aria-relevant="additions" class="ui-helper-hidden-accessible"></div></body></html>