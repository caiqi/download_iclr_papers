<!DOCTYPE html><html xmlns="http://www.w3.org/1999/xhtml" class="no-js"><head>
  <meta charset="utf-8" />
  <meta http-equiv="X-UA-Compatible" content="IE=edge,chrome=1" />
  <meta name="viewport" content="width=device-width, initial-scale=1" />

  <title>Guided Evolutionary Strategies: Escaping the curse of dimensionality in random search | OpenReview</title>
  <meta name="description" content="" />

      <meta name="citation_title" content="Guided Evolutionary Strategies: Escaping the curse of dimensionality in random search" />
        <meta name="citation_author" content="Anonymous" />
      <meta name="citation_publication_date" content="2018/09/27" />
      <meta name="citation_online_date" content="2018/09/27" />
      <meta name="citation_pdf_url" content="https://openreview.net/pdf?id=B1xFxh0cKX" />
      <meta name="twitter:card" content="summary" />
      <meta name="twitter:site" content="@openreviewnet" />
      <meta name="og:title" content="Guided Evolutionary Strategies: Escaping the curse of..." />
      <meta name="og:description" content="Many applications in machine learning require optimizing a function whose true gradient is unknown, but where surrogate gradient information (directions that may be correlated with, but not..." />
      <meta name="og:image" content="https://openreview.net/static/images/openreview_logo_512.png" />

  <link rel="stylesheet" href="https://fonts.googleapis.com/css?family=Noto+Sans:400,400i,700,700i" />

  <link rel="stylesheet" href="/static/css/bootstrap.min.css" />
  <link rel="stylesheet" href="/static/css/jquery-ui.min.css" />
  <link rel="stylesheet" href="/static/css/main.min.css" />
</head>

<body class="forum">
  <nav class="navbar navbar-inverse navbar-fixed-top" role="navigation">
    <div class="container">
  
      <div class="navbar-header">
        <button type="button" class="navbar-toggle collapsed" data-toggle="collapse" data-target="#navbar" aria-expanded="false" aria-controls="navbar">
          <span class="sr-only">Toggle navigation</span>
          <span class="icon-bar"></span>
          <span class="icon-bar"></span>
          <span class="icon-bar"></span>
        </button>
        <a class="navbar-brand home push-link" href="/"><strong>OpenReview</strong>.net</a>
      </div>
  
      <div id="navbar" class="navbar-collapse collapse">
        <form class="navbar-form navbar-left profile-search" role="search">
          <div class="form-group has-feedback">
            <input id="search_input" type="text" class="form-control ui-autocomplete-input" placeholder="Search ICLR 2019 Conference" autocomplete="off" />
            <span class="glyphicon glyphicon-search form-control-feedback" aria-hidden="true"></span>
          </div>
  
          <input id="search_group" type="hidden" value="ICLR.cc/2019/Conference" />
          <input id="search_content" type="hidden" value="all" />
        <ul id="ui-id-1" tabindex="0" class="ui-menu ui-widget ui-widget-content ui-autocomplete ui-front" style="display: none;"></ul></form>
  
        <ul class="nav navbar-nav navbar-right">
        
            <li id="user-menu"><a href="/login">Login</a></li>
        </ul>
      </div>
  
    </div>
  </nav>

  <div id="or-banner" class="banner" style="">
  <div class="container">
    <div class="row">
      <div class="col-xs-12"><a href="/group?id=ICLR.cc/2019/Conference" title="Venue Homepage"><img class="icon" src="/static/images/arrow_left.svg" /> Go to <strong>ICLR 2019 Conference</strong> homepage</a></div>
    </div>
  </div>
</div>
<div id="flash-message-container" class="alert alert-danger" role="alert" style="display: none;">
  <div class="container">
    <div class="row">
      <div class="col-xs-12">
        <div class="alert-content">
          <button type="button" class="close" aria-label="Close"><span aria-hidden="true">×</span></button>
        </div>
      </div>
    </div>
  </div>
</div>

<div class="container">
  <div class="row">
    <div class="col-xs-12">
      <main id="content" class="clearfix openbanner-visible legacy-styles"><div id="note_B1xFxh0cKX" class="note panel"><div class="title_pdf_row clearfix"><h2 class="note_content_title"><a>Guided Evolutionary Strategies: Escaping the curse of dimensionality in random search</a> <a class="note_content_pdf" href="/pdf?id=B1xFxh0cKX" title="Download PDF" target="_blank"><img src="/static/images/pdf_icon_blue.svg" /></a> </h2></div><div class="meta_row"><span class="signatures">Anonymous</span></div><div class="clearfix"><div class="meta_row pull-left"><span class="date item">28 Sep 2018 (modified: 11 Oct 2018)</span><span class="item">ICLR 2019 Conference Blind Submission</span><span class="item">Readers: <span class="readers-icon glyphicon glyphicon-globe"></span> Everyone</span><span class="item"><a class="action-bibtex-modal" data-bibtex="@inproceedings{    &#10;anonymous2019guided,    &#10;title={Guided Evolutionary Strategies: Escaping the curse of dimensionality in random search},    &#10;author={Anonymous},    &#10;booktitle={Submitted to International Conference on Learning Representations},    &#10;year={2019},    &#10;url={https://openreview.net/forum?id=B1xFxh0cKX},    &#10;note={under review}    &#10;}">Show Bibtex</a></span></div><div class="meta_row meta_actions"></div></div><div class="note_contents"><span class="note_content_field">Abstract: </span><span class="note_content_value">Many applications in machine learning require optimizing a function whose true gradient is unknown, but where surrogate gradient information (directions that may be correlated with, but not necessarily identical to, the true gradient) is available instead. This arises when an approximate gradient is easier to compute than the full gradient (e.g. in meta-learning or unrolled optimization), or when a true gradient is intractable and is replaced with a surrogate (e.g. in certain reinforcement learning applications or training networks with discrete variables). We propose Guided Evolutionary Strategies, a method for optimally using surrogate gradient directions along with random search. We define a search distribution for evolutionary strategies that is elongated along a subspace spanned by the surrogate gradients. This allows us to estimate a descent direction which can then be passed to a first-order optimizer. We analytically and numerically characterize the tradeoffs that result from tuning how strongly the search distribution is stretched along the guiding subspace, and use this to derive a setting of the hyperparameters that works well across problems. Finally, we apply our method to example problems including truncated unrolled optimization and training neural networks with discrete variables, demonstrating improvement over both standard evolutionary strategies and first-order methods (that directly follow the surrogate gradient). We provide a demo of Guided ES at: redacted URL</span></div><div class="note_contents"><span class="note_content_field">Keywords: </span><span class="note_content_value">evolutionary strategies, optimization, gradient estimators, biased gradients</span></div><div class="note_contents"><span class="note_content_field">TL;DR: </span><span class="note_content_value">We propose an optimization method for when only biased gradients are available--we define a new gradient estimator for this scenario, derive the bias and variance of this estimator, and apply it to example problems.</span></div><div class="reply_row clearfix"><div class="item" id="reply_count">6 Replies</div></div></div><hr class="small" /><div id="note_children"><div class="note_with_children"><div id="note_Bke4RAO6nQ" class="note panel"><div class="title_pdf_row clearfix"><h2 class="note_content_title"><a>Good idea but its relation with a similar approach is overlooked and analysis is oversimplified</a>  <button class="btn btn-xs btn-default permalink-button" title="Link to this comment" data-permalink-url="https://openreview.net/forum?id=B1xFxh0cKX&amp;noteId=Bke4RAO6nQ"><span class="glyphicon glyphicon-link" aria-hidden="true"></span></button></h2></div><div class="meta_row"><span class="signatures">ICLR 2019 Conference Paper1094 AnonReviewer2</span></div><div class="clearfix"><div class="meta_row pull-left"><span class="date item">05 Nov 2018 (modified: 07 Nov 2018)</span><span class="item">ICLR 2019 Conference Paper1094 Official Review</span><span class="item">Readers: <span class="readers-icon glyphicon glyphicon-globe"></span> Everyone</span></div><div class="meta_row meta_actions"></div></div><div class="note_contents"><span class="note_content_field">Review: </span><span class="note_content_value">In this manuscript, the authors propose an approach that combines random search with the surrogate gradient information. To this end, the proposed method samples from the subspace of the surrogate gradients. This subspace is constructed by storing the previous surrogate gradients. After several assumptions, the authors also a give a discussion on variance-bias trade-off as well as a discussion on hyperparameter optimization. The manuscript ends with numerical experiments.

The proposed guided search seems similar to (stochastic) quasi-Newton methods. For instance the form in (2) is indeed a rank-one update of the gradient. What is authors take on this relationship?

The analysis assumes that the gradient exists. The proposed method is interesting when the gradients are not available. Therefore, it is not clear in what sense this analysis would apply to general functions.  The authors also assume that the second order Taylor expression is exact. Is this absolutely necessary? Would the analysis work when the function is approximated locally with its second order expansion? 

I guess the equation in (2) is satisfied irrespective of the distribution of the \epsilon_i vectors. If I am right, then what is the role of the particular distribution used for sampling from the subspace of surrogate gradients?

The authors state "ES has seen a resurgence in popularity in recent years (Salimans et al., 2017; Mania et al., 2018)." Both cited papers are not published in any conference or journal. Is there some recent but published work to support the statement?

</span></div><div class="note_contents"><span class="note_content_field">Rating: </span><span class="note_content_value">5: Marginally below acceptance threshold</span></div><div class="note_contents"><span class="note_content_field">Confidence: </span><span class="note_content_value">3: The reviewer is fairly confident that the evaluation is correct</span></div><div class="reply_row clearfix"></div></div><div class="children"><div class="note_with_children"><div id="note_ryx-zWDiaX" class="note panel"><div class="title_pdf_row clearfix"><h2 class="note_content_title"><a>Thank you for the review. Responses below:</a>  <button class="btn btn-xs btn-default permalink-button" title="Link to this comment" data-permalink-url="https://openreview.net/forum?id=B1xFxh0cKX&amp;noteId=ryx-zWDiaX"><span class="glyphicon glyphicon-link" aria-hidden="true"></span></button></h2></div><div class="meta_row"><span class="signatures">ICLR 2019 Conference Paper1094 Authors</span></div><div class="clearfix"><div class="meta_row pull-left"><span class="date item">16 Nov 2018</span><span class="item">ICLR 2019 Conference Paper1094 Official Comment</span><span class="item">Readers: <span class="readers-icon glyphicon glyphicon-globe"></span> Everyone</span></div><div class="meta_row meta_actions"></div></div><div class="note_contents"><span class="note_content_field">Comment: </span><span class="note_content_value">Thank you for the review. Responses below:

“The proposed guided search seems similar to (stochastic) quasi-Newton methods. For instance the form in (2) is indeed a rank-one update of the gradient. What is authors take on this relationship?”
Quasi-Newton methods assume access to first-order information about the objective, whereas our focus is on black-box optimization. Critically, we do not assume that the “surrogate” gradient information we are provided is reliable. Providing a way to robustly use this information when it is useful and to discard it when it is not is our primary contribution.

Our update is indeed inspired by quasi-Newton methods, and we now note this in the maintext. In particular, as the author notes, we adapt the search covariance with a history of k past gradient estimates similar to how the approximate inverse Hessian is updated according to the past k gradient evaluations in L-BFGS. However, we are not trying to approximate the inverse Hessian. In our application, we are updating the covariance of the distribution used to perturb parameters.

“The analysis assumes that the gradient exists. The proposed method is interesting when the gradients are not available. Therefore, it is not clear in what sense this analysis would apply to general functions.  The authors also assume that the second order Taylor expression is exact. Is this absolutely necessary? Would the analysis work when the function is approximated locally with its second order expansion?“
These assumptions were made largely to simplify the presentation of the bias-variance analysis.  In the deep learning applications we focused on, the gradient of most loss functions exist, however, they may not be tractable (due to intractable integrals over nuisance variables) or may not be useful (e.g., the gradient of a hard thresholding function is 0). We dropped the higher order Taylor remainder to declutter the exposition, however, the analysis still holds (up to higher order error terms) when the function is locally approximated to 2nd order around each iterate. 

“I guess the equation in (2) is satisfied irrespective of the distribution of the \epsilon_i vectors. If I am right, then what is the role of the particular distribution used for sampling from the subspace of surrogate gradients?”
Correct, the equation in (2) does not depend on the particular distribution for \epsilon. We chose a Gaussian distribution because it is: simple, easy to sample from, and has bounded variance. This is also consistent with previous work on evolutionary strategies (CMA-ES and NES). It would be interesting to, in future work, explore different choices for this distribution either analytically or empirically.

“The authors state "ES has seen a resurgence in popularity in recent years (Salimans et al., 2017; Mania et al., 2018)." Both cited papers are not published in any conference or journal. Is there some recent but published work to support the statement?”
We have added additional published citations [1, 2, 3, 4] all of which use evolutionary strategies in concert with neural networks.

We would appreciate if the reviewer could expand on their justification for the given score, or consider revising it in the context of our responses here.

[1] Cui et al. Evolutionary Stochastic Gradient Descent for Optimization of Deep Neural Networks (NIPS 2018)
[2] Houthooft et. al. Evolved Policy Gradients (NIPS 2018)
[3] Ha and Schmidhuber. World Models. (NIPS 2018)
[4] Ha, David. Neuroevolution for deep reinforcement learning problems. (GECCO 2018)</span></div><div class="reply_row clearfix"></div></div><div class="children"></div></div></div></div><div class="note_with_children"><div id="note_rkxvkSUTnX" class="note panel"><div class="title_pdf_row clearfix"><h2 class="note_content_title"><a>Improve random search by building a subspace of the previous surrogate gradients for derivative-free optimization; good results but paper lacks clarity and is quite hard to follow.</a>  <button class="btn btn-xs btn-default permalink-button" title="Link to this comment" data-permalink-url="https://openreview.net/forum?id=B1xFxh0cKX&amp;noteId=rkxvkSUTnX"><span class="glyphicon glyphicon-link" aria-hidden="true"></span></button></h2></div><div class="meta_row"><span class="signatures">ICLR 2019 Conference Paper1094 AnonReviewer1</span></div><div class="clearfix"><div class="meta_row pull-left"><span class="date item">05 Nov 2018 (modified: 07 Nov 2018)</span><span class="item">ICLR 2019 Conference Paper1094 Official Review</span><span class="item">Readers: <span class="readers-icon glyphicon glyphicon-globe"></span> Everyone</span></div><div class="meta_row meta_actions"></div></div><div class="note_contents"><span class="note_content_field">Review: </span><span class="note_content_value">Summary: The paper proposes a method to improve random search by building a subspace of the previous k surrogate gradients, mixing it with an isotropic Gaussian distribution to improve the search. Results reported shows are good compared to other approaches for learning weights of neural networks. However, paper lacks clarity and is quite hard to follow.

Quality: The paper presents a well-designed approach that is able to deal with optimization in high dimensionality space, by building a lower order surrogate model build upon the previous gradients computed with this surrogate model. The analysis appears to be correct and provide credential to the approach. Results reported are very good. However, testing is relatively limited to few cases. More experimental results on a good set of problems with several methods would have made the paper stronger and more convincing.

Clarity: The paper is hard to follow. Maybe because I am not completely familiar with the topic, but many elements presented lacks some context. The authors appear clearly to be knowledgeable of their topics, but lacks the capacity to provide all required background to follow their thoughts. The method could have been better illustrated, I found that Fig. 1a not enough to explain the method, while Fig. 1b and other training curves not useful to understand the approach. Some pseudo-code to illustrate the use of the proposed method might certainly help to improve clarity. Sec. 3.2 is not enough to understand well the approach.

Originality: The approach is allowing a nice trade-off between pure random search and guide search through a surrogate model over a subspace of limited dimensionality. This is in-line of some work on the use of ES for training neural networks, but I am not aware of other similar work although I am not super knowledgeable of the field. 

Significance: The approach can have its impact for optimizing deep networks with no gradient, but more exhaustive experimental testing would be required.

Pros and cons:
+ Sound approach
+ Good theoretical support of the approach (bias-variance analysis)
+ Great results reported
+ Of importance for optimizing without gradients
- Presentation of the method lacking many details and not very clear
- Overall quality of the paper is subpar, tend to be very textual and hard to follow in several parts
- Experiments are not exhaustive and detailed. Loss plots are provided for some methods compared. Looks more like a preliminary validation. 

I think that if the paper can be rewritten to be more tight, clearer in its presentation, with figures and pseudo-code to illustrate the method better, with more exhaustive testing, it can be really great. Current, the method appears to be great, but the writing quality of the paper is not yet there.
</span></div><div class="note_contents"><span class="note_content_field">Rating: </span><span class="note_content_value">4: Ok but not good enough - rejection</span></div><div class="note_contents"><span class="note_content_field">Confidence: </span><span class="note_content_value">3: The reviewer is fairly confident that the evaluation is correct</span></div><div class="reply_row clearfix"></div></div><div class="children"><div class="note_with_children"><div id="note_S1gvmZPsTQ" class="note panel"><div class="title_pdf_row clearfix"><h2 class="note_content_title"><a>Thank you for your review. Comments below:</a>  <button class="btn btn-xs btn-default permalink-button" title="Link to this comment" data-permalink-url="https://openreview.net/forum?id=B1xFxh0cKX&amp;noteId=S1gvmZPsTQ"><span class="glyphicon glyphicon-link" aria-hidden="true"></span></button></h2></div><div class="meta_row"><span class="signatures">ICLR 2019 Conference Paper1094 Authors</span></div><div class="clearfix"><div class="meta_row pull-left"><span class="date item">16 Nov 2018</span><span class="item">ICLR 2019 Conference Paper1094 Official Comment</span><span class="item">Readers: <span class="readers-icon glyphicon glyphicon-globe"></span> Everyone</span></div><div class="meta_row meta_actions"></div></div><div class="note_contents"><span class="note_content_field">Comment: </span><span class="note_content_value">Thank you for your review. Comments below:

Regarding clarity, we appreciate the reviewer’s comments and have added more context throughout the paper. We have added pseudocode in the main text, and moved more of the bias-variance derivation to the appendix (to make room for more exposition of the method in 3.2).

In addition, we would appreciate it if the reviewer could elaborate on which aspects of the paper they thought could use more context. Specifically, if Fig1a is insufficient to explain the method, what additional information would the reviewer have appreciated?

Regarding experiments, we are running more baseline comparisons against other adaptive evolutionary strategy methods (CMA-ES and natural evolutionary strategies, or NES). If the reviewer has other specific suggestions as to what would make the experiments more exhaustive, we would appreciate them.
</span></div><div class="reply_row clearfix"></div></div><div class="children"></div></div></div></div><div class="note_with_children"><div id="note_rJeWFl_Y37" class="note panel"><div class="title_pdf_row clearfix"><h2 class="note_content_title"><a>Interesting idea, but not really new</a>  <button class="btn btn-xs btn-default permalink-button" title="Link to this comment" data-permalink-url="https://openreview.net/forum?id=B1xFxh0cKX&amp;noteId=rJeWFl_Y37"><span class="glyphicon glyphicon-link" aria-hidden="true"></span></button></h2></div><div class="meta_row"><span class="signatures">ICLR 2019 Conference Paper1094 AnonReviewer3</span></div><div class="clearfix"><div class="meta_row pull-left"><span class="date item">02 Nov 2018 (modified: 07 Nov 2018)</span><span class="item">ICLR 2019 Conference Paper1094 Official Review</span><span class="item">Readers: <span class="readers-icon glyphicon glyphicon-globe"></span> Everyone</span></div><div class="meta_row meta_actions"></div></div><div class="note_contents"><span class="note_content_field">Review: </span><span class="note_content_value">The idea of this paper is to accelerate the OpenAI type evolution strategy by introducing
1. non-isotropic distribution, where the covariance matrix is of form I + UU^t; and
2. external information such as a surrogate gradient to determine U.
The experiments show promising results. I think it is the right direction to go, however at the same time, these ideas are not really new. 

The first point is well studied in the context of evolution strategies in e.g., (Sun et al., arxiv 2011), (Loshchilov, Evolutionary Computation 2015), (Akimoto et al, GECCO 2016). They all have covariance matrix of form I + UU^t or a bit richer. There are mainly two advantages over full CMA-ES or NES: 1) computationally cheap, and 2) faster adaptation of the covariance matrix. The current paper does not adapt the covariance matrix and use an external information to guide the distribution. Therefore, it is different from the above work, but I suggest to compare Guided ES with these methods to see the effect of external information purely.  

The second point, using external information to change the distribution shape, is also investigated in reference (Hansen, INRIA TechRep 2011), where external information such as a good point or a good directions (gradient) is injected in order to adapt the covariance matrix. 
</span></div><div class="note_contents"><span class="note_content_field">Rating: </span><span class="note_content_value">6: Marginally above acceptance threshold</span></div><div class="note_contents"><span class="note_content_field">Confidence: </span><span class="note_content_value">4: The reviewer is confident but not absolutely certain that the evaluation is correct</span></div><div class="reply_row clearfix"></div></div><div class="children"><div class="note_with_children"><div id="note_HkesDWvop7" class="note panel"><div class="title_pdf_row clearfix"><h2 class="note_content_title"><a>Thank you for your review. Comments below:</a>  <button class="btn btn-xs btn-default permalink-button" title="Link to this comment" data-permalink-url="https://openreview.net/forum?id=B1xFxh0cKX&amp;noteId=HkesDWvop7"><span class="glyphicon glyphicon-link" aria-hidden="true"></span></button></h2></div><div class="meta_row"><span class="signatures">ICLR 2019 Conference Paper1094 Authors</span></div><div class="clearfix"><div class="meta_row pull-left"><span class="date item">16 Nov 2018</span><span class="item">ICLR 2019 Conference Paper1094 Official Comment</span><span class="item">Readers: <span class="readers-icon glyphicon glyphicon-globe"></span> Everyone</span></div><div class="meta_row meta_actions"></div></div><div class="note_contents"><span class="note_content_field">Comment: </span><span class="note_content_value">Thank you for your review. Comments below:

Regarding the first point, we agree that comparisons against other adaptive ES methods (CMA-ES and NES) would be very useful, and are currently performing these comparisons. We will update the paper with these results. However, there is a fundamental limitation with CMA-ES and NES in that they are both purely black box (no gradient information) optimizers. This can be especially seen in the current Figure 1 in the paper, where CMA-ES is not able to take advantage of the initial external gradient information available to Guided ES or SGD, thus fails to make quick progress on the problem--it does begin to accelerate past standard ES as the covariance begins to adapt. NES will have the same issue. Again, we are running experiments to confirm this intuition.

Regarding the second point, we carefully read through the Hansen 2011 reference again and could find no reference on using external gradient information to adapt the covariance matrix. As far as we can tell, Hansen 2011 focuses purely on adapting the covariance using information from iterates encountered during the optimization trajectory. To the best of our knowledge, our work is the first to propose incorporating surrogate” gradient information into an ES algorithm, as well as to analyze the bias and variance of the resulting gradient estimate.</span></div><div class="reply_row clearfix"></div></div><div class="children"></div></div></div></div></div></main></div>
  </div>
</div>


    <!-- Footer -->
    <footer class="sitemap">
      <div class="container">
        <div class="row hidden-xs">
          <div class="col-sm-4">
            <ul class="list-unstyled">
              <li><a href="/" class="home">Home</a></li>
              <li><a href="/venues">All Venues</a></li>
            </ul>
          </div>
    
          <div class="col-sm-4">
            <ul class="list-unstyled">
              <li><a href="/about">About OpenReview</a></li>
              <li><a href="/contact">Contact</a></li>
              <li><a href="#" data-toggle="modal" data-target="#feedback-modal">Feedback</a></li>
            </ul>
          </div>
    
          <div class="col-sm-4">
            <ul class="list-unstyled">
              <li><a href="/terms">Terms of Service</a></li>
              <li><a href="/privacy">Privacy Policy</a></li>
            </ul>
          </div>
        </div>
    
        <div class="row visible-xs-block">
          <div class="col-xs-6">
            <ul class="list-unstyled">
              <li><a href="/" class="home">Home</a></li>
              <li><a href="/about">About OpenReview</a></li>
              <li><a href="#" data-toggle="modal" data-target="#feedback-modal">Feedback</a></li>
            </ul>
          </div>
    
          <div class="col-xs-6">
            <ul class="list-unstyled">
              <li><a href="/contact">Contact</a></li>
              <li><a href="/terms">Terms of Service</a></li>
              <li><a href="/privacy">Privacy Policy</a></li>
            </ul>
          </div>
        </div>
      </div>
    </footer>
    
    <footer class="sponsor">
      <div class="container">
        <div class="row">
          <div class="col-sm-10 col-sm-offset-1">
            <p class="text-center">
              OpenReview is created by the <a href="http://www.iesl.cs.umass.edu/" target="_blank">Information Extraction and Synthesis Laboratory</a>, College of Information and Computer Science, University of Massachusetts Amherst. We gratefully acknowledge the support of the OpenReview sponsors:  Google,  Facebook, NSF, the University of Massachusetts Amherst Center for Data Science, and Center for Intelligent Information Retrieval, as well as the Google Cloud Platform for donating the computing and networking services on which OpenReview.net runs.
            </p>
          </div>
        </div>
      </div>
    </footer>

  <div id="feedback-modal" class="modal fade" tabindex="-1" role="dialog">
    <div class="modal-dialog">
      <div class="modal-content">
        <div class="modal-header">
          <button type="button" class="close" data-dismiss="modal" aria-label="Close"><span aria-hidden="true">×</span></button>
          <h3 class="modal-title">Send Feedback</h3>
        </div>
        <div class="modal-body">
          <p>Enter your feedback below and we'll get back to you as soon as possible.</p>
          <form action="/feedback" method="POST">
            <div class="form-group">
              <input type="email" name="from" class="form-control" placeholder="Email" />
            </div>
            <div class="form-group">
              <input type="text" name="subject" class="form-control" placeholder="Subject" />
            </div>
            <div class="form-group">
              <textarea name="message" class="form-control feedback-input" rows="5" placeholder="Message" required=""></textarea>
            </div>
          <ul id="ui-id-2" tabindex="0" class="ui-menu ui-widget ui-widget-content" style="display: none;"></ul></form>
        </div>
        <div class="modal-footer">
          <button type="button" class="btn btn-default" data-dismiss="modal">Cancel</button>
          <button type="button" class="btn btn-primary">Send</button>
        </div>
      </div>
    </div>
  </div>

  <script type="text/javascript" async="" src="https://www.google-analytics.com/analytics.js"></script><script src="//cdnjs.cloudflare.com/ajax/libs/jquery/2.2.4/jquery.min.js"></script>
  <script>window.jQuery || document.write('&lt;script src="/static/js/vendor/jquery-2.2.4.min.js"&gt;&lt;\/script&gt;')</script>

  <script src="/static/dist/vendor.min.js"></script>
  <script src="/static/dist/templates.min.js"></script>
  <script src="/static/dist/openreview.min.js"></script>

    <script>window.legacyScripts = true;</script>


    <script async="" src="https://www.googletagmanager.com/gtag/js?id=UA-108703919-1"></script>
    <script>
      window.dataLayer = window.dataLayer || [];
      function gtag() { dataLayer.push(arguments); }
      gtag('js', new Date());
      gtag('config', 'UA-108703919-1');
    </script>


<div role="status" aria-live="assertive" aria-relevant="additions" class="ui-helper-hidden-accessible"></div></body></html>