<!DOCTYPE html><html xmlns="http://www.w3.org/1999/xhtml" class="no-js"><head>
  <meta charset="utf-8" />
  <meta http-equiv="X-UA-Compatible" content="IE=edge,chrome=1" />
  <meta name="viewport" content="width=device-width, initial-scale=1" />

  <title>Deep Reinforcement Learning of Universal Policies with Diverse Environment Summaries | OpenReview</title>
  <meta name="description" content="" />

      <meta name="citation_title" content="Deep Reinforcement Learning of Universal Policies with Diverse Environment Summaries" />
        <meta name="citation_author" content="Anonymous" />
      <meta name="citation_publication_date" content="2018/09/27" />
      <meta name="citation_online_date" content="2018/09/27" />
      <meta name="citation_pdf_url" content="https://openreview.net/pdf?id=rJe1y3CqtX" />
      <meta name="twitter:card" content="summary" />
      <meta name="twitter:site" content="@openreviewnet" />
      <meta name="og:title" content="Deep Reinforcement Learning of Universal Policies with Diverse..." />
      <meta name="og:description" content="Deep reinforcement learning has enabled robots to complete complex tasks in simulation. However, the resulting policies do not transfer to real robots due to model errors in the simulator. One..." />
      <meta name="og:image" content="https://openreview.net/static/images/openreview_logo_512.png" />

  <link rel="stylesheet" href="https://fonts.googleapis.com/css?family=Noto+Sans:400,400i,700,700i" />

  <link rel="stylesheet" href="/static/css/bootstrap.min.css" />
  <link rel="stylesheet" href="/static/css/jquery-ui.min.css" />
  <link rel="stylesheet" href="/static/css/main.min.css" />
</head>

<body class="forum">
  <nav class="navbar navbar-inverse navbar-fixed-top" role="navigation">
    <div class="container">
  
      <div class="navbar-header">
        <button type="button" class="navbar-toggle collapsed" data-toggle="collapse" data-target="#navbar" aria-expanded="false" aria-controls="navbar">
          <span class="sr-only">Toggle navigation</span>
          <span class="icon-bar"></span>
          <span class="icon-bar"></span>
          <span class="icon-bar"></span>
        </button>
        <a class="navbar-brand home push-link" href="/"><strong>OpenReview</strong>.net</a>
      </div>
  
      <div id="navbar" class="navbar-collapse collapse">
        <form class="navbar-form navbar-left profile-search" role="search">
          <div class="form-group has-feedback">
            <input id="search_input" type="text" class="form-control ui-autocomplete-input" placeholder="Search ICLR 2019 Conference" autocomplete="off" />
            <span class="glyphicon glyphicon-search form-control-feedback" aria-hidden="true"></span>
          </div>
  
          <input id="search_group" type="hidden" value="ICLR.cc/2019/Conference" />
          <input id="search_content" type="hidden" value="all" />
        <ul id="ui-id-1" tabindex="0" class="ui-menu ui-widget ui-widget-content ui-autocomplete ui-front" style="display: none;"></ul></form>
  
        <ul class="nav navbar-nav navbar-right">
        
            <li id="user-menu"><a href="/login">Login</a></li>
        </ul>
      </div>
  
    </div>
  </nav>

  <div id="or-banner" class="banner" style="">
  <div class="container">
    <div class="row">
      <div class="col-xs-12"><a href="/group?id=ICLR.cc/2019/Conference" title="Venue Homepage"><img class="icon" src="/static/images/arrow_left.svg" /> Go to <strong>ICLR 2019 Conference</strong> homepage</a></div>
    </div>
  </div>
</div>
<div id="flash-message-container" class="alert alert-danger" role="alert" style="display: none;">
  <div class="container">
    <div class="row">
      <div class="col-xs-12">
        <div class="alert-content">
          <button type="button" class="close" aria-label="Close"><span aria-hidden="true">×</span></button>
        </div>
      </div>
    </div>
  </div>
</div>

<div class="container">
  <div class="row">
    <div class="col-xs-12">
      <main id="content" class="clearfix openbanner-visible legacy-styles"><div id="note_rJe1y3CqtX" class="note panel"><div class="title_pdf_row clearfix"><h2 class="note_content_title"><a>Deep Reinforcement Learning of Universal Policies with Diverse Environment Summaries</a> <a class="note_content_pdf" href="/pdf?id=rJe1y3CqtX" title="Download PDF" target="_blank"><img src="/static/images/pdf_icon_blue.svg" /></a> </h2></div><div class="meta_row"><span class="signatures">Anonymous</span></div><div class="clearfix"><div class="meta_row pull-left"><span class="date item">28 Sep 2018 (modified: 11 Oct 2018)</span><span class="item">ICLR 2019 Conference Blind Submission</span><span class="item">Readers: <span class="readers-icon glyphicon glyphicon-globe"></span> Everyone</span><span class="item"><a class="action-bibtex-modal" data-bibtex="@inproceedings{    &#10;anonymous2019deep,    &#10;title={Deep Reinforcement Learning of Universal Policies with Diverse Environment Summaries},    &#10;author={Anonymous},    &#10;booktitle={Submitted to International Conference on Learning Representations},    &#10;year={2019},    &#10;url={https://openreview.net/forum?id=rJe1y3CqtX},    &#10;note={under review}    &#10;}">Show Bibtex</a></span></div><div class="meta_row meta_actions"></div></div><div class="note_contents"><span class="note_content_field">Abstract: </span><span class="note_content_value">Deep reinforcement learning has enabled robots to complete complex tasks in simulation. However, the resulting policies do not transfer to real robots due to model errors in the simulator. One solution is to randomize the simulation environment, so that the resulting, trained policy achieves high performance in expectation over a variety of configurations that could represent the real-world. However, the distribution over simulator configurations must be carefully selected to represent the relevant dynamic modes of the system, as otherwise it can be unlikely to sample challenging configurations frequently enough. Moreover, the ideal distribution to improve the policy changes as the policy (un)learns to solve tasks in certain configurations. In this paper, we propose to use an inexpensive, kernel-based summarization method method that identifies configurations that lead to diverse behaviors. Since failure modes for the given task are naturally diverse, the policy trains on a mixture of representative and challenging configurations, which leads to more robust policies. In experiments, we show that the proposed method achieves the same performance as domain randomization in simple cases, but performs better when domain randomization does not lead to diverse dynamic modes.</span></div><div class="note_contents"><span class="note_content_field">Keywords: </span><span class="note_content_value">Domain Randomization, Diverse Summaries, Reinforcement learning</span></div><div class="note_contents"><span class="note_content_field">TL;DR: </span><span class="note_content_value">As an alternative to domain randomization, we summarize simulator configurations to ensure that the policy is trained on a diverse set of induced state-trajectories.</span></div><div class="reply_row clearfix"><div class="item" id="reply_count">3 Replies</div></div></div><hr class="small" /><div id="note_children"><div class="note_with_children"><div id="note_Sye-d_D33X" class="note panel"><div class="title_pdf_row clearfix"><h2 class="note_content_title"><a>The proposed idea is not properly supported and is not convincing.</a>  <button class="btn btn-xs btn-default permalink-button" title="Link to this comment" data-permalink-url="https://openreview.net/forum?id=rJe1y3CqtX&amp;noteId=Sye-d_D33X"><span class="glyphicon glyphicon-link" aria-hidden="true"></span></button></h2></div><div class="meta_row"><span class="signatures">ICLR 2019 Conference Paper949 AnonReviewer1</span></div><div class="clearfix"><div class="meta_row pull-left"><span class="date item">04 Nov 2018 (modified: 07 Nov 2018)</span><span class="item">ICLR 2019 Conference Paper949 Official Review</span><span class="item">Readers: <span class="readers-icon glyphicon glyphicon-globe"></span> Everyone</span></div><div class="meta_row meta_actions"></div></div><div class="note_contents"><span class="note_content_field">Review: </span><span class="note_content_value">This paper presents a new method for learning diverse policies that can potentially transfer better to new environments. The proposed method aims to find simulation configurations that lead to diverse behaviors using “submodular optimizations” technique; an idea stemmed from past data summarization methods.  

Pros:

-The paper deals with an important problem in RL which is learning robust policies that can transfer better

- Simple idea based on prior information theory literature is proposed. 

- Good incorporation of past methods for improving robustness in RL.

Cons:

&gt;&gt;The paper has provided weak evidence and to support the effectiveness and significance of the proposed approach. The current analysis and experimental evaluations are not convincing.

Relevant baselines are missed and limited tasks are explored:
- A relevant prior work of Pinto et al. (2017) is not considered in the baselines for comparison.  
- Only two tasks are considered in experiments which are not representative of the effectiveness of the proposed approach and does not provide convincing evidence that the proposed method performs better than prior works.
-In the HalfCheetah task only the performance of random baseline is shown and comparison with EpOpt is not reported
-Results of the base DDPG policy is reported in the experiments (Fig 3 and Table 1). For a thorough comparison these missing results are necessary. 

&gt;&gt;Correct citation to the prior work of “domain randomization” is necessary:
- The second paragraph of related work section (first paragraph of page 2) explains that the idea of domain randomization has been first used in computer vision by Tobin et al. 2017. The idea of domain randomization was first proposed and deployed on a real robot platform in Sadeghi and Levine 2016 (Sadeghi, Fereshteh, and Sergey Levine. "CAD2RL: Real single-image flight without a single real image." arXiv preprint arXiv:1611.04201 (2016).) and was later used in Tobin et al. 2017 (this is also explained in the Tobin et al. 2017). Adding proper citation to Sadeghi and Levine 2016 is necessary. 

&gt;&gt; Relevant prior works are missed:
- The proposed idea in the paper is relevant to multi-task RL (e.g. Teh Y, Bapst V, Czarnecki WM, Quan J, Kirkpatrick J, Hadsell R, Heess N, Pascanu R. Distral: Robust multitask reinforcement learning. InAdvances in Neural Information Processing Systems 2017 (pp. 4496-4506). Adding related discussion about prior multi-task RL methods is highly recommended for improving the paper.
- Citating several relevant prior RL methods that deal with transfer learning is missed. A few examples are:

Rusu AA, Vecerik M, Rothörl T, Heess N, Pascanu R, Hadsell R. Sim-to-real robot learning from pixels with progressive nets. arXiv preprint arXiv:1610.04286. 2016 Oct 13.

Rusu AA, Rabinowitz NC, Desjardins G, Soyer H, Kirkpatrick J, Kavukcuoglu K, Pascanu R, Hadsell R. Progressive neural networks. arXiv preprint arXiv:1606.04671. 2016 Jun 15.

Rusu AA, Colmenarejo SG, Gulcehre C, Desjardins G, Kirkpatrick J, Pascanu R, Mnih V, Kavukcuoglu K, Hadsell R. Policy distillation. arXiv preprint arXiv:1511.06295. 2015 Nov 19.


&gt;&gt; The format of the paper needs to be improved. 
- The introduction section is rather incomplete and does not properly motivate the problem and the proposed solution.
-The organization of the experimental section should be improved. It is hard to follow the experiments and find the key experimental results in the current version of the paper.
</span></div><div class="note_contents"><span class="note_content_field">Rating: </span><span class="note_content_value">4: Ok but not good enough - rejection</span></div><div class="note_contents"><span class="note_content_field">Confidence: </span><span class="note_content_value">5: The reviewer is absolutely certain that the evaluation is correct and very familiar with the relevant literature</span></div><div class="reply_row clearfix"></div></div><div class="children"></div></div><div class="note_with_children"><div id="note_r1xR9Pucn7" class="note panel"><div class="title_pdf_row clearfix"><h2 class="note_content_title"><a>Efficient robust optimization in simulation through careful selection of diverse set of perturbations</a>  <button class="btn btn-xs btn-default permalink-button" title="Link to this comment" data-permalink-url="https://openreview.net/forum?id=rJe1y3CqtX&amp;noteId=r1xR9Pucn7"><span class="glyphicon glyphicon-link" aria-hidden="true"></span></button></h2></div><div class="meta_row"><span class="signatures">ICLR 2019 Conference Paper949 AnonReviewer2</span></div><div class="clearfix"><div class="meta_row pull-left"><span class="date item">03 Nov 2018 (modified: 07 Nov 2018)</span><span class="item">ICLR 2019 Conference Paper949 Official Review</span><span class="item">Readers: <span class="readers-icon glyphicon glyphicon-globe"></span> Everyone</span></div><div class="meta_row meta_actions"></div></div><div class="note_contents"><span class="note_content_field">Review: </span><span class="note_content_value">This paper addresses the problem of finding a policy that will perform well in a real environment when training in a simulator that may have errors.  It takes the now standard approach of trying to find a policy that performs well in an ensemble of simulated environments that are perturbations of the basic simulator.   The question is:  how can we construct an ensemble that represents the uncertainty about the real world well while being small enough for computational efficiency.  The idea is to construct a diverse set of samples that represents the whole space of important variations in the simulation;  the particular novelty here is to ensure that the sample set attains coverage over the *behaviors* of the simulator rather than the parameters of the simulation.  This problem is made difficult by the fact that there is no finite space of samples to choose from and the fact that we don't have a natural distance metric on the simulator behavior.

The main positive contributions of the paper are:
- The view of the problem of selecting from an infinite set as one of streaming sub-modular optimization.  This is a nice idea that is new to me and seemed appropriate for the problem.
- The idea that we want diversity in behavior, and then the technical approach of defining a kernel on simulator parameter sets that depends on the trajectories that those parameters induce.

I do have a set of questions and concerns:
- Might it not be better (more robust) to use not just trajectories from the current policy, but from other policies as well, to compute the kernel on parameter sets?  
- How do you get the length-scale parameters for the kernel?
- The confidence intervals in table 1 are too big to really justify firm conclusions;  it would be better to run the algorithms several more times, until the intervals pull apart.
- You say: "For ease of implementation  and since, in higher dimensional system, the variance of the policy gradients becomes  a significant factor, we train the  robot  on both the environment summaries and the N_s random rollouts."   This seems like it might be an important point that should be addressed earlier.  And, why does this ease implementation?
- I didn't understand:  "Diverse summaries are more consistent than pure random sampling."  What do you mean by consistent here?
- The metrics used in the empirical comparisons don't seem exactly right to me.  The goal of this work is to learn a policy that is robust in some sense (so  that,  e.g., it will do sim-2-real well).   We  really want  it to  work well in all  possible cases, not just in expectation or according to the sampling distribution you create, (I guess---since the paper said  the minimax criterion was desirable but difficult to work with).  So, then, it seems like  the best performance criterion would be to sample a whole lot of  domain parameters and report performance  on the worst  (rather than reporting performance on a distribution that's like the one you  trained it on or on an easy random one).

Overall, my view is that the idea is good, but somewhat small, and it hasn't really yet been proven to make a big difference.</span></div><div class="note_contents"><span class="note_content_field">Rating: </span><span class="note_content_value">6: Marginally above acceptance threshold</span></div><div class="note_contents"><span class="note_content_field">Confidence: </span><span class="note_content_value">4: The reviewer is confident but not absolutely certain that the evaluation is correct</span></div><div class="reply_row clearfix"></div></div><div class="children"></div></div><div class="note_with_children"><div id="note_HygHJ0cXnX" class="note panel"><div class="title_pdf_row clearfix"><h2 class="note_content_title"><a>Empirical results are not convincing enough</a>  <button class="btn btn-xs btn-default permalink-button" title="Link to this comment" data-permalink-url="https://openreview.net/forum?id=rJe1y3CqtX&amp;noteId=HygHJ0cXnX"><span class="glyphicon glyphicon-link" aria-hidden="true"></span></button></h2></div><div class="meta_row"><span class="signatures">ICLR 2019 Conference Paper949 AnonReviewer3</span></div><div class="clearfix"><div class="meta_row pull-left"><span class="date item">29 Oct 2018 (modified: 07 Nov 2018)</span><span class="item">ICLR 2019 Conference Paper949 Official Review</span><span class="item">Readers: <span class="readers-icon glyphicon glyphicon-globe"></span> Everyone</span></div><div class="meta_row meta_actions"></div></div><div class="note_contents"><span class="note_content_field">Review: </span><span class="note_content_value">This paper studies the problem of robust policy optimization, motivated by the fact that policies that work in simulations do not transfer well to real world. The authors propose to use the diversity measure on the roll out trajectories to select a diverse set of simulator configurations and train policies that can work well for all of those selected configurations. 

Overall, I think the idea is interesting but it is not entirely clear why adding diverse configurations should result in good performance, and the experiments are very limited and not convincing enough. 

Pros:
- The paper is easy to follow.
- The idea of using a diverse summary to do robust policy optimization is interesting. 
- The diversity measure on the trajectories instead of the space of configuration parameters also intuitively makes sense since it takes into account that the similarity between two configuration parameters does not typically mean the similarity between their corresponding policies. 

Cons:
- The setting of this paper seems to only work for the fully observable case with state space being in R^d, deterministic dynamics and deterministic policy (otherwise the diversity measure would be stochastic?). It would be good to clarify these in Sec. 2. 
- For the example in Fig 2 and the first experiment, what I don't understand is why the initial state is not part of the policy.
- It is not clear if the reason that EP-OPT performed worse than the proposed approach is only because there are not enough rollouts for EP-OPT. This could be an unfair comparison.
- It would be good to show the comparison to EP-OPT for the second experiment as well. 
- Two experiments may not be enough to verify valid performance since there could be a lot of randomness in the results.
- In page 6, it would be good to clarify that the summary being optimal is only with respect to f(M_s), but not the original problem of finding optimal policy. </span></div><div class="note_contents"><span class="note_content_field">Rating: </span><span class="note_content_value">5: Marginally below acceptance threshold</span></div><div class="note_contents"><span class="note_content_field">Confidence: </span><span class="note_content_value">4: The reviewer is confident but not absolutely certain that the evaluation is correct</span></div><div class="reply_row clearfix"></div></div><div class="children"></div></div></div></main></div>
  </div>
</div>


    <!-- Footer -->
    <footer class="sitemap">
      <div class="container">
        <div class="row hidden-xs">
          <div class="col-sm-4">
            <ul class="list-unstyled">
              <li><a href="/" class="home">Home</a></li>
              <li><a href="/venues">All Venues</a></li>
            </ul>
          </div>
    
          <div class="col-sm-4">
            <ul class="list-unstyled">
              <li><a href="/about">About OpenReview</a></li>
              <li><a href="/contact">Contact</a></li>
              <li><a href="#" data-toggle="modal" data-target="#feedback-modal">Feedback</a></li>
            </ul>
          </div>
    
          <div class="col-sm-4">
            <ul class="list-unstyled">
              <li><a href="/terms">Terms of Service</a></li>
              <li><a href="/privacy">Privacy Policy</a></li>
            </ul>
          </div>
        </div>
    
        <div class="row visible-xs-block">
          <div class="col-xs-6">
            <ul class="list-unstyled">
              <li><a href="/" class="home">Home</a></li>
              <li><a href="/about">About OpenReview</a></li>
              <li><a href="#" data-toggle="modal" data-target="#feedback-modal">Feedback</a></li>
            </ul>
          </div>
    
          <div class="col-xs-6">
            <ul class="list-unstyled">
              <li><a href="/contact">Contact</a></li>
              <li><a href="/terms">Terms of Service</a></li>
              <li><a href="/privacy">Privacy Policy</a></li>
            </ul>
          </div>
        </div>
      </div>
    </footer>
    
    <footer class="sponsor">
      <div class="container">
        <div class="row">
          <div class="col-sm-10 col-sm-offset-1">
            <p class="text-center">
              OpenReview is created by the <a href="http://www.iesl.cs.umass.edu/" target="_blank">Information Extraction and Synthesis Laboratory</a>, College of Information and Computer Science, University of Massachusetts Amherst. We gratefully acknowledge the support of the OpenReview sponsors:  Google,  Facebook, NSF, the University of Massachusetts Amherst Center for Data Science, and Center for Intelligent Information Retrieval, as well as the Google Cloud Platform for donating the computing and networking services on which OpenReview.net runs.
            </p>
          </div>
        </div>
      </div>
    </footer>

  <div id="feedback-modal" class="modal fade" tabindex="-1" role="dialog">
    <div class="modal-dialog">
      <div class="modal-content">
        <div class="modal-header">
          <button type="button" class="close" data-dismiss="modal" aria-label="Close"><span aria-hidden="true">×</span></button>
          <h3 class="modal-title">Send Feedback</h3>
        </div>
        <div class="modal-body">
          <p>Enter your feedback below and we'll get back to you as soon as possible.</p>
          <form action="/feedback" method="POST">
            <div class="form-group">
              <input type="email" name="from" class="form-control" placeholder="Email" />
            </div>
            <div class="form-group">
              <input type="text" name="subject" class="form-control" placeholder="Subject" />
            </div>
            <div class="form-group">
              <textarea name="message" class="form-control feedback-input" rows="5" placeholder="Message" required=""></textarea>
            </div>
          <ul id="ui-id-2" tabindex="0" class="ui-menu ui-widget ui-widget-content" style="display: none;"></ul></form>
        </div>
        <div class="modal-footer">
          <button type="button" class="btn btn-default" data-dismiss="modal">Cancel</button>
          <button type="button" class="btn btn-primary">Send</button>
        </div>
      </div>
    </div>
  </div>

  <script type="text/javascript" async="" src="https://www.google-analytics.com/analytics.js"></script><script src="//cdnjs.cloudflare.com/ajax/libs/jquery/2.2.4/jquery.min.js"></script>
  <script>window.jQuery || document.write('&lt;script src="/static/js/vendor/jquery-2.2.4.min.js"&gt;&lt;\/script&gt;')</script>

  <script src="/static/dist/vendor.min.js"></script>
  <script src="/static/dist/templates.min.js"></script>
  <script src="/static/dist/openreview.min.js"></script>

    <script>window.legacyScripts = true;</script>


    <script async="" src="https://www.googletagmanager.com/gtag/js?id=UA-108703919-1"></script>
    <script>
      window.dataLayer = window.dataLayer || [];
      function gtag() { dataLayer.push(arguments); }
      gtag('js', new Date());
      gtag('config', 'UA-108703919-1');
    </script>


<div role="status" aria-live="assertive" aria-relevant="additions" class="ui-helper-hidden-accessible"></div></body></html>