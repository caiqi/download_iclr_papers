<!DOCTYPE html><html xmlns="http://www.w3.org/1999/xhtml" class="no-js"><head>
  <meta charset="utf-8" />
  <meta http-equiv="X-UA-Compatible" content="IE=edge,chrome=1" />
  <meta name="viewport" content="width=device-width, initial-scale=1" />

  <title>SGD Converges to Global Minimum in Deep Learning via Star-convex Path | OpenReview</title>
  <meta name="description" content="" />

      <meta name="citation_title" content="SGD Converges to Global Minimum in Deep Learning via Star-convex Path" />
        <meta name="citation_author" content="Anonymous" />
      <meta name="citation_publication_date" content="2018/09/27" />
      <meta name="citation_online_date" content="2018/09/27" />
      <meta name="citation_pdf_url" content="https://openreview.net/pdf?id=BylIciRcYQ" />
      <meta name="twitter:card" content="summary" />
      <meta name="twitter:site" content="@openreviewnet" />
      <meta name="og:title" content="SGD Converges to Global Minimum in Deep Learning via Star-convex Path" />
      <meta name="og:description" content="Stochastic gradient descent (SGD) has been found to be surprisingly effective in training a variety of deep neural networks. However, there is still a lack of understanding on how and why SGD can..." />
      <meta name="og:image" content="https://openreview.net/static/images/openreview_logo_512.png" />

  <link rel="stylesheet" href="https://fonts.googleapis.com/css?family=Noto+Sans:400,400i,700,700i" />

  <link rel="stylesheet" href="/static/css/bootstrap.min.css" />
  <link rel="stylesheet" href="/static/css/jquery-ui.min.css" />
  <link rel="stylesheet" href="/static/css/main.min.css" />
</head>

<body class="forum">
  <nav class="navbar navbar-inverse navbar-fixed-top" role="navigation">
    <div class="container">
  
      <div class="navbar-header">
        <button type="button" class="navbar-toggle collapsed" data-toggle="collapse" data-target="#navbar" aria-expanded="false" aria-controls="navbar">
          <span class="sr-only">Toggle navigation</span>
          <span class="icon-bar"></span>
          <span class="icon-bar"></span>
          <span class="icon-bar"></span>
        </button>
        <a class="navbar-brand home push-link" href="/"><strong>OpenReview</strong>.net</a>
      </div>
  
      <div id="navbar" class="navbar-collapse collapse">
        <form class="navbar-form navbar-left profile-search" role="search">
          <div class="form-group has-feedback">
            <input id="search_input" type="text" class="form-control ui-autocomplete-input" placeholder="Search ICLR 2019 Conference" autocomplete="off" />
            <span class="glyphicon glyphicon-search form-control-feedback" aria-hidden="true"></span>
          </div>
  
          <input id="search_group" type="hidden" value="ICLR.cc/2019/Conference" />
          <input id="search_content" type="hidden" value="all" />
        <ul id="ui-id-1" tabindex="0" class="ui-menu ui-widget ui-widget-content ui-autocomplete ui-front" style="display: none;"></ul></form>
  
        <ul class="nav navbar-nav navbar-right">
        
            <li id="user-menu"><a href="/login">Login</a></li>
        </ul>
      </div>
  
    </div>
  </nav>

  <div id="or-banner" class="banner" style="">
  <div class="container">
    <div class="row">
      <div class="col-xs-12"><a href="/group?id=ICLR.cc/2019/Conference" title="Venue Homepage"><img class="icon" src="/static/images/arrow_left.svg" /> Go to <strong>ICLR 2019 Conference</strong> homepage</a></div>
    </div>
  </div>
</div>
<div id="flash-message-container" class="alert alert-danger" role="alert" style="display: none;">
  <div class="container">
    <div class="row">
      <div class="col-xs-12">
        <div class="alert-content">
          <button type="button" class="close" aria-label="Close"><span aria-hidden="true">×</span></button>
        </div>
      </div>
    </div>
  </div>
</div>

<div class="container">
  <div class="row">
    <div class="col-xs-12">
      <main id="content" class="clearfix openbanner-visible legacy-styles"><div id="note_BylIciRcYQ" class="note panel"><div class="title_pdf_row clearfix"><h2 class="note_content_title"><a>SGD Converges to Global Minimum in Deep Learning via Star-convex Path</a> <a class="note_content_pdf" href="/pdf?id=BylIciRcYQ" title="Download PDF" target="_blank"><img src="/static/images/pdf_icon_blue.svg" /></a> </h2></div><div class="meta_row"><span class="signatures">Anonymous</span></div><div class="clearfix"><div class="meta_row pull-left"><span class="date item">28 Sep 2018 (modified: 20 Nov 2018)</span><span class="item">ICLR 2019 Conference Blind Submission</span><span class="item">Readers: <span class="readers-icon glyphicon glyphicon-globe"></span> Everyone</span><span class="item"><a class="action-bibtex-modal" data-bibtex="@inproceedings{    &#10;anonymous2019sgd,    &#10;title={SGD Converges to Global Minimum in Deep Learning via Star-convex Path},    &#10;author={Anonymous},    &#10;booktitle={Submitted to International Conference on Learning Representations},    &#10;year={2019},    &#10;url={https://openreview.net/forum?id=BylIciRcYQ},    &#10;note={under review}    &#10;}">Show Bibtex</a></span><a class="note_content_pdf item" href="/revisions?id=BylIciRcYQ" target="_blank">Show Revisions</a></div><div class="meta_row meta_actions"></div></div><div class="note_contents"><span class="note_content_field">Abstract: </span><span class="note_content_value">Stochastic gradient descent (SGD) has been found to be surprisingly effective in training a variety of deep neural networks. However, there is still a lack of understanding on how and why SGD can train these complex networks towards a global minimum. In this study, we establish the convergence of SGD to a global minimum for nonconvex optimization problems that are commonly encountered in neural network training. Our argument exploits the following two important properties: 1) the training loss can achieve zero value (approximately), which has been widely observed in deep learning; 2) SGD follows a star-convex path, which is verified by various experiments in this paper.  In such a context, our analysis shows that SGD, although has long been considered as a randomized algorithm, converges in an intrinsically deterministic manner to a global minimum. </span></div><div class="note_contents"><span class="note_content_field">Keywords: </span><span class="note_content_value">SGD, deep learning, global minimum, convergence</span></div><div class="reply_row clearfix"><div class="item" id="reply_count">6 Replies</div></div></div><hr class="small" /><div id="note_children"><div class="note_with_children"><div id="note_Hyg7evSKnQ" class="note panel"><div class="title_pdf_row clearfix"><h2 class="note_content_title"><a>The paper provides interesting idea but the empirical results may be biased due to ill-posed problem </a>  <button class="btn btn-xs btn-default permalink-button" title="Link to this comment" data-permalink-url="https://openreview.net/forum?id=BylIciRcYQ&amp;noteId=Hyg7evSKnQ"><span class="glyphicon glyphicon-link" aria-hidden="true"></span></button></h2></div><div class="meta_row"><span class="signatures">ICLR 2019 Conference Paper535 AnonReviewer1</span></div><div class="clearfix"><div class="meta_row pull-left"><span class="date item">02 Nov 2018 (modified: 07 Nov 2018)</span><span class="item">ICLR 2019 Conference Paper535 Official Review</span><span class="item">Readers: <span class="readers-icon glyphicon glyphicon-globe"></span> Everyone</span></div><div class="meta_row meta_actions"></div></div><div class="note_contents"><span class="note_content_field">Review: </span><span class="note_content_value">The paper proposes a new approach to explain the effective behavior of SGD in training deep neural networks by introducing the notion of star-convexity. A function h is star-convex if its global minimum lies on or above any plane tangent to the function, namely h* &gt;= h(x) + &lt; h'(x), x*-x&gt; for any x. Under such condition, the paper shows that the empirical loss goes to zero and the iterates generated by SGD converges to a global minimum. Extensive experiments has been conducted to empirically validate the assumption. 

The paper is very well organized and is easy to follow. The star-convexity assumption is very interesting which provides new insights about the landscape of the loss function and the trajectory of SGD. It is in general difficult to theoretically check this condition so several empirical verifications has been proposed. My main concern is about these empirical verifications.

1) The minimum of the cross entropy loss lies at infinity 
The experiments are performed respect to the cross entropy loss. However, cross entropy loss violates Fact 1 since for any finite weight, cross entropy loss is always strictly positive. Thus the zero is never attained and the global minimum always lies at infinity. As a result, the star-convexity inequality h* &gt;= h(x) + &lt; h'(x), x*-x&gt; hardly makes sense since x* is at infinity and neither does the theorem followed. 
In this case, a plot of the norm of xk is highly suggested since it is a sanity check to see whether the iterates goes to infinity. 

2) The phenomenon may depend on the reference point, i.e last iterate
Since the minimum is never attained, the empirical check of the star-convexity maybe biased. More precisely, it might be possible that the behavior of the observed phenomenon depends on the reference point, i.e. the last iterate. Therefore, it will be interesting to see if the observed phenomenon still holds when varying the stopping time, for instance plot the star convexity check using the iterates at 60, 80, 100, 120 epochs as reference point. 

In fact, the experiments shown in Figure 4 implicitly supports that the behavior may change dramatically respect to different reference point. The reason is that the loss in these experiments are far away from 0, meaning that we are far from the minimum, thus checking the star-convexity does not make sense because the star-convexity is only defined respect to the minimum. 

Overall, the paper provides interesting idea but the empirical results may be biased due to ill-posed problem </span></div><div class="note_contents"><span class="note_content_field">Rating: </span><span class="note_content_value">5: Marginally below acceptance threshold</span></div><div class="note_contents"><span class="note_content_field">Confidence: </span><span class="note_content_value">4: The reviewer is confident but not absolutely certain that the evaluation is correct</span></div><div class="reply_row clearfix"></div></div><div class="children"><div class="note_with_children"><div id="note_SJxGnBGW07" class="note panel"><div class="title_pdf_row clearfix"><h2 class="note_content_title"><a>Response</a>  <button class="btn btn-xs btn-default permalink-button" title="Link to this comment" data-permalink-url="https://openreview.net/forum?id=BylIciRcYQ&amp;noteId=SJxGnBGW07"><span class="glyphicon glyphicon-link" aria-hidden="true"></span></button></h2></div><div class="meta_row"><span class="signatures">ICLR 2019 Conference Paper535 Authors</span></div><div class="clearfix"><div class="meta_row pull-left"><span class="date item">20 Nov 2018</span><span class="item">ICLR 2019 Conference Paper535 Official Comment</span><span class="item">Readers: <span class="readers-icon glyphicon glyphicon-globe"></span> Everyone</span></div><div class="meta_row meta_actions"></div></div><div class="note_contents"><span class="note_content_field">Comment: </span><span class="note_content_value">We thank the reviewer for the valuable feedbacks. 

This paper aims at reporting an interesting star-convex property of the SGD optimization path that has been observed in training a variety of DL models, including MLP, CNN, residual networks and RNN (verified recently). Moreover, our theory is motivated by such a common observation and attempts to justify the role of this property plays in determining the convergence of the optimization in DL. 

Our response to the reviewer’s comments are provided as follows.

1) The minimum of the cross entropy loss lies at infinity. It is a sanity to check whether the iterates goes to infinity. 

 Response: We thank the reviewer for pointing out this, and we are aware of it. We choose to present the results on cross entropy as it is widely used in DL applications. In fact, we verified the star-convexity property for training other losses that by nature can achieve zero such as the MSE loss (please see the experiment results that we added in Fig.7 in Appendix D in supplementary).
Under the cross-entropy loss, we found that after the training loss is very close to zero, the l_2 norm of the corresponding iterate grows only logarithmically. This can be clearly seen from the experiments that we added in Fig.6 in Appendix D of supplementary. We further note that such a phenomenon has also been observed and justified in Fig.2 of [1]).  Thus, empirically, it is reasonable to treat the loss value to be approximately zero (i.e., reaches minimum) with a bounded weight norm.
[1] ``The Implicit Bias of Gradient Descent on Separable Data’’, Soundry et al 2018

2) The phenomenon may depend on the reference point, i.e., last iterate
Since the minimum is never attained, the empirical check of the star-convexity maybe biased. More precisely, it might be possible that the behavior of the observed phenomenon depends on the reference point, i.e. the last iterate. Therefore, it will be interesting to see if the observed phenomenon still holds when varying the stopping time, for instance plot the star convexity check using the iterates at 60, 80, 100, 120 epochs as reference point. 
In fact, the experiments shown in Figure 4 implicitly supports that the behavior may change dramatically respect to different reference point. The reason is that the loss in these experiments are far away from 0, meaning that we are far from the minimum, thus checking the star-convexity does not make sense because the star-convexity is only defined respect to the minimum. 

Response: As can be seen in the experiments that we added in Fig.5 in Appendix D of the supplementary materials, we have checked the star-convex property by taking the reference point at different intermediate iterates (60, 80, 100, 120) as the reviewer suggested. We found that star-convexity still holds under these choices of reference points, and therefore such a property does not depend on the choice of reference point so long as their loss are (nearly) zero. This observation is common for over-parameterized networks which can achieve near zero loss and therefore can have common global minimum.
We emphasize that the reference point in our star-convexity must be the minimizer that achieves zero loss. Hence, in experiments, we must set the reference point at the epochs where the corresponding loss is nearly zero. The points at intermediate iterates with a high loss value cannot be chosen as reference point of star-convexity, because these points cannot be treated as the common global minimizer.. 
Regarding the experiments in Figure 4, they are conducted on under-parameterized networks where (approximate) zero loss cannot be achieved, and the algorithm in fact does not find a common global minimum. This experiment is to justify the role of the over-parameterization (or the common global minimum) plays in determining the star-convex optimization path.
</span></div><div class="reply_row clearfix"></div></div><div class="children"></div></div></div></div><div class="note_with_children"><div id="note_HyxZqj0O27" class="note panel"><div class="title_pdf_row clearfix"><h2 class="note_content_title"><a>Interesting paper, but maybe less significant than it appears to be</a>  <button class="btn btn-xs btn-default permalink-button" title="Link to this comment" data-permalink-url="https://openreview.net/forum?id=BylIciRcYQ&amp;noteId=HyxZqj0O27"><span class="glyphicon glyphicon-link" aria-hidden="true"></span></button></h2></div><div class="meta_row"><span class="signatures">ICLR 2019 Conference Paper535 AnonReviewer2</span></div><div class="clearfix"><div class="meta_row pull-left"><span class="date item">02 Nov 2018 (modified: 07 Nov 2018)</span><span class="item">ICLR 2019 Conference Paper535 Official Review</span><span class="item">Readers: <span class="readers-icon glyphicon glyphicon-globe"></span> Everyone</span></div><div class="meta_row meta_actions"></div></div><div class="note_contents"><span class="note_content_field">Review: </span><span class="note_content_value">This paper attempts to account for the success of SGD on training deep neural networks. Starting from two empirical observations: (1) deep neural networks can almost achieve zero training loss; (2) the path of iterates generated by SGD on these models follow approximately the “star convex path”, under the assumptions that individual functions share a global minima with respect to which the path of iterates generated by SGD satisfies the star convexity properties, the papers shows that the iterates converges to the global minima. 

In terms of clarity, I think the paper can definitely benefit if the observations/assumptions/definitions/theorems are stated in a more formal and mathematically rigorous manner. For example:
- On page 3, “fact 1”: I don’t think “fact” is the right word here. “Fact” refers to what has been rigorously proved or verified, which is not the case for what is in the paper here. I believe “observation” is more appropriate. Also the assumption that l_i is non-negative should be formally added.
- On page 3, section 3.1: the x^* here is the last iteration produced by SGD. Then how can it be called the “global minima”? The caption of Figure 1 on page 4 is simply misleading.
- On page 4, the statement in definition 1 is more like a theorem than a definition. It is giving readers the impression that any path generated by SGD satisfies the star-convex condition, which is not the case here. A definition should look like “we call a path generated by SGD a star-convex path if it satisfies …”. Definition 2 on page 6 has the similar issue.

In terms of quality, while I believe the paper is technically correct,  I have one minor question here:
Page 3, Fact 1: How can you conclude that the set of common global minimizers are bounded? In fact I don’t believe this is true at all in general. If you have a ReLu network, you can scale the parameters as described in [1], then the model is invariant. Therefore, the set of common minimizer is definitely NOT bounded. 

In terms of significance, I think this paper is very interesting as it attempts to draw the connection between the aforementioned observations and the convergence properties of SGD. Unfortunately I think that this paper is less significant than it has appeared to be, although the analysis appears to be correct. 

First of all, all the analysis of this paper is based on one very important and very strong assumption, namely, all individual functions $l_i$ share at least one common global minimizer. The authors have attempted to justify this assumption by empirical evidences (figure 1). However, achieving near-zero loss is completely different from achieving exact zero because only when the model achieves exact zero can you argue that a common global minimizer exists. 

Secondly, the claim that the iterate converges to the global minima is based on the assumption that the path follows an “epoch-wise star-convex” property. From this property, it only takes simple convex analysis to reach the conclusion of theorem 1 and 2. Meanwhile, the assumption that the path does follow the “epoch-wise start-convex” properties is not at all informative. It is not clear why or when the path would follow such a path. Therefore theorem 1 and 2 are not more informative than simply assuming the sequence converges to a global minimizer. 

In fact, it is well-known that SGD with constant stepsize converges to the unique minimizer if one assumes the loss function F is strongly convex and the variance of the stochastic gradient g_k is bounded by a multiple of the norm-square of the true gradient:
Var(g_k) &lt;= M ||∇F(x_k)||^2
Which is naturally satisfied if all individual functions share a common minimizer. Therefore, I don’t think the results shown in the paper is that surprising or novel. 

With respect to the empirical evidence, the loss function l_i is assumed to be continuously differentiable with Lipschitz continuous gradients, which is not true for networks using ReLU-like activations. Then how can the paper use models like Alexnet to justify the theory? Also, if what the authors claim is true, then the stochastic gradient would have vanishing variance as it approaches x^*. Can the authors show this empirically?

In summary, I think this paper is definitely interesting, but the significance is not as much as it would appear.

Ref: 
[1] Dinh, L., Pascanu, R., Bengio, S., &amp; Bengio, Y. (2017). Sharp minima can generalize for deep nets. arXiv preprint arXiv:1703.04933.</span></div><div class="note_contents"><span class="note_content_field">Rating: </span><span class="note_content_value">5: Marginally below acceptance threshold</span></div><div class="note_contents"><span class="note_content_field">Confidence: </span><span class="note_content_value">5: The reviewer is absolutely certain that the evaluation is correct and very familiar with the relevant literature</span></div><div class="reply_row clearfix"></div></div><div class="children"><div class="note_with_children"><div id="note_HkgubIm-Am" class="note panel"><div class="title_pdf_row clearfix"><h2 class="note_content_title"><a>Response</a>  <button class="btn btn-xs btn-default permalink-button" title="Link to this comment" data-permalink-url="https://openreview.net/forum?id=BylIciRcYQ&amp;noteId=HkgubIm-Am"><span class="glyphicon glyphicon-link" aria-hidden="true"></span></button></h2></div><div class="meta_row"><span class="signatures">ICLR 2019 Conference Paper535 Authors</span></div><div class="clearfix"><div class="meta_row pull-left"><span class="date item">20 Nov 2018</span><span class="item">ICLR 2019 Conference Paper535 Official Comment</span><span class="item">Readers: <span class="readers-icon glyphicon glyphicon-globe"></span> Everyone</span></div><div class="meta_row meta_actions"></div></div><div class="note_contents"><span class="note_content_field">Comment: </span><span class="note_content_value">We thank the reviewer for the valuable feedbacks. 

This paper aims at reporting an interesting star-convex property of the SGD optimization path that has been observed in training a variety of DL models, including MLP, CNN, residual networks and RNN (verified recently). Moreover, our theory is motivated by such a common observation and attempts to justify the role of this property plays in determining the convergence of the optimization in DL. 

Our response to the reviewer’s comments are provided as follows.

Quality:  In Fact 1: How can you conclude that the set of common global minimizers are bounded? 

A: We thank the reviewer for pointing out this. In fact, our theoretical results do not require that all the global minima be bounded. To be precise, we only need the star-convexity to hold for a bounded subset of the common global minimum, under which our theory guarantees that SGD converges to one of the elements in that set.  We clarify this in the revision.

Clarity: On page 3, “fact” is the right word here. On Page 3 the x^* here is the last iteration produced by SGD. On page 4, the statement in definition 1 is more like a theorem.

A: We thank the reviewer for valuable suggestions. In the revision, we use ``observation’’ instead of “fact”. We add the non-negativity assumption. We now refer to x^* as the output of SGD. We restate definition 1&amp;2.

Significance 1) The analysis of this paper is based on ... 

A: We have added new experiments to demonstrate that for the star-convexity of SGD holds for the MSE loss function (which can achieve zero loss).  We agree that cross-entropy achieves only near-zero loss, but such an approximation is not that unreasonable, as can be observed by the experiments that we added as Fig. 6 in Appendix D, which illustrates that the cross-entropy loss is nearly zero for certain finite norm of the weight parameters. Hence, we do expect that such approximation can convey useful information.  
We believe that we should not restrict ourselves only to theory that exactly matches what happens in practice. Near-zero loss is widely observed for training over-parameterized neural networks. Thus, the common minimizer assumption is motivated by this observation, and has led us to discover the star-convexity of SGD paths empirically, and further develop the convergence of SGD based on such a property. Hence, the approximate common global minimizer does yield consistent practical and theoretical results that explain what happens in deep learning.

2) Secondly, …. 

A: We first want to point out that the “epoch-wise star-convexity” is an accumulative effect of the residual error of every component loss over one epoch, which is a nontrivial and much weaker condition than that the entire loss function is star-convex over the points that SGD visits. Thus, assuming “epoch-wise star-convexity”, the proofs of theorems 1 and 2 do not follow the conventional convex analysis. One can of course argue that it is simple, but we think the focus here should be the information that it conveys in such a context.
Second, we report a star-convex path property over a wide range of DL training tasks, which have not been reported in the existing literature to our knowledge. We do think this is an informative discovery. Of course, understanding when and why such a property hold for DL training is definitely important and deserves exploration in the future work. 

3) In fact, it is well-known that SGD with constant step size … 

A: We want to point out the difference between our theory and the result mentioned by the reviewer. The star-convexity is much more relaxed than that the loss function F being strongly convex. Second, the bounded variance assumption that Var(g_k) &lt;= M ||∇F(x_k)||^2 is hard to justify in general, and it is not clear to what extent can it be justified in DL tasks. Not to mention that it is clearly not true that the loss function is strong convex! In contrast, our star-convexity assumption is verified by various DL experiments as we report in the paper. Moreover, under strong convexity, traditional analysis only guarantees the convergence of the sequence in probability, which is much weaker than our deterministic convergence results in Theorem 3. 

4) With respect to the empirical evidence, ... 

A: We thank the reviewer for pointing out this, and we are aware of it. We use ReLU activation as it is commonly used in DL tasks. Of course, one can use a smoothed version of ReLU (softplus) and obtain nearly the same result. We clarified this and added more experiments on this in the revision.
For the variance, we do observe that the variance vanishes as SGD converges, and in fact report such a property as Corollary 1 in the originally submitted version. This is a necessary observation when SGD converges to a common global minimizer, and therefore also justifies the existence of common global minimum to some extent. We add experiments on this in the revision.
</span></div><div class="reply_row clearfix"></div></div><div class="children"></div></div></div></div><div class="note_with_children"><div id="note_B1xsEN2d37" class="note panel"><div class="title_pdf_row clearfix"><h2 class="note_content_title"><a>Good theoretical paper</a>  <button class="btn btn-xs btn-default permalink-button" title="Link to this comment" data-permalink-url="https://openreview.net/forum?id=BylIciRcYQ&amp;noteId=B1xsEN2d37"><span class="glyphicon glyphicon-link" aria-hidden="true"></span></button></h2></div><div class="meta_row"><span class="signatures">ICLR 2019 Conference Paper535 AnonReviewer3</span></div><div class="clearfix"><div class="meta_row pull-left"><span class="date item">02 Nov 2018 (modified: 20 Nov 2018)</span><span class="item">ICLR 2019 Conference Paper535 Official Review</span><span class="item">Readers: <span class="readers-icon glyphicon glyphicon-globe"></span> Everyone</span><a class="note_content_pdf item" href="/revisions?id=B1xsEN2d37" target="_blank">Show Revisions</a></div><div class="meta_row meta_actions"></div></div><div class="note_contents"><span class="note_content_field">Review: </span><span class="note_content_value">This paper analyzed the global convergence property of SGD in deep learning based on the star-convexity assumption. The claims seem correct and validated empirically with some observations in deep learning. The writing is good and easy to follow.

My understanding of the analysis is that all the claims seem to be valid when the solution is in a wide valley of the loss surface where the star-convexity holds, in general. This has been observed empirically in previous work, and the experiments on cifar10 in Fig. 2 support my hypothesis. My questions are:

1. How to guarantee the star-convexity will be valid in deep learning?
2. What network or data properties can lead to such assumption?

Also, this is a missing related work from the algorithmic perspective to explore the global optimization in deep learning: 

Zhang et. al. CVPR'18. "BPGrad: Towards Global Optimality in Deep Learning via Branch and Pruning".
</span></div><div class="note_contents"><span class="note_content_field">Rating: </span><span class="note_content_value">7: Good paper, accept</span></div><div class="note_contents"><span class="note_content_field">Confidence: </span><span class="note_content_value">4: The reviewer is confident but not absolutely certain that the evaluation is correct</span></div><div class="reply_row clearfix"></div></div><div class="children"><div class="note_with_children"><div id="note_B1equOGWCm" class="note panel"><div class="title_pdf_row clearfix"><h2 class="note_content_title"><a>Response</a>  <button class="btn btn-xs btn-default permalink-button" title="Link to this comment" data-permalink-url="https://openreview.net/forum?id=BylIciRcYQ&amp;noteId=B1equOGWCm"><span class="glyphicon glyphicon-link" aria-hidden="true"></span></button></h2></div><div class="meta_row"><span class="signatures">ICLR 2019 Conference Paper535 Authors</span></div><div class="clearfix"><div class="meta_row pull-left"><span class="date item">20 Nov 2018</span><span class="item">ICLR 2019 Conference Paper535 Official Comment</span><span class="item">Readers: <span class="readers-icon glyphicon glyphicon-globe"></span> Everyone</span></div><div class="meta_row meta_actions"></div></div><div class="note_contents"><span class="note_content_field">Comment: </span><span class="note_content_value">We thank the reviewer for the valuable feedbacks. 

This paper aims at reporting an interesting star-convex property of the SGD optimization path that has been observed in training a variety of DL models, including MLP, CNN, residual networks and RNN (verified recently). Moreover, our theory is motivated by such a common observation and attempts to justify the role of this property plays in determining the convergence of the optimization in DL. 

Our response to the reviewer’s comments are provided as follows.

1. How to guarantee the star-convexity will be valid in deep learning?

Response: We thank the reviewer for pointing out this question. It is definitely interesting to explore the underlying mechanism that leads to such a common observation.  We think that over-parameterization can be one of the important factors. We are currently investigating this issue theoretically on some simple networks, and our understanding so far favors such a direction.

2. What network or data properties can lead to such assumption?

Response All our experiments are conducted on practical neural network training tasks with real datasets. From the experiments, we find that the property holds for a variety of network architectures (MLP, CNN, Inception, RNN) and different datasets (image, text, etc). We think that this can be an amenable property of over-parameterized network. 
In fact, several recent works ([1,2]) show that the optimization trajectories of SGD is generally smooth despite the nonconvexity and depth of the networks, and our star-convexity property can be viewed as another aspect that further promotes theoretical justification to deep learning optimization. We will explore these two questions more in future work.

3. There is a missing related work from the algorithmic perspective to explore the global optimization in deep learning: 
Zhang et. al. CVPR'18. "BPGrad: Towards Global Optimality in Deep Learning via Branch and Pruning".

Response: We thank the reviewer for pointing out this interesting related work. We will cite this work in the upcoming revision. 

[1] Li et al. Visualizing the loss landscape of neural nets. To appear in NIPS 2018
[2] Eliana Lorch. Visualizing deep network training trajectories with pca. In ICML Workshop on Visualization for Deep Learning, 2016.
</span></div><div class="reply_row clearfix"></div></div><div class="children"></div></div></div></div></div></main></div>
  </div>
</div>


    <!-- Footer -->
    <footer class="sitemap">
      <div class="container">
        <div class="row hidden-xs">
          <div class="col-sm-4">
            <ul class="list-unstyled">
              <li><a href="/" class="home">Home</a></li>
              <li><a href="/venues">All Venues</a></li>
            </ul>
          </div>
    
          <div class="col-sm-4">
            <ul class="list-unstyled">
              <li><a href="/about">About OpenReview</a></li>
              <li><a href="/contact">Contact</a></li>
              <li><a href="#" data-toggle="modal" data-target="#feedback-modal">Feedback</a></li>
            </ul>
          </div>
    
          <div class="col-sm-4">
            <ul class="list-unstyled">
              <li><a href="/terms">Terms of Service</a></li>
              <li><a href="/privacy">Privacy Policy</a></li>
            </ul>
          </div>
        </div>
    
        <div class="row visible-xs-block">
          <div class="col-xs-6">
            <ul class="list-unstyled">
              <li><a href="/" class="home">Home</a></li>
              <li><a href="/about">About OpenReview</a></li>
              <li><a href="#" data-toggle="modal" data-target="#feedback-modal">Feedback</a></li>
            </ul>
          </div>
    
          <div class="col-xs-6">
            <ul class="list-unstyled">
              <li><a href="/contact">Contact</a></li>
              <li><a href="/terms">Terms of Service</a></li>
              <li><a href="/privacy">Privacy Policy</a></li>
            </ul>
          </div>
        </div>
      </div>
    </footer>
    
    <footer class="sponsor">
      <div class="container">
        <div class="row">
          <div class="col-sm-10 col-sm-offset-1">
            <p class="text-center">
              OpenReview is created by the <a href="http://www.iesl.cs.umass.edu/" target="_blank">Information Extraction and Synthesis Laboratory</a>, College of Information and Computer Science, University of Massachusetts Amherst. We gratefully acknowledge the support of the OpenReview sponsors:  Google,  Facebook, NSF, the University of Massachusetts Amherst Center for Data Science, and Center for Intelligent Information Retrieval, as well as the Google Cloud Platform for donating the computing and networking services on which OpenReview.net runs.
            </p>
          </div>
        </div>
      </div>
    </footer>

  <div id="feedback-modal" class="modal fade" tabindex="-1" role="dialog">
    <div class="modal-dialog">
      <div class="modal-content">
        <div class="modal-header">
          <button type="button" class="close" data-dismiss="modal" aria-label="Close"><span aria-hidden="true">×</span></button>
          <h3 class="modal-title">Send Feedback</h3>
        </div>
        <div class="modal-body">
          <p>Enter your feedback below and we'll get back to you as soon as possible.</p>
          <form action="/feedback" method="POST">
            <div class="form-group">
              <input type="email" name="from" class="form-control" placeholder="Email" />
            </div>
            <div class="form-group">
              <input type="text" name="subject" class="form-control" placeholder="Subject" />
            </div>
            <div class="form-group">
              <textarea name="message" class="form-control feedback-input" rows="5" placeholder="Message" required=""></textarea>
            </div>
          <ul id="ui-id-2" tabindex="0" class="ui-menu ui-widget ui-widget-content" style="display: none;"></ul></form>
        </div>
        <div class="modal-footer">
          <button type="button" class="btn btn-default" data-dismiss="modal">Cancel</button>
          <button type="button" class="btn btn-primary">Send</button>
        </div>
      </div>
    </div>
  </div>

  <script type="text/javascript" async="" src="https://www.google-analytics.com/analytics.js"></script><script src="//cdnjs.cloudflare.com/ajax/libs/jquery/2.2.4/jquery.min.js"></script>
  <script>window.jQuery || document.write('&lt;script src="/static/js/vendor/jquery-2.2.4.min.js"&gt;&lt;\/script&gt;')</script>

  <script src="/static/dist/vendor.min.js"></script>
  <script src="/static/dist/templates.min.js"></script>
  <script src="/static/dist/openreview.min.js"></script>

    <script>window.legacyScripts = true;</script>


    <script async="" src="https://www.googletagmanager.com/gtag/js?id=UA-108703919-1"></script>
    <script>
      window.dataLayer = window.dataLayer || [];
      function gtag() { dataLayer.push(arguments); }
      gtag('js', new Date());
      gtag('config', 'UA-108703919-1');
    </script>


<div role="status" aria-live="assertive" aria-relevant="additions" class="ui-helper-hidden-accessible"></div></body></html>