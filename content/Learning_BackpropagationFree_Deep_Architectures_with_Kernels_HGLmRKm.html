<!DOCTYPE html><html xmlns="http://www.w3.org/1999/xhtml" class="no-js"><head>
  <meta charset="utf-8" />
  <meta http-equiv="X-UA-Compatible" content="IE=edge,chrome=1" />
  <meta name="viewport" content="width=device-width, initial-scale=1" />

  <title>Learning Backpropagation-Free Deep Architectures with Kernels | OpenReview</title>
  <meta name="description" content="" />

      <meta name="citation_title" content="Learning Backpropagation-Free Deep Architectures with Kernels" />
        <meta name="citation_author" content="Anonymous" />
      <meta name="citation_publication_date" content="2018/09/27" />
      <meta name="citation_online_date" content="2018/09/27" />
      <meta name="citation_pdf_url" content="https://openreview.net/pdf?id=H1GLm2R9Km" />
      <meta name="twitter:card" content="summary" />
      <meta name="twitter:site" content="@openreviewnet" />
      <meta name="og:title" content="Learning Backpropagation-Free Deep Architectures with Kernels" />
      <meta name="og:description" content="One can substitute each neuron in any neural network with a kernel machine and obtain a counterpart powered by kernel machines. The new network inherits the expressive power and architecture of the..." />
      <meta name="og:image" content="https://openreview.net/static/images/openreview_logo_512.png" />

  <link rel="stylesheet" href="https://fonts.googleapis.com/css?family=Noto+Sans:400,400i,700,700i" />

  <link rel="stylesheet" href="/static/css/bootstrap.min.css" />
  <link rel="stylesheet" href="/static/css/jquery-ui.min.css" />
  <link rel="stylesheet" href="/static/css/main.min.css" />
</head>

<body class="forum">
  <nav class="navbar navbar-inverse navbar-fixed-top" role="navigation">
    <div class="container">
  
      <div class="navbar-header">
        <button type="button" class="navbar-toggle collapsed" data-toggle="collapse" data-target="#navbar" aria-expanded="false" aria-controls="navbar">
          <span class="sr-only">Toggle navigation</span>
          <span class="icon-bar"></span>
          <span class="icon-bar"></span>
          <span class="icon-bar"></span>
        </button>
        <a class="navbar-brand home push-link" href="/"><strong>OpenReview</strong>.net</a>
      </div>
  
      <div id="navbar" class="navbar-collapse collapse">
        <form class="navbar-form navbar-left profile-search" role="search">
          <div class="form-group has-feedback">
            <input id="search_input" type="text" class="form-control ui-autocomplete-input" placeholder="Search ICLR 2019 Conference" autocomplete="off" />
            <span class="glyphicon glyphicon-search form-control-feedback" aria-hidden="true"></span>
          </div>
  
          <input id="search_group" type="hidden" value="ICLR.cc/2019/Conference" />
          <input id="search_content" type="hidden" value="all" />
        <ul id="ui-id-1" tabindex="0" class="ui-menu ui-widget ui-widget-content ui-autocomplete ui-front" style="display: none;"></ul></form>
  
        <ul class="nav navbar-nav navbar-right">
        
            <li id="user-menu"><a href="/login">Login</a></li>
        </ul>
      </div>
  
    </div>
  </nav>

  <div id="or-banner" class="banner" style="">
  <div class="container">
    <div class="row">
      <div class="col-xs-12"><a href="/group?id=ICLR.cc/2019/Conference" title="Venue Homepage"><img class="icon" src="/static/images/arrow_left.svg" /> Go to <strong>ICLR 2019 Conference</strong> homepage</a></div>
    </div>
  </div>
</div>
<div id="flash-message-container" class="alert alert-danger" role="alert" style="display: none;">
  <div class="container">
    <div class="row">
      <div class="col-xs-12">
        <div class="alert-content">
          <button type="button" class="close" aria-label="Close"><span aria-hidden="true">Ã—</span></button>
        </div>
      </div>
    </div>
  </div>
</div>

<div class="container">
  <div class="row">
    <div class="col-xs-12">
      <main id="content" class="clearfix openbanner-visible legacy-styles"><div id="note_H1GLm2R9Km" class="note panel"><div class="title_pdf_row clearfix"><h2 class="note_content_title"><a>Learning Backpropagation-Free Deep Architectures with Kernels</a> <a class="note_content_pdf" href="/pdf?id=H1GLm2R9Km" title="Download PDF" target="_blank"><img src="/static/images/pdf_icon_blue.svg" /></a> </h2></div><div class="meta_row"><span class="signatures">Anonymous</span></div><div class="clearfix"><div class="meta_row pull-left"><span class="date item">28 Sep 2018 (modified: 19 Nov 2018)</span><span class="item">ICLR 2019 Conference Blind Submission</span><span class="item">Readers: <span class="readers-icon glyphicon glyphicon-globe"></span> Everyone</span><span class="item"><a class="action-bibtex-modal" data-bibtex="@inproceedings{    &#10;anonymous2019learning,    &#10;title={Learning More Interpretable, Backpropagation-Free Deep Architectures with Kernels},    &#10;author={Anonymous},    &#10;booktitle={Submitted to International Conference on Learning Representations},    &#10;year={2019},    &#10;url={https://openreview.net/forum?id=H1GLm2R9Km},    &#10;note={under review}    &#10;}">Show Bibtex</a></span><a class="note_content_pdf item" href="/revisions?id=H1GLm2R9Km" target="_blank">Show Revisions</a></div><div class="meta_row meta_actions"></div></div><div class="note_contents"><span class="note_content_field">Abstract: </span><span class="note_content_value">One can substitute each neuron in any neural network with a kernel machine and obtain a counterpart powered by kernel machines. The new network inherits the expressive power and architecture of the original but works in a more intuitive way since each node enjoys the simple interpretation as a hyperplane (in a reproducing kernel Hilbert space). Further, using the kernel multilayer perceptron as an example, we prove that in classification, an optimal representation that minimizes the risk of the network can be characterized for each hidden layer. This result removes the need of backpropagation in learning the model and can be generalized to any feedforward kernel network. Moreover, unlike backpropagation, which turns models into black boxes, the optimal hidden representation enjoys an intuitive geometric interpretation, making the dynamics of learning in a deep kernel network simple to understand. Empirical results are provided to validate our theory.</span></div><div class="note_contents"><span class="note_content_field">Keywords: </span><span class="note_content_value">supervised learning, backpropagation-free deep architecture, kernel method</span></div><div class="note_contents"><span class="note_content_field">TL;DR: </span><span class="note_content_value">We combine kernel method with connectionist models and show that the resulting deep architectures can be trained layer-wise and have more transparent learning dynamics. </span></div><div class="reply_row clearfix"><div class="item" id="reply_count">23 Replies</div></div></div><hr class="small" /><div id="note_children"><div class="note_with_children"><div id="note_ryejatHZaQ" class="note panel"><div class="title_pdf_row clearfix"><h2 class="note_content_title"><a>Reply to all reviewers</a>  <button class="btn btn-xs btn-default permalink-button" title="Link to this comment" data-permalink-url="https://openreview.net/forum?id=H1GLm2R9Km&amp;noteId=ryejatHZaQ"><span class="glyphicon glyphicon-link" aria-hidden="true"></span></button></h2></div><div class="meta_row"><span class="signatures">ICLR 2019 Conference Paper1364 Authors</span></div><div class="clearfix"><div class="meta_row pull-left"><span class="date item">08 Nov 2018</span><span class="item">ICLR 2019 Conference Paper1364 Official Comment</span><span class="item">Readers: <span class="readers-icon glyphicon glyphicon-globe"></span> Everyone</span></div><div class="meta_row meta_actions"></div></div><div class="note_contents"><span class="note_content_field">Comment: </span><span class="note_content_value">
CONCERN ON INTERPRETABILITY:

All three reviewers pointed out that it is somewhat imprecise to claim that KN or the layerwise learning paradigm is more interpretable than NN trained with BP. We appreciate the reviewers for bringing up this important issue and we will try our best to clarify our claim.

First of all, we think that KN is more interpretable in the sense that thanks to the use of kernel function, the model can be embedded in an inner product space in which it is linear. The inner product space provides constructions to interpret learning geometrically and the model being linear makes it easy to visualize and to work with. This enables us to reduce rather abstract problems such as what is the best hidden representation at a given layer to geometric ones. Our derivation of the optimal hidden representations essentially utilized this nice property of KN. Also, this provided a straightforward geometric interpretation of the learning dynamics in greedily-trained KN, as we have discussed in page 5 and page 6.

For NN, the nonlinearities lack such a natural mathematical construction in which we can easily talk about geometric concepts such as distance, angle, etc. Note that although we can embed the hidden activations of an NN in some proper Euclidean space, but the model is still not linear and hence is difficult to deal with in that space, defeating the purpose of such an embedding. Moreover, in contrast to KN learned layer-by-layer, the interpretation of the learning dynamics of NN learned with BP remains a challenging theoretical problem. Interestingly, the most notable work along this line of research showed (empirically) that the learning dynamics in NN seem to agree with our theory for KN [1].

Moreover, the design process of a KN is now more transparent and intuitive with layer-wise training. This is because by construction of the layer-wise learning algorithm, the quality of hidden representations can be evaluated explicitly at each layer, which makes it possible to trace the bad performance of the overall model to a certain layer and debug the layers individually. 

Although, we are not claiming that KN is as transparent as a simple linear model in which the contribution of each input feature to the output can be directly identified. We agree that in that sense, KN and NN are both difficult, if not impossible, to interpret. We agree that this is perhaps the more commonly used definition for model interpretability in machine learning. And we have changed the title of the paper as well as the parts in which interpretability of KN is discussed to avoid any confusion. The changes are reflected in the newest manuscript.

---------------------------------------------

UPDATED MANUSCRIPT:

We have uploaded a new version of the paper. Some changes were made to the text according to reviewers' requests. And results from MLPs trained with Adam, RMSProp together with batch normalization were added as suggested.

We are currently working on getting results of kMLPs trained with standard BP as suggested. Also, we are trying to get results on more datasets. We will keep the reviewers informed when we add new results.

---------------------------------------------

PAPER LENGTH:

Due to the additions of a few comments as requested by some of the reviewers, the main text in our newest manuscript is currently about 8.5 pages. WE WILL BE WORKING ON THIS OVER THE NEXT FEW DAYS AND WILL ENSURE THAT THE MAIN TEXT IN THE FINAL VERSION IS CONTAINED IN 8 PAGES.



[1] Raghu, M., Gilmer, J., Yosinski, J., &amp; Sohl-Dickstein, J. (2017). Svcca: Singular vector canonical correlation analysis for deep learning dynamics and interpretability. In Advances in Neural Information Processing Systems (pp. 6076-6085).</span></div><div class="reply_row clearfix"></div></div><div class="children"></div></div><div class="note_with_children"><div id="note_rJelxUjc2X" class="note panel"><div class="title_pdf_row clearfix"><h2 class="note_content_title"><a>Good paper, some things are oversold</a>  <button class="btn btn-xs btn-default permalink-button" title="Link to this comment" data-permalink-url="https://openreview.net/forum?id=H1GLm2R9Km&amp;noteId=rJelxUjc2X"><span class="glyphicon glyphicon-link" aria-hidden="true"></span></button></h2></div><div class="meta_row"><span class="signatures">ICLR 2019 Conference Paper1364 AnonReviewer2</span></div><div class="clearfix"><div class="meta_row pull-left"><span class="date item">03 Nov 2018 (modified: 07 Nov 2018)</span><span class="item">ICLR 2019 Conference Paper1364 Official Review</span><span class="item">Readers: <span class="readers-icon glyphicon glyphicon-globe"></span> Everyone</span></div><div class="meta_row meta_actions"></div></div><div class="note_contents"><span class="note_content_field">Review: </span><span class="note_content_value">This paper attempts to learn layers of NNs greedily one at a time by using kernel machines as nodes instead of standard nonlinearities. The paper is well-written and was an interesting read, despite being notation heavy. 

I think the interpretability claims have some merits but are over-stated. Furthermore, the expressive power of universal approximation through kernels holds only asymptotically. So I am not sure if the authors can claim equivalence in expressive powers to more traditional NNs theoretically. I have some additional questions about the paper, and I am reserving my recommendation on this paper till the authors answer them. 

1) Since individual node is simply a hyperplane in the induced kernel space, why not just specify the cost function as the risk + \tau * norm(weights) ?  What is the benefit of explicitly talking about gaussian complexities and delineating Theorem 4.2 when the same can be achieved by writing a much simpler form? Lemmas 4.4 and 4.5 should be straightforward extensions too if just used in this form since Lemma C.1 follows easily, and again could be simplified a lot by just using the regularized cost function. Am I missing something here?


2) Lemma 4.3 assumes separability (since c should be &gt; a for \tau to be positive) of classes, and also balanced classes (since number of positives = number of negatives). Why are these assumptions reasonable ? I understand that the empirical evaluation presented do justify the methodology, but I am wondering if based on these assumptions the theoretical results are of any use in the way they are currently presented. 

Minor :
Below Def 4.1  "to a standard normal distribution " should be "according to P".
Some typos, please proof read e.g. spelling error "represnetation ". </span></div><div class="note_contents"><span class="note_content_field">Rating: </span><span class="note_content_value">6: Marginally above acceptance threshold</span></div><div class="note_contents"><span class="note_content_field">Confidence: </span><span class="note_content_value">4: The reviewer is confident but not absolutely certain that the evaluation is correct</span></div><div class="reply_row clearfix"></div></div><div class="children"><div class="note_with_children"><div id="note_S1x314iyCm" class="note panel"><div class="title_pdf_row clearfix"><h2 class="note_content_title"><a>New manuscript has been uploaded</a>  <button class="btn btn-xs btn-default permalink-button" title="Link to this comment" data-permalink-url="https://openreview.net/forum?id=H1GLm2R9Km&amp;noteId=S1x314iyCm"><span class="glyphicon glyphicon-link" aria-hidden="true"></span></button></h2></div><div class="meta_row"><span class="signatures">ICLR 2019 Conference Paper1364 Authors</span></div><div class="clearfix"><div class="meta_row pull-left"><span class="date item">19 Nov 2018</span><span class="item">ICLR 2019 Conference Paper1364 Official Comment</span><span class="item">Readers: <span class="readers-icon glyphicon glyphicon-globe"></span> Everyone</span></div><div class="meta_row meta_actions"></div></div><div class="note_contents"><span class="note_content_field">Comment: </span><span class="note_content_value">
Dear Reviewer 2,

Weâ€™ve uploaded a new version of our paper. The main text is now 8-page long. The changes were mainly made to Section 6 (Experiments) and the rest of the paper is essentially the same as the previous version.

As the rebuttal period is coming to an end, we would like to thank you for your earlier comments and questions. They provided us with new perspectives and helped us tremendously in improving our paper. We would really appreciate it if you could please let us know your thoughts on our reply. In particular, we hope we have adequately answered your two major technical questions. Please let us know if you have any further concerns.

We understand that you must be very busy so any comment, detailed or brief, would be greatly appreciated. Thank you very much.</span></div><div class="reply_row clearfix"></div></div><div class="children"></div></div><div class="note_with_children"><div id="note_B1xshY_qp7" class="note panel"><div class="title_pdf_row clearfix"><h2 class="note_content_title"><a>New manuscript with more experimental results has been uploaded</a>  <button class="btn btn-xs btn-default permalink-button" title="Link to this comment" data-permalink-url="https://openreview.net/forum?id=H1GLm2R9Km&amp;noteId=B1xshY_qp7"><span class="glyphicon glyphicon-link" aria-hidden="true"></span></button></h2></div><div class="meta_row"><span class="signatures">ICLR 2019 Conference Paper1364 Authors</span></div><div class="clearfix"><div class="meta_row pull-left"><span class="date item">15 Nov 2018</span><span class="item">ICLR 2019 Conference Paper1364 Official Comment</span><span class="item">Readers: <span class="readers-icon glyphicon glyphicon-globe"></span> Everyone</span></div><div class="meta_row meta_actions"></div></div><div class="note_contents"><span class="note_content_field">Comment: </span><span class="note_content_value">
Dear Reviewer 2,

We have finished new experiments on the standard MNIST dataset. In these experiments, we focused on further empirically studying the proposed layer-wise learning algorithm. Results from kMLPs and MLPs trained with BP are provided. The MLPs were optimized using RMSProp. Moreover, batch normalization and dropout were used to boost the performance of the MLPs. For both the single-hidden-layer and the two-hidden layer kMLPs, the layer-wise algorithm consistently outperformed BP. And the greedily-trained kMLPs compared favorably with the MLPs even though neither batch normalization nor dropout was used for the former.

Also, two standard acceleration methods for kernel machines, including the popular random Fourier features [1], were compared with the proposed acceleration trick. The resulting accelerated BP-trained kMLPs were still outperformed by the accelerated greedy kMLPs. Note that the proposed trick is extremely simple and requires almost no extra overhead (it requires just a random sampling of the training set after the previous layer has been trained).

The main text of this version of our paper is 8.5 pages. We are working on it and will upload an 8-page version in the next few days. Thank you very much.



[1] Rahimi, A., &amp; Recht, B. (2008). Random features for large-scale kernel machines. In Advances in neural information processing systems (pp. 1177-1184).</span></div><div class="reply_row clearfix"></div></div><div class="children"></div></div><div class="note_with_children"><div id="note_r1xNY6K867" class="note panel"><div class="title_pdf_row clearfix"><h2 class="note_content_title"><a>New manuscript has been uploaded</a>  <button class="btn btn-xs btn-default permalink-button" title="Link to this comment" data-permalink-url="https://openreview.net/forum?id=H1GLm2R9Km&amp;noteId=r1xNY6K867"><span class="glyphicon glyphicon-link" aria-hidden="true"></span></button></h2></div><div class="meta_row"><span class="signatures">ICLR 2019 Conference Paper1364 Authors</span></div><div class="clearfix"><div class="meta_row pull-left"><span class="date item">12 Nov 2018</span><span class="item">ICLR 2019 Conference Paper1364 Official Comment</span><span class="item">Readers: <span class="readers-icon glyphicon glyphicon-globe"></span> Everyone</span></div><div class="meta_row meta_actions"></div></div><div class="note_contents"><span class="note_content_field">Comment: </span><span class="note_content_value">
Dear Reviewer 2,

We have updated our manuscript. The main change is a reformulation of the two key lemmas (Lemma 4.3 and Lemma 4.5), as requested by Reviewer 1. We think this new formulation is clearer than the previous one. We have also rewritten the definition of f^(i)_j in par. 3, Section 2, since Reviewer 3 mentioned that the original one was unclear. Also, the main text of the paper is now contained in 8 pages. Thank you very much.</span></div><div class="reply_row clearfix"></div></div><div class="children"></div></div><div class="note_with_children"><div id="note_B1l4k2B-am" class="note panel"><div class="title_pdf_row clearfix"><h2 class="note_content_title"><a>Reply to Reviewer 2 [2/2]</a>  <button class="btn btn-xs btn-default permalink-button" title="Link to this comment" data-permalink-url="https://openreview.net/forum?id=H1GLm2R9Km&amp;noteId=B1l4k2B-am"><span class="glyphicon glyphicon-link" aria-hidden="true"></span></button></h2></div><div class="meta_row"><span class="signatures">ICLR 2019 Conference Paper1364 Authors</span></div><div class="clearfix"><div class="meta_row pull-left"><span class="date item">08 Nov 2018</span><span class="item">ICLR 2019 Conference Paper1364 Official Comment</span><span class="item">Readers: <span class="readers-icon glyphicon glyphicon-globe"></span> Everyone</span></div><div class="meta_row meta_actions"></div></div><div class="note_contents"><span class="note_content_field">Comment: </span><span class="note_content_value">
-----------------
QUESTIONS:
-----------------

- 1) Since individual node is simply a hyperplane in the induced kernel space, why not just specify the cost function as the risk + \tau norm(weights) ?  What is the benefit of explicitly talking about gaussian complexities and delineating Theorem 4.2 when the same can be achieved by writing a much simpler form?

[The fact that minimizing empirical loss + \tau norm (weights) effectively minimizes the true (expected) risk is ultimately justified for NN by bounds analogous to that in Theorem 4.2 [6][7][8]. And since KN is structurally different from NN, we wanted to be more rigorous by deriving similar bounds for our novel architecture and justifying the effectiveness of the layer-wise training algorithm from first principles. In other words, we wanted to prove that the learning algorithm we designed for KN would actually minimize the true risk and hence guarantee generalization to unseen data.

Despite that, as the reviewer has pointed out, each node in a KN is a hyperplane in an RKHS, it is not entirely clear to us how the above result should follow easily as, after all, the hyperplane is in an RKHS instead of the input space.]

- Lemmas 4.4 and 4.5 should be straightforward extensions too if just used in this form since Lemma C.1 follows easily, and again could be simplified a lot by just using the regularized cost function. Am I missing something here?

[The difficulty in going from Theorem 4.2 + Lemma 4.3 to Lemma 4.4 + Lemma 4.5 is that the losses are different and so are the dimensions of the layers. These two factors eventually required a different bound (Lemma 4.4) and also a new proof of optimality for the so-defined optimal representation in Lemma 4.3.]

---------------------------------------------

- 2) Lemma 4.3 assumes separability (since c should be &gt; a for \tau to be positive) of classes, and also balanced classes (since number of positives = number of negatives). Why are these assumptions reasonable ? I understand that the empirical evaluation presented do justify the methodology, but I am wondering if based on these assumptions the theoretical results are of any use in the way they are currently presented. 

[About the first assumption, recall from Section 3 that a := min_{x, y} k(x, y) and c := max_{x, y} k(x, y). Hence, unless the kernel chosen is a constant (which is of course not of interest in practice), a &lt; c by construction. So we are really making no assumption here. Moreover, just to clarify, neither Lemma 4.3 nor Lemma 4.5 requires a separability assumption.

In terms of the assumption that the classes are balanced, we have been working on it since the initial submission and it turns out that this assumption is not needed at all. We have updated the proof. Please see the new proof on page 16 for details. We thank the reviewer for bringing this up.]

----------
MINOR:
----------

- Below Def 4.1  "to a standard normal distribution" should be "according to P".

[Here, we refer to the sequence g_1, ..., g_N as the sequence that is being ``fitted''. And the g_n's are i.i.d. standard normal random variables. This interpretation was taken directly from [7]. We have updated the manuscript to clarify.]

---------------------------------------------

- Some typos

[We have corrected the typos in the newly uploaded version and we thank the reviewer for brining them into our attention.]



[1] Park, J., &amp; Sandberg, I. W. (1991). Universal approximation using radial-basis-function networks. Neural computation, 3(2), 246-257.

[2] Cybenko, G. (1989). Approximation by superpositions of a sigmoidal function. Mathematics of control, signals and systems, 2(4), 303-314.

[3] Funahashi, K. I. (1989). On the approximate realization of continuous mappings by neural networks. Neural networks, 2(3), 183-192.

[4] Barron, A. R. (1993). Universal approximation bounds for superpositions of a sigmoidal function. IEEE Transactions on Information theory, 39(3), 930-945.

[5] Sun, S., Chen, W., Wang, L., Liu, X., &amp; Liu, T. Y. (2016, February). On the Depth of Deep Neural Networks: A Theoretical View. In AAAI (pp. 2066-2072).

[6] Bartlett, P. L. (1997). For valid generalization the size of the weights is more important than the size of the network. In Advances in neural information processing systems (pp. 134-140).

[7] Bartlett, P. L., &amp; Mendelson, S. (2002). Rademacher and Gaussian complexities: Risk bounds and structural results. Journal of Machine Learning Research, 3(Nov), 463-482.

[8] Shalev-Shwartz, S., &amp; Ben-David, S. (2014). Understanding machine learning: From theory to algorithms. Cambridge university press.
</span></div><div class="reply_row clearfix"></div></div><div class="children"></div></div><div class="note_with_children"><div id="note_rkeHpoBWaQ" class="note panel"><div class="title_pdf_row clearfix"><h2 class="note_content_title"><a>Reply to Reviewer 2 [1/2]</a>  <button class="btn btn-xs btn-default permalink-button" title="Link to this comment" data-permalink-url="https://openreview.net/forum?id=H1GLm2R9Km&amp;noteId=rkeHpoBWaQ"><span class="glyphicon glyphicon-link" aria-hidden="true"></span></button></h2></div><div class="meta_row"><span class="signatures">ICLR 2019 Conference Paper1364 Authors</span></div><div class="clearfix"><div class="meta_row pull-left"><span class="date item">08 Nov 2018</span><span class="item">ICLR 2019 Conference Paper1364 Official Comment</span><span class="item">Readers: <span class="readers-icon glyphicon glyphicon-globe"></span> Everyone</span></div><div class="meta_row meta_actions"></div></div><div class="note_contents"><span class="note_content_field">Comment: </span><span class="note_content_value">
Firstly, we would like to thank Reviewer 2 for the insightful review. We have found the comments and questions really helpful and we now address them in details. We do hope that Reviewer 2 finds our response satisfying.

Comments from the reviewer are listed first with each preceded by a dash. Our replies are put in brackets.

-----------------
COMMENTS:
-----------------

- I think the interpretability claims have some merits but are over-stated.

[We are thankful that the reviewer brought up this important issue. Please see our reply to all reviewers for our response.]

---------------------------------------------

- Furthermore, the expressive power of universal approximation through kernels holds only asymptotically. So I am not sure if the authors can claim equivalence in expressive powers to more traditional NNs theoretically.

[Generally, we think that the issue of expressive power is a highly abstract one and it is usually difficult to argue which model possesses stronger expressive power in very concrete terms. Although, we do agree with the reviewer that the universal approximation property of kernel machines holds only when we do not limit the number and the positions of its centroids [1]. Nevertheless, to the best of our knowledge, all classical universal approximation results for NNs are also asymptotical results [2][3][4].

Moreover, intuitively, a single node in a KN (a kernel machine) is already (asymptotically) a universal function approximator. In contrast, it takes at least two layers of NN to be (asymptotically) a universal function approximator. 

Further, in terms of some complexity measures such as Gaussian complexity, Lemma B.2 in our paper seems to show that the model complexity of kMLP is comparable to that of MLP [5]. And they also scale in a similar way in the depth and width of the network.

Combining the arguments above, we expect KN to be at least comparable to NN in terms of expressive power, which is corroborated by our experimental results in the paper.]

</span></div><div class="reply_row clearfix"></div></div><div class="children"></div></div></div></div><div class="note_with_children"><div id="note_H1xZy96tnQ" class="note panel"><div class="title_pdf_row clearfix"><h2 class="note_content_title"><a>Interesting theoretical work which could be improved by further experimental work</a>  <button class="btn btn-xs btn-default permalink-button" title="Link to this comment" data-permalink-url="https://openreview.net/forum?id=H1GLm2R9Km&amp;noteId=H1xZy96tnQ"><span class="glyphicon glyphicon-link" aria-hidden="true"></span></button></h2></div><div class="meta_row"><span class="signatures">ICLR 2019 Conference Paper1364 AnonReviewer1</span></div><div class="clearfix"><div class="meta_row pull-left"><span class="date item">02 Nov 2018 (modified: 07 Nov 2018)</span><span class="item">ICLR 2019 Conference Paper1364 Official Review</span><span class="item">Readers: <span class="readers-icon glyphicon glyphicon-globe"></span> Everyone</span></div><div class="meta_row meta_actions"></div></div><div class="note_contents"><span class="note_content_field">Review: </span><span class="note_content_value">This paper explores integration of kernel machines with neural networks based on replacing the non-linear function represented by each neuron with a function living in some pre-defined RKHS. From the theoretical standpoint, this work is a clear improvement upon the work of Zhang et al. (2017). Authors further propose a layer-wise training algorithm based on optimisation of a particular similarity measure between embeddings based on their class assignments at each layer, which eliminates necessity of gradient-based training. However, the experimental performance of the proposed algorithm is somewhat lacking in comparison, perhaps because the authors focus on kernelised equivalents of MLPs instead of CNNs as Zhang et al.

My rating of the paper is mainly due to the lack of experimental evidence for usefulness of the layer-wise training, and absence of experimental comparison with several baselines (see details below). It is also unclear whether the structure of KNs is significantly better than that of NNs in terms of interpretability. Apart from the comments below, I would like to ask the authors to discuss relation to the following related papers:

	1) Kulkarni &amp; Karande, 2017: "Layer-wise training of deep networks using kernel similarity" <a href="https://arxiv.org/pdf/1703.07115.pdf" target="_blank" rel="nofollow">https://arxiv.org/pdf/1703.07115.pdf</a>

	2) Scardapanea et al., 2017: "Kafnets: kernel-based non-parametric activation functions for neural networks" https://arxiv.org/pdf/1707.04035.pdf


Detailed comments:

Theory

- (Sec 4.1) Backpropagation (BP) is being criticised: BP is only a particular implementation of gradient calculation. It seems to me that your criticisms are thus more related to use of iterative gradient-based optimisation algorithms, rather than to obtaining gradients through BP?! Regarding the criticism that BP forces intermediate layers to correct for "mistakes" made by layers higher up: it seems your layer-wise algorithm attempts to learn the best possible representation in first layer, and then progresses to the next layer where it tries to correct for the potential error of the first layer and so on. In other words it seems that the errors of layers are propagated from first to last, instead of last to first as in BP, but are still being propagated in a sense. I do not immediately see why propagation forward should be preferable. Can you please further explain this point?

- It is proven in the appendix (Lemma B.3) that under certain conditions stacking additional layers never leads to degradation of training loss. Can you please clarify whether additional layers can be helpful even in the case where previous layers already succeeded in learning the optimal representation?

- (Sec 4.1) Layer-wise vs. network-wise optimality: I find the claim that BP-based learner is not aware of the network-wise optimality confusing. BP explicitly optimises for network-wise optimality and the relative contribution to the network-wise error of each weight is propagated accordingly. I suppose my confusion stems from lack of a clear description of what defines a learner "aware" or "blind" to network-wise optimality. In general, I am not convinced layer-wise optimality is a useful criterion when what we want to achieve is network-wise optimality. As you show in the appendix, if layer-wise optimality is achieved then it implies network-wise optimality; however, layer-wise optimality is only a sufficient condition and likely not a necessary one (except for the simplified scenario studied in B.3). It is thus not clear to me why layer-wise training would always be preferable to network-wise training (e.g. using BP) especially because its greedy nature might intuitively prevent learning of hierarchical representations which are commonly claimed to be key to the success of neural networks. Can you please clarify?

- (Sec 4.2) I think it would be beneficial to state in the introduction that the "risk" is with respect to the hinge loss which is common in the SVM/kernel literature but much less in the deep learning literature and thus could surprise a few people when they reach this point. 
Futher questions:
	- From Lemma 4.3, it seems that the derived representation is only optimal with respect to the **upper bound** on the empirical risk (which for \tau &gt;= 2 will be an upper bound on the population risk). I got slightly confused at this point as my interpretation of the previous text was that the representation is optimal with respect to the population risk itself. Does the upper bound have the same set of optima? Please clarify.

	- (p.5) There are two assumptions that I find somewhat restrictive. Just before Lemma 4.3 you assume that the number of points in each class must be the same. Can you comment on whether you expect the same representation to be optimal for classification problems with significantly imbalanced number of samples per class? The second assumption is after Lemma 4.4 where you state that the stationary kernel k^{l-1} should attain its infinum for all x, y s.t. || x - y || greater than some threshold. This does not hold for many of the popular kernels like RBF, Matern, or inverse multiquadric. Do you think this assumption can be relaxed?

	- (p.5) Choice of the dissimilarity measure for G: Can you provide more intuition about why you selected L^1 distance and whether you would expect different results with L^2 or other common metrics?

- (Sec 4.3) Can you please provide more detaild about the relation of the proposed objective (\hat(R)(F) + \tau max_j ||f_j||_H) to Lemmas 4.3 and 4.5 where the optimal representation was derived for functions that optimise an upper bound in terms of Gaussian complexity (e.g. is the representation that minimises risk w.r.t. the Gaussian bound also optimal with respect to functions that optimise this objective)?


Experiments

- I would appreciate addition of some standard baselines, like MLP combined with dropout or batch normalisation, and optimised with RMSProp (or similar). These would greatly help with assessing competitiveness with current SOTA results.

- It would be nice to see the relative contribution of the two main components of the paper. Specifically, an experiment which would evaluate empirical performance of KNs optimised by some form of gradient descent vs. by your layer-wise training rule would be very insightful.


Other

- (p.2, 1st par in Sec 2) [minor] You state "a kernel machine is a universal function approximator". I suppose that might be true for a certain class of kernels but not in general?! Please clarify.

- (p.2, 3rd par in Sec 2) [minor] Are you using a particular version of the representer theorem in the representation of f_j^{(i)} as linear combination of feature maps? Please clarify.

- (p.2, end of 1st par in Sec 3) L^{(i)} is defined as sup over X_i. It is not clear to me that this constant is necessarily finite and I suspect it will not be in general (it will for the RBF kernel (and most stationary kernels) used in experiments though). Finiteness of L^{(i)} is necessary for the bound in Eq. (2) to be non-vacuous. Please clarify.

- (p.3, after 1st display in Sec 4.2.1) [minor] Missing dot after "that we wish to minimise". Next sentence states "**the** optimal F" (emphasis mine) -- I am sorry if I overlooked it, but I did not notice a proof that a solution exists and is unique, and am not familiar enough with the literature to immediately see the answer. Perhaps a footnote clarifying the statement would help.

- (p.4, 1st par in Sec 4) You say "A generalisation to regression is reserved for future work". I did not expect that based on the first few pages. On high-level, it seems that generalisation to regression need not be trivial as, for example, the optimal representation derived in Lemma 4.3 and Lemma 4.5 explicitly relies on the classification nature of the problem. Can you comment on expected difficulty of extension to regression? Possibly state in the introduction that only classification is considered in this paper.
	- (p.7, 1st par in Sec 6) [related] "However they did not extend the idea to any **arbitrary** NN" (emphasis mine). Can you please be more specific here?

- (p.5-6) [minor] Last sentence in Lemmas 4.3 and 4.5 is slightly confusing. Can you rephrase please?

- (p.6) [minor] You say "the learned decision boundary would generalise better to unseen data". Can you please clarify the last sentence (e.g. being more precise about the meaning of the word "simple" in the same sentence) and provide reference for why this is necessarily the case?</span></div><div class="note_contents"><span class="note_content_field">Rating: </span><span class="note_content_value">5: Marginally below acceptance threshold</span></div><div class="note_contents"><span class="note_content_field">Confidence: </span><span class="note_content_value">4: The reviewer is confident but not absolutely certain that the evaluation is correct</span></div><div class="reply_row clearfix"></div></div><div class="children"><div class="note_with_children"><div id="note_r1epFXo1Rm" class="note panel"><div class="title_pdf_row clearfix"><h2 class="note_content_title"><a>New manuscript has been uploaded</a>  <button class="btn btn-xs btn-default permalink-button" title="Link to this comment" data-permalink-url="https://openreview.net/forum?id=H1GLm2R9Km&amp;noteId=r1epFXo1Rm"><span class="glyphicon glyphicon-link" aria-hidden="true"></span></button></h2></div><div class="meta_row"><span class="signatures">ICLR 2019 Conference Paper1364 Authors</span></div><div class="clearfix"><div class="meta_row pull-left"><span class="date item">19 Nov 2018</span><span class="item">ICLR 2019 Conference Paper1364 Official Comment</span><span class="item">Readers: <span class="readers-icon glyphicon glyphicon-globe"></span> Everyone</span></div><div class="meta_row meta_actions"></div></div><div class="note_contents"><span class="note_content_field">Comment: </span><span class="note_content_value">
Dear Reviewer 1,

Weâ€™ve uploaded a new version of our paper. The main text is now 8-page long. The changes were mainly made to Section 6 (Experiments) and the rest of the paper is essentially the same as the previous version.

As the rebuttal period is coming to an end, we would like to thank you for your earlier comments and questions. They provided us with new perspectives and helped us tremendously in improving our paper. We would really appreciate it if you could please let us know your thoughts on our reply. We hope we have now added enough experimental results and addressed your other concerns. Please let us know if you think more experimental work is needed or if you have any further comments or questions.

We understand that you must be very busy so any comment, detailed or brief, would be greatly appreciated. Thank you very much.</span></div><div class="reply_row clearfix"></div></div><div class="children"></div></div><div class="note_with_children"><div id="note_H1evQ5_96X" class="note panel"><div class="title_pdf_row clearfix"><h2 class="note_content_title"><a>New manuscript with more experimental results has been uploaded</a>  <button class="btn btn-xs btn-default permalink-button" title="Link to this comment" data-permalink-url="https://openreview.net/forum?id=H1GLm2R9Km&amp;noteId=H1evQ5_96X"><span class="glyphicon glyphicon-link" aria-hidden="true"></span></button></h2></div><div class="meta_row"><span class="signatures">ICLR 2019 Conference Paper1364 Authors</span></div><div class="clearfix"><div class="meta_row pull-left"><span class="date item">15 Nov 2018</span><span class="item">ICLR 2019 Conference Paper1364 Official Comment</span><span class="item">Readers: <span class="readers-icon glyphicon glyphicon-globe"></span> Everyone</span></div><div class="meta_row meta_actions"></div></div><div class="note_contents"><span class="note_content_field">Comment: </span><span class="note_content_value">
Dear Reviewer 1,

We have finished new experiments on the standard MNIST dataset. In these experiments, we focused on further empirically studying the proposed layer-wise learning algorithm. Results from kMLPs and MLPs trained with BP are provided. The MLPs were optimized using RMSProp. Moreover, batch normalization and dropout were used to boost the performance of the MLPs. For both the single-hidden-layer and the two-hidden layer kMLPs, the layer-wise algorithm consistently outperformed BP. And the greedily-trained kMLPs compared favorably with the MLPs even though neither batch normalization nor dropout was used for the former.

Also, two standard acceleration methods for kernel machines, including the popular random Fourier features [1], were compared with the proposed acceleration trick. The resulting accelerated BP-trained kMLPs were still outperformed by the accelerated greedy kMLPs. Note that the proposed trick is extremely simple and requires almost no extra overhead (it requires just a random sampling of the training set after the previous layer has been trained).

The main text of this version of our paper is 8.5 pages. We are working on it and will upload an 8-page version in the next few days. Thank you very much.



[1] Rahimi, A., &amp; Recht, B. (2008). Random features for large-scale kernel machines. In Advances in neural information processing systems (pp. 1177-1184).</span></div><div class="reply_row clearfix"></div></div><div class="children"></div></div><div class="note_with_children"><div id="note_ryg8TpKL67" class="note panel"><div class="title_pdf_row clearfix"><h2 class="note_content_title"><a>New manuscript has been uploaded</a>  <button class="btn btn-xs btn-default permalink-button" title="Link to this comment" data-permalink-url="https://openreview.net/forum?id=H1GLm2R9Km&amp;noteId=ryg8TpKL67"><span class="glyphicon glyphicon-link" aria-hidden="true"></span></button></h2></div><div class="meta_row"><span class="signatures">ICLR 2019 Conference Paper1364 Authors</span></div><div class="clearfix"><div class="meta_row pull-left"><span class="date item">12 Nov 2018</span><span class="item">ICLR 2019 Conference Paper1364 Official Comment</span><span class="item">Readers: <span class="readers-icon glyphicon glyphicon-globe"></span> Everyone</span></div><div class="meta_row meta_actions"></div></div><div class="note_contents"><span class="note_content_field">Comment: </span><span class="note_content_value">
Dear Reviewer 1,

We have updated our manuscript. The main change is a reformulation of the two key lemmas (Lemma 4.3 and Lemma 4.5), as requested by Reviewer 1. We think this new formulation is clearer than the previous one. We have also rewritten the definition of f^(i)_j in par. 3, Section 2, since Reviewer 3 mentioned that the original one was unclear. Also, the main text of the paper is now contained in 8 pages. Thank you very much.</span></div><div class="reply_row clearfix"></div></div><div class="children"></div></div><div class="note_with_children"><div id="note_rJeHa0rZ6Q" class="note panel"><div class="title_pdf_row clearfix"><h2 class="note_content_title"><a>Reply to Reviewer 1 [6/6]</a>  <button class="btn btn-xs btn-default permalink-button" title="Link to this comment" data-permalink-url="https://openreview.net/forum?id=H1GLm2R9Km&amp;noteId=rJeHa0rZ6Q"><span class="glyphicon glyphicon-link" aria-hidden="true"></span></button></h2></div><div class="meta_row"><span class="signatures">ICLR 2019 Conference Paper1364 Authors</span></div><div class="clearfix"><div class="meta_row pull-left"><span class="date item">08 Nov 2018</span><span class="item">ICLR 2019 Conference Paper1364 Official Comment</span><span class="item">Readers: <span class="readers-icon glyphicon glyphicon-globe"></span> Everyone</span></div><div class="meta_row meta_actions"></div></div><div class="note_contents"><span class="note_content_field">Comment: </span><span class="note_content_value">
- (p.7, 1st par in Sec 6) [related] "However they did not extend the idea to any **arbitrary** NN" (emphasis mine). Can you please be more specific here?

[We meant that the early works that tried to "kernelize" NNs considered only certain NN architectures. As we have listed in Section 6, the most general work in this regard proposed only kMLP and the KN equivalent of CNN [7]. In contrast, we proposed that one can convert any NN to an equivalent KN by the simple procedure described in Section 2.]

---------------------------------------------

- (p.5-6) [minor] Last sentence in Lemmas 4.3 and 4.5 is slightly confusing. Can you rephrase please?

[We meant the following: For each given representation of data, denoted F(S), the learning paradigm would return an f^\star that minimizes the loss. And one can compute the loss of this particular f^\star on this specific representation. Then denote as U the set of all representations to which the corresponding f^\star satisfies the condition stated in the last sentence of Lemma 4.3 or 4.5. Now, if a certain representation satisfies Eq. 1 in Lemma 4.3, then the f^\star returned by the learning paradigm under this representation would achieve a loss that is smaller than that corresponding to any representation in U.

Note that the set U is of practical interest as the conditions stated in the last sentence of Lemma 4.3 and 4.5 are merely mild conditions on how well the returned solution minimizes the empirical loss.

We will rephrase the last sentences of the two lemmas, but this change is not included in the current draft as we have not found a good way to rephrase them concisely yet.]

---------------------------------------------

- (p.6) [minor] You say "the learned decision boundary would generalise better to unseen data". Can you please clarify the last sentence (e.g. being more precise about the meaning of the word "simple" in the same sentence) and provide reference for why this is necessarily the case?

[That the learned decision boundary would generalize better to unseen data follows from the fact that the optimal hidden representation at the last layer is so defined such that the bound on the expected classification error of the classifier is minimized. Hence if the l-1^th layer learns a representation that is close to this optimal one, it is of course reasonable to expect the classifier to generalize better to unseen data (sampled from the same distribution as that of the training set).

By "simple", we meant that examples from distinct classes would be far apart in the RKHS and those in the same class would be close. This observation is justified in the proof of Lemma 4.3. And intuitively, this representation is "simple" to classify.]



[1] Raghu, M., Gilmer, J., Yosinski, J., &amp; Sohl-Dickstein, J. (2017). Svcca: Singular vector canonical correlation analysis for deep learning dynamics and interpretability. In Advances in Neural Information Processing Systems (pp. 6076-6085).

[2] Bartlett, P. L. (1997). For valid generalization the size of the weights is more important than the size of the network. In Advances in neural information processing systems (pp. 134-140).

[3] Bartlett, P. L., &amp; Mendelson, S. (2002). Rademacher and Gaussian complexities: Risk bounds and structural results. Journal of Machine Learning Research, 3(Nov), 463-482.

[4] Park, J., &amp; Sandberg, I. W. (1991). Universal approximation using radial-basis-function networks. Neural computation, 3(2), 246-257.

[5] Micchelli, C. A., Xu, Y., &amp; Zhang, H. (2006). Universal kernels. Journal of Machine Learning Research, 7(Dec), 2651-2667.

[6] SchÃ¶lkopf, B., Herbrich, R., &amp; Smola, A. J. (2001, July). A generalized representer theorem. In International conference on computational learning theory (pp. 416-426). Springer, Berlin, Heidelberg.

[7] Zhang, S., Li, J., Xie, P., Zhang, Y., Shao, M., Zhou, H., &amp; Yan, M. (2017). Stacked Kernel Network. arXiv preprint arXiv:1711.09219.
</span></div><div class="reply_row clearfix"></div></div><div class="children"></div></div><div class="note_with_children"><div id="note_SJgeY0rZTX" class="note panel"><div class="title_pdf_row clearfix"><h2 class="note_content_title"><a>Reply to Reviewer 1 [5/6]</a>  <button class="btn btn-xs btn-default permalink-button" title="Link to this comment" data-permalink-url="https://openreview.net/forum?id=H1GLm2R9Km&amp;noteId=SJgeY0rZTX"><span class="glyphicon glyphicon-link" aria-hidden="true"></span></button></h2></div><div class="meta_row"><span class="signatures">ICLR 2019 Conference Paper1364 Authors</span></div><div class="clearfix"><div class="meta_row pull-left"><span class="date item">08 Nov 2018</span><span class="item">ICLR 2019 Conference Paper1364 Official Comment</span><span class="item">Readers: <span class="readers-icon glyphicon glyphicon-globe"></span> Everyone</span></div><div class="meta_row meta_actions"></div></div><div class="note_contents"><span class="note_content_field">Comment: </span><span class="note_content_value">
---------------------
EXPERIMENTS:
---------------------

- I would appreciate addition of some standard baselines, like MLP combined with dropout or batch normalisation, and optimised with RMSProp (or similar). These would greatly help with assessing competitiveness with current SOTA results.

- It would be nice to see the relative contribution of the two main components of the paper. Specifically, an experiment which would evaluate empirical performance of KNs optimised by some form of gradient descent vs. by your layer-wise training rule would be very insightful.

[It would indeed be interesting to see these comparisons. In our initial submission, the results of MLPs were from models optimized with Adam and standard SGD, whichever is better. In the newest manuscript, we have added results from MLPs optimized with standard SGD, Adam and RMSProp + batch normalization. We are running more experiments and will inform the reviewer once we have the results.]

-----------
OTHER:
-----------

- (p.2, 1st par in Sec 2) [minor] You state "a kernel machine is a universal function approximator". I suppose that might be true for a certain class of kernels but not in general?! Please clarify.

[Indeed, this is true for the certain kernels only [4][5]. We have updated the manuscript to be more specific about this and we thank the reviewer for pointing it out.]

---------------------------------------------

- (p.2, 3rd par in Sec 2) [minor] Are you using a particular version of the representer theorem in the representation of f_j^(i) as linear combination of feature maps? Please clarify.

[We are only using the most standard version of the representer theorem [6]. And we have updated the manuscript to clarify this.]

---------------------------------------------

- (p.2, end of 1st par in Sec 3) L^(i) is defined as sup over X_i. It is not clear to me that this constant is necessarily finite and I suspect it will not be in general (it will for the RBF kernel (and most stationary kernels) used in experiments though). Finiteness of L^(i) is necessary for the bound in Eq. (2) to be non-vacuous. Please clarify.

[It is indeed part of the assumption that L^(i) &lt; \infty. And we have updated the manuscript accordingly. Fortunately, as the reviewer pointed out, this assumption is satisfied by most popular kernels.]

---------------------------------------------

- (p.3, after 1st display in Sec 4.2.1) [minor] Missing dot after "that we wish to minimise".

[Could the reviewer please clarify on this comment? We could not figure out what the reviewer meant by "missing dot". Thanks in advance.]

- Next sentence states "**the** optimal F" (emphasis mine) -- I am sorry if I overlooked it, but I did not notice a proof that a solution exists and is unique, and am not familiar enough with the literature to immediately see the answer. Perhaps a footnote clarifying the statement would help.

[Without further assumption on the loss and the hypothesis space, such a global minimum may not be found in practice and also may not be unique (multiple risk-equivalent solutions may exist). Here we are only referring to the best F the learner can find that minimizes R_l. And it is not important whether that F is "the best" or if "the best" exists at all. We have updated the draft to clarify.]

---------------------------------------------

- (p.4, 1st par in Sec 4) You say "A generalisation to regression is reserved for future work". I did not expect that based on the first few pages. On high-level, it seems that generalisation to regression need not be trivial as, for example, the optimal representation derived in Lemma 4.3 and Lemma 4.5 explicitly relies on the classification nature of the problem. Can you comment on expected difficulty of extension to regression? 

[Undoubtedly, generalization of our layer-wise learning algorithm to regression is a nontrivial task. At this point, we only have some very early ideas as to how to proceed. The main difficulty, in our attempt to come up with an equivalent theory for regression, is that the domain is now uncountable instead of a finite set.]

- Possibly state in the introduction that only classification is considered in this paper.

[We have now stated in the Abstract and also in the Introduction that our theoretical results regarding optimal hidden representations are only for classification and are proven only under certain losses.]

---------------------------------------------


</span></div><div class="reply_row clearfix"></div></div><div class="children"></div></div><div class="note_with_children"><div id="note_Byebf0BbT7" class="note panel"><div class="title_pdf_row clearfix"><h2 class="note_content_title"><a>Reply to Reviewer 1 [4/6]</a>  <button class="btn btn-xs btn-default permalink-button" title="Link to this comment" data-permalink-url="https://openreview.net/forum?id=H1GLm2R9Km&amp;noteId=Byebf0BbT7"><span class="glyphicon glyphicon-link" aria-hidden="true"></span></button></h2></div><div class="meta_row"><span class="signatures">ICLR 2019 Conference Paper1364 Authors</span></div><div class="clearfix"><div class="meta_row pull-left"><span class="date item">08 Nov 2018</span><span class="item">ICLR 2019 Conference Paper1364 Official Comment</span><span class="item">Readers: <span class="readers-icon glyphicon glyphicon-globe"></span> Everyone</span></div><div class="meta_row meta_actions"></div></div><div class="note_contents"><span class="note_content_field">Comment: </span><span class="note_content_value">
[The reviewer is absolutely correct. For \tau &gt;= 4\sqrt{c/N} in Lemma 4.3 (\tau &gt;= \frac{8L^(l)Cd_{l-1}}{\max (|c|, |a|)} \sqrt{c/N} in Lemma 4.5), the representation characterized by Lemma 4.3 (Lemma 4.5) is optimal w.r.t. an upper bound on the population risk. Note that N is the size of the training set and hence, for reasonably-sized datasets, these two conditions are relatively mild. 

The true population risk is almost never computable under machine learning settings since we do not make distributional assumptions. For this reason, we believe that it is commonly accepted and also useful to have optimality w.r.t. a bound on the population risk instead of the true population risk. In fact, optimality of this kind justifies many commonly used learning paradigms. For example, for NN in classification, the standard minimization of an empirical risk and a regularization term is considered to be optimal w.r.t. an upper bound on the true risk [2][3].

With that said, we do think that it can be benefitial to investigate the possibility of tightening the bound or deriving new bounds using different techniques, as this may shed light on new optimal hidden representations and hence also new layer-wise learning algorithms. This is one of the directions that we plan to study in the future.]

---------------------------------------------

- (p.5) There are two assumptions that I find somewhat restrictive. Just before Lemma 4.3 you assume that the number of points in each class must be the same. Can you comment on whether you expect the same representation to be optimal for classification problems with significantly imbalanced number of samples per class? 

[We thank the reviewer for pointing this out. This assumption has been removed in the newest manuscript and we have made changes to the proof to justify the removal. Please refer to page 16 for details.]

- The second assumption is after Lemma 4.4 where you state that the stationary kernel k^{l-1} should attain its infinum for all x, y s.t. || x - y || greater than some threshold. This does not hold for many of the popular kernels like RBF, Matern, or inverse multiquadric. Do you think this assumption can be relaxed?

[We agree with the reviewer that this assumption is somewhat restrictive. Unfortunately, this assumption cannot be removed in our current proof of the lemma. We are actively working towards improving or completely removing this assumption. Nevertheless, we think that for kernels with light tails such as the RBF kernel, the value would decay quickly away from the origin (for RBF, the decay is exponentially fast). Hence in practice, we expect that this assumption would not be too far away from reality. In all of our experiments, we have used RBF kernels with varied kernel widths. And we have pointed out that RBF kernels do not strictly satisfy this assumption in Section 7.]

---------------------------------------------

- (p.5) Choice of the dissimilarity measure for G: Can you provide more intuition about why you selected L^1 distance and whether you would expect different results with L^2 or other common metrics?

[The choice of dissimilarity measure is somewhat arbitrary. We do not have a specific reason as to why L^1 distance should be favored over L^2 distance or alignment. And we chose L^1 distance just so that we could obtain a concrete result (Lemma 4.5). Although, we should point out that the proof for Lemma 4.5 used the fact that the loss is L^1 distance. We are currently working on producing quivalent results when the loss is L^2 distance or alignment. In practice, however, we have not noticed any significant performance difference among using different loss functions for the hidden layers in our experiments. Hence we expect theoretical results equivalent to Lemma 4.5 to continue to hold for different losses.]

---------------------------------------------

- (Sec 4.3) Can you please provide more details about the relation of the proposed objective (\hat{R}(F) + \tau max_j ||f_j||_H) to Lemmas 4.3 and 4.5 where the optimal representation was derived for functions that optimise an upper bound in terms of Gaussian complexity (e.g. is the representation that minimises risk w.r.t. the Gaussian bound also optimal with respect to functions that optimise this objective)?

[The bounds in Lemma 4.3 and 4.5 using Gaussian complexity should be combined with the bound on Gaussian complexity in Theorem 4.2. This would give the objective \hat{R}_l(f^(l)) + \tau ||f^(l)||_{H_l} for Lemma 4.3 and \hat{R}_{l-1}(F^(l-1)) + \tau max_j ||f_j^(l-1)||_{H_{l-1}} for Lemma 4.5. We have updated the statements of Lemma 4.3 and 4.5 to clarify and we thank the reviewer for pointing this out.

Note that a bound containing Gaussian complexity cannot be computed or used as a loss function since Gaussian complexity is not computable in practice (it involves computing the expectation of i.i.d. random variables of an unknown distribution).]</span></div><div class="reply_row clearfix"></div></div><div class="children"></div></div><div class="note_with_children"><div id="note_rklkp6H-6X" class="note panel"><div class="title_pdf_row clearfix"><h2 class="note_content_title"><a>Reply to Reviewer 1 [3/6]</a>  <button class="btn btn-xs btn-default permalink-button" title="Link to this comment" data-permalink-url="https://openreview.net/forum?id=H1GLm2R9Km&amp;noteId=rklkp6H-6X"><span class="glyphicon glyphicon-link" aria-hidden="true"></span></button></h2></div><div class="meta_row"><span class="signatures">ICLR 2019 Conference Paper1364 Authors</span></div><div class="clearfix"><div class="meta_row pull-left"><span class="date item">08 Nov 2018</span><span class="item">ICLR 2019 Conference Paper1364 Official Comment</span><span class="item">Readers: <span class="readers-icon glyphicon glyphicon-globe"></span> Everyone</span></div><div class="meta_row meta_actions"></div></div><div class="note_contents"><span class="note_content_field">Comment: </span><span class="note_content_value">
- In general, I am not convinced layer-wise optimality is a useful criterion when what we want to achieve is network-wise optimality. As you show in the appendix, if layer-wise optimality is achieved then it implies network-wise optimality; however, layer-wise optimality is only a sufficient condition and likely not a necessary one (except for the simplified scenario studied in B.3). 

[It is important to note that the proposed layer-wise learning algorithm learns network-wise optimality at each layer given that the regularization coefficient is chosen properly. We now provide details to justify this claim.

Layer-wise optimality simply refers to the target function of each layer, which is crutial for making possible a layer-wise learning algorithm. Note that this is a concept that is local in each layer. As the reviewer pointed out, the end goal for each layer, however, is to learn the network-wise optimality. And as we have shown in the paper, this optimality may not coincide with layer-wise optimality. But note that the notion of network-wise optimality inherently involves the entire network and it is not clear how this can be learned one layer at a time.

One of our contributions in this paper is that we have shown in Lemma 4.6 that learning network-wise optimality is equivalent (upon the choice of a proper regularization coefficient) to learning layer-wise optimality plus minimizing a layer-wise regularization term. Note that the latter quantity involves only terms that are locally defined for each layer. Hence, Lemma 4.6 essentially justifies learning network-wise optimality in a completely layer-wise fashion.

Moreover, this objective coincides with the learning objectives in Lemmas 4.3 and 4.5. Thus, the optimal hidden representations characterized therein concretely describe the target function of each layer, clearing all obstacles for a layer-wise learning scheme. And again, we are guaranteed by Lemma 4.6 that this learning objective, albeit involving only a single layer, would lead us to network-wise optimality.]

- It is thus not clear to me why layer-wise training would always be preferable to network-wise training (e.g. using BP) especially because its greedy nature might intuitively prevent learning of hierarchical representations which are commonly claimed to be key to the success of neural networks. Can you please clarify?

[Some of the benefits of layer-wise learning as compared to BP have been presented in our response to the reviewer's first comment in the DETAILED COMMENTS section. Please see above for details.

Regarding learning hierarchical representations, we think that even if the learning algorithm is greedy, the representations are still built on top of each other and hence, they can still be hierarchical.

Moreover, we think that it is still an open question whether BP in fact also implicitly learns a deep NN in a greedy, bottom-up fashion. Some empirical results seem to suggest that the answer is positive, i.e., hierarchical representations learned with BP might have also been learned implicitly in a layer-by-layer manner [1].]

---------------------------------------------

- (Sec 4.2) I think it would be beneficial to state in the introduction that the "risk" is with respect to the hinge loss which is common in the SVM/kernel literature but much less in the deep learning literature and thus could surprise a few people when they reach this point. 

[We have updated the manuscript according to the reviewer's comment and we thank the reviewer for bringing this potential confusion into our attention.]

-------------------------------
FURTHER QUESTIONS:
-------------------------------

- From Lemma 4.3, it seems that the derived representation is only optimal with respect to the **upper bound** on the empirical risk (which for \tau &gt;= 2 will be an upper bound on the population risk). I got slightly confused at this point as my interpretation of the previous text was that the representation is optimal with respect to the population risk itself. Does the upper bound have the same set of optima? Please clarify.</span></div><div class="reply_row clearfix"></div></div><div class="children"></div></div><div class="note_with_children"><div id="note_rJe4OaB-67" class="note panel"><div class="title_pdf_row clearfix"><h2 class="note_content_title"><a>Reply to Reviewer 1 [2/6]</a>  <button class="btn btn-xs btn-default permalink-button" title="Link to this comment" data-permalink-url="https://openreview.net/forum?id=H1GLm2R9Km&amp;noteId=rJe4OaB-67"><span class="glyphicon glyphicon-link" aria-hidden="true"></span></button></h2></div><div class="meta_row"><span class="signatures">ICLR 2019 Conference Paper1364 Authors</span></div><div class="clearfix"><div class="meta_row pull-left"><span class="date item">08 Nov 2018</span><span class="item">ICLR 2019 Conference Paper1364 Official Comment</span><span class="item">Readers: <span class="readers-icon glyphicon glyphicon-globe"></span> Everyone</span></div><div class="meta_row meta_actions"></div></div><div class="note_contents"><span class="note_content_field">Comment: </span><span class="note_content_value">   
   3) BP and in fact, end-to-end training algorithms in general, turns the model into a black box in the following senses. First, they make the design procedure of a deep architecture unintuitive in the sense that it is difficult, if not impossible, to attribute the poor performance of a model to an improper design choice in a certain part or layer of the network. Also, it is hard to assess the quality of training in the hidden layers or interpret the effect of the learning algorithm on the model during training. 

    As we have argued in our reply to all reviewers regarding interpretability, the layer-wise learning approach makes it possible to debug each layer individually since we now have a metric against which we can evaluate the performance of each layer separately. And it also has clearer learning dynamics compared to end-to-end training schemes.]


- Regarding the criticism that BP forces intermediate layers to correct for "mistakes" made by layers higher up: it seems your layer-wise algorithm attempts to learn the best possible representation in first layer, and then progresses to the next layer where it tries to correct for the potential error of the first layer and so on. In other words it seems that the errors of layers are propagated from first to last, instead of last to first as in BP, but are still being propagated in a sense. I do not immediately see why propagation forward should be preferable. Can you please further explain this point?

[We are not sure that we understood this comment correctly. Could the reviewer please be more specific about which part of the paper this comment is referred to? We do not think we have made this particular criticism about BP in Section 4.1 or any part of the paper. It would be helpful if the reviewer can identify the claim in our paper that caused this confusion so that we can rephrase it in a clearer way. Many thanks in advance.]

---------------------------------------------

- It is proven in the appendix (Lemma B.3) that under certain conditions stacking additional layers never leads to degradation of training loss. Can you please clarify whether additional layers can be helpful even in the case where previous layers already succeeded in learning the optimal representation?

[If by succeeding in learning the optimal representaion, the reviewer meant achieving zero loss, then Lemma B.3 would only give a guarantee that we can stack on more layers and still have the resulting model achieve zero loss at the latest layer added. In other words, stacking more layers will certainly not help with further reducing training loss in that case.

However, in practice, although identical training losses were achieved sometimes while experimenting with models of different depth, many benefits were observed by choosing the deeper model in this case. For example, the upper layers usually converge significantly faster to a smaller or identical training loss compared to the lower layers and hence for example, a three-layer kMLP may take fewer total iterations to reach a certain training loss than a two-layer one. Also, as we have argued in Appendix B.5, upper layers usually have a certain degree of robustness to bad kernel parameterization, hence, making them easier to fine-tune.]

---------------------------------------------

- (Sec 4.1) Layer-wise vs. network-wise optimality: I find the claim that BP-based learner is not aware of the network-wise optimality confusing. BP explicitly optimises for network-wise optimality and the relative contribution to the network-wise error of each weight is propagated accordingly. I suppose my confusion stems from lack of a clear description of what defines a learner "aware" or "blind" to network-wise optimality. 

[We do not recall having made the claim that BP is not aware of the network-wise optimality. We certainly agree with the reviewer. Note that the fundamental difficulties in Section 4.1 are for layer-wise learning only. Could the reviewer please be more specific about which sentence/part in Section 4.1 caused this confusion? We will rephrase accordingly.

To clarify further, a learner is "aware" of network-wise optimality for each layer when it is an end-to-end learner since in that case, as the reviewer has pointed out, each weight update would be toward minimizing the loss for the entire network. In contrast, a learner is "blind" to network-wise optimality when it works in a layer-wise fashion, optimizing a loss that is not the loss for the entire network one layer at a time.]
</span></div><div class="reply_row clearfix"></div></div><div class="children"></div></div><div class="note_with_children"><div id="note_rJxLgTrWp7" class="note panel"><div class="title_pdf_row clearfix"><h2 class="note_content_title"><a>Reply to Reviewer 1 [1/6]</a>  <button class="btn btn-xs btn-default permalink-button" title="Link to this comment" data-permalink-url="https://openreview.net/forum?id=H1GLm2R9Km&amp;noteId=rJxLgTrWp7"><span class="glyphicon glyphicon-link" aria-hidden="true"></span></button></h2></div><div class="meta_row"><span class="signatures">ICLR 2019 Conference Paper1364 Authors</span></div><div class="clearfix"><div class="meta_row pull-left"><span class="date item">08 Nov 2018</span><span class="item">ICLR 2019 Conference Paper1364 Official Comment</span><span class="item">Readers: <span class="readers-icon glyphicon glyphicon-globe"></span> Everyone</span></div><div class="meta_row meta_actions"></div></div><div class="note_contents"><span class="note_content_field">Comment: </span><span class="note_content_value">
First, we thank Reviewer 1 for the very insightful comments. We can see that Reviewer 1 has read through our paper thoroughly and we are truly thankful for that. We will try our best to address the concerns and answer the questions from the reviewer and we hope that the reviewer finds our reply satisfying.

Comments from the reviewer are listed first with each preceded by a dash. Our replies are put in brackets.

-------------------------------
GENERAL COMMENTS:
-------------------------------

- ...which eliminates necessity of gradient-based training.

[Just to clarify, our layer-wise learning algorithm only eliminates the need of obtaining gradients using BP. It is still a gradient-based optimization per se.]

---------------------------------------------

- My rating of the paper is mainly due to the lack of experimental evidence for usefulness of the layer-wise training, and absence of experimental comparison with several baselines (see details below).

[The objective of the current paper was to provide a comprehensive solution to the theoretical problem and therefore, majority of the time was spent on trying to achieve this goal. Nevertheless, we completely agree with the reviewer that more empirical results would complement the theory and henceforth, we are working hard to produce more results as suggested. We shall notify all reviewers once we have new results and have updated our manuscript accordingly.]

---------------------------------------------

- It is also unclear whether the structure of KNs is significantly better than that of NNs in terms of interpretability.

[We are thankful that the reviewer brought up this important issue. Please see our reply to all reviewers for our response.]

--------------------------------
TWO RELATED PAPERS:
--------------------------------

We were not aware of these two papers and we thank Reviewer 1 for bringing them into our attention. Below are our comments on these two works, which we have added to the newest manuscript as well.

1) Kulkarni &amp; Karande, 2017: "Layer-wise training of deep networks using kernel similarity" <a href="https://arxiv.org/pdf/1703.07115.pdf" target="_blank" rel="nofollow">https://arxiv.org/pdf/1703.07115.pdf</a>

[This work used the idea of an ideal kernel matrix to train NNs layer-by-layer. The activation of each layer, together with a kernel function that is separate from the architecture, is used to compute a kernel matrix. And the training of each layer amounts to aligning that kernel matrix to an ideal one. The ideal kernel matrix used therein is a special case of the ideal kernel matrix characterized by our Lemma 4.3 and Lemma 4.5. However, this work did not discuss or prove the optimality of the underlying hidden representations for NNs.]

---------------------------------------------

2) Scardapanea et al., 2017: "Kafnets: kernel-based non-parametric activation functions for neural networks" https://arxiv.org/pdf/1707.04035.pdf

[This work explored the possibility of substituting the nonlinearities of NNs with kernel expansions. While the resulting networks are similar to our KNs, the authors did not further study specially-tailored training methods as we did in our work. Instead, the resulting models are simply optimized with gradient-based optimization together with BP.]

--------------------------------
DETAILED COMMENTS:
--------------------------------

- (Sec 4.1) Backpropagation (BP) is being criticised: BP is only a particular implementation of gradient calculation. It seems to me that your criticisms are thus more related to use of iterative gradient-based optimisation algorithms, rather than to obtaining gradients through BP?!

[Our criticism for BP in Section 1 is on the scheme of obtaining gradients via BP instead of gradient-based optimization algorithms. Our layer-wise learning algorithm is also gradient-based, as pointed out earlier in our reply. To further clarify, we now provide more details backing up our comments on BP in Section 1:

    1) BP can be computationally intensive and memory inefficient. This is because in standard BP, gradients for all layers have to be computed at each update. And one can either save these gradients while updating each layer (memory inefficient), or compute gradient for each layer on the fly (requires a lot of redundant computations since one has to differentiate through all layers between the output and the layer being updated). Clearly, a layer-wise learning approach mitigates this issue.

    2) Obtaining gradients through BP can cause the vanishing gradient problem when the model contains a composition of many nonlinear layers. And it is clear that a layer-wise, gradient-based optimization approach is less subject to this issue since one no longer needs to differentiate through multiple layers for each gradient computation.
</span></div><div class="reply_row clearfix"></div></div><div class="children"></div></div></div></div><div class="note_with_children"><div id="note_rklAnxYt2X" class="note panel"><div class="title_pdf_row clearfix"><h2 class="note_content_title"><a>interesting theoretical analysis of layer-wise training of kernel-based neural networks, concerns about practicality </a>  <button class="btn btn-xs btn-default permalink-button" title="Link to this comment" data-permalink-url="https://openreview.net/forum?id=H1GLm2R9Km&amp;noteId=rklAnxYt2X"><span class="glyphicon glyphicon-link" aria-hidden="true"></span></button></h2></div><div class="meta_row"><span class="signatures">ICLR 2019 Conference Paper1364 AnonReviewer3</span></div><div class="clearfix"><div class="meta_row pull-left"><span class="date item">02 Nov 2018 (modified: 07 Nov 2018)</span><span class="item">ICLR 2019 Conference Paper1364 Official Review</span><span class="item">Readers: <span class="readers-icon glyphicon glyphicon-globe"></span> Everyone</span></div><div class="meta_row meta_actions"></div></div><div class="note_contents"><span class="note_content_field">Review: </span><span class="note_content_value">Summary: The paper considers so-called kernel neural networks where the non-linear activation function at each neuron is replaced by a kernelized linear operation, and analyses a layer-wise training scheme to train such networks. The theoretical claims are that (i) the optimal representation at each hidden layer can be determined by getting the similarity between two kernel matrices and (ii) this procedure gives a more interpretable training procedure and can avoid the vanishing gradient problems. Some small-scale experiments are provided.

Evaluation: I have a mixed feeling about this paper: the theoretical contributions seem interesting but its interpretation and practicality are somewhat non-intuitive and philosophically troubling, in my opinion. I did not check the proofs in the appendix so I might have missed some critical info or have not fully understood the experimental set-up.

- interpretability: it's not clear to me if this training scheme is any more interpretable than backprop training (not to mention it's not clear to me how to define interpretability for neural networks). Whether BP or any layer-wise training schemes is used, isn't the goal is to get S_{l-1} to the state where S_{l-1}s for examples of different classes are far away from each other as this is easier for the classifier?
- function representation: in section 2, fj^i(x) is parameterized as a sum of kernel values evaluated at x and the training points. It's unclear to me what is x here -- input to the network or output of the previous layer? This also has a sum over all training points, so is training kMLPs in a layer-wise fashion more efficient than traditional kernel methods? 
- training scheme: what is the order of layers being trained? input to output or output to input? I'm slightly hazy on how to obtain F^{(l-1)}(S) to compute G_{l-1}. 
- the intuition of layer-wise optimality: on page 4, the paper states that "the global min of R_l wrt S_{l-1} can be explicitly identified prior to any training" but intuitively this must condition on some known function/function class F^(l). Could you please enlighten me on this?
- the experiments are of small-scale and, as the paper pointed out, only demonstrating the concepts. What are the main practical difficulties preventing this from being applied to bigger networks/bigger datasets?
- vanishing gradients: I'm not clear how layer-wise training can avoid this issue - could you please explain this?
- some typos: p1 emplying -&gt; employing, p4 supress -&gt; suppress, p5 represnetation -&gt; representation</span></div><div class="note_contents"><span class="note_content_field">Rating: </span><span class="note_content_value">5: Marginally below acceptance threshold</span></div><div class="note_contents"><span class="note_content_field">Confidence: </span><span class="note_content_value">3: The reviewer is fairly confident that the evaluation is correct</span></div><div class="reply_row clearfix"></div></div><div class="children"><div class="note_with_children"><div id="note_SJgTm7sJRm" class="note panel"><div class="title_pdf_row clearfix"><h2 class="note_content_title"><a>New manuscript has been uploaded</a>  <button class="btn btn-xs btn-default permalink-button" title="Link to this comment" data-permalink-url="https://openreview.net/forum?id=H1GLm2R9Km&amp;noteId=SJgTm7sJRm"><span class="glyphicon glyphicon-link" aria-hidden="true"></span></button></h2></div><div class="meta_row"><span class="signatures">ICLR 2019 Conference Paper1364 Authors</span></div><div class="clearfix"><div class="meta_row pull-left"><span class="date item">19 Nov 2018</span><span class="item">ICLR 2019 Conference Paper1364 Official Comment</span><span class="item">Readers: <span class="readers-icon glyphicon glyphicon-globe"></span> Everyone</span></div><div class="meta_row meta_actions"></div></div><div class="note_contents"><span class="note_content_field">Comment: </span><span class="note_content_value">
Dear Reviewer 3,

Weâ€™ve uploaded a new version of our paper. The main text is now 8-page long. The changes were mainly made to Section 6 (Experiments) and the rest of the paper is essentially the same as the previous version.

As the rebuttal period is coming to an end, we would like to thank you for your earlier comments and questions. They provided us with new perspectives and helped us tremendously in improving our paper. We would really appreciate it if you could please let us know your thoughts on our reply. In particular, we hope our response addressed your concern regarding the interpretation and practicality of our theoretical results. Please let us know if you think there are further questions that need to be answered.

We understand that you must be very busy so any comment, detailed or brief, would be greatly appreciated. Thank you very much.</span></div><div class="reply_row clearfix"></div></div><div class="children"></div></div><div class="note_with_children"><div id="note_HJeGi9_cTQ" class="note panel"><div class="title_pdf_row clearfix"><h2 class="note_content_title"><a>New manuscript with more experimental results has been uploaded</a>  <button class="btn btn-xs btn-default permalink-button" title="Link to this comment" data-permalink-url="https://openreview.net/forum?id=H1GLm2R9Km&amp;noteId=HJeGi9_cTQ"><span class="glyphicon glyphicon-link" aria-hidden="true"></span></button></h2></div><div class="meta_row"><span class="signatures">ICLR 2019 Conference Paper1364 Authors</span></div><div class="clearfix"><div class="meta_row pull-left"><span class="date item">15 Nov 2018</span><span class="item">ICLR 2019 Conference Paper1364 Official Comment</span><span class="item">Readers: <span class="readers-icon glyphicon glyphicon-globe"></span> Everyone</span></div><div class="meta_row meta_actions"></div></div><div class="note_contents"><span class="note_content_field">Comment: </span><span class="note_content_value">
Dear Reviewer 3,

We have finished new experiments on the standard MNIST dataset. In these experiments, we focused on further empirically studying the proposed layer-wise learning algorithm. Results from kMLPs and MLPs trained with BP are provided. The MLPs were optimized using RMSProp. Moreover, batch normalization and dropout were used to boost the performance of the MLPs. For both the single-hidden-layer and the two-hidden layer kMLPs, the layer-wise algorithm consistently outperformed BP. And the greedily-trained kMLPs compared favorably with the MLPs even though neither batch normalization nor dropout was used for the former.

Also, two standard acceleration methods for kernel machines, including the popular random Fourier features [1], were compared with the proposed acceleration trick. The resulting accelerated BP-trained kMLPs were still outperformed by the accelerated greedy kMLPs. Note that the proposed trick is extremely simple and requires almost no extra overhead (it requires just a random sampling of the training set after the previous layer has been trained).

The main text of this version of our paper is 8.5 pages. We are working on it and will upload an 8-page version in the next few days. Thank you very much.



[1] Rahimi, A., &amp; Recht, B. (2008). Random features for large-scale kernel machines. In Advances in neural information processing systems (pp. 1177-1184).</span></div><div class="reply_row clearfix"></div></div><div class="children"></div></div><div class="note_with_children"><div id="note_BJxwGCYLpX" class="note panel"><div class="title_pdf_row clearfix"><h2 class="note_content_title"><a>New manuscript has been uploaded</a>  <button class="btn btn-xs btn-default permalink-button" title="Link to this comment" data-permalink-url="https://openreview.net/forum?id=H1GLm2R9Km&amp;noteId=BJxwGCYLpX"><span class="glyphicon glyphicon-link" aria-hidden="true"></span></button></h2></div><div class="meta_row"><span class="signatures">ICLR 2019 Conference Paper1364 Authors</span></div><div class="clearfix"><div class="meta_row pull-left"><span class="date item">12 Nov 2018</span><span class="item">ICLR 2019 Conference Paper1364 Official Comment</span><span class="item">Readers: <span class="readers-icon glyphicon glyphicon-globe"></span> Everyone</span></div><div class="meta_row meta_actions"></div></div><div class="note_contents"><span class="note_content_field">Comment: </span><span class="note_content_value">
Dear Reviewer 3,

We have updated our manuscript. The main change is a reformulation of the two key lemmas (Lemma 4.3 and Lemma 4.5), as requested by Reviewer 1. We think this new formulation is clearer than the previous one. We have also rewritten the definition of f^(i)_j in par. 3, Section 2, since Reviewer 3 mentioned that the original one was unclear. Also, the main text of the paper is now contained in 8 pages. Thank you very much.</span></div><div class="reply_row clearfix"></div></div><div class="children"></div></div><div class="note_with_children"><div id="note_SkeGvhSW67" class="note panel"><div class="title_pdf_row clearfix"><h2 class="note_content_title"><a>Reply to Reviewer 3 [2/2]</a>  <button class="btn btn-xs btn-default permalink-button" title="Link to this comment" data-permalink-url="https://openreview.net/forum?id=H1GLm2R9Km&amp;noteId=SkeGvhSW67"><span class="glyphicon glyphicon-link" aria-hidden="true"></span></button></h2></div><div class="meta_row"><span class="signatures">ICLR 2019 Conference Paper1364 Authors</span></div><div class="clearfix"><div class="meta_row pull-left"><span class="date item">08 Nov 2018</span><span class="item">ICLR 2019 Conference Paper1364 Official Comment</span><span class="item">Readers: <span class="readers-icon glyphicon glyphicon-globe"></span> Everyone</span></div><div class="meta_row meta_actions"></div></div><div class="note_contents"><span class="note_content_field">Comment: </span><span class="note_content_value">
- the intuition of layer-wise optimality: on page 4, the paper states that "the global min of R_l wrt S_{l-1} can be explicitly identified prior to any training" but intuitively this must condition on some known function/function class F^(l). Could you please enlighten me on this?

[This is a great question and the reviewer is indeed correct. This "conditioning" takes the form of the assumption in Lemma 4.3 (and Lemma 4.5) that the actual F^(l) considered is the one returned by a learning paradigm minimizing the loss. Putting this in a more concise form, the optimal representation S^\star_{l-1} is defined as S^\star_{l-1} := argmin_{S_{l-1}} min_{F^(l)} R_l(F^(l), S_{l-1}), where the minimum over F^(l) is understood as the minimum for each fixed S_{l-1}.]

---------------------------------------------

- the experiments are of small-scale and, as the paper pointed out, only demonstrating the concepts. What are the main practical difficulties preventing this from being applied to bigger networks/bigger datasets?

[The main reason that we have not performed experiments of larger scales was that we have been spending most of our time on the theory. With that said, we are currently running more experiments and will be adding results as soon as possible. We will notify the reviewers after we update the manuscript with new results.]

---------------------------------------------

- vanishing gradients: I'm not clear how layer-wise training can avoid this issue - could you please explain this?

[As the reviewer knows, vanishing gradient occurs when one has to compute gradients of the form df_l \circ ...\circ f_1(w)/dw, where f_l, ..., f_1 are layers of a deep architecture containing some nonlinear functions. Typically, the magnitude of the resulting gradient at each layer can be small due to some properties of the nonlinearities such as their ranges being between +-1 and having plateaus everywhere but in a small region around origin, etc. And the larger the l, the smaller the magnitude of the gradient at a layer closer to the input since it would be a product of the gradients from upper layers, all of which being some numbers with small absolute value (likely less than 1).

Although layer-wise training does not eliminate the computation for gradients or change these troublesome properties of the nonlinearities mentioned above, it only requires computing gradients of the form df_i(w)/dw. And since the composition of layers does not occur, the issue mentioned above would be mitigated to some extent.]

---------------------------------------------

- some typos: p1 emplying -&gt; employing, p4 supress -&gt; suppress, p5 represnetation -&gt; representation

[We have corrected the typos in the newest manuscript. And we are truly thankful that Reviewer 3 brought them into our attention.]



[1] Rumelhart, D. E., Hinton, G. E., &amp; Williams, R. J. (1986). Learning representations by back-propagating errors. nature, 323(6088), 533.</span></div><div class="reply_row clearfix"></div></div><div class="children"></div></div><div class="note_with_children"><div id="note_rJgDEhHWT7" class="note panel"><div class="title_pdf_row clearfix"><h2 class="note_content_title"><a>Reply to Reviewer 3 [1/2]</a>  <button class="btn btn-xs btn-default permalink-button" title="Link to this comment" data-permalink-url="https://openreview.net/forum?id=H1GLm2R9Km&amp;noteId=rJgDEhHWT7"><span class="glyphicon glyphicon-link" aria-hidden="true"></span></button></h2></div><div class="meta_row"><span class="signatures">ICLR 2019 Conference Paper1364 Authors</span></div><div class="clearfix"><div class="meta_row pull-left"><span class="date item">08 Nov 2018</span><span class="item">ICLR 2019 Conference Paper1364 Official Comment</span><span class="item">Readers: <span class="readers-icon glyphicon glyphicon-globe"></span> Everyone</span></div><div class="meta_row meta_actions"></div></div><div class="note_contents"><span class="note_content_field">Comment: </span><span class="note_content_value">
We would like to first thank Reviewer 3 for the helpful comments and questions. Our detailed reply is provided below. And we hope that it answers the questions from Reviewer 3.

Comments from the reviewer are listed first with each preceded by a dash. Our replies are put in brackets.

---------------------------------------------

- the theoretical contributions seem interesting but its interpretation and practicality are somewhat non-intuitive and philosophically troubling

[In terms of the interpretation of the theoretical results, our work showed that, thanks to the use of kernel functions as nonlinearities in a connectionist architecture, one can concretely answer the question: What is the best representation for each hidden layer? 

This is the fundamental question behind training deep connectionist models [1]. Before, the most widely-accepted answer to this problem was an indirect one: Use BP. This is indirect since, despite that BP does the job of training the model well, it does not provide any interpretable or generalizable knowledge as to what defines a good hidden representation. In contrast, our Lemma 4.3 and Lemma 4.5 provided explicit and general conditions characterizing such optimal hidden representations.

In terms of practicality, these theoretical results removed the need for BP and directly made possible a layer-wise learning algorithm with optimality guarantee equivalent to that offered by BP. Among works that try to replace BP, to the best of our knowledge, ours is the first to provide such an optimality guarantee, thanks to the theoretical results in this paper.]

---------------------------------------------

- interpretability: it's not clear to me if this training scheme is any more interpretable than backprop training (not to mention it's not clear to me how to define interpretability for neural networks).

[We are thankful that the reviewer brought up this important issue. Please see our reply to all reviewers for our response.]

- Whether BP or any layer-wise training schemes is used, isn't the goal is to get S_{l-1} to the state where S_{l-1}s for examples of different classes are far away from each other as this is easier for the classifier?

[For the hidden layers in NN, it is difficult to even talk about the notion of examples being "far away from each other" since there is no natural metric space in which the training can be geometrically interpreted or discussed. Of course, one may use the Euclidean space in which the hidden activation vectors live, but it is not entirely trivial (at least to us) how to prove that what backpropagation does is to push examples from distinct classes as far apart as possible in the metric of that Euclidean space.

For KN trained with the proposed layer-wise algorithm, on the other hand, such an interpretation can be readily applied, as we have shown in Sections 4.2.2 and 4.2.3. And this makes its learning dynamics more transparent and straightforward than NN.]

---------------------------------------------

- function representation: in section 2, f_j^i(x) is parameterized as a sum of kernel values evaluated at x and the training points. It's unclear to me what is x here -- input to the network or output of the previous layer?

[The f_j^i(x) there is just a generic kernel machine, i.e., a generic node in a kMLP. What x is depends on the position of the node in the network in the same way as in MLP.]

- This also has a sum over all training points, so is training kMLPs in a layer-wise fashion more efficient than traditional kernel methods? 

[In terms of computational complexity, a kMLP is more demanding than a traditional kernel machine since the latter corresponds to a single node in the former. Section 4.4 provided a natural accelerating approach to mitigate this issue in practice.

Nevertheless, our results in Appendix B.5.1 suggest that kMLP performs much better than traditional kernel machine and kernel machine enhanced by state-of-the-art multiple kernel learning algorithms.]

---------------------------------------------

- training scheme: what is the order of layers being trained? input to output or output to input? I'm slightly hazy on how to obtain F^(l-1)(S) to compute G_{l-1}. 

[The training proceeds from input to output. Each layer is trained and frozen afterwards. For example, one first train F^(1) to minimize some dissimilarity measure between the ideal kernel matrix G^\star and the actual kernel matrix G_1 defined as (G_1)_{mn} = k^(2)(F^(1)(x_m), F^(1)(x_n)). After the training of F^(1), freeze it and call the frozen state F^{(1)*}. Now start training F^(2) to minimize some dissimilarity measure between G^\star and (G_2)_{mn} = k^(3)(F^(2) \circ F^{(1)*}(x_m), F^(2) \circ F^{(1)*}(x_n)). And so on.]

---------------------------------------------</span></div><div class="reply_row clearfix"></div></div><div class="children"></div></div></div></div></div></main></div>
  </div>
</div>


    <!-- Footer -->
    <footer class="sitemap">
      <div class="container">
        <div class="row hidden-xs">
          <div class="col-sm-4">
            <ul class="list-unstyled">
              <li><a href="/" class="home">Home</a></li>
              <li><a href="/venues">All Venues</a></li>
            </ul>
          </div>
    
          <div class="col-sm-4">
            <ul class="list-unstyled">
              <li><a href="/about">About OpenReview</a></li>
              <li><a href="/contact">Contact</a></li>
              <li><a href="#" data-toggle="modal" data-target="#feedback-modal">Feedback</a></li>
            </ul>
          </div>
    
          <div class="col-sm-4">
            <ul class="list-unstyled">
              <li><a href="/terms">Terms of Service</a></li>
              <li><a href="/privacy">Privacy Policy</a></li>
            </ul>
          </div>
        </div>
    
        <div class="row visible-xs-block">
          <div class="col-xs-6">
            <ul class="list-unstyled">
              <li><a href="/" class="home">Home</a></li>
              <li><a href="/about">About OpenReview</a></li>
              <li><a href="#" data-toggle="modal" data-target="#feedback-modal">Feedback</a></li>
            </ul>
          </div>
    
          <div class="col-xs-6">
            <ul class="list-unstyled">
              <li><a href="/contact">Contact</a></li>
              <li><a href="/terms">Terms of Service</a></li>
              <li><a href="/privacy">Privacy Policy</a></li>
            </ul>
          </div>
        </div>
      </div>
    </footer>
    
    <footer class="sponsor">
      <div class="container">
        <div class="row">
          <div class="col-sm-10 col-sm-offset-1">
            <p class="text-center">
              OpenReview is created by the <a href="http://www.iesl.cs.umass.edu/" target="_blank">Information Extraction and Synthesis Laboratory</a>, College of Information and Computer Science, University of Massachusetts Amherst. We gratefully acknowledge the support of the OpenReview sponsors:  Google,  Facebook, NSF, the University of Massachusetts Amherst Center for Data Science, and Center for Intelligent Information Retrieval, as well as the Google Cloud Platform for donating the computing and networking services on which OpenReview.net runs.
            </p>
          </div>
        </div>
      </div>
    </footer>

  <div id="feedback-modal" class="modal fade" tabindex="-1" role="dialog">
    <div class="modal-dialog">
      <div class="modal-content">
        <div class="modal-header">
          <button type="button" class="close" data-dismiss="modal" aria-label="Close"><span aria-hidden="true">Ã—</span></button>
          <h3 class="modal-title">Send Feedback</h3>
        </div>
        <div class="modal-body">
          <p>Enter your feedback below and we'll get back to you as soon as possible.</p>
          <form action="/feedback" method="POST">
            <div class="form-group">
              <input type="email" name="from" class="form-control" placeholder="Email" />
            </div>
            <div class="form-group">
              <input type="text" name="subject" class="form-control" placeholder="Subject" />
            </div>
            <div class="form-group">
              <textarea name="message" class="form-control feedback-input" rows="5" placeholder="Message" required=""></textarea>
            </div>
          <ul id="ui-id-2" tabindex="0" class="ui-menu ui-widget ui-widget-content" style="display: none;"></ul></form>
        </div>
        <div class="modal-footer">
          <button type="button" class="btn btn-default" data-dismiss="modal">Cancel</button>
          <button type="button" class="btn btn-primary">Send</button>
        </div>
      </div>
    </div>
  </div>

  <script type="text/javascript" async="" src="https://www.google-analytics.com/analytics.js"></script><script src="//cdnjs.cloudflare.com/ajax/libs/jquery/2.2.4/jquery.min.js"></script>
  <script>window.jQuery || document.write('&lt;script src="/static/js/vendor/jquery-2.2.4.min.js"&gt;&lt;\/script&gt;')</script>

  <script src="/static/dist/vendor.min.js"></script>
  <script src="/static/dist/templates.min.js"></script>
  <script src="/static/dist/openreview.min.js"></script>

    <script>window.legacyScripts = true;</script>


    <script async="" src="https://www.googletagmanager.com/gtag/js?id=UA-108703919-1"></script>
    <script>
      window.dataLayer = window.dataLayer || [];
      function gtag() { dataLayer.push(arguments); }
      gtag('js', new Date());
      gtag('config', 'UA-108703919-1');
    </script>


<div role="status" aria-live="assertive" aria-relevant="additions" class="ui-helper-hidden-accessible"></div></body></html>