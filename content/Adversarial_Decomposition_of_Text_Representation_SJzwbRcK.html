<!DOCTYPE html><html xmlns="http://www.w3.org/1999/xhtml" class="no-js"><head>
  <meta charset="utf-8" />
  <meta http-equiv="X-UA-Compatible" content="IE=edge,chrome=1" />
  <meta name="viewport" content="width=device-width, initial-scale=1" />

  <title>Adversarial Decomposition of Text Representation | OpenReview</title>
  <meta name="description" content="" />

      <meta name="citation_title" content="Adversarial Decomposition of Text Representation" />
        <meta name="citation_author" content="Alexey Romanov" />
        <meta name="citation_author" content="Anna Rumshisky" />
        <meta name="citation_author" content="Anna Rogers" />
        <meta name="citation_author" content="David Donahue" />
      <meta name="citation_publication_date" content="2018/09/27" />
      <meta name="citation_online_date" content="2018/09/27" />
      <meta name="citation_pdf_url" content="https://openreview.net/pdf?id=SJzwb2RcK7" />
      <meta name="twitter:card" content="summary" />
      <meta name="twitter:site" content="@openreviewnet" />
      <meta name="og:title" content="Adversarial Decomposition of Text Representation" />
      <meta name="og:description" content="In this paper, we present a method for adversarial decomposition of text representation. This method can be used to decompose a representation of an input sentence into several independent vectors,..." />
      <meta name="og:image" content="https://openreview.net/static/images/openreview_logo_512.png" />

  <link rel="stylesheet" href="https://fonts.googleapis.com/css?family=Noto+Sans:400,400i,700,700i" />

  <link rel="stylesheet" href="/static/css/bootstrap.min.css" />
  <link rel="stylesheet" href="/static/css/jquery-ui.min.css" />
  <link rel="stylesheet" href="/static/css/main.min.css" />
</head>

<body class="forum">
  <nav class="navbar navbar-inverse navbar-fixed-top" role="navigation">
    <div class="container">
  
      <div class="navbar-header">
        <button type="button" class="navbar-toggle collapsed" data-toggle="collapse" data-target="#navbar" aria-expanded="false" aria-controls="navbar">
          <span class="sr-only">Toggle navigation</span>
          <span class="icon-bar"></span>
          <span class="icon-bar"></span>
          <span class="icon-bar"></span>
        </button>
        <a class="navbar-brand home push-link" href="/"><strong>OpenReview</strong>.net</a>
      </div>
  
      <div id="navbar" class="navbar-collapse collapse">
        <form class="navbar-form navbar-left profile-search" role="search">
          <div class="form-group has-feedback">
            <input id="search_input" type="text" class="form-control ui-autocomplete-input" placeholder="Search ICLR 2019 Conference" autocomplete="off" />
            <span class="glyphicon glyphicon-search form-control-feedback" aria-hidden="true"></span>
          </div>
  
          <input id="search_group" type="hidden" value="ICLR.cc/2019/Conference" />
          <input id="search_content" type="hidden" value="all" />
        <ul id="ui-id-1" tabindex="0" class="ui-menu ui-widget ui-widget-content ui-autocomplete ui-front" style="display: none;"></ul></form>
  
        <ul class="nav navbar-nav navbar-right">
        
            <li id="user-menu"><a href="/login">Login</a></li>
        </ul>
      </div>
  
    </div>
  </nav>

  <div id="or-banner" class="banner" style="">
  <div class="container">
    <div class="row">
      <div class="col-xs-12"><a href="/group?id=ICLR.cc/2019/Conference" title="Venue Homepage"><img class="icon" src="/static/images/arrow_left.svg" /> Go to <strong>ICLR 2019 Conference</strong> homepage</a></div>
    </div>
  </div>
</div>
<div id="flash-message-container" class="alert alert-danger" role="alert" style="display: none;">
  <div class="container">
    <div class="row">
      <div class="col-xs-12">
        <div class="alert-content">
          <button type="button" class="close" aria-label="Close"><span aria-hidden="true">×</span></button>
        </div>
      </div>
    </div>
  </div>
</div>

<div class="container">
  <div class="row">
    <div class="col-xs-12">
      <main id="content" class="clearfix openbanner-visible legacy-styles"><div id="note_SJzwb2RcK7" class="note panel"><div class="title_pdf_row clearfix"><h2 class="note_content_title"><a>Adversarial Decomposition of Text Representation</a> <a class="note_content_pdf" href="/pdf?id=SJzwb2RcK7" title="Download PDF" target="_blank"><img src="/static/images/pdf_icon_blue.svg" /></a> </h2></div><div class="meta_row"><span class="signatures"><a href="/profile?email=jgc128%40outlook.com" class="profile-link" data-toggle="tooltip" data-placement="bottom" title="" data-original-title="jgc128@outlook.com">Alexey Romanov</a>, <a href="/profile?email=arum%40cs.uml.edu" class="profile-link" data-toggle="tooltip" data-placement="bottom" title="" data-original-title="arum@cs.uml.edu">Anna Rumshisky</a>, <a href="/profile?email=arogers%40cs.uml.edu" class="profile-link" data-toggle="tooltip" data-placement="bottom" title="" data-original-title="arogers@cs.uml.edu">Anna Rogers</a>, <a href="/profile?email=david_donahue%40student.uml.edu" class="profile-link" data-toggle="tooltip" data-placement="bottom" title="" data-original-title="david_donahue@student.uml.edu">David Donahue</a></span></div><div class="clearfix"><div class="meta_row pull-left"><span class="date item">28 Sep 2018 (modified: 17 Nov 2018)</span><span class="item">ICLR 2019 Conference Withdrawn Submission</span><span class="item">Readers: <span class="readers-icon glyphicon glyphicon-globe"></span> Everyone</span></div><div class="meta_row meta_actions"></div></div><div class="note_contents"><span class="note_content_field">Abstract: </span><span class="note_content_value">In this paper, we present a method for adversarial decomposition of text representation. This method can be used to decompose a representation of an input sentence into several independent vectors, where each vector is responsible for a specific aspect of the input sentence. We evaluate the proposed method on two case studies: the conversion between different social registers and diachronic language change. We show that the proposed method is capable of fine-grained con- trolled change of these aspects of the input sentence. For example, our model is capable of learning a continuous (rather than categorical) representation of the style of the sentence, in line with the reality of language use. The model uses adversarial-motivational training and includes a special motivational loss, which acts opposite to the discriminator and encourages a better decomposition. Finally, we evaluate the obtained meaning embeddings on a downstream task of para- phrase detection and show that they are significantly better than embeddings of a regular autoencoder.</span></div><div class="note_contents"><span class="note_content_field">Keywords: </span><span class="note_content_value">learning representation, decomposition, adversarial training, style transfer</span></div><div class="note_contents"><span class="note_content_field">TL;DR: </span><span class="note_content_value">A method which learns separate representations for the meaning and the form of a sentence</span></div><div class="reply_row clearfix"><div class="item" id="reply_count">8 Replies</div></div></div><hr class="small" /><div id="note_children"><div class="note_with_children"><div id="note_rJxz6e_mp7" class="note panel"><div class="title_pdf_row clearfix"><h2 class="note_content_title"><a>Submission Withdrawn by the Authors</a>  <button class="btn btn-xs btn-default permalink-button" title="Link to this comment" data-permalink-url="https://openreview.net/forum?id=SJzwb2RcK7&amp;noteId=rJxz6e_mp7"><span class="glyphicon glyphicon-link" aria-hidden="true"></span></button></h2></div><div class="meta_row"><span class="signatures">ICLR 2019 Conference Paper1182 Authors</span></div><div class="clearfix"><div class="meta_row pull-left"><span class="date item">10 Nov 2018</span><span class="item">ICLR 2019 Conference Paper1182 Withdraw Submission</span><span class="item">Readers: <span class="readers-icon glyphicon glyphicon-globe"></span> Everyone</span></div><div class="meta_row meta_actions"></div></div><div class="note_contents"><span class="note_content_field">Withdrawal confirmation: </span><span class="note_content_value">I have read and agree with the withdrawal statement on behalf of myself and my co-authors.</span></div><div class="reply_row clearfix"></div></div><div class="children"></div></div><div class="note_with_children"><div id="note_ryewhxuXaX" class="note panel"><div class="title_pdf_row clearfix"><h2 class="note_content_title"><a>Paper withdrawal</a>  <button class="btn btn-xs btn-default permalink-button" title="Link to this comment" data-permalink-url="https://openreview.net/forum?id=SJzwb2RcK7&amp;noteId=ryewhxuXaX"><span class="glyphicon glyphicon-link" aria-hidden="true"></span></button></h2></div><div class="meta_row"><span class="signatures">ICLR 2019 Conference Paper1182 Authors</span></div><div class="clearfix"><div class="meta_row pull-left"><span class="date item">10 Nov 2018</span><span class="item">ICLR 2019 Conference Paper1182 Official Comment</span><span class="item">Readers: <span class="readers-icon glyphicon glyphicon-globe"></span> Everyone</span></div><div class="meta_row meta_actions"></div></div><div class="note_contents"><span class="note_content_field">Comment: </span><span class="note_content_value">We thank all reviewers for their useful comments. Since additional experiments will require some time, we decided to withdraw the paper from ICLR to thoroughly perform the suggested experiments and incroporate the changes.</span></div><div class="reply_row clearfix"></div></div><div class="children"></div></div><div class="note_with_children"><div id="note_HyeYo1Jj37" class="note panel"><div class="title_pdf_row clearfix"><h2 class="note_content_title"><a>The model seems to be really intriguing but the formulation ...</a>  <button class="btn btn-xs btn-default permalink-button" title="Link to this comment" data-permalink-url="https://openreview.net/forum?id=SJzwb2RcK7&amp;noteId=HyeYo1Jj37"><span class="glyphicon glyphicon-link" aria-hidden="true"></span></button></h2></div><div class="meta_row"><span class="signatures">ICLR 2019 Conference Paper1182 AnonReviewer1</span></div><div class="clearfix"><div class="meta_row pull-left"><span class="date item">03 Nov 2018 (modified: 07 Nov 2018)</span><span class="item">ICLR 2019 Conference Paper1182 Official Review</span><span class="item">Readers: <span class="readers-icon glyphicon glyphicon-globe"></span> Everyone</span></div><div class="meta_row meta_actions"></div></div><div class="note_contents"><span class="note_content_field">Review: </span><span class="note_content_value">The paper presents a model to separately learn two parts of sentence representation in vectors: meaning and form. Why this is needed is easy to underatand and well explained. On the contrary, the forumulation of the model is indeed unnecessarely complex. It seems the authors want to convince that the model is extremely complicated. Yet, the formulation does not help in explaining the complexity of the model. 

The formulation (Section 3) is a really difficult part to understand. It does not follow an easy path. First of all, what is the diffence beween \textbf{X} and \mathcal{X}. More importantly, where \textbf{X} used in the formulation? What is p(.) in the main equation of the formulation? Is it a probabilistic distribution over vectors \vec{f}^a \vec{f}^b. Then, if p(.) is a probabilistic distribution and \alphas are scalars, \vec{f}_i is a scalar? 
p(.) shuold be better described. 

If the aim of furmulation is to help readers to go into the paper, it does not seem to be reached. On the contrary, it makes reading more complex.

The main innovation of the paper seems the introduction of two layers in a standard GAN. These two layers are: (1) the Motivator; (2) the Discriminator. These two models should address two different perspectives for sentence modeling. To better explain the model, the athors could use Fig. 1 by adding the outputs and the inputs of the layers. If the input and the output of the generator are intuitive, the output of the Motivator and the Discriminator is not that clear. An example could be very useful to clarify the model. What is exactly the output of the Discriminator? And the one of the Motivator? These two layers force the separation of the meaning and the form, respectively. But how? What is the target output for the first and the target output for the second?

Minor issues
====
There are some abbreviations that are not explained or are explained after their first use, e.g., Natural Language Processing (NLP), CV?, Generative Adversial Network (GAN), Long-Short Term Memory (LSTM), ...
 </span></div><div class="note_contents"><span class="note_content_field">Rating: </span><span class="note_content_value">3: Clear rejection</span></div><div class="note_contents"><span class="note_content_field">Confidence: </span><span class="note_content_value">4: The reviewer is confident but not absolutely certain that the evaluation is correct</span></div><div class="reply_row clearfix"></div></div><div class="children"><div class="note_with_children"><div id="note_ryeLeaPmT7" class="note panel"><div class="title_pdf_row clearfix"><h2 class="note_content_title"><a>Response to the AnonReviewer1</a>  <button class="btn btn-xs btn-default permalink-button" title="Link to this comment" data-permalink-url="https://openreview.net/forum?id=SJzwb2RcK7&amp;noteId=ryeLeaPmT7"><span class="glyphicon glyphicon-link" aria-hidden="true"></span></button></h2></div><div class="meta_row"><span class="signatures">ICLR 2019 Conference Paper1182 Authors</span></div><div class="clearfix"><div class="meta_row pull-left"><span class="date item">10 Nov 2018</span><span class="item">ICLR 2019 Conference Paper1182 Official Comment</span><span class="item">Readers: <span class="readers-icon glyphicon glyphicon-globe"></span> Everyone</span></div><div class="meta_row meta_actions"></div></div><div class="note_contents"><span class="note_content_field">Comment: </span><span class="note_content_value">We apologize for the confusion. \mathcal{X} reflects the domain of the sentences in one particular style (say, all sentences in the Old Shakespeare style). \textbf{X}, in contrast, denotes the specific corpus of text in this style on which the model is trained. This distinction is important because when the model generates a new sentence, it would be in \mathcal{X} and not \textbf{X}. \textbf{X} is used in the section 4. 

Yes, the alphas are scalars and f are vectors. This equation was meant to clarify that the style is not just binary but rather a mixture of several (in this example, two) distributions. This is a unique characteristic of our model and no previous approaches do that, so we wanted to emphasize it. We will clarify this section.

The discriminator is the same as used in all GAN models. As explained in the section 4.2, it is a classifier that performs the binary classification and tries to distinguish between sentences from two domains (i.e. original Shakespeare vs. contemporary translation). Since we adopt the Wasserstein GAN loss, it outputs an unbounded scalar for every sample in the batch. The motivator is the same, except the discriminator works on the meaning vectors, and the motivator receives the style vectors as input.
</span></div><div class="reply_row clearfix"></div></div><div class="children"></div></div></div></div><div class="note_with_children"><div id="note_SJgGqz-F2m" class="note panel"><div class="title_pdf_row clearfix"><h2 class="note_content_title"><a>Neat little contribution.</a>  <button class="btn btn-xs btn-default permalink-button" title="Link to this comment" data-permalink-url="https://openreview.net/forum?id=SJzwb2RcK7&amp;noteId=SJgGqz-F2m"><span class="glyphicon glyphicon-link" aria-hidden="true"></span></button></h2></div><div class="meta_row"><span class="signatures">ICLR 2019 Conference Paper1182 AnonReviewer2</span></div><div class="clearfix"><div class="meta_row pull-left"><span class="date item">02 Nov 2018 (modified: 07 Nov 2018)</span><span class="item">ICLR 2019 Conference Paper1182 Official Review</span><span class="item">Readers: <span class="readers-icon glyphicon glyphicon-globe"></span> Everyone</span></div><div class="meta_row meta_actions"></div></div><div class="note_contents"><span class="note_content_field">Review: </span><span class="note_content_value">The paper presents a method for style transfer using latent vectors that are trained to separately capture meaning and form (style). 

The main claims are that (i) the form aspect is modeled in a continuous fashion as opposed to prior work, and (ii) the use of a motivator (in addition to a discriminator) to encourage packing information about the form. 

The paper presents experiments that compare how controllable the generation is under this model. 

Here are my concerns with the paper:

1. Why is decomposing information from a packed sentence embedding better than say separately tracking meaning and form related information using two separate LSTMs? It is not obvious why one is better than the other. -- One can argue that there are more parameters with two LSTMs. But then how does this compare with the parameters of the 2-layer MLP? 

2. One of the main distinctions being claimed is that this model treats form as truly continuous. But the discriminator seems to be trained to predict the form of the sentence -- where form is one of the two classes (middle vs. modern)? 
-- If this is indeed the case then how is this treatment different from prior work (e.g. Hu et al. (2017) which treat the latent variable as a binary feature but with continuous values). The mechanism is different but the treatment seems to be the same. Is it?
-- If this is not the case then how does the discriminator know what the continuous form vector should be for each sentence?

3. As far as I can tell the evaluations don’t seem to test the benefit of the form being truly continuous in the sense of controllable generation. 
Is there a way to say generate this sentence in a high degree of middle english? I cannot imagine why/how this would be useful as a task.

4. There is one evaluation in Figure 2, which suggests that the ADNet model generated sentences have higher transfer strength but this evaluation is not conclusive in the sense that you have too much drop in content. It would be compelling to have a manual evaluation to supplement this evidence. 

5. Table 1 examples look good but Table 2 results seem to indicate that the model doesn’t do as well for headlines to scientific article titles. Why is this the case?

6. Table 3 results are encouraging. It would be useful to also include a result where you use both the form and meaning vectors (say concatenated) as a control test to see if it is indeed the meaning vectors that are useful or if it is this kind of separation that is somehow useful. 

7. The intro has a sentence that says the method decomposes the sentence embedding into several vectors -- this has only been tested for form and meaning. There is nothing in the model that says it can't have more than two vectors but just that the empirical evidence is only for form and meaning. 

Some typos:
better decomposition → better decompose
dissociation meaning and form → dissociating meaning from form
complimentary → complementary
</span></div><div class="note_contents"><span class="note_content_field">Rating: </span><span class="note_content_value">6: Marginally above acceptance threshold</span></div><div class="note_contents"><span class="note_content_field">Confidence: </span><span class="note_content_value">3: The reviewer is fairly confident that the evaluation is correct</span></div><div class="reply_row clearfix"></div></div><div class="children"><div class="note_with_children"><div id="note_B1lzYpwQpm" class="note panel"><div class="title_pdf_row clearfix"><h2 class="note_content_title"><a>Response to the AnonReviewer2</a>  <button class="btn btn-xs btn-default permalink-button" title="Link to this comment" data-permalink-url="https://openreview.net/forum?id=SJzwb2RcK7&amp;noteId=B1lzYpwQpm"><span class="glyphicon glyphicon-link" aria-hidden="true"></span></button></h2></div><div class="meta_row"><span class="signatures">ICLR 2019 Conference Paper1182 Authors</span></div><div class="clearfix"><div class="meta_row pull-left"><span class="date item">10 Nov 2018</span><span class="item">ICLR 2019 Conference Paper1182 Official Comment</span><span class="item">Readers: <span class="readers-icon glyphicon glyphicon-globe"></span> Everyone</span></div><div class="meta_row meta_actions"></div></div><div class="note_contents"><span class="note_content_field">Comment: </span><span class="note_content_value">Thank you for such a detailed review. Please find answers to the specific points below:

1. Yes, the motivation for separation is to reduce the number of parameters. LSTMs have a lot of parameters, and in our case we introduce only a small fully-connected layer.

2. Thank you for this question, and we will update the paper to clarify that. Despite the fact the the discriminator is trained using binary labels, the resulting style vectors are truly continuous.  They form a style space that you can traverse; as you move from one extreme location to another the style would gradually change accordingly. This is easy to see in Figure 4 in the appendix where we show style embeddings of sentences from novels of six different authors. In that case, the discriminator predicts one of 6 classes. But when plotted with t-SNE, style embeddings form a star-shaped region, with author-specific sentences located in the tips of the star, and common sentences used by all authors (e.g. “Good morning”) in the center. 

3. This is an excellent point: the evaluation we performed is the same as in the previous work that assumed binary nature of style; it does allow for direct comparison to those approaches, which is essential, but it does not reflect the full potential of our approach. However, it should be in principle possible to generate sentences of decreasing “middle-Englishness” as decrease in the number of markedly middle-English vocabulary (e.g. “Fare thee well, my lord” &gt; “Fare you well, my lord” &gt; “Good bye, my lord”). We agree that it is unclear what practical applications of this would be, but it is a much more linguistically plausible model of style and, as such, it should yield higher accuracy on the style transfer task.

4. A high content preservation score coupled with low transfer strength is actually bad - that means that the model just repeats the original sentence. The lower content preservation coupled with high transfer strength means that the model not just repeating the original sentence but indeed performs the transfer. We will add the human evaluation.

5. The model requires the meaning distribution of the two training corpora to be the same. While this is true for the shakespeare dataset, this is not exactly the case for the headlines dataset. What we saw in our experiments is that the model outputs synonyms and similar words from the same domain (e.g. if the paper’s headlines talks about computer vision, the generated news headlines would talk about 3d graphics in colloquial terms; see also “LTE systems” -&gt; “telecommunication management” in the Table 2.) We will address this point in the paper.

6. Thank you for the suggestion, we will add these experiments.

7. The proposed approach is general and can used to decompose representation into multiple vectors, provided that there is sufficient training data that reflects the distribution of the target phenomena. For example, for the literary styles one could imagine having three embeddings: one for the content, one for the overall style of the epoch (e.g. victorian period vs modernism), and one for the individual style of the author. It would be interesting to perform such experiments in future work.
</span></div><div class="reply_row clearfix"></div></div><div class="children"></div></div></div></div><div class="note_with_children"><div id="note_H1xYz3MN27" class="note panel"><div class="title_pdf_row clearfix"><h2 class="note_content_title"><a>Interesting method, but some important questions are not answered, and experiments didn't clearly justify the contributions</a>  <button class="btn btn-xs btn-default permalink-button" title="Link to this comment" data-permalink-url="https://openreview.net/forum?id=SJzwb2RcK7&amp;noteId=H1xYz3MN27"><span class="glyphicon glyphicon-link" aria-hidden="true"></span></button></h2></div><div class="meta_row"><span class="signatures">ICLR 2019 Conference Paper1182 AnonReviewer3</span></div><div class="clearfix"><div class="meta_row pull-left"><span class="date item">29 Oct 2018 (modified: 07 Nov 2018)</span><span class="item">ICLR 2019 Conference Paper1182 Official Review</span><span class="item">Readers: <span class="readers-icon glyphicon glyphicon-globe"></span> Everyone</span></div><div class="meta_row meta_actions"></div></div><div class="note_contents"><span class="note_content_field">Review: </span><span class="note_content_value">This paper proposed a method to decompose text representation into two vectors for meaning and form respectively. The method used an adversarial discriminator to eliminate form information in meaning vectors and used a motivator on form vectors. The authors evaluated the model on a form transfer task and a downstream task of paraphrase detection.

Pros of the paper:
1. The paper proposed to use a motivator to encourage the model to keep more form information in form vectors.

2. Learned meaning embedding gives a better performance than other unsupervised method on a downstream paraphrasing detection task.

Cons of the paper, and questions:

1. One big concern about the method is why f (form vector) would contain only form information, while m (meaning vector) would contain all semantic information. I might miss something, but it seems totally possible to me that some semantic information will shift to f rather than to m, because f has access to the whole input, and that's why previous work used a categorial vector. The authors didn't explain how they addressed this issue.

2. In the form transfer experiments, the proposed model had lower content preservation scores. This probably means semantic meaning will shift after switching f, so the issue mentioned above does exist, right? The authors should show more results and analysis on how well meaning is preserved.

3. The procedure of constructing the opposite form vector in Section 5.1.1 is a bit ad-hoc, and it doesn't really motivate the use of continuous form representation. It would be nice if the authors could show other use cases where continuous form representation is clearly a better choice.

4. Why not report experiments on sentiment data? I think the model is generic enough and should be able to handle sentiment decomposition and transfer. I would be curious to see the results on sentiment data.

5. The perplexity change from 6.89 to 9.74 seems huge to me. Even though the absolute change seems small, the relative change is huge (~30%) according to 1-billion-words-benchmark of language model. It would be useful to show more evidence that this change doesn't impact the fluency.

Overall the originality of the method is marginal. Some questions about the method need to be answered. Evaluations are a bit weak and they don't clearly justify the contributions of the paper.</span></div><div class="note_contents"><span class="note_content_field">Rating: </span><span class="note_content_value">4: Ok but not good enough - rejection</span></div><div class="note_contents"><span class="note_content_field">Confidence: </span><span class="note_content_value">3: The reviewer is fairly confident that the evaluation is correct</span></div><div class="reply_row clearfix"></div></div><div class="children"><div class="note_with_children"><div id="note_HJe4RawmpX" class="note panel"><div class="title_pdf_row clearfix"><h2 class="note_content_title"><a>Response to the AnonReviewer3</a>  <button class="btn btn-xs btn-default permalink-button" title="Link to this comment" data-permalink-url="https://openreview.net/forum?id=SJzwb2RcK7&amp;noteId=HJe4RawmpX"><span class="glyphicon glyphicon-link" aria-hidden="true"></span></button></h2></div><div class="meta_row"><span class="signatures">ICLR 2019 Conference Paper1182 Authors</span></div><div class="clearfix"><div class="meta_row pull-left"><span class="date item">10 Nov 2018</span><span class="item">ICLR 2019 Conference Paper1182 Official Comment</span><span class="item">Readers: <span class="readers-icon glyphicon glyphicon-globe"></span> Everyone</span></div><div class="meta_row meta_actions"></div></div><div class="note_contents"><span class="note_content_field">Comment: </span><span class="note_content_value">Thank you for the review. Please find answers to the specific points below:

1. Indeed, there’s no theoretical guarantee that this would not happen, and this is why we experimented with different sizes of the form vector. The assumption is that the size limitation would ensure that storing non-form-related information elsewhere would improve model performance.  Figure 2 shows that as the meaning vectors get smaller, and the form vectors - larger, the higher is transfer strength and the lower is content preservation. If the model would store meaning in the form vector, then the content preservation would not be hurt by reducing the size of the meaning vector. This shows that the model does not store the meaning in the form vector. We will clarify this point in the paper.

2. Yes, the content preservation scores are lower, but the transfer strength is higher. As evident from the picture, these are two opposite objectives that can be balanced by choosing the appropriate sizes of the meaning and form vectors.

3. This is the evaluation performed in previous work that enables direct comparison with it. It indeed does not show the main strength of our model. We’ll perform additional evaluation as described in the response to the AnonReviewer2, item #3.

4. We believe it is incorrect to use the term “style transfer” on sentiment data, as change in sentiment is rather a change in the specific aspect of meaning. However we did perform such experiments. The model was able to change the objects of the sentences, as well as polarity. We deliberately did not report the results of these experiments, as we would like the research community to stop using the term “style transfer” in such inappropriate cases. See also Yoav Goldberg’s post: <a href="https://twitter.com/yoavgo/status/1059406530267750400" target="_blank" rel="nofollow">https://twitter.com/yoavgo/status/1059406530267750400</a> 

5. Could you please explain what do you mean? The language model was trained on the corresponding shakespeare data.
</span></div><div class="reply_row clearfix"></div></div><div class="children"></div></div></div></div></div></main></div>
  </div>
</div>


    <!-- Footer -->
    <footer class="sitemap">
      <div class="container">
        <div class="row hidden-xs">
          <div class="col-sm-4">
            <ul class="list-unstyled">
              <li><a href="/" class="home">Home</a></li>
              <li><a href="/venues">All Venues</a></li>
            </ul>
          </div>
    
          <div class="col-sm-4">
            <ul class="list-unstyled">
              <li><a href="/about">About OpenReview</a></li>
              <li><a href="/contact">Contact</a></li>
              <li><a href="#" data-toggle="modal" data-target="#feedback-modal">Feedback</a></li>
            </ul>
          </div>
    
          <div class="col-sm-4">
            <ul class="list-unstyled">
              <li><a href="/terms">Terms of Service</a></li>
              <li><a href="/privacy">Privacy Policy</a></li>
            </ul>
          </div>
        </div>
    
        <div class="row visible-xs-block">
          <div class="col-xs-6">
            <ul class="list-unstyled">
              <li><a href="/" class="home">Home</a></li>
              <li><a href="/about">About OpenReview</a></li>
              <li><a href="#" data-toggle="modal" data-target="#feedback-modal">Feedback</a></li>
            </ul>
          </div>
    
          <div class="col-xs-6">
            <ul class="list-unstyled">
              <li><a href="/contact">Contact</a></li>
              <li><a href="/terms">Terms of Service</a></li>
              <li><a href="/privacy">Privacy Policy</a></li>
            </ul>
          </div>
        </div>
      </div>
    </footer>
    
    <footer class="sponsor">
      <div class="container">
        <div class="row">
          <div class="col-sm-10 col-sm-offset-1">
            <p class="text-center">
              OpenReview is created by the <a href="http://www.iesl.cs.umass.edu/" target="_blank">Information Extraction and Synthesis Laboratory</a>, College of Information and Computer Science, University of Massachusetts Amherst. We gratefully acknowledge the support of the OpenReview sponsors:  Google,  Facebook, NSF, the University of Massachusetts Amherst Center for Data Science, and Center for Intelligent Information Retrieval, as well as the Google Cloud Platform for donating the computing and networking services on which OpenReview.net runs.
            </p>
          </div>
        </div>
      </div>
    </footer>

  <div id="feedback-modal" class="modal fade" tabindex="-1" role="dialog">
    <div class="modal-dialog">
      <div class="modal-content">
        <div class="modal-header">
          <button type="button" class="close" data-dismiss="modal" aria-label="Close"><span aria-hidden="true">×</span></button>
          <h3 class="modal-title">Send Feedback</h3>
        </div>
        <div class="modal-body">
          <p>Enter your feedback below and we'll get back to you as soon as possible.</p>
          <form action="/feedback" method="POST">
            <div class="form-group">
              <input type="email" name="from" class="form-control" placeholder="Email" />
            </div>
            <div class="form-group">
              <input type="text" name="subject" class="form-control" placeholder="Subject" />
            </div>
            <div class="form-group">
              <textarea name="message" class="form-control feedback-input" rows="5" placeholder="Message" required=""></textarea>
            </div>
          <ul id="ui-id-2" tabindex="0" class="ui-menu ui-widget ui-widget-content" style="display: none;"></ul></form>
        </div>
        <div class="modal-footer">
          <button type="button" class="btn btn-default" data-dismiss="modal">Cancel</button>
          <button type="button" class="btn btn-primary">Send</button>
        </div>
      </div>
    </div>
  </div>

  <script type="text/javascript" async="" src="https://www.google-analytics.com/analytics.js"></script><script src="//cdnjs.cloudflare.com/ajax/libs/jquery/2.2.4/jquery.min.js"></script>
  <script>window.jQuery || document.write('&lt;script src="/static/js/vendor/jquery-2.2.4.min.js"&gt;&lt;\/script&gt;')</script>

  <script src="/static/dist/vendor.min.js"></script>
  <script src="/static/dist/templates.min.js"></script>
  <script src="/static/dist/openreview.min.js"></script>

    <script>window.legacyScripts = true;</script>


    <script async="" src="https://www.googletagmanager.com/gtag/js?id=UA-108703919-1"></script>
    <script>
      window.dataLayer = window.dataLayer || [];
      function gtag() { dataLayer.push(arguments); }
      gtag('js', new Date());
      gtag('config', 'UA-108703919-1');
    </script>


<div role="status" aria-live="assertive" aria-relevant="additions" class="ui-helper-hidden-accessible"></div></body></html>