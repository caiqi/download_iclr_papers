<!DOCTYPE html><html xmlns="http://www.w3.org/1999/xhtml" class="no-js"><head>
  <meta charset="utf-8" />
  <meta http-equiv="X-UA-Compatible" content="IE=edge,chrome=1" />
  <meta name="viewport" content="width=device-width, initial-scale=1" />

  <title>AD-VAT: An Asymmetric Dueling mechanism for learning Visual Active Tracking | OpenReview</title>
  <meta name="description" content="" />

      <meta name="citation_title" content="AD-VAT: An Asymmetric Dueling mechanism for learning Visual Active Tracking" />
        <meta name="citation_author" content="Anonymous" />
      <meta name="citation_publication_date" content="2018/09/27" />
      <meta name="citation_online_date" content="2018/09/27" />
      <meta name="citation_pdf_url" content="https://openreview.net/pdf?id=HkgYmhR9KX" />
      <meta name="twitter:card" content="summary" />
      <meta name="twitter:site" content="@openreviewnet" />
      <meta name="og:title" content="AD-VAT: An Asymmetric Dueling mechanism for learning Visual Active..." />
      <meta name="og:description" content="Visual active tracking (VAT) aims at following a target object by autonomously controlling the motion system of a tracker given visual observations. In this paper, we propose a novel method which..." />
      <meta name="og:image" content="https://openreview.net/static/images/openreview_logo_512.png" />

  <link rel="stylesheet" href="https://fonts.googleapis.com/css?family=Noto+Sans:400,400i,700,700i" />

  <link rel="stylesheet" href="/static/css/bootstrap.min.css" />
  <link rel="stylesheet" href="/static/css/jquery-ui.min.css" />
  <link rel="stylesheet" href="/static/css/main.min.css" />
</head>

<body class="forum">
  <nav class="navbar navbar-inverse navbar-fixed-top" role="navigation">
    <div class="container">
  
      <div class="navbar-header">
        <button type="button" class="navbar-toggle collapsed" data-toggle="collapse" data-target="#navbar" aria-expanded="false" aria-controls="navbar">
          <span class="sr-only">Toggle navigation</span>
          <span class="icon-bar"></span>
          <span class="icon-bar"></span>
          <span class="icon-bar"></span>
        </button>
        <a class="navbar-brand home push-link" href="/"><strong>OpenReview</strong>.net</a>
      </div>
  
      <div id="navbar" class="navbar-collapse collapse">
        <form class="navbar-form navbar-left profile-search" role="search">
          <div class="form-group has-feedback">
            <input id="search_input" type="text" class="form-control ui-autocomplete-input" placeholder="Search ICLR 2019 Conference" autocomplete="off" />
            <span class="glyphicon glyphicon-search form-control-feedback" aria-hidden="true"></span>
          </div>
  
          <input id="search_group" type="hidden" value="ICLR.cc/2019/Conference" />
          <input id="search_content" type="hidden" value="all" />
        <ul id="ui-id-1" tabindex="0" class="ui-menu ui-widget ui-widget-content ui-autocomplete ui-front" style="display: none;"></ul></form>
  
        <ul class="nav navbar-nav navbar-right">
        
            <li id="user-menu"><a href="/login">Login</a></li>
        </ul>
      </div>
  
    </div>
  </nav>

  <div id="or-banner" class="banner" style="">
  <div class="container">
    <div class="row">
      <div class="col-xs-12"><a href="/group?id=ICLR.cc/2019/Conference" title="Venue Homepage"><img class="icon" src="/static/images/arrow_left.svg" /> Go to <strong>ICLR 2019 Conference</strong> homepage</a></div>
    </div>
  </div>
</div>
<div id="flash-message-container" class="alert alert-danger" role="alert" style="display: none;">
  <div class="container">
    <div class="row">
      <div class="col-xs-12">
        <div class="alert-content">
          <button type="button" class="close" aria-label="Close"><span aria-hidden="true">×</span></button>
        </div>
      </div>
    </div>
  </div>
</div>

<div class="container">
  <div class="row">
    <div class="col-xs-12">
      <main id="content" class="clearfix openbanner-visible legacy-styles"><div id="note_HkgYmhR9KX" class="note panel"><div class="title_pdf_row clearfix"><h2 class="note_content_title"><a>AD-VAT: An Asymmetric Dueling mechanism for learning Visual Active Tracking</a> <a class="note_content_pdf" href="/pdf?id=HkgYmhR9KX" title="Download PDF" target="_blank"><img src="/static/images/pdf_icon_blue.svg" /></a> </h2></div><div class="meta_row"><span class="signatures">Anonymous</span></div><div class="clearfix"><div class="meta_row pull-left"><span class="date item">28 Sep 2018 (modified: 09 Nov 2018)</span><span class="item">ICLR 2019 Conference Blind Submission</span><span class="item">Readers: <span class="readers-icon glyphicon glyphicon-globe"></span> Everyone</span><span class="item"><a class="action-bibtex-modal" data-bibtex="@inproceedings{    &#10;anonymous2019ad-vat:,    &#10;title={AD-VAT: An Asymmetric Dueling mechanism for learning Visual Active Tracking},    &#10;author={Anonymous},    &#10;booktitle={Submitted to International Conference on Learning Representations},    &#10;year={2019},    &#10;url={https://openreview.net/forum?id=HkgYmhR9KX},    &#10;note={under review}    &#10;}">Show Bibtex</a></span><a class="note_content_pdf item" href="/revisions?id=HkgYmhR9KX" target="_blank">Show Revisions</a></div><div class="meta_row meta_actions"></div></div><div class="note_contents"><span class="note_content_field">Abstract: </span><span class="note_content_value">Visual active tracking (VAT) aims at following a target object by autonomously controlling the motion system of a tracker given visual observations. In this paper, we propose a novel method which adopts an asymmetric dueling mechanism for learning visual active tracking, namely AD-VAT. In AD-VAT, the target and the tracker are mutual opponents, i.e, the tracker manages to lockup the target, and the target tries to escape from the tracker. In the implementation, both the tracker and the target are approximated by deep networks, and their policies that map environment observations to control actions can be learned via reinforcement learning in an end-to-end manner. The tracker and the target are asymmetric in observations, network structures and reward functions. Different from the tracker, the target is modeled with a tracker-aware network, i.e, besides its own observation, the tracker's observations and actions are also fed as input to the network. In addition, it learns to predict the tracker's reward as an auxiliary task.  We argue that such an asymmetric adversarial mechanism is able to learn a stronger target,  which vice versa induces a more robust tracker. The experimental results, in both 2D and 3D environments, demonstrate that the proposed method leads to a faster convergence in training the tracker and more robust tracking behaviors in different testing scenarios. For supplementary videos, see: <a href="https://www.youtube.com/playlist?list=PL9rZj4Mea7wOZkdajK1TsprRg8iUf51BS" target="_blank" rel="nofollow">https://www.youtube.com/playlist?list=PL9rZj4Mea7wOZkdajK1TsprRg8iUf51BS</a></span></div><div class="note_contents"><span class="note_content_field">Keywords: </span><span class="note_content_value">Active tracking, reinforcement learning, adversarial learning, multi agent</span></div><div class="note_contents"><span class="note_content_field">TL;DR: </span><span class="note_content_value">We propose AD-VAT, where the tracker and the target object, viewed as two learnable agents, are opponents and can mutually enhance during training.</span></div><div class="reply_row clearfix"><div class="item" id="reply_count">7 Replies</div></div></div><hr class="small" /><div id="note_children"><div class="note_with_children"><div id="note_HkefcSaM6m" class="note panel"><div class="title_pdf_row clearfix"><h2 class="note_content_title"><a>Change Logs</a>  <button class="btn btn-xs btn-default permalink-button" title="Link to this comment" data-permalink-url="https://openreview.net/forum?id=HkgYmhR9KX&amp;noteId=HkefcSaM6m"><span class="glyphicon glyphicon-link" aria-hidden="true"></span></button></h2></div><div class="meta_row"><span class="signatures">ICLR 2019 Conference Paper1378 Authors</span></div><div class="clearfix"><div class="meta_row pull-left"><span class="date item">09 Nov 2018</span><span class="item">ICLR 2019 Conference Paper1378 Official Comment</span><span class="item">Readers: <span class="readers-icon glyphicon glyphicon-globe"></span> Everyone</span></div><div class="meta_row meta_actions"></div></div><div class="note_contents"><span class="note_content_field">Comment: </span><span class="note_content_value">We have updated our paper during the rebuttal period, which could be summarized as below:

a) Supplementary videos are updated in:  <a href="https://www.youtube.com/playlist?list=PL9rZj4Mea7wOZkdajK1TsprRg8iUf51BS" target="_blank" rel="nofollow">https://www.youtube.com/playlist?list=PL9rZj4Mea7wOZkdajK1TsprRg8iUf51BS</a> 
     The videos contains: 
         1. Training the target and tracker jointly via AD-VAT (2D);
         2. Testing the AD-VAT tracker in four testing environments (2D);
         3. Using the learned target to attack the baseline trackers (2D);
         4. Training the target and tracker via AD-VAT in DR Room (3D);
         5. Testing the tracker in Realistic Environments (3D);
         6. Passively testing tracker on real-world video clips.
b) Appendix. A is modified for better explaining the partial zero-sum reward.
c) Appendix. B is added. It visualizes the training process by drawing the position distribution in different training stages.
d) Appendix.C is added. It provides evaluation results on video clips to demonstrate the potential of transferring the tracking ability to the real world.</span></div><div class="reply_row clearfix"></div></div><div class="children"></div></div><div class="note_with_children"><div id="note_Hkg8D92gTm" class="note panel"><div class="title_pdf_row clearfix"><h2 class="note_content_title"><a>Incremental contribution and unclear rationales</a>  <button class="btn btn-xs btn-default permalink-button" title="Link to this comment" data-permalink-url="https://openreview.net/forum?id=HkgYmhR9KX&amp;noteId=Hkg8D92gTm"><span class="glyphicon glyphicon-link" aria-hidden="true"></span></button></h2></div><div class="meta_row"><span class="signatures">ICLR 2019 Conference Paper1378 AnonReviewer2</span></div><div class="clearfix"><div class="meta_row pull-left"><span class="date item">08 Nov 2018</span><span class="item">ICLR 2019 Conference Paper1378 Official Review</span><span class="item">Readers: <span class="readers-icon glyphicon glyphicon-globe"></span> Everyone</span></div><div class="meta_row meta_actions"></div></div><div class="note_contents"><span class="note_content_field">Review: </span><span class="note_content_value">This work aims to address the visual active tracking problem in which the tracker is automatically adjusted to follow the target. A training mechanism in which tracker and the target serve as mutual opponents is derived to learning the active tracker. Experimental evaluation in both 2D and 3D environments is conducted.

I think the contributions of this work is incremental compared with [Luo et al (2018)] in which the major difference is the partial zero sum reward structure is used and the observations and actions information from the tracker are incorporated into the target network, while the network architecture is quite similar to [Luo et al (2018)].
In addition, the explanation about importance of the tracker awareness to the target network seems not sufficient. The ancient Chinese proverb is not a good explanation. It would be better if some theoretical support can be provided for such design.

For active object tracking in real-world/3D environment, designing the reward function only based on the distance between the expected position and the tracked object position can not well reflect the tracker capacity. The scale changes of the target should also be considered when designing the reward function of the tracker. However, the proposed method does not consider the issue, and the evaluation using the reward function based on the position distance may not be sufficient.
</span></div><div class="note_contents"><span class="note_content_field">Rating: </span><span class="note_content_value">5: Marginally below acceptance threshold</span></div><div class="note_contents"><span class="note_content_field">Confidence: </span><span class="note_content_value">4: The reviewer is confident but not absolutely certain that the evaluation is correct</span></div><div class="reply_row clearfix"></div></div><div class="children"><div class="note_with_children"><div id="note_ryl__Au-a7" class="note panel"><div class="title_pdf_row clearfix"><h2 class="note_content_title"><a>Reply to AnonReviewer2</a>  <button class="btn btn-xs btn-default permalink-button" title="Link to this comment" data-permalink-url="https://openreview.net/forum?id=HkgYmhR9KX&amp;noteId=ryl__Au-a7"><span class="glyphicon glyphicon-link" aria-hidden="true"></span></button></h2></div><div class="meta_row"><span class="signatures">ICLR 2019 Conference Paper1378 Authors</span></div><div class="clearfix"><div class="meta_row pull-left"><span class="date item">08 Nov 2018</span><span class="item">ICLR 2019 Conference Paper1378 Official Comment</span><span class="item">Readers: <span class="readers-icon glyphicon glyphicon-globe"></span> Everyone</span></div><div class="meta_row meta_actions"></div></div><div class="note_contents"><span class="note_content_field">Comment: </span><span class="note_content_value">Thanks for the review. Our feedback goes below.

Q1: "I think the contributions of this work is incremental compared with [Luo et al (2018)] in which the major difference is the partial zero sum reward structure is used and the observations and actions information from the tracker are incorporated into the target network"
A1:  Our method is fundamentally different from Luo et al. (2018), please see our reply to R#1 (the Q2-A2) for detailed explanations. In short, the major difference is that we employ Multi-Agent RL to train both the tracker and the target object, while Luo et al. (2018) only train the tracker with Single-Agent RL (where they pre-define/hand-tune the moving path for the target object). Our method turns out better in the sense that it produces a stronger tracker via the proposed asymmetrical dueling training. 

The Multi-Agent RL training in our VAT task is unstable and slow to converge. To address these issues, we derived the two techniques: the partial zero sum and the asymmetrical target object model.


Q2: "In addition, the explanation about importance of the tracker awareness to the target network seems not sufficient. The ancient Chinese proverb is not a good explanation. It would be better if some theoretical support can be provided for such design."
A2: The tracker awareness mechanism for the target object is "cheating". This way, the target object would appear to be "stronger" than the tracker as it knows what the tracker knows. Such a treatment accelerates the training by inducing a reasonable curriculum to the tracker and finally helps training a much stronger and more generalizable tracker. Note we cannot apply this trick to the tracker as it cannot cheat when deploying. See also our reply to R#1 (Q3-A3).

As for the details of the tracker-aware model, it not only uses the observation and action of the tracker as extra input information but also employs an auxiliary task to predict the tracker's immediate reward. The auxiliary task could help the tracker learn a better representation for the adversarial policy to challenge the tracker.


Q3: "For active object tracking in real-world/3D environment, designing the reward function only based on the distance between the expected position and the tracked object position can not well reflect the tracker capacity. The scale changes of the target should also be considered when designing the reward function of the tracker. However, the proposed method does not consider the issue, and the evaluation using the reward function based on the position distance may not be sufficient."
A3: The scale of a target object showing up in the tracker's image observation will be implied by the distance between tracker and object, which we've considered when designing the reward function. 

Consider a simple case of projecting a line in 3D space onto a camera plane. The length (l) of the line on the 2D image plane is derived by an equation as below:
                                                                                      l = L*f/d, 
where L is the original length in 3D space, f is the distance between the 2D plane and the focal center, and d is the distance between the line and the focal center.
In the VAT problem，f depends on the intrinsic parameters of the camera model, which is fixed; L depends on the 3D model of the target object, which also could be regarded as constant. Thus, the scale of the object in the 2D image plane is impacted only by d, the distance between the target and the tracker. It is not difficult to derive that, the farther the distance d is, the smaller the target is observed. This suggests that the designed distance-based reward function has well considered the scale of the object.

Note that calculating the scale of the target in an image is of high computational complexity. It requires to extract the object mask and calculate the area of the mask. In contrast, our distance-based reward is computationally cheap, thanks to the simulator's APIs by which we can easily access the tracker's and target's world coordinate in the bird view map.</span></div><div class="reply_row clearfix"></div></div><div class="children"></div></div></div></div><div class="note_with_children"><div id="note_rkxScidq27" class="note panel"><div class="title_pdf_row clearfix"><h2 class="note_content_title"><a>Contrived task</a>  <button class="btn btn-xs btn-default permalink-button" title="Link to this comment" data-permalink-url="https://openreview.net/forum?id=HkgYmhR9KX&amp;noteId=rkxScidq27"><span class="glyphicon glyphicon-link" aria-hidden="true"></span></button></h2></div><div class="meta_row"><span class="signatures">ICLR 2019 Conference Paper1378 AnonReviewer1</span></div><div class="clearfix"><div class="meta_row pull-left"><span class="date item">03 Nov 2018 (modified: 07 Nov 2018)</span><span class="item">ICLR 2019 Conference Paper1378 Official Review</span><span class="item">Readers: <span class="readers-icon glyphicon glyphicon-globe"></span> Everyone</span></div><div class="meta_row meta_actions"></div></div><div class="note_contents"><span class="note_content_field">Review: </span><span class="note_content_value">This paper presents a simple multi-agent Deep RL task where a moving tracker tries to follow a moving target. The tracker receives, from its own perspective, partially observed visual information o_t^{alpha} about the target (e.g., an image that may show the target) and the target receives both observations from its own perspective o_t^{beta} and a copy of the information from the tracker's perspective. Both agents are standard convnet + LSTM neural architectures trained using A3C and are evaluated in 2D and 3D environments. The reward function is not completely zero-sum, as the tracked agent's reward vanishes when it gets too far from a reference point in the maze.

The work is very incremental over Luo et al (2018) "End-to-end Active Object Tracking and Its Real-world Deployment via Reinforcement Learning", as the only two additions are extra observations o_t^{alpha} for the target, and a reward function that has a fudge factor when the target gets too far away. Citing Sun Tzu's "Art of War" (please use the correct citation format) is not convincing enough for adding the tracker's observations as inputs for the target agent. Should not the asymmetrical relationship work the other way round, with the tracker knowing more about the target?

Experiments are conducted using two baselines for the target agent, one a random walk and another an agent that navigates to a target according to a shortest path planning algorithm. The ablation study shows that the tracker-aware observations and a target's reward structure that penalizes when it gets too far do help the tracker's performance, and that training the target agent helps the tracker agent achieve higher scores. The improvement is however quite small and the task is ad-hoc.
 
The paper would have benefitted from a proper analysis of the trajectories taken by the adversarial target as opposed to the heuristic ones, and from comparison with non-RL state-of-the-art on tracking tasks. Further multi-agent tasks could also have been considered, such as capture the flag tasks as in "Human-level performance in first-person multiplayer games with population-based deep reinforcement learning".</span></div><div class="note_contents"><span class="note_content_field">Rating: </span><span class="note_content_value">4: Ok but not good enough - rejection</span></div><div class="note_contents"><span class="note_content_field">Confidence: </span><span class="note_content_value">3: The reviewer is fairly confident that the evaluation is correct</span></div><div class="reply_row clearfix"></div></div><div class="children"><div class="note_with_children"><div id="note_r1xYj34yp7" class="note panel"><div class="title_pdf_row clearfix"><h2 class="note_content_title"><a>Thanks for the review. </a>  <button class="btn btn-xs btn-default permalink-button" title="Link to this comment" data-permalink-url="https://openreview.net/forum?id=HkgYmhR9KX&amp;noteId=r1xYj34yp7"><span class="glyphicon glyphicon-link" aria-hidden="true"></span></button></h2></div><div class="meta_row"><span class="signatures">ICLR 2019 Conference Paper1378 Authors</span></div><div class="clearfix"><div class="meta_row pull-left"><span class="date item">07 Nov 2018 (modified: 08 Nov 2018)</span><span class="item">ICLR 2019 Conference Paper1378 Official Comment</span><span class="item">Readers: <span class="readers-icon glyphicon glyphicon-globe"></span> Everyone</span></div><div class="meta_row meta_actions"></div></div><div class="note_contents"><span class="note_content_field">Comment: </span><span class="note_content_value">Thanks for the review. Our feedback goes below.

Q1: "Contrived task"
A1: Visual object tracking is widely recognized as an important task in Computer Vision. In this study, we propose a principled approach of how to train a robust tracker.


Q2: "The work is very incremental over Luo et al. (2018) "End-to-end Active Object Tracking and Its Real-world Deployment via Reinforcement Learning", as the only two additions are extra observations o_t^{alpha} for the target, and a reward function that has a fudge factor when the target gets too far away"
A2: Our method is fundamentally different from Luo et al. (2018), as explained below.

Luo et al. (2018) adopted pre-defined target object moving path, coded in hand-tuned scripts. Thus, only the tracker is trainable, and the settings are single-agent RL. 

In our method, the target object is also implemented by a neural network, learning how to escape the tracker during training. Both the tracker and the target object are trained jointly in an adversary/dueling way, and the settings are multi-agent RL. 

We show the advantage of our method over Luo et al. (2018). Note that the pre-defined target object moving path in Luo et al. (2018) can hurt the generalizability of the tracker. In reality, the target object can move in various hard patterns: Z-turn, U-turn, sudden stop, walk-towards-wall-then-turn, etc., which can pose non-trivial difficulties to the tracker during both training and deployment. Moreover, such moving patterns are difficult to be thoroughly covered and coded by the hand-tuning scripts as in Luo et al. (2018).

The trainable target object in our method, however, can learn the proper moving path in order to escape from the tracker solely by the adversary/dueling training, without hand-tuned path. The smart target object, in turn, induces a tracker that well follows the target no matter how wild the target object moves. Eventually, we obtain a much stronger tracker than that of Luo et al. (2018), achieving the very purpose of our study: to train a robust tracker for VAT task.


Q3: "Should not the asymmetrical relationship work the other way round, with the tracker knowing more about the target?"
A3: We should not do that. 

Note that the additional "asymmetrical" information is way of "cheating". As our goal is to train a tracker, we don't need to consider deploying a target object. Therefore, we can simply let the target object cheat during training by feeding to it the tracker's observation/reward/action. Such a "peeking" treatment accelerates the training and ultimately improves the tracker's training quality, as is shown in the submitted paper.

The tracker, however, is unable to "cheat" when deployed (e.g., in a real-world robot). It has to predict the action using its own observations. There is no way for the tracker to acquire the information (observation/reward/action) from a target object. 


Q4: "The paper would have benefitted from a proper analysis of the trajectories taken by the adversarial target as opposed to the heuristic ones, ..."
A4: We have added to Appendix some texts for the analysis, see Appendix.B in the updated submission. The target object does show intriguing behaviors when escaping the tracker, see the supplementary videos available at <a href="https://www.youtube.com/playlist?list=PL9rZj4Mea7wOZkdajK1TsprRg8iUf51BS" target="_blank" rel="nofollow">https://www.youtube.com/playlist?list=PL9rZj4Mea7wOZkdajK1TsprRg8iUf51BS</a>


Q5: "...and from comparison with non-RL state-of-the-art on tracking tasks."
A5: Luo et al. (2018) had done the comparisons and shown their method improves over several representative non-RL trackers in the literature.
Our method outperforms that of Luo et al. (2018).


Q6: "Citing Sun Tzu's "Art of War" (please use the correct citation format)..."
A6: We have fixed this in the updated submission.


Q7: "Further multi-agent tasks could also have been considered, such as capture the flag tasks as in "Human-level performance in first-person multiplayer games with population-based deep reinforcement learning""
A7: The method developed in that paper is for playing the First Person Shooting game, where it has to ensure the fairness among the intra- and inter-team players. In our study, the primary goal is to train a tracker (player 1), permitting us to leverage the asymmetrical mechanism for the target object (player 2). This technique effectively improves the adversary/dueling training and eventually produces a strong tracker.</span></div><div class="reply_row clearfix"></div></div><div class="children"></div></div></div></div><div class="note_with_children"><div id="note_Hyxga8D52m" class="note panel"><div class="title_pdf_row clearfix"><h2 class="note_content_title"><a>novel reward function in adversarial VAT appliation</a>  <button class="btn btn-xs btn-default permalink-button" title="Link to this comment" data-permalink-url="https://openreview.net/forum?id=HkgYmhR9KX&amp;noteId=Hyxga8D52m"><span class="glyphicon glyphicon-link" aria-hidden="true"></span></button></h2></div><div class="meta_row"><span class="signatures">ICLR 2019 Conference Paper1378 AnonReviewer3</span></div><div class="clearfix"><div class="meta_row pull-left"><span class="date item">03 Nov 2018 (modified: 07 Nov 2018)</span><span class="item">ICLR 2019 Conference Paper1378 Official Review</span><span class="item">Readers: <span class="readers-icon glyphicon glyphicon-globe"></span> Everyone</span></div><div class="meta_row meta_actions"></div></div><div class="note_contents"><span class="note_content_field">Review: </span><span class="note_content_value">This is in a visual active tracking application. The paper proposes a novel reward function - "partial zero sum", which only encourages the tracker-target competition when they are close and penalizes whey they are too far.

This is a very interesting problem and I see why their contribution could improve the system performance. 

Clarity: the paper is well-written. I also like how the author provides both formulas and a lot of details on implementation of the end-to-end system. 

Originality: Most of the components are pretty standard, however I value the part that seems pretty novel to me - which is the "partial zero-sum" idea.

Evaluation: the result obtained from the simulated environment in 2d and 3d are convincing. However, if 1) real-world test and results  2) a stronger baseline can be used, that would be a stronger acceptance. </span></div><div class="note_contents"><span class="note_content_field">Rating: </span><span class="note_content_value">6: Marginally above acceptance threshold</span></div><div class="note_contents"><span class="note_content_field">Confidence: </span><span class="note_content_value">4: The reviewer is confident but not absolutely certain that the evaluation is correct</span></div><div class="reply_row clearfix"></div></div><div class="children"><div class="note_with_children"><div id="note_rJlb9G2fp7" class="note panel"><div class="title_pdf_row clearfix"><h2 class="note_content_title"><a>Reply to AnonReviewer3</a>  <button class="btn btn-xs btn-default permalink-button" title="Link to this comment" data-permalink-url="https://openreview.net/forum?id=HkgYmhR9KX&amp;noteId=rJlb9G2fp7"><span class="glyphicon glyphicon-link" aria-hidden="true"></span></button></h2></div><div class="meta_row"><span class="signatures">ICLR 2019 Conference Paper1378 Authors</span></div><div class="clearfix"><div class="meta_row pull-left"><span class="date item">09 Nov 2018</span><span class="item">ICLR 2019 Conference Paper1378 Official Comment</span><span class="item">Readers: <span class="readers-icon glyphicon glyphicon-globe"></span> Everyone</span></div><div class="meta_row meta_actions"></div></div><div class="note_contents"><span class="note_content_field">Comment: </span><span class="note_content_value">Thanks for appreciating our partial-zero-sum idea. Our primary contribution is the adversary/dueling RL mechanism for training a robust tracker. To stabilize and accelerates the training, we devised the techniques of the partial-zero-sum and the asymmetrical target model. These two techniques are critical for a successful training, and we hope to see their applications to other domains involving adversary/dueling training.

As for the comments on "real-world test and results", we've taken a qualitative testing on some real-world video clips from VOT dataset [Kristan et al. (2016)]. In this evaluation, we feed the video clips to the tracker and observe the network output actions. In general, the results show that the output action is consistent with the position and scale of the target. For example, when the target moves from the image center to the left until disappearing, the tracker outputs actions ``move forward", ``move forward-left", and ``turn left" sequentially. The testing demonstrates the potential of transferring the tracking ability to real-world. 

Please see Appendix.C in our updated submission and watch the demo video here: <a href="https://youtu.be/jv-5HVg_Sf4" target="_blank" rel="nofollow">https://youtu.be/jv-5HVg_Sf4</a> </span></div><div class="reply_row clearfix"></div></div><div class="children"></div></div></div></div></div></main></div>
  </div>
</div>


    <!-- Footer -->
    <footer class="sitemap">
      <div class="container">
        <div class="row hidden-xs">
          <div class="col-sm-4">
            <ul class="list-unstyled">
              <li><a href="/" class="home">Home</a></li>
              <li><a href="/venues">All Venues</a></li>
            </ul>
          </div>
    
          <div class="col-sm-4">
            <ul class="list-unstyled">
              <li><a href="/about">About OpenReview</a></li>
              <li><a href="/contact">Contact</a></li>
              <li><a href="#" data-toggle="modal" data-target="#feedback-modal">Feedback</a></li>
            </ul>
          </div>
    
          <div class="col-sm-4">
            <ul class="list-unstyled">
              <li><a href="/terms">Terms of Service</a></li>
              <li><a href="/privacy">Privacy Policy</a></li>
            </ul>
          </div>
        </div>
    
        <div class="row visible-xs-block">
          <div class="col-xs-6">
            <ul class="list-unstyled">
              <li><a href="/" class="home">Home</a></li>
              <li><a href="/about">About OpenReview</a></li>
              <li><a href="#" data-toggle="modal" data-target="#feedback-modal">Feedback</a></li>
            </ul>
          </div>
    
          <div class="col-xs-6">
            <ul class="list-unstyled">
              <li><a href="/contact">Contact</a></li>
              <li><a href="/terms">Terms of Service</a></li>
              <li><a href="/privacy">Privacy Policy</a></li>
            </ul>
          </div>
        </div>
      </div>
    </footer>
    
    <footer class="sponsor">
      <div class="container">
        <div class="row">
          <div class="col-sm-10 col-sm-offset-1">
            <p class="text-center">
              OpenReview is created by the <a href="http://www.iesl.cs.umass.edu/" target="_blank">Information Extraction and Synthesis Laboratory</a>, College of Information and Computer Science, University of Massachusetts Amherst. We gratefully acknowledge the support of the OpenReview sponsors:  Google,  Facebook, NSF, the University of Massachusetts Amherst Center for Data Science, and Center for Intelligent Information Retrieval, as well as the Google Cloud Platform for donating the computing and networking services on which OpenReview.net runs.
            </p>
          </div>
        </div>
      </div>
    </footer>

  <div id="feedback-modal" class="modal fade" tabindex="-1" role="dialog">
    <div class="modal-dialog">
      <div class="modal-content">
        <div class="modal-header">
          <button type="button" class="close" data-dismiss="modal" aria-label="Close"><span aria-hidden="true">×</span></button>
          <h3 class="modal-title">Send Feedback</h3>
        </div>
        <div class="modal-body">
          <p>Enter your feedback below and we'll get back to you as soon as possible.</p>
          <form action="/feedback" method="POST">
            <div class="form-group">
              <input type="email" name="from" class="form-control" placeholder="Email" />
            </div>
            <div class="form-group">
              <input type="text" name="subject" class="form-control" placeholder="Subject" />
            </div>
            <div class="form-group">
              <textarea name="message" class="form-control feedback-input" rows="5" placeholder="Message" required=""></textarea>
            </div>
          <ul id="ui-id-2" tabindex="0" class="ui-menu ui-widget ui-widget-content" style="display: none;"></ul></form>
        </div>
        <div class="modal-footer">
          <button type="button" class="btn btn-default" data-dismiss="modal">Cancel</button>
          <button type="button" class="btn btn-primary">Send</button>
        </div>
      </div>
    </div>
  </div>

  <script type="text/javascript" async="" src="https://www.google-analytics.com/analytics.js"></script><script src="//cdnjs.cloudflare.com/ajax/libs/jquery/2.2.4/jquery.min.js"></script>
  <script>window.jQuery || document.write('&lt;script src="/static/js/vendor/jquery-2.2.4.min.js"&gt;&lt;\/script&gt;')</script>

  <script src="/static/dist/vendor.min.js"></script>
  <script src="/static/dist/templates.min.js"></script>
  <script src="/static/dist/openreview.min.js"></script>

    <script>window.legacyScripts = true;</script>


    <script async="" src="https://www.googletagmanager.com/gtag/js?id=UA-108703919-1"></script>
    <script>
      window.dataLayer = window.dataLayer || [];
      function gtag() { dataLayer.push(arguments); }
      gtag('js', new Date());
      gtag('config', 'UA-108703919-1');
    </script>


<div role="status" aria-live="assertive" aria-relevant="additions" class="ui-helper-hidden-accessible"></div></body></html>