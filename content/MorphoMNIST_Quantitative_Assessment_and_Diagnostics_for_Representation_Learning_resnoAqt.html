<!DOCTYPE html><html xmlns="http://www.w3.org/1999/xhtml" class="no-js"><head>
  <meta charset="utf-8" />
  <meta http-equiv="X-UA-Compatible" content="IE=edge,chrome=1" />
  <meta name="viewport" content="width=device-width, initial-scale=1" />

  <title>Morpho-MNIST: Quantitative Assessment and Diagnostics for Representation Learning | OpenReview</title>
  <meta name="description" content="" />

      <meta name="citation_title" content="Morpho-MNIST: Quantitative Assessment and Diagnostics for Representation Learning" />
        <meta name="citation_author" content="Anonymous" />
      <meta name="citation_publication_date" content="2018/09/27" />
      <meta name="citation_online_date" content="2018/09/27" />
      <meta name="citation_pdf_url" content="https://openreview.net/pdf?id=r1esnoAqt7" />
      <meta name="twitter:card" content="summary" />
      <meta name="twitter:site" content="@openreviewnet" />
      <meta name="og:title" content="Morpho-MNIST: Quantitative Assessment and Diagnostics for..." />
      <meta name="og:description" content="Revealing latent structure in data is an active field of research, having brought exciting new models such as variational autoencoders and generative adversarial networks, and is essential to push..." />
      <meta name="og:image" content="https://openreview.net/static/images/openreview_logo_512.png" />

  <link rel="stylesheet" href="https://fonts.googleapis.com/css?family=Noto+Sans:400,400i,700,700i" />

  <link rel="stylesheet" href="/static/css/bootstrap.min.css" />
  <link rel="stylesheet" href="/static/css/jquery-ui.min.css" />
  <link rel="stylesheet" href="/static/css/main.min.css" />
</head>

<body class="forum">
  <nav class="navbar navbar-inverse navbar-fixed-top" role="navigation">
    <div class="container">
  
      <div class="navbar-header">
        <button type="button" class="navbar-toggle collapsed" data-toggle="collapse" data-target="#navbar" aria-expanded="false" aria-controls="navbar">
          <span class="sr-only">Toggle navigation</span>
          <span class="icon-bar"></span>
          <span class="icon-bar"></span>
          <span class="icon-bar"></span>
        </button>
        <a class="navbar-brand home push-link" href="/"><strong>OpenReview</strong>.net</a>
      </div>
  
      <div id="navbar" class="navbar-collapse collapse">
        <form class="navbar-form navbar-left profile-search" role="search">
          <div class="form-group has-feedback">
            <input id="search_input" type="text" class="form-control ui-autocomplete-input" placeholder="Search ICLR 2019 Conference" autocomplete="off" />
            <span class="glyphicon glyphicon-search form-control-feedback" aria-hidden="true"></span>
          </div>
  
          <input id="search_group" type="hidden" value="ICLR.cc/2019/Conference" />
          <input id="search_content" type="hidden" value="all" />
        <ul id="ui-id-1" tabindex="0" class="ui-menu ui-widget ui-widget-content ui-autocomplete ui-front" style="display: none;"></ul></form>
  
        <ul class="nav navbar-nav navbar-right">
        
            <li id="user-menu"><a href="/login">Login</a></li>
        </ul>
      </div>
  
    </div>
  </nav>

  <div id="or-banner" class="banner" style="">
  <div class="container">
    <div class="row">
      <div class="col-xs-12"><a href="/group?id=ICLR.cc/2019/Conference" title="Venue Homepage"><img class="icon" src="/static/images/arrow_left.svg" /> Go to <strong>ICLR 2019 Conference</strong> homepage</a></div>
    </div>
  </div>
</div>
<div id="flash-message-container" class="alert alert-danger" role="alert" style="display: none;">
  <div class="container">
    <div class="row">
      <div class="col-xs-12">
        <div class="alert-content">
          <button type="button" class="close" aria-label="Close"><span aria-hidden="true">×</span></button>
        </div>
      </div>
    </div>
  </div>
</div>

<div class="container">
  <div class="row">
    <div class="col-xs-12">
      <main id="content" class="clearfix openbanner-visible legacy-styles"><div id="note_r1esnoAqt7" class="note panel"><div class="title_pdf_row clearfix"><h2 class="note_content_title"><a>Morpho-MNIST: Quantitative Assessment and Diagnostics for Representation Learning</a> <a class="note_content_pdf" href="/pdf?id=r1esnoAqt7" title="Download PDF" target="_blank"><img src="/static/images/pdf_icon_blue.svg" /></a> </h2></div><div class="meta_row"><span class="signatures">Anonymous</span></div><div class="clearfix"><div class="meta_row pull-left"><span class="date item">28 Sep 2018 (modified: 11 Oct 2018)</span><span class="item">ICLR 2019 Conference Blind Submission</span><span class="item">Readers: <span class="readers-icon glyphicon glyphicon-globe"></span> Everyone</span><span class="item"><a class="action-bibtex-modal" data-bibtex="@inproceedings{    &#10;anonymous2019morpho-mnist:,    &#10;title={Morpho-MNIST: Quantitative Assessment and Diagnostics for Representation Learning},    &#10;author={Anonymous},    &#10;booktitle={Submitted to International Conference on Learning Representations},    &#10;year={2019},    &#10;url={https://openreview.net/forum?id=r1esnoAqt7},    &#10;note={under review}    &#10;}">Show Bibtex</a></span></div><div class="meta_row meta_actions"></div></div><div class="note_contents"><span class="note_content_field">Abstract: </span><span class="note_content_value">Revealing latent structure in data is an active field of research, having brought exciting new models such as variational autoencoders and generative adversarial networks, and is essential to push machine learning towards unsupervised knowledge discovery. However, a major challenge is the lack of suitable benchmarks for an objective and quantitative evaluation of learned representations. To address this issue we introduce Morpho-MNIST. We extend the popular MNIST dataset by adding a morphometric analysis enabling quantitative comparison of different models, identification of the roles of latent variables, and characterisation of sample diversity. We further propose a set of quantifiable perturbations to assess the performance of unsupervised and supervised methods on challenging tasks such as outlier detection and domain adaptation.</span></div><div class="note_contents"><span class="note_content_field">Keywords: </span><span class="note_content_value">quantitative evaluation, diagnostics, generative models, representation learning, morphometrics, image perturbations</span></div><div class="note_contents"><span class="note_content_field">TL;DR: </span><span class="note_content_value">This paper introduces Morpho-MNIST, a collection of shape metrics and perturbations, in a step towards quantitative evaluation of representation learning in computer vision.</span></div><div class="reply_row clearfix"><div class="item" id="reply_count">7 Replies</div></div></div><hr class="small" /><div id="note_children"><div class="note_with_children"><div id="note_S1eAb9jpTX" class="note panel"><div class="title_pdf_row clearfix"><h2 class="note_content_title"><a>asking for reviewer feedback</a>  <button class="btn btn-xs btn-default permalink-button" title="Link to this comment" data-permalink-url="https://openreview.net/forum?id=r1esnoAqt7&amp;noteId=S1eAb9jpTX"><span class="glyphicon glyphicon-link" aria-hidden="true"></span></button></h2></div><div class="meta_row"><span class="signatures">ICLR 2019 Conference Paper742 Authors</span></div><div class="clearfix"><div class="meta_row pull-left"><span class="date item">17 Nov 2018</span><span class="item">ICLR 2019 Conference Paper742 Official Comment</span><span class="item">Readers: <span class="readers-icon glyphicon glyphicon-globe"></span> Everyone</span></div><div class="meta_row meta_actions"></div></div><div class="note_contents"><span class="note_content_field">Comment: </span><span class="note_content_value">Dear Reviewers,

We appreciate your time on assessing our paper, and we would highly value to hear back whether our responses to the criticism and our additional argumentation may change your recommendation. We believe that our work could be of great interest to the ICLR community and may initiate a very much needed discussion about objective and quantitative evaluation in representation learning.

As we believe most of the main criticism was based on misunderstanding that can be easily addressed in an updated version of the paper, we would love to hear what you think. Is there anything you believe we missed in our responses, or any other points you would like us to address in order to improve our paper?

Many thanks again for your time and valuable feedback.</span></div><div class="reply_row clearfix"></div></div><div class="children"></div></div><div class="note_with_children"><div id="note_SJghAtFWaX" class="note panel"><div class="title_pdf_row clearfix"><h2 class="note_content_title"><a>Review: Morpho-MNIST: Quantitative Assessment and Diagnostics for Representation Learning </a>  <button class="btn btn-xs btn-default permalink-button" title="Link to this comment" data-permalink-url="https://openreview.net/forum?id=r1esnoAqt7&amp;noteId=SJghAtFWaX"><span class="glyphicon glyphicon-link" aria-hidden="true"></span></button></h2></div><div class="meta_row"><span class="signatures">ICLR 2019 Conference Paper742 AnonReviewer1</span></div><div class="clearfix"><div class="meta_row pull-left"><span class="date item">08 Nov 2018</span><span class="item">ICLR 2019 Conference Paper742 Official Review</span><span class="item">Readers: <span class="readers-icon glyphicon glyphicon-globe"></span> Everyone</span></div><div class="meta_row meta_actions"></div></div><div class="note_contents"><span class="note_content_field">Review: </span><span class="note_content_value">This paper discusses the problem of evaluating and diagnosing the representations learnt using a generative model. This is a very important and necessary problem.

However, this paper lacks in terms of experimental evaluation and has some technical flaws.
1. Morphological properties deals with only the "shape" properties of the image object. However, when the entire image is subject to the generative model, it learns multiple properties from the image apart from shape too - such as texture and color. Additionally, there are lot of low level pixel relations that the model learns to fit the distribution of the given images. However, here the authors have assumed that the latent space of the generative models are influenced only by the morphological properties of the image - which is wrong. Latent space features could be affected by the color or texture of the image as well.

2. Extracting morphological properties of the image is straight-foward for MNIST kind of objects. However, it becomes really difficult for other datasets such as CIFAR or some real world images. Studying the properties of a generative model on such datasets is very challenging and the authors have not added a discussion around that. 

3. Now assuming that my GAN model has learnt good representation in Morpho-MNIST dataset, is it guaranteed to learn good representations in other datasets as well? There is no guarantee on generalizability or extensibility of the work. </span></div><div class="note_contents"><span class="note_content_field">Rating: </span><span class="note_content_value">3: Clear rejection</span></div><div class="note_contents"><span class="note_content_field">Confidence: </span><span class="note_content_value">4: The reviewer is confident but not absolutely certain that the evaluation is correct</span></div><div class="reply_row clearfix"></div></div><div class="children"><div class="note_with_children"><div id="note_rylzwEUXa7" class="note panel"><div class="title_pdf_row clearfix"><h2 class="note_content_title"><a>Incorrect assumptions</a>  <button class="btn btn-xs btn-default permalink-button" title="Link to this comment" data-permalink-url="https://openreview.net/forum?id=r1esnoAqt7&amp;noteId=rylzwEUXa7"><span class="glyphicon glyphicon-link" aria-hidden="true"></span></button></h2></div><div class="meta_row"><span class="signatures">ICLR 2019 Conference Paper742 Authors</span></div><div class="clearfix"><div class="meta_row pull-left"><span class="date item">10 Nov 2018</span><span class="item">ICLR 2019 Conference Paper742 Official Comment</span><span class="item">Readers: <span class="readers-icon glyphicon glyphicon-globe"></span> Everyone</span></div><div class="meta_row meta_actions"></div></div><div class="note_contents"><span class="note_content_field">Comment: </span><span class="note_content_value">We thank the reviewer for acknowledging the importance of the problem we aimed to address, however, we very much disagree with the statements made regarding our assumptions.

Regarding the reviewer’s first point, we believe there is a misunderstanding. We absolutely agree that a generative model needs to learn about colour, texture, and low-level pixel relations to be able to extract its representations and to produce reasonable samples. Regarding the reviewer’s statement that “the authors have assumed that the latent space of the generative models are influenced only by the morphological properties of the image”, we would like to stress that we never made such assumptions nor have we claimed that the latent space of models trained on MNIST capture exclusively shape variations. What the Morpho-MNIST methodology aims to answer is: “to what extent has my model learned to represent these specific factors of variation in the data?” If colour and texture are important factors for a given application or dataset, it suffices to design the relevant scalar metrics and include them in the very same framework.

This brings us to the second point. As far as we are aware, this is the first attempt in *any* context to quantitatively characterise inferential and generative behaviour of learned representations. We propose to do it in terms of measurable features: here we exploit shape attributes, and in the conclusion we point to various possible extensions involving colours or object properties. In our view, it just makes sense that the first step in that direction builds on a simple dataset with well understood and easily measurable factors of variation.

Finally, although it is correct that there are no generalisability guarantees, that is the case for any model evaluated on MNIST, CIFAR-10, or even ImageNet (cf. <a href="https://arxiv.org/abs/1806.00451," target="_blank" rel="nofollow">https://arxiv.org/abs/1806.00451,</a> for example). As argued above, we are proposing a toolset to inspect and diagnose trained generative models that works with any collection of measurable attributes. Evidently conclusions may not be transferable if the datasets have different relevant attributes.</span></div><div class="reply_row clearfix"></div></div><div class="children"></div></div></div></div><div class="note_with_children"><div id="note_Hygjmwnqhm" class="note panel"><div class="title_pdf_row clearfix"><h2 class="note_content_title"><a>Interesting characterisation and extension of MNIST </a>  <button class="btn btn-xs btn-default permalink-button" title="Link to this comment" data-permalink-url="https://openreview.net/forum?id=r1esnoAqt7&amp;noteId=Hygjmwnqhm"><span class="glyphicon glyphicon-link" aria-hidden="true"></span></button></h2></div><div class="meta_row"><span class="signatures">ICLR 2019 Conference Paper742 AnonReviewer2</span></div><div class="clearfix"><div class="meta_row pull-left"><span class="date item">03 Nov 2018 (modified: 07 Nov 2018)</span><span class="item">ICLR 2019 Conference Paper742 Official Review</span><span class="item">Readers: <span class="readers-icon glyphicon glyphicon-globe"></span> Everyone</span></div><div class="meta_row meta_actions"></div></div><div class="note_contents"><span class="note_content_field">Review: </span><span class="note_content_value">Authors present a set of criteria to categorize MNISt digists (e.g. slant, stroke length, ..) and a set of interesting perturbations (swelling, fractures, ...) to modify MNIST dataset. They suggest analysing performance of generative models based on these tools. By extracting this kind of features, they effectively decrease the dimmension of  data. Therefore, statistically comparing the distribution of generated vs test data and binning the generated data is now possible. They perform a thorough study regarding MNIST. Their tools are a handy addition to the analytical surveys in several applications (e.g. how classification fails), but not convincingly for generation. 

Since their method is manually designed for MNIST, the manuscript would benefit from a justification or discussion on the  common pitfalls and the correlation between MNIST generation and more complex natural image generation tasks. Since the presented metrics do not show a significant difference between the VAE and Vanilla GAN model, the question remains whether evaluating on MNIST is a good proxy for the performance of the model on colored images with backgrounds or not. For example sharpness and attending to details is not typically a challenge in MNIST generation where in other datasets this is usually the first challenge to be addressed. I'm not convinced that ability of a model in disentangling thickness correlates to their ability in natural image generation.</span></div><div class="note_contents"><span class="note_content_field">Rating: </span><span class="note_content_value">5: Marginally below acceptance threshold</span></div><div class="note_contents"><span class="note_content_field">Confidence: </span><span class="note_content_value">3: The reviewer is fairly confident that the evaluation is correct</span></div><div class="reply_row clearfix"></div></div><div class="children"><div class="note_with_children"><div id="note_HkehOTkW6X" class="note panel"><div class="title_pdf_row clearfix"><h2 class="note_content_title"><a>Representation learning vs. sample quality</a>  <button class="btn btn-xs btn-default permalink-button" title="Link to this comment" data-permalink-url="https://openreview.net/forum?id=r1esnoAqt7&amp;noteId=HkehOTkW6X"><span class="glyphicon glyphicon-link" aria-hidden="true"></span></button></h2></div><div class="meta_row"><span class="signatures">ICLR 2019 Conference Paper742 Authors</span></div><div class="clearfix"><div class="meta_row pull-left"><span class="date item">08 Nov 2018</span><span class="item">ICLR 2019 Conference Paper742 Official Comment</span><span class="item">Readers: <span class="readers-icon glyphicon glyphicon-globe"></span> Everyone</span></div><div class="meta_row meta_actions"></div></div><div class="note_contents"><span class="note_content_field">Comment: </span><span class="note_content_value">We appreciate the thoughtful review and suggestions for adding clarifications, in particular with respect to the aspect of natural image generation.

We completely agree that MNIST generation is not to be mistaken as a surrogate for the generation of natural images, and nowhere in our paper did we intend to suggest otherwise. We thank the reviewer for bringing up this potential confusion, and we will add further clarification to make sure there is no ambiguity about this in our paper.

As a matter of fact, we believe that the current focus of research towards generating natural images can be misleading (or at least gives an incomplete picture) in the context of representation learning, as the quality of sampled images generally tells us little about how well the learned representations capture the known factors of variation in the training distribution.

With Morpho-MNIST, we aimed to address this issue by providing an objective methodology for evaluating representation learning, i.e. quantitatively measuring how expressive a trained generative model is and how well it covers the variability in the data, in our case defined by morphometry of shapes represented as grayscale images. In fact, we make no statements about measuring sample _quality_, only the _diversity_ of shape attributes. Our conclusion does point to possible extensions of this framework to other measurable content attributes for different types of images.

The fact that VAE and GAN performed similarly under our metrics shows only that, for this data and similar model capacities, the representations they learned are comparably expressive. We actually believe this is an important message to convey, as a large body of work focusing on the crispness of generated images might incorrectly lead to a conclusion that VAEs are generally inferior to GANs with respect to representation learning. Here, we can demonstrate quantitatively that for the considered type of distribution (morphometry of rasterised shapes) this is not the case.

On your point of “whether evaluating on MNIST is a good proxy for the performance of the model on colored images with backgrounds or not” we would say the answer is clearly no. But, as mentioned above and hopefully made more clear in our revision, it was never our intention to imply otherwise. We also couldn’t agree more with your statement “I'm not convinced that [the] ability of a model in disentangling thickness correlates to their ability in natural image generation.” These are very different problems.

However, we would like to reiterate some of our reasons for focusing on MNIST, as presented in the introduction: few and simple factors of variation, sufficient size for its complexity, low computational requirements, and availability. Importantly, MNIST is a standard baseline on which a great number of generative models proposed in the literature are evaluated. This means that our framework can also be applied to these models retrospectively, adding novel insights about their performance in a more objective and quantitative manner. We do believe this is an important contribution to the area of representation learning.

Here are a few such prominent works, in addition to the ones cited in our paper:
- Goodfellow et al. (NIPS 2014). Generative Adversarial Nets.
- Nowozin et al. (NIPS 2016). f-GAN: Training Generative Neural Samplers using Variational Divergence Minimization.
- Radford et al. (ICLR 2016). Unsupervised Representation Learning with Deep Convolutional Generative Adversarial Networks.
- Salimans et al. (NIPS 2016). Improved Techniques for Training GANs.
- Rezende &amp; Mohamed (ICML 2015). Variational Inference with Normalizing Flows.
- Mescheder et al. (ICML 2017). Adversarial Variational Bayes: Unifying Variational Autoencoders and Generative Adversarial Networks.</span></div><div class="reply_row clearfix"></div></div><div class="children"></div></div></div></div><div class="note_with_children"><div id="note_S1etazq92Q" class="note panel"><div class="title_pdf_row clearfix"><h2 class="note_content_title"><a>not enough contribution</a>  <button class="btn btn-xs btn-default permalink-button" title="Link to this comment" data-permalink-url="https://openreview.net/forum?id=r1esnoAqt7&amp;noteId=S1etazq92Q"><span class="glyphicon glyphicon-link" aria-hidden="true"></span></button></h2></div><div class="meta_row"><span class="signatures">ICLR 2019 Conference Paper742 AnonReviewer3</span></div><div class="clearfix"><div class="meta_row pull-left"><span class="date item">03 Nov 2018 (modified: 07 Nov 2018)</span><span class="item">ICLR 2019 Conference Paper742 Official Review</span><span class="item">Readers: <span class="readers-icon glyphicon glyphicon-globe"></span> Everyone</span></div><div class="meta_row meta_actions"></div></div><div class="note_contents"><span class="note_content_field">Review: </span><span class="note_content_value">The author proposed an extended version of MNIS where they introduced thickening/thinning/swelling/fracture. The operation is done using binary morphological operations.

* Providing benchmark data for tasks such disentanglement is important but I am not sure generating data is sufficient contribution for a paper. 
* I am not sure what conclusion I should draw from Fig 5 and Fig 6 about the data.
* Eventually this data can become a benchmark data when it is paired with a method. Then that method/data are a benchmark.

 </span></div><div class="note_contents"><span class="note_content_field">Rating: </span><span class="note_content_value">4: Ok but not good enough - rejection</span></div><div class="note_contents"><span class="note_content_field">Confidence: </span><span class="note_content_value">3: The reviewer is fairly confident that the evaluation is correct</span></div><div class="reply_row clearfix"></div></div><div class="children"><div class="note_with_children"><div id="note_SkgGgAJZTm" class="note panel"><div class="title_pdf_row clearfix"><h2 class="note_content_title"><a>Inadequate review, main contributions ignored</a>  <button class="btn btn-xs btn-default permalink-button" title="Link to this comment" data-permalink-url="https://openreview.net/forum?id=r1esnoAqt7&amp;noteId=SkgGgAJZTm"><span class="glyphicon glyphicon-link" aria-hidden="true"></span></button></h2></div><div class="meta_row"><span class="signatures">ICLR 2019 Conference Paper742 Authors</span></div><div class="clearfix"><div class="meta_row pull-left"><span class="date item">08 Nov 2018</span><span class="item">ICLR 2019 Conference Paper742 Official Comment</span><span class="item">Readers: <span class="readers-icon glyphicon glyphicon-globe"></span> Everyone</span></div><div class="meta_row meta_actions"></div></div><div class="note_contents"><span class="note_content_field">Comment: </span><span class="note_content_value">With all due respect, the reviewer’s summary of our work is inaccurate, incomplete and ignores the key part of our contributions. This suggests that the reviewer may not have carefully read our paper. The review consists of three lines, which we will address below. We strongly believe that the overall recommendation based on the reviewer’s comments is unjustified.

Regarding the first statement “I am not sure generating data is sufficient contribution for a paper”:

Generating a dataset is a relatively minor portion of this work; please refer to Sec. 1.1 for a clear outline of the main points. The bulk of our paper is about how morphometrics enable quantitative evaluation of learned representations (Section 4), and how the proposed image perturbations can enrich this evaluation and also open up a variety of new supervised tasks (explored in Appendix D). 

Regarding the second statement “I am not sure what conclusion I should draw from Fig 5 and Fig 6 about the data”:

As described clearly in the text, Fig. 5 shows quantitative results for _inferential_ disentanglement (i.e. from real MNIST test data to latent codes) of two different InfoGANs, and Fig. 6 for _generative_ disentanglement (from latent codes to generated samples). In the paper we state “as the tables are mostly indistinguishable, we may argue that in this case the inference and generator networks have learned to consistently encode and decode the digit shape attributes.” To the best of our knowledge, this is the first time that partial correlations have been used to illustrate and quantitatively characterise the performance of representation learning, thanks to extracted morphometric attributes as proposed in the paper.

The final statement “Eventually this data can become a benchmark data when it is paired with a method. Then that method/data are a benchmark” is not very clear to us. Our paper introduces both a novel quantitative assessment _methodology_ and new _datasets_ for experimentation and benchmarking of representation learning methods. We provide baseline results for recent approaches, including different variants of GANs and VAEs.

In this light, we would like to reiterate that we strongly believe the reviewer’s assessment of our paper is inadequate.</span></div><div class="reply_row clearfix"></div></div><div class="children"></div></div></div></div></div></main></div>
  </div>
</div>


    <!-- Footer -->
    <footer class="sitemap">
      <div class="container">
        <div class="row hidden-xs">
          <div class="col-sm-4">
            <ul class="list-unstyled">
              <li><a href="/" class="home">Home</a></li>
              <li><a href="/venues">All Venues</a></li>
            </ul>
          </div>
    
          <div class="col-sm-4">
            <ul class="list-unstyled">
              <li><a href="/about">About OpenReview</a></li>
              <li><a href="/contact">Contact</a></li>
              <li><a href="#" data-toggle="modal" data-target="#feedback-modal">Feedback</a></li>
            </ul>
          </div>
    
          <div class="col-sm-4">
            <ul class="list-unstyled">
              <li><a href="/terms">Terms of Service</a></li>
              <li><a href="/privacy">Privacy Policy</a></li>
            </ul>
          </div>
        </div>
    
        <div class="row visible-xs-block">
          <div class="col-xs-6">
            <ul class="list-unstyled">
              <li><a href="/" class="home">Home</a></li>
              <li><a href="/about">About OpenReview</a></li>
              <li><a href="#" data-toggle="modal" data-target="#feedback-modal">Feedback</a></li>
            </ul>
          </div>
    
          <div class="col-xs-6">
            <ul class="list-unstyled">
              <li><a href="/contact">Contact</a></li>
              <li><a href="/terms">Terms of Service</a></li>
              <li><a href="/privacy">Privacy Policy</a></li>
            </ul>
          </div>
        </div>
      </div>
    </footer>
    
    <footer class="sponsor">
      <div class="container">
        <div class="row">
          <div class="col-sm-10 col-sm-offset-1">
            <p class="text-center">
              OpenReview is created by the <a href="http://www.iesl.cs.umass.edu/" target="_blank">Information Extraction and Synthesis Laboratory</a>, College of Information and Computer Science, University of Massachusetts Amherst. We gratefully acknowledge the support of the OpenReview sponsors:  Google,  Facebook, NSF, the University of Massachusetts Amherst Center for Data Science, and Center for Intelligent Information Retrieval, as well as the Google Cloud Platform for donating the computing and networking services on which OpenReview.net runs.
            </p>
          </div>
        </div>
      </div>
    </footer>

  <div id="feedback-modal" class="modal fade" tabindex="-1" role="dialog">
    <div class="modal-dialog">
      <div class="modal-content">
        <div class="modal-header">
          <button type="button" class="close" data-dismiss="modal" aria-label="Close"><span aria-hidden="true">×</span></button>
          <h3 class="modal-title">Send Feedback</h3>
        </div>
        <div class="modal-body">
          <p>Enter your feedback below and we'll get back to you as soon as possible.</p>
          <form action="/feedback" method="POST">
            <div class="form-group">
              <input type="email" name="from" class="form-control" placeholder="Email" />
            </div>
            <div class="form-group">
              <input type="text" name="subject" class="form-control" placeholder="Subject" />
            </div>
            <div class="form-group">
              <textarea name="message" class="form-control feedback-input" rows="5" placeholder="Message" required=""></textarea>
            </div>
          <ul id="ui-id-2" tabindex="0" class="ui-menu ui-widget ui-widget-content" style="display: none;"></ul></form>
        </div>
        <div class="modal-footer">
          <button type="button" class="btn btn-default" data-dismiss="modal">Cancel</button>
          <button type="button" class="btn btn-primary">Send</button>
        </div>
      </div>
    </div>
  </div>

  <script type="text/javascript" async="" src="https://www.google-analytics.com/analytics.js"></script><script src="//cdnjs.cloudflare.com/ajax/libs/jquery/2.2.4/jquery.min.js"></script>
  <script>window.jQuery || document.write('&lt;script src="/static/js/vendor/jquery-2.2.4.min.js"&gt;&lt;\/script&gt;')</script>

  <script src="/static/dist/vendor.min.js"></script>
  <script src="/static/dist/templates.min.js"></script>
  <script src="/static/dist/openreview.min.js"></script>

    <script>window.legacyScripts = true;</script>


    <script async="" src="https://www.googletagmanager.com/gtag/js?id=UA-108703919-1"></script>
    <script>
      window.dataLayer = window.dataLayer || [];
      function gtag() { dataLayer.push(arguments); }
      gtag('js', new Date());
      gtag('config', 'UA-108703919-1');
    </script>


<div role="status" aria-live="assertive" aria-relevant="additions" class="ui-helper-hidden-accessible"></div></body></html>