<!DOCTYPE html><html xmlns="http://www.w3.org/1999/xhtml" class="no-js"><head>
  <meta charset="utf-8" />
  <meta http-equiv="X-UA-Compatible" content="IE=edge,chrome=1" />
  <meta name="viewport" content="width=device-width, initial-scale=1" />

  <title>Low-Rank Matrix Factorization of LSTM as Effective Model Compression | OpenReview</title>
  <meta name="description" content="" />

      <meta name="citation_title" content="Low-Rank Matrix Factorization of LSTM as Effective Model Compression" />
        <meta name="citation_author" content="Genta Indra Winata" />
        <meta name="citation_author" content="Andrea Madotto" />
        <meta name="citation_author" content="Jamin Shin" />
        <meta name="citation_author" content="Elham J. Barezi" />
      <meta name="citation_publication_date" content="2018/09/27" />
      <meta name="citation_online_date" content="2018/09/27" />
      <meta name="citation_pdf_url" content="https://openreview.net/pdf?id=BylahsR9tX" />
      <meta name="twitter:card" content="summary" />
      <meta name="twitter:site" content="@openreviewnet" />
      <meta name="og:title" content="Low-Rank Matrix Factorization of LSTM as Effective Model Compression" />
      <meta name="og:description" content="Large-scale Long Short-Term Memory (LSTM) cells are often the building blocks of many state-of-the-art algorithms for tasks in Natural Language Processing (NLP). However, LSTMs are known to be..." />
      <meta name="og:image" content="https://openreview.net/static/images/openreview_logo_512.png" />

  <link rel="stylesheet" href="https://fonts.googleapis.com/css?family=Noto+Sans:400,400i,700,700i" />

  <link rel="stylesheet" href="/static/css/bootstrap.min.css" />
  <link rel="stylesheet" href="/static/css/jquery-ui.min.css" />
  <link rel="stylesheet" href="/static/css/main.min.css" />
</head>

<body class="forum">
  <nav class="navbar navbar-inverse navbar-fixed-top" role="navigation">
    <div class="container">
  
      <div class="navbar-header">
        <button type="button" class="navbar-toggle collapsed" data-toggle="collapse" data-target="#navbar" aria-expanded="false" aria-controls="navbar">
          <span class="sr-only">Toggle navigation</span>
          <span class="icon-bar"></span>
          <span class="icon-bar"></span>
          <span class="icon-bar"></span>
        </button>
        <a class="navbar-brand home push-link" href="/"><strong>OpenReview</strong>.net</a>
      </div>
  
      <div id="navbar" class="navbar-collapse collapse">
        <form class="navbar-form navbar-left profile-search" role="search">
          <div class="form-group has-feedback">
            <input id="search_input" type="text" class="form-control ui-autocomplete-input" placeholder="Search ICLR 2019 Conference" autocomplete="off" />
            <span class="glyphicon glyphicon-search form-control-feedback" aria-hidden="true"></span>
          </div>
  
          <input id="search_group" type="hidden" value="ICLR.cc/2019/Conference" />
          <input id="search_content" type="hidden" value="all" />
        <ul id="ui-id-1" tabindex="0" class="ui-menu ui-widget ui-widget-content ui-autocomplete ui-front" style="display: none;"></ul></form>
  
        <ul class="nav navbar-nav navbar-right">
        
            <li id="user-menu"><a href="/login">Login</a></li>
        </ul>
      </div>
  
    </div>
  </nav>

  <div id="or-banner" class="banner" style="">
  <div class="container">
    <div class="row">
      <div class="col-xs-12"><a href="/group?id=ICLR.cc/2019/Conference" title="Venue Homepage"><img class="icon" src="/static/images/arrow_left.svg" /> Go to <strong>ICLR 2019 Conference</strong> homepage</a></div>
    </div>
  </div>
</div>
<div id="flash-message-container" class="alert alert-danger" role="alert" style="display: none;">
  <div class="container">
    <div class="row">
      <div class="col-xs-12">
        <div class="alert-content">
          <button type="button" class="close" aria-label="Close"><span aria-hidden="true">×</span></button>
        </div>
      </div>
    </div>
  </div>
</div>

<div class="container">
  <div class="row">
    <div class="col-xs-12">
      <main id="content" class="clearfix openbanner-visible legacy-styles"><div id="note_BylahsR9tX" class="note panel"><div class="title_pdf_row clearfix"><h2 class="note_content_title"><a>Low-Rank Matrix Factorization of LSTM as Effective Model Compression</a> <a class="note_content_pdf" href="/pdf?id=BylahsR9tX" title="Download PDF" target="_blank"><img src="/static/images/pdf_icon_blue.svg" /></a> </h2></div><div class="meta_row"><span class="signatures"><a href="/profile?email=giwinata%40connect.ust.hk" class="profile-link" data-toggle="tooltip" data-placement="bottom" title="" data-original-title="giwinata@connect.ust.hk">Genta Indra Winata</a>, <a href="/profile?email=amadotto%40connect.ust.hk" class="profile-link" data-toggle="tooltip" data-placement="bottom" title="" data-original-title="amadotto@connect.ust.hk">Andrea Madotto</a>, <a href="/profile?email=jay.shin%40connect.ust.hk" class="profile-link" data-toggle="tooltip" data-placement="bottom" title="" data-original-title="jay.shin@connect.ust.hk">Jamin Shin</a>, <a href="/profile?email=ejs%40connect.ust.hk" class="profile-link" data-toggle="tooltip" data-placement="bottom" title="" data-original-title="ejs@connect.ust.hk">Elham J. Barezi</a></span></div><div class="clearfix"><div class="meta_row pull-left"><span class="date item">28 Sep 2018 (modified: 17 Nov 2018)</span><span class="item">ICLR 2019 Conference Withdrawn Submission</span><span class="item">Readers: <span class="readers-icon glyphicon glyphicon-globe"></span> Everyone</span><a class="note_content_pdf item" href="/revisions?id=BylahsR9tX" target="_blank">Show Revisions</a></div><div class="meta_row meta_actions"></div></div><div class="note_contents"><span class="note_content_field">Abstract: </span><span class="note_content_value">Large-scale Long Short-Term Memory (LSTM) cells are often the building blocks of many state-of-the-art algorithms for tasks in Natural Language Processing (NLP). However, LSTMs are known to be computationally inefficient because the memory capacity of the models depends on the number of parameters, and the inherent recurrence that models the temporal dependency is not parallelizable. In this paper, we propose simple, but effective, low-rank matrix factorization (MF) algorithms to compress network parameters and significantly speed up LSTMs with almost no loss of performance (and sometimes even gain). To show the effectiveness of our method across different tasks, we examine two settings: 1) compressing core LSTM layers in Language Models, 2) compressing biLSTM layers of ELMo~\citep{ELMo} and evaluate in three downstream NLP tasks (Sentiment Analysis, Textual Entailment, and Question Answering). The latter is particularly interesting as embeddings from large pre-trained biLSTM Language Models are often used as contextual word representations. Finally, we discover that matrix factorization performs better in general, additive recurrence is often more important than multiplicative recurrence, and we identify an interesting correlation between matrix norms and compression performance.

</span></div><div class="note_contents"><span class="note_content_field">Keywords: </span><span class="note_content_value">NLP, LSTM, Compression, Low Rank, Norm Analysis</span></div><div class="note_contents"><span class="note_content_field">TL;DR: </span><span class="note_content_value">We propose simple, but effective, low-rank matrix factorization (MF) algorithms to speed up in running time, save memory, and improve the performance of LSTMs.</span></div><div class="reply_row clearfix"><div class="item" id="reply_count">9 Replies</div></div></div><hr class="small" /><div id="note_children"><div class="note_with_children"><div id="note_ryxj-ub7a7" class="note panel"><div class="title_pdf_row clearfix"><h2 class="note_content_title"><a>Submission Withdrawn by the Authors</a>  <button class="btn btn-xs btn-default permalink-button" title="Link to this comment" data-permalink-url="https://openreview.net/forum?id=BylahsR9tX&amp;noteId=ryxj-ub7a7"><span class="glyphicon glyphicon-link" aria-hidden="true"></span></button></h2></div><div class="meta_row"><span class="signatures">ICLR 2019 Conference Paper753 Authors</span></div><div class="clearfix"><div class="meta_row pull-left"><span class="date item">09 Nov 2018</span><span class="item">ICLR 2019 Conference Paper753 Withdraw Submission</span><span class="item">Readers: <span class="readers-icon glyphicon glyphicon-globe"></span> Everyone</span></div><div class="meta_row meta_actions"></div></div><div class="note_contents"><span class="note_content_field">Withdrawal confirmation: </span><span class="note_content_value">I have read and agree with the withdrawal statement on behalf of myself and my co-authors.</span></div><div class="reply_row clearfix"></div></div><div class="children"></div></div><div class="note_with_children"><div id="note_S1xGr_JxaX" class="note panel"><div class="title_pdf_row clearfix"><h2 class="note_content_title"><a>Good work with extensive experiments but it seems the novelty is somewhat limited</a>  <button class="btn btn-xs btn-default permalink-button" title="Link to this comment" data-permalink-url="https://openreview.net/forum?id=BylahsR9tX&amp;noteId=S1xGr_JxaX"><span class="glyphicon glyphicon-link" aria-hidden="true"></span></button></h2></div><div class="meta_row"><span class="signatures">ICLR 2019 Conference Paper753 AnonReviewer2</span></div><div class="clearfix"><div class="meta_row pull-left"><span class="date item">07 Nov 2018</span><span class="item">ICLR 2019 Conference Paper753 Official Review</span><span class="item">Readers: <span class="readers-icon glyphicon glyphicon-globe"></span> Everyone</span></div><div class="meta_row meta_actions"></div></div><div class="note_contents"><span class="note_content_field">Review: </span><span class="note_content_value">This paper proposes to accelerate LSTM by using MF as the post-processing compression strategy. Extensive experiments are conducted to show the performance. However, there are some concerns.

-The novelty of this paper is somewhat limited. The MF methods such as SVD and NMF are directly used in LSTM without any analysis. What are the motivations? Some necessary discussions should be added. There are many MF methods ("Deep Collaborative Embedding for Social Image Understanding "). Is another one suitable?
-The research paper should propose a new idea rather than provide many results to report to readers. The current version seems a good technical report rather than a research paper.
-There are extensive experimental results. How about the running time? It is better to show some results about the running time. Besides, why not implement the method with GPU, which is widely used in the research community?</span></div><div class="note_contents"><span class="note_content_field">Rating: </span><span class="note_content_value">5: Marginally below acceptance threshold</span></div><div class="note_contents"><span class="note_content_field">Confidence: </span><span class="note_content_value">4: The reviewer is confident but not absolutely certain that the evaluation is correct</span></div><div class="reply_row clearfix"></div></div><div class="children"><div class="note_with_children"><div id="note_ByxvNzyX6X" class="note panel"><div class="title_pdf_row clearfix"><h2 class="note_content_title"><a>Re: Reviewer2</a>  <button class="btn btn-xs btn-default permalink-button" title="Link to this comment" data-permalink-url="https://openreview.net/forum?id=BylahsR9tX&amp;noteId=ByxvNzyX6X"><span class="glyphicon glyphicon-link" aria-hidden="true"></span></button></h2></div><div class="meta_row"><span class="signatures">ICLR 2019 Conference Paper753 Authors</span></div><div class="clearfix"><div class="meta_row pull-left"><span class="date item">09 Nov 2018</span><span class="item">ICLR 2019 Conference Paper753 Official Comment</span><span class="item">Readers: <span class="readers-icon glyphicon glyphicon-globe"></span> Everyone</span></div><div class="meta_row meta_actions"></div></div><div class="note_contents"><span class="note_content_field">Comment: </span><span class="note_content_value">Thank you for your review.

We agree that SVD and Semi-NMF are widely known MF methods, which is actually the point of our work. The main contribution of our work is the analysis of our proposed factorization strategy, which shows that any low-rank decomposition method can be effective in model compression with high scalability compared to more complex methods, and perform well in both Language Modeling and Extrinsic NLP tasks. We highlighted the effectiveness of the compression on addictive recurrence, multiplicative recurrence and both. It also points out the characteristics of the low-rank matrix factorization particularly on LSTM. We are aware that we are not the first that used MF methods (as pointed out by other reviewers), but most of the previous article reported results on single tasks (LM mostly) and with few comparisons. 

Regarding analysis and discussion, we have some interesting and novel matrix norm analysis in Section 5 that talk about the two different types of recurrence found in LSTMs (additive and multiplicative) and how they affected by compression. 

Our contribution to the research community is not proposing a new matrix factorization method, but rather showing that it is very useful in major NLP tasks. Instead, we provide very extensive experimentation and analysis of various compression methods on a wide array of NLP tasks (Language Modeling, Question Answering, Sentiment Analysis, and Textual Entailment) to support our claim.

Finally, regarding the running time on GPU, we wanted to express the usefulness in terms of inference time in a more practical setting which doesn't require GPUs. Without specific optimizations, we observed similar running time on GPU.</span></div><div class="reply_row clearfix"></div></div><div class="children"></div></div></div></div><div class="note_with_children"><div id="note_SyxprpUYnX" class="note panel"><div class="title_pdf_row clearfix"><h2 class="note_content_title"><a>Detailed experiments and nice work but it seems the idea is not new</a>  <button class="btn btn-xs btn-default permalink-button" title="Link to this comment" data-permalink-url="https://openreview.net/forum?id=BylahsR9tX&amp;noteId=SyxprpUYnX"><span class="glyphicon glyphicon-link" aria-hidden="true"></span></button></h2></div><div class="meta_row"><span class="signatures">ICLR 2019 Conference Paper753 AnonReviewer3</span></div><div class="clearfix"><div class="meta_row pull-left"><span class="date item">02 Nov 2018 (modified: 07 Nov 2018)</span><span class="item">ICLR 2019 Conference Paper753 Official Review</span><span class="item">Readers: <span class="readers-icon glyphicon glyphicon-globe"></span> Everyone</span></div><div class="meta_row meta_actions"></div></div><div class="note_contents"><span class="note_content_field">Review: </span><span class="note_content_value">This paper focuses on compressing pretrained LSTM models using matrix factorization methods, and have detailed analysis on compressing different parts of LSTM models.

It seems the idea is very similar to [1]. Both use matrix factorization to compress pretrained RNN models. The method is not exactly the same and the experiments are different tasks. It would be nice if it could be added as a baseline.

The LSTMP model proposed in [2] also uses matrix factorization as a method to do LSTM model compression and speedup. The difference here is that this paper focuses on compression of a pretrained model. However, when fine-tuning is used, the difference becomes less.  So I think for comparison, another baseline is needed, the LSTM model with projection layer that is trained from scratch. 

[1] Prabhavalkar, Rohit, et al. "On the compression of recurrent neural networks with an application to LVCSR acoustic modeling for embedded speech recognition." arXiv preprint arXiv:1603.08042 (2016).
[2] Sak, Haşim, Andrew Senior, and Françoise Beaufays. "Long short-term memory recurrent neural network architectures for large scale acoustic modeling." Fifteenth annual conference of the international speech communication association. 2014.
</span></div><div class="note_contents"><span class="note_content_field">Rating: </span><span class="note_content_value">5: Marginally below acceptance threshold</span></div><div class="note_contents"><span class="note_content_field">Confidence: </span><span class="note_content_value">2: The reviewer is willing to defend the evaluation, but it is quite likely that the reviewer did not understand central parts of the paper</span></div><div class="reply_row clearfix"></div></div><div class="children"><div class="note_with_children"><div id="note_SJeCL-1QTX" class="note panel"><div class="title_pdf_row clearfix"><h2 class="note_content_title"><a>Re: Reviewer3</a>  <button class="btn btn-xs btn-default permalink-button" title="Link to this comment" data-permalink-url="https://openreview.net/forum?id=BylahsR9tX&amp;noteId=SJeCL-1QTX"><span class="glyphicon glyphicon-link" aria-hidden="true"></span></button></h2></div><div class="meta_row"><span class="signatures">ICLR 2019 Conference Paper753 Authors</span></div><div class="clearfix"><div class="meta_row pull-left"><span class="date item">09 Nov 2018</span><span class="item">ICLR 2019 Conference Paper753 Official Comment</span><span class="item">Readers: <span class="readers-icon glyphicon glyphicon-globe"></span> Everyone</span></div><div class="meta_row meta_actions"></div></div><div class="note_contents"><span class="note_content_field">Comment: </span><span class="note_content_value">Thank you for your review.

We agree that SVD and Semi-NMF are widely known MF methods, which is actually the point of our work. The significance of our work is on the factorization of weight recurrences and we evaluated the factorization only on "W_i", "W_h", and both of them. Moreover, we wanted to show that any low-rank decomposition method can be effective in model compression with high scalability compared to more complex methods, and perform well in both Language Modeling and Extrinsic NLP tasks.

Our contribution to the research community is not proposing a new matrix factorization method, but rather showing the possible improvement we can make using a simple method on weight recurrences that is very useful in major NLP tasks. We provide very extensive experimentation and analysis of various compression methods on a wide array of NLP tasks (Language Modeling, Question Answering, Sentiment Analysis, and Textual Entailment) to support our claim.

Thank you for showing us more existing work. However, we could not directly compare with these work because they did not report "Perplexity" as the metric (they reported "Word Error Rate (WER)"). We will include these papers in related works. </span></div><div class="reply_row clearfix"></div></div><div class="children"></div></div></div></div><div class="note_with_children"><div id="note_HylTAeS8hm" class="note panel"><div class="title_pdf_row clearfix"><h2 class="note_content_title"><a>Good technical report of applying decomposition methods to weights of LSTM. But lack of originality and significance. </a>  <button class="btn btn-xs btn-default permalink-button" title="Link to this comment" data-permalink-url="https://openreview.net/forum?id=BylahsR9tX&amp;noteId=HylTAeS8hm"><span class="glyphicon glyphicon-link" aria-hidden="true"></span></button></h2></div><div class="meta_row"><span class="signatures">ICLR 2019 Conference Paper753 AnonReviewer1</span></div><div class="clearfix"><div class="meta_row pull-left"><span class="date item">31 Oct 2018 (modified: 07 Nov 2018)</span><span class="item">ICLR 2019 Conference Paper753 Official Review</span><span class="item">Readers: <span class="readers-icon glyphicon glyphicon-globe"></span> Everyone</span></div><div class="meta_row meta_actions"></div></div><div class="note_contents"><span class="note_content_field">Review: </span><span class="note_content_value">
[PROS]

[quality]

The paper did extensive experimental studies on compressing weights of LSTM with two matrix decomposition methods (SVD and NMF). Their experiments cover three NLP downstream tasks and two benchmark datasets, and performed the decomposition methods on state-of-the-art language models. They explored effects of different methods on different weights of LSTM, and also examined how the results are influenced by different norms (used in the objective function of decomposition method). The experimental studies are very solid and convincing---they can well support their claims. 

[clarity]

The paper is clearly-written and has a very detailed appendix.  

[CONS]

[originality]

One major weakness of this paper is its lack of originality. The paper directly applied two well-known and popularly used methods---SVD and NMF---to the weights of LSTM, but did not provide any insights on the fitness of these methods. For example, what the non-negativity means in the case of LSTM weights? 

Moreover, why LSTM? The way of applying these methods in this paper has nothing to do with LSTM’s unique properties that distinguish it from other neural models. None of the experimental findings seem tied to any of LSTM’s properties. We can always expect a speed-up in runtime but a drop in performance after applying these methods to another neural model. As long as we still maintain certain rank, the performance will not drop much. The effect of using different norms seems only dependent on the properties of the norms and the matrices, but not anything related to the structure of a LSTM. 

[significance]

As mentioned in [quality], the experiments are extensive and results are convincing. But the results are largely predictable as discussed in [originality], and not necessarily tied to LSTM. That being said, this submission is certainly a really high-quality technical report, but does not seem significant enough to be accepted to ICLR. 

[questions for authors]

Why are experiments only performed on CPUs but not GPUs? I understand that the time complexity does not depend on the device, but the actual runtime does---because of massive cores in GPUs. So when people apply large RNNs like ELMO, they would prefer using GPUs. So do methods proposed in this work also provide any edge in that case? </span></div><div class="note_contents"><span class="note_content_field">Rating: </span><span class="note_content_value">4: Ok but not good enough - rejection</span></div><div class="note_contents"><span class="note_content_field">Confidence: </span><span class="note_content_value">4: The reviewer is confident but not absolutely certain that the evaluation is correct</span></div><div class="reply_row clearfix"></div></div><div class="children"><div class="note_with_children"><div id="note_B1eoj4Jmp7" class="note panel"><div class="title_pdf_row clearfix"><h2 class="note_content_title"><a>Re: Reviewer1</a>  <button class="btn btn-xs btn-default permalink-button" title="Link to this comment" data-permalink-url="https://openreview.net/forum?id=BylahsR9tX&amp;noteId=B1eoj4Jmp7"><span class="glyphicon glyphicon-link" aria-hidden="true"></span></button></h2></div><div class="meta_row"><span class="signatures">ICLR 2019 Conference Paper753 Authors</span></div><div class="clearfix"><div class="meta_row pull-left"><span class="date item">09 Nov 2018</span><span class="item">ICLR 2019 Conference Paper753 Official Comment</span><span class="item">Readers: <span class="readers-icon glyphicon glyphicon-globe"></span> Everyone</span></div><div class="meta_row meta_actions"></div></div><div class="note_contents"><span class="note_content_field">Comment: </span><span class="note_content_value">Thank you for your review.

We agree that SVD and Semi-NMF are widely known MF methods, which is actually the point of our work. The motivation of our work is to show that any low-rank decomposition method can be effective in model compression with high scalability compared to more complex methods, and perform well in both Language Modeling and Extrinsic NLP tasks. We also propose to analyze the effectiveness of compression in additive and multiplicative recurrences, which improves our understanding of LSTM. We are aware that we are not the first that used such matrix factorization method such as "Semi-NMF" (as pointed out by other reviewers), but most of the previous article reported results on single tasks (LM mostly) and they didn't apply factorization to particular recurrences and provide extensive discussion.

Regarding "Why LSTM," most of the compression algorithms were applied on Feedforward Neural Networks and CNNs, while LSTMs are relatively unexplored, and LSTMs are widely adopted in the NLP community as the building block of models (ELMo).  About the "unique properties of LSTMS", we agree that our method is not specifically "tied" to LSTMs, but we were interested in showing how MF methods perform in RNNs. We actually do experiment such by separately factorizing "W_i" and "W_h" (additive and multiplicative recurrences) in order to compare and analyze the effect of these properties in LSTMs. 

Finally, regarding the running time on GPU, we wanted to express the usefulness in terms of inference time in a more practical setting which doesn't require GPUs. Without specific optimizations, we observed similar running time on GPU.</span></div><div class="reply_row clearfix"></div></div><div class="children"></div></div></div></div><div class="note_with_children"><div id="note_H1ePyfkoiX" class="note panel"><div class="title_pdf_row clearfix"><h2 class="note_content_title"><a>Possibly Relevant Work</a>  <button class="btn btn-xs btn-default permalink-button" title="Link to this comment" data-permalink-url="https://openreview.net/forum?id=BylahsR9tX&amp;noteId=H1ePyfkoiX"><span class="glyphicon glyphicon-link" aria-hidden="true"></span></button></h2></div><div class="meta_row"><span class="signatures">(anonymous)</span></div><div class="clearfix"><div class="meta_row pull-left"><span class="date item">22 Oct 2018</span><span class="item">ICLR 2019 Conference Paper753 Public Comment</span><span class="item">Readers: <span class="readers-icon glyphicon glyphicon-globe"></span> Everyone</span></div><div class="meta_row meta_actions"></div></div><div class="note_contents"><span class="note_content_field">Comment: </span><span class="note_content_value">There is a work from this year's ICML [1], they also tried to compress the LSTM trained on PTB language modeling task. Maybe you can mention their results in the paper?

[1] Towards Binary-Valued Gates for Robust LSTM Training, <a href="https://arxiv.org/abs/1806.02988," target="_blank" rel="nofollow">https://arxiv.org/abs/1806.02988,</a> ICML 2018</span></div><div class="reply_row clearfix"></div></div><div class="children"><div class="note_with_children"><div id="note_Hylqtj1no7" class="note panel"><div class="title_pdf_row clearfix"><h2 class="note_content_title"><a>Re: Possibly Relevant Work</a>  <button class="btn btn-xs btn-default permalink-button" title="Link to this comment" data-permalink-url="https://openreview.net/forum?id=BylahsR9tX&amp;noteId=Hylqtj1no7"><span class="glyphicon glyphicon-link" aria-hidden="true"></span></button></h2></div><div class="meta_row"><span class="signatures">ICLR 2019 Conference Paper753 Authors</span></div><div class="clearfix"><div class="meta_row pull-left"><span class="date item">23 Oct 2018</span><span class="item">ICLR 2019 Conference Paper753 Official Comment</span><span class="item">Readers: <span class="readers-icon glyphicon glyphicon-globe"></span> Everyone</span></div><div class="meta_row meta_actions"></div></div><div class="note_contents"><span class="note_content_field">Comment: </span><span class="note_content_value">Thank you for the comment and for letting us know, this work is pretty recent. We are going to include the results in the next version.</span></div><div class="reply_row clearfix"></div></div><div class="children"></div></div></div></div></div></main></div>
  </div>
</div>


    <!-- Footer -->
    <footer class="sitemap">
      <div class="container">
        <div class="row hidden-xs">
          <div class="col-sm-4">
            <ul class="list-unstyled">
              <li><a href="/" class="home">Home</a></li>
              <li><a href="/venues">All Venues</a></li>
            </ul>
          </div>
    
          <div class="col-sm-4">
            <ul class="list-unstyled">
              <li><a href="/about">About OpenReview</a></li>
              <li><a href="/contact">Contact</a></li>
              <li><a href="#" data-toggle="modal" data-target="#feedback-modal">Feedback</a></li>
            </ul>
          </div>
    
          <div class="col-sm-4">
            <ul class="list-unstyled">
              <li><a href="/terms">Terms of Service</a></li>
              <li><a href="/privacy">Privacy Policy</a></li>
            </ul>
          </div>
        </div>
    
        <div class="row visible-xs-block">
          <div class="col-xs-6">
            <ul class="list-unstyled">
              <li><a href="/" class="home">Home</a></li>
              <li><a href="/about">About OpenReview</a></li>
              <li><a href="#" data-toggle="modal" data-target="#feedback-modal">Feedback</a></li>
            </ul>
          </div>
    
          <div class="col-xs-6">
            <ul class="list-unstyled">
              <li><a href="/contact">Contact</a></li>
              <li><a href="/terms">Terms of Service</a></li>
              <li><a href="/privacy">Privacy Policy</a></li>
            </ul>
          </div>
        </div>
      </div>
    </footer>
    
    <footer class="sponsor">
      <div class="container">
        <div class="row">
          <div class="col-sm-10 col-sm-offset-1">
            <p class="text-center">
              OpenReview is created by the <a href="http://www.iesl.cs.umass.edu/" target="_blank">Information Extraction and Synthesis Laboratory</a>, College of Information and Computer Science, University of Massachusetts Amherst. We gratefully acknowledge the support of the OpenReview sponsors:  Google,  Facebook, NSF, the University of Massachusetts Amherst Center for Data Science, and Center for Intelligent Information Retrieval, as well as the Google Cloud Platform for donating the computing and networking services on which OpenReview.net runs.
            </p>
          </div>
        </div>
      </div>
    </footer>

  <div id="feedback-modal" class="modal fade" tabindex="-1" role="dialog">
    <div class="modal-dialog">
      <div class="modal-content">
        <div class="modal-header">
          <button type="button" class="close" data-dismiss="modal" aria-label="Close"><span aria-hidden="true">×</span></button>
          <h3 class="modal-title">Send Feedback</h3>
        </div>
        <div class="modal-body">
          <p>Enter your feedback below and we'll get back to you as soon as possible.</p>
          <form action="/feedback" method="POST">
            <div class="form-group">
              <input type="email" name="from" class="form-control" placeholder="Email" />
            </div>
            <div class="form-group">
              <input type="text" name="subject" class="form-control" placeholder="Subject" />
            </div>
            <div class="form-group">
              <textarea name="message" class="form-control feedback-input" rows="5" placeholder="Message" required=""></textarea>
            </div>
          <ul id="ui-id-2" tabindex="0" class="ui-menu ui-widget ui-widget-content" style="display: none;"></ul></form>
        </div>
        <div class="modal-footer">
          <button type="button" class="btn btn-default" data-dismiss="modal">Cancel</button>
          <button type="button" class="btn btn-primary">Send</button>
        </div>
      </div>
    </div>
  </div>

  <script type="text/javascript" async="" src="https://www.google-analytics.com/analytics.js"></script><script src="//cdnjs.cloudflare.com/ajax/libs/jquery/2.2.4/jquery.min.js"></script>
  <script>window.jQuery || document.write('&lt;script src="/static/js/vendor/jquery-2.2.4.min.js"&gt;&lt;\/script&gt;')</script>

  <script src="/static/dist/vendor.min.js"></script>
  <script src="/static/dist/templates.min.js"></script>
  <script src="/static/dist/openreview.min.js"></script>

    <script>window.legacyScripts = true;</script>


    <script async="" src="https://www.googletagmanager.com/gtag/js?id=UA-108703919-1"></script>
    <script>
      window.dataLayer = window.dataLayer || [];
      function gtag() { dataLayer.push(arguments); }
      gtag('js', new Date());
      gtag('config', 'UA-108703919-1');
    </script>


<div role="status" aria-live="assertive" aria-relevant="additions" class="ui-helper-hidden-accessible"></div></body></html>