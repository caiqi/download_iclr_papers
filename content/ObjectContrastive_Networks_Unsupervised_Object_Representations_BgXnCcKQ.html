<!DOCTYPE html><html xmlns="http://www.w3.org/1999/xhtml" class="no-js"><head>
  <meta charset="utf-8" />
  <meta http-equiv="X-UA-Compatible" content="IE=edge,chrome=1" />
  <meta name="viewport" content="width=device-width, initial-scale=1" />

  <title>Object-Contrastive Networks: Unsupervised Object Representations | OpenReview</title>
  <meta name="description" content="" />

      <meta name="citation_title" content="Object-Contrastive Networks: Unsupervised Object Representations" />
        <meta name="citation_author" content="Soeren Pirk" />
        <meta name="citation_author" content="Mohi Khansari" />
        <meta name="citation_author" content="Yunfei Bai" />
        <meta name="citation_author" content="Corey Lynch" />
        <meta name="citation_author" content="Pierre Sermanet" />
      <meta name="citation_publication_date" content="2018/09/27" />
      <meta name="citation_online_date" content="2018/09/27" />
      <meta name="citation_pdf_url" content="https://openreview.net/pdf?id=B1g6XnCcKQ" />
      <meta name="twitter:card" content="summary" />
      <meta name="twitter:site" content="@openreviewnet" />
      <meta name="og:title" content="Object-Contrastive Networks: Unsupervised Object Representations" />
      <meta name="og:description" content="Discovering objects and their attributes is of great importance for autonomous agents to effectively operate in human environments. This task is particularly challenging due to the ubiquitousness..." />
      <meta name="og:image" content="https://openreview.net/static/images/openreview_logo_512.png" />

  <link rel="stylesheet" href="https://fonts.googleapis.com/css?family=Noto+Sans:400,400i,700,700i" />

  <link rel="stylesheet" href="/static/css/bootstrap.min.css" />
  <link rel="stylesheet" href="/static/css/jquery-ui.min.css" />
  <link rel="stylesheet" href="/static/css/main.min.css" />
</head>

<body class="forum">
  <nav class="navbar navbar-inverse navbar-fixed-top" role="navigation">
    <div class="container">
  
      <div class="navbar-header">
        <button type="button" class="navbar-toggle collapsed" data-toggle="collapse" data-target="#navbar" aria-expanded="false" aria-controls="navbar">
          <span class="sr-only">Toggle navigation</span>
          <span class="icon-bar"></span>
          <span class="icon-bar"></span>
          <span class="icon-bar"></span>
        </button>
        <a class="navbar-brand home push-link" href="/"><strong>OpenReview</strong>.net</a>
      </div>
  
      <div id="navbar" class="navbar-collapse collapse">
        <form class="navbar-form navbar-left profile-search" role="search">
          <div class="form-group has-feedback">
            <input id="search_input" type="text" class="form-control ui-autocomplete-input" placeholder="Search ICLR 2019 Conference" autocomplete="off" />
            <span class="glyphicon glyphicon-search form-control-feedback" aria-hidden="true"></span>
          </div>
  
          <input id="search_group" type="hidden" value="ICLR.cc/2019/Conference" />
          <input id="search_content" type="hidden" value="all" />
        <ul id="ui-id-1" tabindex="0" class="ui-menu ui-widget ui-widget-content ui-autocomplete ui-front" style="display: none;"></ul></form>
  
        <ul class="nav navbar-nav navbar-right">
        
            <li id="user-menu"><a href="/login">Login</a></li>
        </ul>
      </div>
  
    </div>
  </nav>

  <div id="or-banner" class="banner" style="">
  <div class="container">
    <div class="row">
      <div class="col-xs-12"><a href="/group?id=ICLR.cc/2019/Conference" title="Venue Homepage"><img class="icon" src="/static/images/arrow_left.svg" /> Go to <strong>ICLR 2019 Conference</strong> homepage</a></div>
    </div>
  </div>
</div>
<div id="flash-message-container" class="alert alert-danger" role="alert" style="display: none;">
  <div class="container">
    <div class="row">
      <div class="col-xs-12">
        <div class="alert-content">
          <button type="button" class="close" aria-label="Close"><span aria-hidden="true">×</span></button>
        </div>
      </div>
    </div>
  </div>
</div>

<div class="container">
  <div class="row">
    <div class="col-xs-12">
      <main id="content" class="clearfix openbanner-visible legacy-styles"><div id="note_B1g6XnCcKQ" class="note panel"><div class="title_pdf_row clearfix"><h2 class="note_content_title"><a>Object-Contrastive Networks: Unsupervised Object Representations</a> <a class="note_content_pdf" href="/pdf?id=B1g6XnCcKQ" title="Download PDF" target="_blank"><img src="/static/images/pdf_icon_blue.svg" /></a> </h2></div><div class="meta_row"><span class="signatures"><a href="/profile?email=pirk%40google.com" class="profile-link" data-toggle="tooltip" data-placement="bottom" title="" data-original-title="pirk@google.com">Soeren Pirk</a>, <a href="/profile?email=khansari%40google.com" class="profile-link" data-toggle="tooltip" data-placement="bottom" title="" data-original-title="khansari@google.com">Mohi Khansari</a>, <a href="/profile?email=yunfeibai%40google.com" class="profile-link" data-toggle="tooltip" data-placement="bottom" title="" data-original-title="yunfeibai@google.com">Yunfei Bai</a>, <a href="/profile?email=coreylynch%40google.com" class="profile-link" data-toggle="tooltip" data-placement="bottom" title="" data-original-title="coreylynch@google.com">Corey Lynch</a>, <a href="/profile?email=sermanet%40google.com" class="profile-link" data-toggle="tooltip" data-placement="bottom" title="" data-original-title="sermanet@google.com">Pierre Sermanet</a></span></div><div class="clearfix"><div class="meta_row pull-left"><span class="date item">28 Sep 2018 (modified: 17 Nov 2018)</span><span class="item">ICLR 2019 Conference Withdrawn Submission</span><span class="item">Readers: <span class="readers-icon glyphicon glyphicon-globe"></span> Everyone</span></div><div class="meta_row meta_actions"></div></div><div class="note_contents"><span class="note_content_field">Abstract: </span><span class="note_content_value">Discovering objects and their attributes is of great importance for autonomous agents to effectively operate in human environments. This task is particularly challenging due to the ubiquitousness of objects and all their nuances in perceptual and semantic detail. In this paper we present an unsupervised approach for learning disentangled representations of objects entirely from unlabeled monocular videos. These continuous representations are not biased by or limited by a discrete set of labels determined by human labelers. The proposed representation is trained with a metric learning loss, where objects with homogeneous features are pushed together, while those with heterogeneous features are pulled apart. We show these unsupervised embeddings allow to discover object attributes and can enable robots to self-supervise in previously unseen environments. We quantitatively evaluate performance on a large-scale synthetic dataset with 12k object models, as well as on a real dataset collected by a robot and show that our unsupervised object understanding generalizes to previously unseen objects. Specifically, we demonstrate the effectiveness of our approach on robotic manipulation tasks, such as pointing at and grasping of objects. An interesting and perhaps surprising finding in this approach is that given a limited set of objects, object correspondences will naturally emerge when using metric learning without requiring explicit positive pairs.</span></div><div class="note_contents"><span class="note_content_field">Keywords: </span><span class="note_content_value">self-supervised robotics, object understanding, object representations, metric learning, unsupervised vision</span></div><div class="note_contents"><span class="note_content_field">TL;DR: </span><span class="note_content_value">An unsupervised approach for learning disentangled representations of objects entirely from unlabeled monocular videos.</span></div><div class="reply_row clearfix"><div class="item" id="reply_count">4 Replies</div></div></div><hr class="small" /><div id="note_children"><div class="note_with_children"><div id="note_BJxX7Msi6X" class="note panel"><div class="title_pdf_row clearfix"><h2 class="note_content_title"><a>Submission Withdrawn by the Authors</a>  <button class="btn btn-xs btn-default permalink-button" title="Link to this comment" data-permalink-url="https://openreview.net/forum?id=B1g6XnCcKQ&amp;noteId=BJxX7Msi6X"><span class="glyphicon glyphicon-link" aria-hidden="true"></span></button></h2></div><div class="meta_row"><span class="signatures">ICLR 2019 Conference Paper1400 Authors</span></div><div class="clearfix"><div class="meta_row pull-left"><span class="date item">16 Nov 2018</span><span class="item">ICLR 2019 Conference Paper1400 Withdraw Submission</span><span class="item">Readers: <span class="readers-icon glyphicon glyphicon-globe"></span> Everyone</span></div><div class="meta_row meta_actions"></div></div><div class="note_contents"><span class="note_content_field">Withdrawal confirmation: </span><span class="note_content_value">I have read and agree with the withdrawal statement on behalf of myself and my co-authors.</span></div><div class="reply_row clearfix"></div></div><div class="children"></div></div><div class="note_with_children"><div id="note_rygGx5rT37" class="note panel"><div class="title_pdf_row clearfix"><h2 class="note_content_title"><a>Simplistic experimental setup, no technical novelty, missing baselines and experimental details</a>  <button class="btn btn-xs btn-default permalink-button" title="Link to this comment" data-permalink-url="https://openreview.net/forum?id=B1g6XnCcKQ&amp;noteId=rygGx5rT37"><span class="glyphicon glyphicon-link" aria-hidden="true"></span></button></h2></div><div class="meta_row"><span class="signatures">ICLR 2019 Conference Paper1400 AnonReviewer2</span></div><div class="clearfix"><div class="meta_row pull-left"><span class="date item">05 Nov 2018 (modified: 07 Nov 2018)</span><span class="item">ICLR 2019 Conference Paper1400 Official Review</span><span class="item">Readers: <span class="readers-icon glyphicon glyphicon-globe"></span> Everyone</span></div><div class="meta_row meta_actions"></div></div><div class="note_contents"><span class="note_content_field">Review: </span><span class="note_content_value">Summary:
This paper aim for learning a feature representation from video sequences captured from a scene from different view points. The proposed approach is tested on a table top scenario for synthetic and real scenes. Pairs of frames from captured video is selected, then a pre-trained object detector finds the object proposal bounding boxes. The positive pairs are found using nearest neighbor between cropped bounding boxed from two random frames and finally a network is trained using an n-pair contrastive loss function and hence called object-contrastive network.

Pros: Unsupervised feature learning is an interesting area in computer vision and ML and this paper tries to tackle this problem for objects seen from different viewpoints. 
 
Cons:
-Not enough technical novelty compared to current unsupervised feature learning approaches. The proposed approach uses two random frame from a sequence and use nearest neighbor match based on some pre-trained network and compute an n-pair contrastive loss of Sohn 2016 on top. 

-Experimental set up for the real experiment is very simplistic and objects with similar appearance and colors are appearing in both train and test sets which is far from random selection of object instances and categories into test and train (plates, bowls and cups with similar colors and similar shapes).
Why the proposed method is not trained and tested on tasks similar to [a]? There can be similar setup in training videos of [a] and tested on object detection task on videos of natural scenes (rather than a particular indoor table top scenario). [a] is a relevant baseline which is missed.
[a] Xiaolong Wang and Abhinav Gupta. Unsupervised learning of visual representations using videos. In ICCV, 2015. 

Missing Baselines:
-Comparing against learned embedding feature with feature trained on (a) ResNet50 pre-trained ImageNet or (b) ResNet50 pre-trained COCO for both NN and linear setup is missed. There is only ResNet50 embedding pre-trained on ImageNet shown in table 1.
-Comparing against previous self-supervised methods that use tracking is missed.
-Comparing against previous methods that learn embedding on delta time and/or camera location is missed.

Issues in experimental setups:

-Section 5.2 with title “Instance Detection and Tracking” only shows three qualitative example if instance retrieval and ranking for a pair of views. There is no standard quantitative result for instance tracking in this section such accuracy of trajectory over time. Also the detail of experimental setup for table 2 is missing. Number of instances, pairs, real or synthetic, etc.

-The object appearance is not similar from different view. In the current experimental setup (which is less than 90 degrees different viewpoint) the appearance can be similar. It is not clear if the proposed approach can work with more variation of camera viewpoint.

-There are many hand designed assumptions in the experimental setup which makes it unnatural in real scenario. For instance, the number of objects in all frames are approximately equal and all objects are visible in all frames. In real scenario the objects can appear and disappear from the camera viewpoint based on camera field of view and can can cause drastic changes in the nearest neighbor set up in the method formulation. What happen if in extreme case there is no object in one of the frames when wants to find the pairs? It can match with some random patches then? 

-In Page 5, section 4.1, it is mentioned “We randomly define the number of objects (up to 20) in a scene and select half of the objects from two randomly selected categories. The other half is selected from the remaining object categories.”. What is the logistic behind this choice? The reason for this setup is not explained in the paper.

-Throughout the paper the words “attribute”, “class”, “semantic”, “label” are used in a confusing manner based on the current literature. For example, “…naturally encoding attributes like class, color, texture and function…” in Introduction section. Class is not an object attribute.
</span></div><div class="note_contents"><span class="note_content_field">Rating: </span><span class="note_content_value">3: Clear rejection</span></div><div class="note_contents"><span class="note_content_field">Confidence: </span><span class="note_content_value">5: The reviewer is absolutely certain that the evaluation is correct and very familiar with the relevant literature</span></div><div class="reply_row clearfix"></div></div><div class="children"></div></div><div class="note_with_children"><div id="note_BygcBES6nQ" class="note panel"><div class="title_pdf_row clearfix"><h2 class="note_content_title"><a>Technically not novel, experimentally weak, unsupported arguments.</a>  <button class="btn btn-xs btn-default permalink-button" title="Link to this comment" data-permalink-url="https://openreview.net/forum?id=B1g6XnCcKQ&amp;noteId=BygcBES6nQ"><span class="glyphicon glyphicon-link" aria-hidden="true"></span></button></h2></div><div class="meta_row"><span class="signatures">ICLR 2019 Conference Paper1400 AnonReviewer1</span></div><div class="clearfix"><div class="meta_row pull-left"><span class="date item">05 Nov 2018 (modified: 07 Nov 2018)</span><span class="item">ICLR 2019 Conference Paper1400 Official Review</span><span class="item">Readers: <span class="readers-icon glyphicon glyphicon-globe"></span> Everyone</span></div><div class="meta_row meta_actions"></div></div><div class="note_contents"><span class="note_content_field">Review: </span><span class="note_content_value">In this paper, an unsupervised representation learning method for visual inputs is proposed. The proposed method incorporates a metric learning approach that pulls nearest neighbor pairs of image patches close to each other in the embedding space while pushing apart other pairs. The train and test scenarios are captured based on a table top scenario with multiple objects of different colors such as cups and bowls. Standard datasets are not used for benchmarking. 

Pros:
- Unsupervised feature learning is an active and interesting area of research.
- The proposed method and network architecture is simple.

Cons:

-The paper does not present technical novelty. The proposed network architecture is a ResNet50 that uses the object proposals obtained by Faster RCNN (Ren et al. 2015) and incorporates the n-pair loss function proposed by Sohn 2016. 

-Technical details are missing. 

&gt;When using object proposals obtained by Faster RCNN (Ren et al. 2015) to generate pairs, do you use all of the object proposals or do you select a subset of them by some threshold? 
&gt;How is the robotic control performed in the robot grasping and pointing tasks? It seems that grasping and pointing tasks are not learned and are based on conventional motion planning and state estimation. These details are not included in the manuscript.
&gt;Section 4.2 mentions that a mobile robot was needed for collecting the data. What kind of mobile robot was used? Why was a mobile robot needed? There is no details about the robot platform in the manuscript and no visualization is provided for the robot setup.
&gt;Why is a ResNet pretrained with ImageNet is used throughout the experiments (and not pretrained with COCO?) while object proposals are obtained by Faster RCNN which is pretrained by COCO. How would that affect the results? 

-The paper uses imprecise, confusing and sometimes incorrect phrases and wordings throughout the draft. Here are a few examples:

&gt; It is unclear what it is exactly meant by “object correspondences” that naturally emerge (e.g. in the last four lines of abstract). Would a correct object correspondence refer to “similar instances”, “similar colors”, “similar object categories”, “similar object poses”, “similar functionality”? For example, the first column of Fig. 1 shows an example of two cups with “similar green color”, “similar tall shapes” and “similar functionality” and “similar background” that are considered as “negative pairs (with red arrows)” while in the same Fig 1. Two cups with drastically different appearances one of which is “yellow with handle” (second column) and the another is “green without handle” (third column) are considered to be positive pairs (blue arrows). Similar confusing examples and arguments can repeatedly be found in the experiments and embedding visualizations:  Fig. 4, Fig.5, Fig. 10- Fig. 15. 

&gt; Throughout the draft and in Figures (e.g. Fig. 1) it is emphasized the the data collection is done by “robot” or it is “robotic”. Why was a robot needed to capture images or videos of around a table? A hand held moving camera could also be simply used.  Capturing images and videos around a space is a very well-known and simple practice done in many previous works in computer vision and machine learning. It does not need a robot and it is not robotic. 

&gt; First sentence of the second paragraph of the introduction is not a technically correct statement based on the well-known computer vision literature. Specifically, “class” is not considered as an “attribute”.  What does it mean to disentangle “perceptual” and “semantic” object attributes? This is a very vague statement. “color” is neither a “semantic” attribute nor a “perceptual” attribute. Also, in section 3.2 and Fig. 2, it is confusing to consider “class” label as an attribute. Please refer to the well-known prior work of “Farhadi A, Endres I, Hoiem D, Forsyth D. Describing objects by their attributes. In Computer Vision and Pattern Recognition, 2009. CVPR 2009. IEEE Conference on 2009 Jun 20 (pp. 1778-1785). IEEE.” for a clear and well-known explanation of object attributes vs categories. If your definition of “attributes” is technically different than that of this aforementioned prior work, it should be clarified to avoid confusion. 

-Unconvincing usefulness for robotics applications: The paper explains that the proposed method is effective and useful for robotic tasks however tis claim is not backed up with convincing robotic tasks or scenarios. Also, the performance in the robotic grasping and pointing tasks are not compared with any baselines.

&gt; While usefulness of the method for improving robotic grasping task is listed in the contributions (number 3 in the last paragraph of Section 1) there are only two qualitative grasping scenarios shown in the paper. It is not explained how the robot control is done. In both grasping scenarios the target object is the one in the middle. It seems that the grasping control policies are not learned and are motion planned or are a scripted policy and only the visual matching of the target object with the objects of the scene are done with the proposed method. For evaluating such scenario no robot is needed and only similarity score could be reported. It would had been interesting to see how representation learning can be seamlessly incorporated for robotic grasping which involves control as a tangible contribution however, the presented scenario does is not contributing to that problem and is only doing visual matching. No baseline comparison is provided here.

&gt; The details of the robot control for the robot pointing scenario is also not provided. The objects are places on two rows in all scenarios and it looks like the the actual pointing is done by motion planning after the visual matching step done by the proposed method. The presented method in this paper tries to find the closest visual match to the target object amongst the objects on the table and does not integrate it with any policy learning for improving the “robotic pointing task” so there is no robotic contribution here. No baseline is provided performance comparison in this robotic task as well.  

- The experimental results are weak and unconvincing:

&gt; In the experiments corresponding to Table 6 and Table 1, what is the performance of ResNet 50 embedding (linear)? Can you provide these results?

&gt; Table 6 shows that the performance gain of the proposed method compared to the supervised and unsupervised baselines is marginal. 

&gt; What is the performance of a baseline network with similar architecture that uses “contrastive loss” (based on object pairs with similar attributes). This baseline is missed.

&gt; Qualitatively, the visualizations in Fig. 1, Fig. 4-5, Fig. 10-15 show that OCN embedding is noisy. In all these figures, there are many less similar instances that are retrieved on the top and many similar instances that are ranked down in the list.    

&gt; The paper refers to ad-hoc experimental protocols which makes the evaluations unclear and invalid: what is meant to report “class” and “container” attributes in Table 3 and section 5.3? Why are “bottles and cans” are considered in a same group while there were referred to as different objects in all previous explanations of the object types and categories used in the training and testing? What is the difference between “cups and Mugs” and “glasses” that are separated? How are “Balls” , “Bowls”, “Plates”, etc listed as *attributes*?  Aren’t these previously referred to as object categories? 

&gt; The paper has some inconsistent experimental setups. Why in Fig 16, the same instances of were removed for the real objects and not for the synthetic objects?


-The presentation of the paper can be improved. Here are a few examples:

&gt;Grammatical or unclear wordings and typos (e.g. Caption of Fig. 2 “attracting embedding nearest neighbors”; is not a readable. Repetitive words, Last line of section 3. , etc)
&gt;In Fig. 10-15, using t-SNE (Maaten LV, Hinton G. Visualizing data using t-SNE. Journal of machine learning research. 2008;9(Nov):2579-605.) instead of a sorted list provides much more expressive visualization of the effectiveness of the approach. It is highly recommended that t-SNE be used instead of sorted lists if image patches.

</span></div><div class="note_contents"><span class="note_content_field">Rating: </span><span class="note_content_value">3: Clear rejection</span></div><div class="note_contents"><span class="note_content_field">Confidence: </span><span class="note_content_value">5: The reviewer is absolutely certain that the evaluation is correct and very familiar with the relevant literature</span></div><div class="reply_row clearfix"></div></div><div class="children"></div></div><div class="note_with_children"><div id="note_Ske0gl6o27" class="note panel"><div class="title_pdf_row clearfix"><h2 class="note_content_title"><a>An interesting idea in an unnatural setup</a>  <button class="btn btn-xs btn-default permalink-button" title="Link to this comment" data-permalink-url="https://openreview.net/forum?id=B1g6XnCcKQ&amp;noteId=Ske0gl6o27"><span class="glyphicon glyphicon-link" aria-hidden="true"></span></button></h2></div><div class="meta_row"><span class="signatures">ICLR 2019 Conference Paper1400 AnonReviewer3</span></div><div class="clearfix"><div class="meta_row pull-left"><span class="date item">04 Nov 2018 (modified: 07 Nov 2018)</span><span class="item">ICLR 2019 Conference Paper1400 Official Review</span><span class="item">Readers: <span class="readers-icon glyphicon glyphicon-globe"></span> Everyone</span></div><div class="meta_row meta_actions"></div></div><div class="note_contents"><span class="note_content_field">Review: </span><span class="note_content_value">This paper explored self-supervised learning of object representations. The main idea is to encourage objects with similar features to get further ‘attracted’ to each other. The authors demonstrated that the system works on real objects with simple geometry. 

The problem of self-supervised learning from video data is an important one. This paper is making progress along this direction. The approach is new. The paper is in general well written and clear. 

My main concern is the problem setup is unnatural. I can imagine two types of approaches with video input: object-based or pixel-based. An object-based approach detects and tracks objects over time, and then learns object representations upon correspondence. Tracking objects is hard, but gives object correspondence as the basis for learning. A pixel-based approach does not detect objects, but learns dense feature representations for each pixel via signals such as optical flow. Here, training signals become noisier, but the algorithm no longer suffers from the errors that may arise in object detection. It can also generalize to objects that are hard to be detected, such as soft bodies and liquids.

The proposed system however lies in an uncanny valley: it performs object detection per frame (which is hard and noisy), but it then discards the temporal signals in the video and therefore loses object correspondence. To compensate that, the authors proposed a sometimes incorrect heuristic (based on nearest neighbors) as supervision. The motivation behind such design is unclear, and I’d love to hear the authors’ feedback on it. 

The authors should also cite and discuss those pixel-based methods (see below).

The experimental results are neat, but not too impressive. The objects used are all rigid with simple geometry. As said before, such an approach would have a hard time generalize to deformable objects and liquids. The results on the real robot is not very convincing, as the system doesn’t really learn object representations that can be used for manipulation. For example, for the results in Fig 7, I assume the way of grasping is handcrafted, instead of learned by the network. Please let me know if I’m wrong.

In general, I feel this paper interesting but not exciting, and it’s unclear what we can really learn from it. I’m therefore on the border, leaning slightly toward rejection. I’m happy to adjust my rating based on the discussion and revision.

Related work

Self-supervised Visual Descriptor Learning for Dense Correspondence. ICRA 17.
Dense Object Nets: Learning Dense Visual Object Descriptors By and For Robotic Manipulation. CORL 18 (concurrent work, though on arxiv since June).
Unsupervised learning of object frames by dense equivariant image labelling. NIPS 17.
</span></div><div class="note_contents"><span class="note_content_field">Rating: </span><span class="note_content_value">5: Marginally below acceptance threshold</span></div><div class="note_contents"><span class="note_content_field">Confidence: </span><span class="note_content_value">4: The reviewer is confident but not absolutely certain that the evaluation is correct</span></div><div class="reply_row clearfix"></div></div><div class="children"></div></div></div></main></div>
  </div>
</div>


    <!-- Footer -->
    <footer class="sitemap">
      <div class="container">
        <div class="row hidden-xs">
          <div class="col-sm-4">
            <ul class="list-unstyled">
              <li><a href="/" class="home">Home</a></li>
              <li><a href="/venues">All Venues</a></li>
            </ul>
          </div>
    
          <div class="col-sm-4">
            <ul class="list-unstyled">
              <li><a href="/about">About OpenReview</a></li>
              <li><a href="/contact">Contact</a></li>
              <li><a href="#" data-toggle="modal" data-target="#feedback-modal">Feedback</a></li>
            </ul>
          </div>
    
          <div class="col-sm-4">
            <ul class="list-unstyled">
              <li><a href="/terms">Terms of Service</a></li>
              <li><a href="/privacy">Privacy Policy</a></li>
            </ul>
          </div>
        </div>
    
        <div class="row visible-xs-block">
          <div class="col-xs-6">
            <ul class="list-unstyled">
              <li><a href="/" class="home">Home</a></li>
              <li><a href="/about">About OpenReview</a></li>
              <li><a href="#" data-toggle="modal" data-target="#feedback-modal">Feedback</a></li>
            </ul>
          </div>
    
          <div class="col-xs-6">
            <ul class="list-unstyled">
              <li><a href="/contact">Contact</a></li>
              <li><a href="/terms">Terms of Service</a></li>
              <li><a href="/privacy">Privacy Policy</a></li>
            </ul>
          </div>
        </div>
      </div>
    </footer>
    
    <footer class="sponsor">
      <div class="container">
        <div class="row">
          <div class="col-sm-10 col-sm-offset-1">
            <p class="text-center">
              OpenReview is created by the <a href="http://www.iesl.cs.umass.edu/" target="_blank">Information Extraction and Synthesis Laboratory</a>, College of Information and Computer Science, University of Massachusetts Amherst. We gratefully acknowledge the support of the OpenReview sponsors:  Google,  Facebook, NSF, the University of Massachusetts Amherst Center for Data Science, and Center for Intelligent Information Retrieval, as well as the Google Cloud Platform for donating the computing and networking services on which OpenReview.net runs.
            </p>
          </div>
        </div>
      </div>
    </footer>

  <div id="feedback-modal" class="modal fade" tabindex="-1" role="dialog">
    <div class="modal-dialog">
      <div class="modal-content">
        <div class="modal-header">
          <button type="button" class="close" data-dismiss="modal" aria-label="Close"><span aria-hidden="true">×</span></button>
          <h3 class="modal-title">Send Feedback</h3>
        </div>
        <div class="modal-body">
          <p>Enter your feedback below and we'll get back to you as soon as possible.</p>
          <form action="/feedback" method="POST">
            <div class="form-group">
              <input type="email" name="from" class="form-control" placeholder="Email" />
            </div>
            <div class="form-group">
              <input type="text" name="subject" class="form-control" placeholder="Subject" />
            </div>
            <div class="form-group">
              <textarea name="message" class="form-control feedback-input" rows="5" placeholder="Message" required=""></textarea>
            </div>
          <ul id="ui-id-2" tabindex="0" class="ui-menu ui-widget ui-widget-content" style="display: none;"></ul></form>
        </div>
        <div class="modal-footer">
          <button type="button" class="btn btn-default" data-dismiss="modal">Cancel</button>
          <button type="button" class="btn btn-primary">Send</button>
        </div>
      </div>
    </div>
  </div>

  <script type="text/javascript" async="" src="https://www.google-analytics.com/analytics.js"></script><script src="//cdnjs.cloudflare.com/ajax/libs/jquery/2.2.4/jquery.min.js"></script>
  <script>window.jQuery || document.write('&lt;script src="/static/js/vendor/jquery-2.2.4.min.js"&gt;&lt;\/script&gt;')</script>

  <script src="/static/dist/vendor.min.js"></script>
  <script src="/static/dist/templates.min.js"></script>
  <script src="/static/dist/openreview.min.js"></script>

    <script>window.legacyScripts = true;</script>


    <script async="" src="https://www.googletagmanager.com/gtag/js?id=UA-108703919-1"></script>
    <script>
      window.dataLayer = window.dataLayer || [];
      function gtag() { dataLayer.push(arguments); }
      gtag('js', new Date());
      gtag('config', 'UA-108703919-1');
    </script>


<div role="status" aria-live="assertive" aria-relevant="additions" class="ui-helper-hidden-accessible"></div></body></html>