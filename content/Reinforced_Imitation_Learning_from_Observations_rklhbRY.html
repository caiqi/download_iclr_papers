<!DOCTYPE html><html xmlns="http://www.w3.org/1999/xhtml" class="no-js"><head>
  <meta charset="utf-8" />
  <meta http-equiv="X-UA-Compatible" content="IE=edge,chrome=1" />
  <meta name="viewport" content="width=device-width, initial-scale=1" />

  <title>Reinforced Imitation Learning from Observations | OpenReview</title>
  <meta name="description" content="" />

      <meta name="citation_title" content="Reinforced Imitation Learning from Observations" />
        <meta name="citation_author" content="Anonymous" />
      <meta name="citation_publication_date" content="2018/09/27" />
      <meta name="citation_online_date" content="2018/09/27" />
      <meta name="citation_pdf_url" content="https://openreview.net/pdf?id=rklhb2R9Y7" />
      <meta name="twitter:card" content="summary" />
      <meta name="twitter:site" content="@openreviewnet" />
      <meta name="og:title" content="Reinforced Imitation Learning from Observations" />
      <meta name="og:description" content="Imitation learning is an effective alternative approach to learn a policy when the reward function is sparse. In this paper, we consider a challenging setting where an agent has access to a sparse..." />
      <meta name="og:image" content="https://openreview.net/static/images/openreview_logo_512.png" />

  <link rel="stylesheet" href="https://fonts.googleapis.com/css?family=Noto+Sans:400,400i,700,700i" />

  <link rel="stylesheet" href="/static/css/bootstrap.min.css" />
  <link rel="stylesheet" href="/static/css/jquery-ui.min.css" />
  <link rel="stylesheet" href="/static/css/main.min.css" />
</head>

<body class="forum">
  <nav class="navbar navbar-inverse navbar-fixed-top" role="navigation">
    <div class="container">
  
      <div class="navbar-header">
        <button type="button" class="navbar-toggle collapsed" data-toggle="collapse" data-target="#navbar" aria-expanded="false" aria-controls="navbar">
          <span class="sr-only">Toggle navigation</span>
          <span class="icon-bar"></span>
          <span class="icon-bar"></span>
          <span class="icon-bar"></span>
        </button>
        <a class="navbar-brand home push-link" href="/"><strong>OpenReview</strong>.net</a>
      </div>
  
      <div id="navbar" class="navbar-collapse collapse">
        <form class="navbar-form navbar-left profile-search" role="search">
          <div class="form-group has-feedback">
            <input id="search_input" type="text" class="form-control ui-autocomplete-input" placeholder="Search ICLR 2019 Conference" autocomplete="off" />
            <span class="glyphicon glyphicon-search form-control-feedback" aria-hidden="true"></span>
          </div>
  
          <input id="search_group" type="hidden" value="ICLR.cc/2019/Conference" />
          <input id="search_content" type="hidden" value="all" />
        <ul id="ui-id-1" tabindex="0" class="ui-menu ui-widget ui-widget-content ui-autocomplete ui-front" style="display: none;"></ul></form>
  
        <ul class="nav navbar-nav navbar-right">
        
            <li id="user-menu"><a href="/login">Login</a></li>
        </ul>
      </div>
  
    </div>
  </nav>

  <div id="or-banner" class="banner" style="">
  <div class="container">
    <div class="row">
      <div class="col-xs-12"><a href="/group?id=ICLR.cc/2019/Conference" title="Venue Homepage"><img class="icon" src="/static/images/arrow_left.svg" /> Go to <strong>ICLR 2019 Conference</strong> homepage</a></div>
    </div>
  </div>
</div>
<div id="flash-message-container" class="alert alert-danger" role="alert" style="display: none;">
  <div class="container">
    <div class="row">
      <div class="col-xs-12">
        <div class="alert-content">
          <button type="button" class="close" aria-label="Close"><span aria-hidden="true">×</span></button>
        </div>
      </div>
    </div>
  </div>
</div>

<div class="container">
  <div class="row">
    <div class="col-xs-12">
      <main id="content" class="clearfix openbanner-visible legacy-styles"><div id="note_rklhb2R9Y7" class="note panel"><div class="title_pdf_row clearfix"><h2 class="note_content_title"><a>Reinforced Imitation Learning from Observations</a> <a class="note_content_pdf" href="/pdf?id=rklhb2R9Y7" title="Download PDF" target="_blank"><img src="/static/images/pdf_icon_blue.svg" /></a> </h2></div><div class="meta_row"><span class="signatures">Anonymous</span></div><div class="clearfix"><div class="meta_row pull-left"><span class="date item">28 Sep 2018 (modified: 11 Oct 2018)</span><span class="item">ICLR 2019 Conference Blind Submission</span><span class="item">Readers: <span class="readers-icon glyphicon glyphicon-globe"></span> Everyone</span><span class="item"><a class="action-bibtex-modal" data-bibtex="@inproceedings{    &#10;anonymous2019reinforced,    &#10;title={Reinforced Imitation Learning from Observations},    &#10;author={Anonymous},    &#10;booktitle={Submitted to International Conference on Learning Representations},    &#10;year={2019},    &#10;url={https://openreview.net/forum?id=rklhb2R9Y7},    &#10;note={under review}    &#10;}">Show Bibtex</a></span></div><div class="meta_row meta_actions"></div></div><div class="note_contents"><span class="note_content_field">Abstract: </span><span class="note_content_value">Imitation learning is an effective alternative approach to learn a policy when the reward function is sparse. In this paper, we consider a challenging setting where an agent has access to a sparse reward function and state-only expert observations. We propose a method which gradually balances between the imitation learning cost and the reinforcement learning objective.~Built upon an existing imitation learning method, our approach works with state-only observations. We show, through navigation scenarios, that (i) an agent is able to efficiently leverage sparse rewards to outperform standard state-only imitation learning, (ii) it can learn a policy even when learner's actions are different from the expert, and (iii) the performance of the agent is not bounded by that of the expert due to the optimized usage of sparse rewards.</span></div><div class="reply_row clearfix"><div class="item" id="reply_count">7 Replies</div></div></div><hr class="small" /><div id="note_children"><div class="note_with_children"><div id="note_S1gXhcwkCm" class="note panel"><div class="title_pdf_row clearfix"><h2 class="note_content_title"><a>Some comment on novelty</a>  <button class="btn btn-xs btn-default permalink-button" title="Link to this comment" data-permalink-url="https://openreview.net/forum?id=rklhb2R9Y7&amp;noteId=S1gXhcwkCm"><span class="glyphicon glyphicon-link" aria-hidden="true"></span></button></h2></div><div class="meta_row"><span class="signatures">(anonymous)</span></div><div class="clearfix"><div class="meta_row pull-left"><span class="date item">19 Nov 2018</span><span class="item">ICLR 2019 Conference Paper1212 Public Comment</span><span class="item">Readers: <span class="readers-icon glyphicon glyphicon-globe"></span> Everyone</span></div><div class="meta_row meta_actions"></div></div><div class="note_contents"><span class="note_content_field">Comment: </span><span class="note_content_value">I think this paper basically makes a incremental follow-up to [Kang et.al., 2018] and [Li et.al., 2017]. However, it seems that the authors do not show enough respect for these work. From the perspective of [Kang et.al., 2018], this paper simply changes GAIL over (s, a) into GAIL over (s_t,.. s_t+n), which has been repeatedly proposed in [Bradly et.al., 2016][Josh et.al., 2017][Faraz et.al., 2018]. On the other hand, from [Li ei.al., 2017], we could also consider this paper as a variant of reward augmentation trick.

Overall, I think the novelty of this paper is quite limited and may not fit into ICLR requirements.

[Kang et.al., 2018] Policy Optimization with Demonstrations, Kang et.al., in ICML, 2018
[Li et.al., 2017] InfoGAIL: Interpretable Imitation Learning from Visual Demonstrations, Li et.al., in NIPS, 2017
[Bradly et.al., 2016] Third-Person Imitation Learning, Bradly et.al., in ICLR, 2017
[Josh et.al., 2017] Learning human behaviors from motion capture by adversarial imitation, Josh et.al., arXiv 1707.02201
[Faraz et.al., 2018] Generative Adversarial Imitation from Observation, Faraz et.al., arXiv 1807.06158</span></div><div class="reply_row clearfix"></div></div><div class="children"></div></div><div class="note_with_children"><div id="note_S1eBU1kkaX" class="note panel"><div class="title_pdf_row clearfix"><h2 class="note_content_title"><a>heuristic combining environment rewards with an IRL-style rewards</a>  <button class="btn btn-xs btn-default permalink-button" title="Link to this comment" data-permalink-url="https://openreview.net/forum?id=rklhb2R9Y7&amp;noteId=S1eBU1kkaX"><span class="glyphicon glyphicon-link" aria-hidden="true"></span></button></h2></div><div class="meta_row"><span class="signatures">ICLR 2019 Conference Paper1212 AnonReviewer3</span></div><div class="clearfix"><div class="meta_row pull-left"><span class="date item">06 Nov 2018 (modified: 07 Nov 2018)</span><span class="item">ICLR 2019 Conference Paper1212 Official Review</span><span class="item">Readers: <span class="readers-icon glyphicon glyphicon-globe"></span> Everyone</span></div><div class="meta_row meta_actions"></div></div><div class="note_contents"><span class="note_content_field">Review: </span><span class="note_content_value">The draft proposes a heuristic combining environment rewards with an IRL-style rewards recovered from expert demonstrations, seeking to extend the GAIL approach to IRL to the case of mismatching action spaces between the expert and the learner. The interesting contribution is, in my opinion, the self-exploration parameter that reduces the reliance of learning on demonstrations once they have been learned sufficiently well.

Questions:

- In general, it's known that behavioral cloning, of which this work seem to be an example in so much it learns state distributions that are indistinguishable from the expert ones, can fail spectacularly because of the distribution shift (Kaariainen@ALW06, Ross&amp;Bagnell@AISTATS10, Ross&amp;Bagnell@AISTATS11). Can you comment if GAN-based methods are immune or susceptible to this?
   
- Would this work for tasks where the state-space has to be learned together with the policy? E.g. image captioning tasks or Atari games.

- Is it possible to quantify the ease of learning or the frequency of use of the "new" actions, i.e. $A^l \setminus A^e$?. Won't learning these actions effectively be as difficult as RL with sparse rewards? Say, in a grid world where 4-way diagonal moves allow reaching the goal faster, learner is king 8-way, demonstrations come from a 4-way expert, rewards are sparse and each step receives a -1 reward and the final goal is large positive -- does the learner's final policy actually use the diagonals and when?

Related work:
   
- Is it possible to make a connection to (data or policy) aggregation methods in IL. Such methods (e.g. Chang et al.@ICML15) can also sometimes learn policies better than the expert.

Experiments:
- why GAIL wasn't evaluated in Fig. 3 and Fig. 4?

Minor:
- what's BCE in algorithm 1?    
- Fig.1: "the the"
- sec 3.2: but avoid -&gt; but avoids
- sec 3.2: be to considers -&gt; be to consider
- sec 3.2: any hyperparameter -&gt; any hyperparameters
- colors in Fig 2 are indistinguishable
- Table 1: headers saying which method is prior work and which is contribution would be helpful
- Fig. 3: if possible try to find a way of communicating the relation of action spaces between expert and learner (e.g. a subset of/superset of). Using the same figure to depict self-exploration make it complicated to analyse.
- sec 3.2: wording in the last paragraph on p.4 (positive scaling won't _make_ anything positive if it wasn't before)</span></div><div class="note_contents"><span class="note_content_field">Rating: </span><span class="note_content_value">6: Marginally above acceptance threshold</span></div><div class="note_contents"><span class="note_content_field">Confidence: </span><span class="note_content_value">2: The reviewer is willing to defend the evaluation, but it is quite likely that the reviewer did not understand central parts of the paper</span></div><div class="reply_row clearfix"></div></div><div class="children"><div class="note_with_children"><div id="note_H1l-XOhNam" class="note panel"><div class="title_pdf_row clearfix"><h2 class="note_content_title"><a>Initial reply</a>  <button class="btn btn-xs btn-default permalink-button" title="Link to this comment" data-permalink-url="https://openreview.net/forum?id=rklhb2R9Y7&amp;noteId=H1l-XOhNam"><span class="glyphicon glyphicon-link" aria-hidden="true"></span></button></h2></div><div class="meta_row"><span class="signatures">ICLR 2019 Conference Paper1212 Authors</span></div><div class="clearfix"><div class="meta_row pull-left"><span class="date item">11 Nov 2018</span><span class="item">ICLR 2019 Conference Paper1212 Official Comment</span><span class="item">Readers: <span class="readers-icon glyphicon glyphicon-globe"></span> Everyone</span></div><div class="meta_row meta_actions"></div></div><div class="note_contents"><span class="note_content_field">Comment: </span><span class="note_content_value">Thanks for the review and the feedback.
We are especially thankful for raising the topic of exploring the "new" actions, i.e. $A^l \setminus A^e$. Our method, in contrary to previously presented, does not limit the learner to use the same actions as used in expert demonstrations. As shown in our experiments, we were able to successfully train our agent even when action spaces are disjoint. Considering your specific example (with 8-way-king learner and 4-way expert), we already have a short comment on that in the paper -- the learner on average gets to the goal quicker (in respect to number of steps), hence it uses “new” actions. We believe that it is important remark and we will edit paper to make this more clear and visible.
We did not have any experiments supporting the hypothesis of being immune to domain shift in our setup. That’s a very interesting direction, but we believe it is general question for all GAN-based methods and should be a part of another work. However, we would be glad to explore this in future research.
I believe that our idea will work in any setup but it is well-suited for cases when expert and learner have different action spaces. The Atari games are created in a such way that all buttons (actions) are important and usually necessary to perform well. However, we started experimenting on complex environment, Vizdoom (3D POMPD), where the agent is still able to perform well using just a subset of all possible actions. Preliminary results lead to the similar conclusions, but we need more time to make any statement in this setting.
We have not shown GAIL for comparison in Fig 3. and Fig. 4 because the straightforward application of GAIL method is possible only when learner action space is a subset of expert action space which is rarely case in our experiments.
Thanks you again for providing related work and all minor remarks. We will update our paper according to them.</span></div><div class="reply_row clearfix"></div></div><div class="children"></div></div></div></div><div class="note_with_children"><div id="note_ByeSUJ03nX" class="note panel"><div class="title_pdf_row clearfix"><h2 class="note_content_title"><a>State only demonstrations but in deterministic environments </a>  <button class="btn btn-xs btn-default permalink-button" title="Link to this comment" data-permalink-url="https://openreview.net/forum?id=rklhb2R9Y7&amp;noteId=ByeSUJ03nX"><span class="glyphicon glyphicon-link" aria-hidden="true"></span></button></h2></div><div class="meta_row"><span class="signatures">ICLR 2019 Conference Paper1212 AnonReviewer2</span></div><div class="clearfix"><div class="meta_row pull-left"><span class="date item">05 Nov 2018 (modified: 07 Nov 2018)</span><span class="item">ICLR 2019 Conference Paper1212 Official Review</span><span class="item">Readers: <span class="readers-icon glyphicon glyphicon-globe"></span> Everyone</span></div><div class="meta_row meta_actions"></div></div><div class="note_contents"><span class="note_content_field">Review: </span><span class="note_content_value">The paper proposes to combine expert demonstration together with reinforcement learning to speed up learning of control policies. To do so, the authors modify the GAIL algorithm and create a composite reward function as a linear combination of the extrinsic reward and the imitation reward. They test their approach on several toy problems (small grid worlds). 

The idea of combining GAIL reward and extrinsic reward is not really new and quite straight forward so I wouldn't consider this as a contribution. Also, using state only demonstration in the framework of GAIL is not new as the authors also acknowledge in the paper. Finally, I don't think the experiments are convincing since the chosen problems are rather simple. 

But my main concern is that the major claim of the authors is that they don't use expert actions as input to their algorithm, but only sequences of states. Yet they test their algorithm on deterministic environments. In such a case, two consecutive states kind of encode the action and all the information is their. Even if the action sets are different in some of the experiments, the are still very close to each other and the encoding of the expert actions in the state sequence is probably helping a lot. So I would like to see how this method works in stochastic environments. </span></div><div class="note_contents"><span class="note_content_field">Rating: </span><span class="note_content_value">5: Marginally below acceptance threshold</span></div><div class="note_contents"><span class="note_content_field">Confidence: </span><span class="note_content_value">5: The reviewer is absolutely certain that the evaluation is correct and very familiar with the relevant literature</span></div><div class="reply_row clearfix"></div></div><div class="children"><div class="note_with_children"><div id="note_ryeub04zpQ" class="note panel"><div class="title_pdf_row clearfix"><h2 class="note_content_title"><a>Initial reply</a>  <button class="btn btn-xs btn-default permalink-button" title="Link to this comment" data-permalink-url="https://openreview.net/forum?id=rklhb2R9Y7&amp;noteId=ryeub04zpQ"><span class="glyphicon glyphicon-link" aria-hidden="true"></span></button></h2></div><div class="meta_row"><span class="signatures">ICLR 2019 Conference Paper1212 Authors</span></div><div class="clearfix"><div class="meta_row pull-left"><span class="date item">09 Nov 2018</span><span class="item">ICLR 2019 Conference Paper1212 Official Comment</span><span class="item">Readers: <span class="readers-icon glyphicon glyphicon-globe"></span> Everyone</span></div><div class="meta_row meta_actions"></div></div><div class="note_contents"><span class="note_content_field">Comment: </span><span class="note_content_value">We thank the reviewer for the detailed comments.

We fully agree (as acknowledged that in the paper) that using state-only demonstration in the GAIL framework is not new.  However, to the best of our knowledge, no systematic experiments studying state-only demonstrations have been done. We propose baseline methods that are straightforward implication of previous works and we compare them to our methods (RTGD, ATD, self-exploration). We show quantitatively how all methods presented perform in different situations.

We agree to the fact that in deterministic environments the consecutive states encode the action performed. The choice of deterministic environment was made on purpose. We believe that this choice makes the problem more challenging because discriminator is more likely to ‘decode’ action spaces used by two agents and easily discriminate based on that -- which leads to providing non-informative rewards (rewards that are not linked with the policy).
This intuition is confirmed in the paper: CSD performs bad when action spaces are disjoint. We report results for methods that can not decode actions (RTGD, ATD and SSD) and they perform better than CSD (unless action states are not disjoint, then all methods perform similar). We will modify the paper to make this more clear.

Although the experiments show the main properties of the proposed algorithm, we also agree that more experiments on different tasks would definitely be helpful.
We explored the method in another gridworld setting, in which the agent does not have access to the whole map, but only partial observations (we use a 5x5 subgrid surrounding the agent). We used the same action spaces for both expert and learner to be able to compare the result directly. Results in the POMDP setting are very similar to the fully-observed setting.
We started experimenting on more complex environment, Vizdoom (3D POMPD). This environment is much more (computationally) demanding, so conducting systematic experiment set (as done for the original problem) is more challenging. Preliminary results lead to the similar conclusions, but we need more time to make any statement in this setting.
Vizdoom environment dynamic implements inertia and hence the previous action cannot be inferred from consecutive states. Also the behavior of another agents (bots) is non-deterministic.

We would appreciate any suggestions on the experimental setting that could improve our work.
</span></div><div class="reply_row clearfix"></div></div><div class="children"></div></div></div></div><div class="note_with_children"><div id="note_SkeVxQuo27" class="note panel"><div class="title_pdf_row clearfix"><h2 class="note_content_title"><a>First review</a>  <button class="btn btn-xs btn-default permalink-button" title="Link to this comment" data-permalink-url="https://openreview.net/forum?id=rklhb2R9Y7&amp;noteId=SkeVxQuo27"><span class="glyphicon glyphicon-link" aria-hidden="true"></span></button></h2></div><div class="meta_row"><span class="signatures">ICLR 2019 Conference Paper1212 AnonReviewer1</span></div><div class="clearfix"><div class="meta_row pull-left"><span class="date item">04 Nov 2018 (modified: 07 Nov 2018)</span><span class="item">ICLR 2019 Conference Paper1212 Official Review</span><span class="item">Readers: <span class="readers-icon glyphicon glyphicon-globe"></span> Everyone</span></div><div class="meta_row meta_actions"></div></div><div class="note_contents"><span class="note_content_field">Review: </span><span class="note_content_value">This paper proposes some new angles to the problem of imitation learning from state only observations (not state-action pairs which are more expensive). 
Specifically, the paper proposes "self exploration", in which it mixes the imitation reward with environment reward from the MDP itself in a gradual manner, guided by the rate of learning.
It also proposes a couple of variants of imitation rewards, RTGD and ATD inparticular, which formulate the imitation rewards for random or exhaustive pairs of states in the observation data, as opposed to the rewards proposed in existing works (CSD, SSD), which are based on either consecutive or single states, which constitute the baseline methods for comparison.
The authors then perform a systematic experiment using a particular navigation problem on a grid world, and inspect under what scenarios (e.g. when the action spaces of the expert and learner are the same, disjoint or in a containment relationship) which of the methods perform well relative to the baselines. 
Some moderately interesting observations are reported, which largely confirm one's intuition about when these methods may perform relatively well. 
There is not very much theoretical support for the proposed methods per se, the paper is mostly an empirical study on these competing reward schemes for imitation learning.
The empirical evaluation is done in a single domain/problem, and in that sense it is questionable how far the observed trends on the relative performance of the competing methods generalizes to other problems and domains. 
Also the proposed ideas are all reasonable but relatively simple and unsurprising, casting some doubt as to the extent to which the paper contributes to the state of understanding of this area of research. </span></div><div class="note_contents"><span class="note_content_field">Rating: </span><span class="note_content_value">4: Ok but not good enough - rejection</span></div><div class="note_contents"><span class="note_content_field">Confidence: </span><span class="note_content_value">4: The reviewer is confident but not absolutely certain that the evaluation is correct</span></div><div class="reply_row clearfix"></div></div><div class="children"><div class="note_with_children"><div id="note_BkxIVC4fTm" class="note panel"><div class="title_pdf_row clearfix"><h2 class="note_content_title"><a>Initial reply</a>  <button class="btn btn-xs btn-default permalink-button" title="Link to this comment" data-permalink-url="https://openreview.net/forum?id=rklhb2R9Y7&amp;noteId=BkxIVC4fTm"><span class="glyphicon glyphicon-link" aria-hidden="true"></span></button></h2></div><div class="meta_row"><span class="signatures">ICLR 2019 Conference Paper1212 Authors</span></div><div class="clearfix"><div class="meta_row pull-left"><span class="date item">09 Nov 2018</span><span class="item">ICLR 2019 Conference Paper1212 Official Comment</span><span class="item">Readers: <span class="readers-icon glyphicon glyphicon-globe"></span> Everyone</span></div><div class="meta_row meta_actions"></div></div><div class="note_contents"><span class="note_content_field">Comment: </span><span class="note_content_value">We thank you for the constructive comments. 
We agree that these results corroborates with one's intuition. And this is precisely why we think the results are interesting. 

Although the experiments show the main properties of the proposed algorithm, we also agree that more experiments on different tasks would definitely be helpful.
We explored the method in another gridworld setting, in which the agent does not have access to the whole map, but only partial observations (we use a 5x5 subgrid surrounding the agent). We used the same action spaces for both expert and learner to be able to compare the result directly. Results in the POMDP setting are very similar to the fully-observed setting.
We started experimenting on more complex environment, Vizdoom (3D POMPD). This environment is much more (computationally) demanding, so conducting a systematic set of  experiment (as done for the original case) is more challenging. Preliminary results lead to the similar conclusions, but we need more time to make any statement in this setting.

We would appreciate to hear your opinion about choices for additional experiments and suggestion on what we can add to the paper to make it better.
</span></div><div class="reply_row clearfix"></div></div><div class="children"></div></div></div></div></div></main></div>
  </div>
</div>


    <!-- Footer -->
    <footer class="sitemap">
      <div class="container">
        <div class="row hidden-xs">
          <div class="col-sm-4">
            <ul class="list-unstyled">
              <li><a href="/" class="home">Home</a></li>
              <li><a href="/venues">All Venues</a></li>
            </ul>
          </div>
    
          <div class="col-sm-4">
            <ul class="list-unstyled">
              <li><a href="/about">About OpenReview</a></li>
              <li><a href="/contact">Contact</a></li>
              <li><a href="#" data-toggle="modal" data-target="#feedback-modal">Feedback</a></li>
            </ul>
          </div>
    
          <div class="col-sm-4">
            <ul class="list-unstyled">
              <li><a href="/terms">Terms of Service</a></li>
              <li><a href="/privacy">Privacy Policy</a></li>
            </ul>
          </div>
        </div>
    
        <div class="row visible-xs-block">
          <div class="col-xs-6">
            <ul class="list-unstyled">
              <li><a href="/" class="home">Home</a></li>
              <li><a href="/about">About OpenReview</a></li>
              <li><a href="#" data-toggle="modal" data-target="#feedback-modal">Feedback</a></li>
            </ul>
          </div>
    
          <div class="col-xs-6">
            <ul class="list-unstyled">
              <li><a href="/contact">Contact</a></li>
              <li><a href="/terms">Terms of Service</a></li>
              <li><a href="/privacy">Privacy Policy</a></li>
            </ul>
          </div>
        </div>
      </div>
    </footer>
    
    <footer class="sponsor">
      <div class="container">
        <div class="row">
          <div class="col-sm-10 col-sm-offset-1">
            <p class="text-center">
              OpenReview is created by the <a href="http://www.iesl.cs.umass.edu/" target="_blank">Information Extraction and Synthesis Laboratory</a>, College of Information and Computer Science, University of Massachusetts Amherst. We gratefully acknowledge the support of the OpenReview sponsors:  Google,  Facebook, NSF, the University of Massachusetts Amherst Center for Data Science, and Center for Intelligent Information Retrieval, as well as the Google Cloud Platform for donating the computing and networking services on which OpenReview.net runs.
            </p>
          </div>
        </div>
      </div>
    </footer>

  <div id="feedback-modal" class="modal fade" tabindex="-1" role="dialog">
    <div class="modal-dialog">
      <div class="modal-content">
        <div class="modal-header">
          <button type="button" class="close" data-dismiss="modal" aria-label="Close"><span aria-hidden="true">×</span></button>
          <h3 class="modal-title">Send Feedback</h3>
        </div>
        <div class="modal-body">
          <p>Enter your feedback below and we'll get back to you as soon as possible.</p>
          <form action="/feedback" method="POST">
            <div class="form-group">
              <input type="email" name="from" class="form-control" placeholder="Email" />
            </div>
            <div class="form-group">
              <input type="text" name="subject" class="form-control" placeholder="Subject" />
            </div>
            <div class="form-group">
              <textarea name="message" class="form-control feedback-input" rows="5" placeholder="Message" required=""></textarea>
            </div>
          <ul id="ui-id-2" tabindex="0" class="ui-menu ui-widget ui-widget-content" style="display: none;"></ul></form>
        </div>
        <div class="modal-footer">
          <button type="button" class="btn btn-default" data-dismiss="modal">Cancel</button>
          <button type="button" class="btn btn-primary">Send</button>
        </div>
      </div>
    </div>
  </div>

  <script type="text/javascript" async="" src="https://www.google-analytics.com/analytics.js"></script><script src="//cdnjs.cloudflare.com/ajax/libs/jquery/2.2.4/jquery.min.js"></script>
  <script>window.jQuery || document.write('&lt;script src="/static/js/vendor/jquery-2.2.4.min.js"&gt;&lt;\/script&gt;')</script>

  <script src="/static/dist/vendor.min.js"></script>
  <script src="/static/dist/templates.min.js"></script>
  <script src="/static/dist/openreview.min.js"></script>

    <script>window.legacyScripts = true;</script>


    <script async="" src="https://www.googletagmanager.com/gtag/js?id=UA-108703919-1"></script>
    <script>
      window.dataLayer = window.dataLayer || [];
      function gtag() { dataLayer.push(arguments); }
      gtag('js', new Date());
      gtag('config', 'UA-108703919-1');
    </script>


<div role="status" aria-live="assertive" aria-relevant="additions" class="ui-helper-hidden-accessible"></div></body></html>