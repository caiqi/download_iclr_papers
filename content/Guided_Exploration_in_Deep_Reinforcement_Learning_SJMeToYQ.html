<!DOCTYPE html><html xmlns="http://www.w3.org/1999/xhtml" class="no-js"><head>
  <meta charset="utf-8" />
  <meta http-equiv="X-UA-Compatible" content="IE=edge,chrome=1" />
  <meta name="viewport" content="width=device-width, initial-scale=1" />

  <title>Guided Exploration in Deep Reinforcement Learning | OpenReview</title>
  <meta name="description" content="" />

      <meta name="citation_title" content="Guided Exploration in Deep Reinforcement Learning" />
        <meta name="citation_author" content="Anonymous" />
      <meta name="citation_publication_date" content="2018/09/27" />
      <meta name="citation_online_date" content="2018/09/27" />
      <meta name="citation_pdf_url" content="https://openreview.net/pdf?id=SJMeTo09YQ" />
      <meta name="twitter:card" content="summary" />
      <meta name="twitter:site" content="@openreviewnet" />
      <meta name="og:title" content="Guided Exploration in Deep Reinforcement Learning" />
      <meta name="og:description" content="This paper proposes a new method to drastically speed up deep reinforcement learning (deep RL) training for problems that have the property of \textit{state-action permissibility} (SAP). Two types..." />
      <meta name="og:image" content="https://openreview.net/static/images/openreview_logo_512.png" />

  <link rel="stylesheet" href="https://fonts.googleapis.com/css?family=Noto+Sans:400,400i,700,700i" />

  <link rel="stylesheet" href="/static/css/bootstrap.min.css" />
  <link rel="stylesheet" href="/static/css/jquery-ui.min.css" />
  <link rel="stylesheet" href="/static/css/main.min.css" />
</head>

<body class="forum">
  <nav class="navbar navbar-inverse navbar-fixed-top" role="navigation">
    <div class="container">
  
      <div class="navbar-header">
        <button type="button" class="navbar-toggle collapsed" data-toggle="collapse" data-target="#navbar" aria-expanded="false" aria-controls="navbar">
          <span class="sr-only">Toggle navigation</span>
          <span class="icon-bar"></span>
          <span class="icon-bar"></span>
          <span class="icon-bar"></span>
        </button>
        <a class="navbar-brand home push-link" href="/"><strong>OpenReview</strong>.net</a>
      </div>
  
      <div id="navbar" class="navbar-collapse collapse">
        <form class="navbar-form navbar-left profile-search" role="search">
          <div class="form-group has-feedback">
            <input id="search_input" type="text" class="form-control ui-autocomplete-input" placeholder="Search ICLR 2019 Conference" autocomplete="off" />
            <span class="glyphicon glyphicon-search form-control-feedback" aria-hidden="true"></span>
          </div>
  
          <input id="search_group" type="hidden" value="ICLR.cc/2019/Conference" />
          <input id="search_content" type="hidden" value="all" />
        <ul id="ui-id-1" tabindex="0" class="ui-menu ui-widget ui-widget-content ui-autocomplete ui-front" style="display: none;"></ul></form>
  
        <ul class="nav navbar-nav navbar-right">
        
            <li id="user-menu"><a href="/login">Login</a></li>
        </ul>
      </div>
  
    </div>
  </nav>

  <div id="or-banner" class="banner" style="">
  <div class="container">
    <div class="row">
      <div class="col-xs-12"><a href="/group?id=ICLR.cc/2019/Conference" title="Venue Homepage"><img class="icon" src="/static/images/arrow_left.svg" /> Go to <strong>ICLR 2019 Conference</strong> homepage</a></div>
    </div>
  </div>
</div>
<div id="flash-message-container" class="alert alert-danger" role="alert" style="display: none;">
  <div class="container">
    <div class="row">
      <div class="col-xs-12">
        <div class="alert-content">
          <button type="button" class="close" aria-label="Close"><span aria-hidden="true">×</span></button>
        </div>
      </div>
    </div>
  </div>
</div>

<div class="container">
  <div class="row">
    <div class="col-xs-12">
      <main id="content" class="clearfix openbanner-visible legacy-styles"><div id="note_SJMeTo09YQ" class="note panel"><div class="title_pdf_row clearfix"><h2 class="note_content_title"><a>Guided Exploration in Deep Reinforcement Learning</a> <a class="note_content_pdf" href="/pdf?id=SJMeTo09YQ" title="Download PDF" target="_blank"><img src="/static/images/pdf_icon_blue.svg" /></a> </h2></div><div class="meta_row"><span class="signatures">Anonymous</span></div><div class="clearfix"><div class="meta_row pull-left"><span class="date item">28 Sep 2018 (modified: 11 Oct 2018)</span><span class="item">ICLR 2019 Conference Blind Submission</span><span class="item">Readers: <span class="readers-icon glyphicon glyphicon-globe"></span> Everyone</span><span class="item"><a class="action-bibtex-modal" data-bibtex="@inproceedings{    &#10;anonymous2019guided,    &#10;title={Guided Exploration in Deep Reinforcement Learning},    &#10;author={Anonymous},    &#10;booktitle={Submitted to International Conference on Learning Representations},    &#10;year={2019},    &#10;url={https://openreview.net/forum?id=SJMeTo09YQ},    &#10;note={under review}    &#10;}">Show Bibtex</a></span></div><div class="meta_row meta_actions"></div></div><div class="note_contents"><span class="note_content_field">Abstract: </span><span class="note_content_value">This paper proposes a new method to drastically speed up deep reinforcement learning (deep RL) training for problems that have the property of \textit{state-action permissibility} (SAP). Two types of permissibility are defined under SAP. The first type says that after an action $a_t$ is performed in a state $s_t$ and the agent reaches the new state $s_{t+1}$, the agent can decide whether the action $a_t$ is \textit{permissible} or \textit{not permissible} in state $s_t$. The second type says that even without performing the action $a_t$ in state $s_t$, the agent can already decide whether $a_t$ is permissible or not in $s_t$. An action is not permissible in a state if the action can never lead to an optimal solution and thus should not be tried. We incorporate the proposed SAP property into two state-of-the-art deep RL algorithms to guide their state-action exploration. Results show that the SAP guidance can markedly speed up training.</span></div><div class="note_contents"><span class="note_content_field">Keywords: </span><span class="note_content_value">deep reinforcement learning, guided exploration, RL training speed up</span></div><div class="note_contents"><span class="note_content_field">TL;DR: </span><span class="note_content_value">introduces a guided action exploration mechanism that drastically speed up RL training</span></div><div class="reply_row clearfix"><div class="item" id="reply_count">4 Replies</div></div></div><hr class="small" /><div id="note_children"><div class="note_with_children"><div id="note_BJgm5hW6nm" class="note panel"><div class="title_pdf_row clearfix"><h2 class="note_content_title"><a>A constrained learning of permissable action-state space for speeding up RL </a>  <button class="btn btn-xs btn-default permalink-button" title="Link to this comment" data-permalink-url="https://openreview.net/forum?id=SJMeTo09YQ&amp;noteId=BJgm5hW6nm"><span class="glyphicon glyphicon-link" aria-hidden="true"></span></button></h2></div><div class="meta_row"><span class="signatures">ICLR 2019 Conference Paper773 AnonReviewer3</span></div><div class="clearfix"><div class="meta_row pull-left"><span class="date item">05 Nov 2018 (modified: 07 Nov 2018)</span><span class="item">ICLR 2019 Conference Paper773 Official Review</span><span class="item">Readers: <span class="readers-icon glyphicon glyphicon-globe"></span> Everyone</span></div><div class="meta_row meta_actions"></div></div><div class="note_contents"><span class="note_content_field">Review: </span><span class="note_content_value">The authors introduce an approach for constraining the action-state space of RL algorithms, with the premise to speed up their learning. To this end, two types of constraints are introduced, coupled and embedded into the traditional policy learning for RL. The main idea of using a binary predictors for predictions of permissible actions leading to desired  states is interesting and novel. It is an intuitive approach for constraining the space and the authors showed in their experiments that it leads to significant speed up in learning of two common RL methods (DDQN and DDPG). The approach is also motivated by recent trends in meta-learning (of the binary predictor) and it would be good if the authors relate it to that (also citing some literature on meta learning). 

While I am in favor of accepting this paper, I think there are several aspects that need be commented on/addressed:

- what would be a simple baseline for constraining the action-state space? One possibility could be to use the learned model to simulate the trajectories and based on that hard code the constraints? Any other ideas, task-specific?

- what is the relation to the model-based RL? In model-based RL we try to learn the transition probabilities from action to states. Could we impose any sparsity constraints on such a model to achieve a similar performance. While the proposed model is more elegant in that it allows the learning of the predictors on the fly, I feel there is a lack of comparisons with approaches that could easily be implemented using heuristics. Please comment. 

- could you be more precise about how often the prediction model is updated? What are potential adverse effects if this models keeps overfitting?

There are also limitations in terms of the number of hyperparameters that need be fine-tuned. I would like that the authors include one paragraph discussing in more detail the limitations of their approach.

</span></div><div class="note_contents"><span class="note_content_field">Rating: </span><span class="note_content_value">7: Good paper, accept</span></div><div class="note_contents"><span class="note_content_field">Confidence: </span><span class="note_content_value">5: The reviewer is absolutely certain that the evaluation is correct and very familiar with the relevant literature</span></div><div class="reply_row clearfix"></div></div><div class="children"></div></div><div class="note_with_children"><div id="note_HJefUYx9nQ" class="note panel"><div class="title_pdf_row clearfix"><h2 class="note_content_title"><a>A simple but nice idea. However, there are issues with the algorithm in the continuous action case and the evaluation could be more exhaustive. </a>  <button class="btn btn-xs btn-default permalink-button" title="Link to this comment" data-permalink-url="https://openreview.net/forum?id=SJMeTo09YQ&amp;noteId=HJefUYx9nQ"><span class="glyphicon glyphicon-link" aria-hidden="true"></span></button></h2></div><div class="meta_row"><span class="signatures">ICLR 2019 Conference Paper773 AnonReviewer1</span></div><div class="clearfix"><div class="meta_row pull-left"><span class="date item">03 Nov 2018 (modified: 07 Nov 2018)</span><span class="item">ICLR 2019 Conference Paper773 Official Review</span><span class="item">Readers: <span class="readers-icon glyphicon glyphicon-globe"></span> Everyone</span></div><div class="meta_row meta_actions"></div></div><div class="note_contents"><span class="note_content_field">Review: </span><span class="note_content_value">The paper introduces permissible actions to reinforcement learning problems. A action is non-permissible if it is known to not lead to the optimal solution. The agent can, after executing an action a_t in state s_t and ending up in s_t+1, estimate whether the action is a_t is non-permissible. This data is used to train a new classifier that predicts the permissibility of an action in a state. The exploration of the RL algorithm can now be guided by the permissibility estimate, i.e., non-permissible actions are not executed. 

The paper is well written and presents a simple, but promising idea to simplify reinforcement learning methods. I so far have not seen the definition of non-permissible actions in the literature so I believe this is novel and makes intuitively also sense, as permissible actions can be identified in many scenarios. However, the paper has a few issues that I want the authors to address:
- The amount of newly introduced hyperparameters is quite big and I am not sure whether the improved performance justifies the increased number of hyperparameters justifies. 
- How many trials have been used to generate the results? Fig3 says "Avg. reward over past 100 training steps". Does that mean only one trial and you average over the last 100 rewards? In order to be significant, at least 5 to 10 trials have to be used as deep RL is known to show highly varying results depending on the random seed. Please also report error bars.
- Why are there no learning curves for Flappy Bird?
- The method for creating the action set if the selected action is permissible seems very adhoc for me, at least in the continuous action case. Would it not make more sense to include the gradient of the classifier into the actor update of DDPG such that the policy would also learn to avoid non-permissible actions? The presented method is in my opinions very hard to scale to higher dimensional action spaces (&gt;2), which is quite a limitation of the approach.
- The description of Section 4, in particular of the construction of the candidate actions could be made more clear. 
- Results are only shown for a rather low dimensional action set (driving) and a discrete action example. 1-2 more illustrations where AP1 could be useful would be highly appreciated. </span></div><div class="note_contents"><span class="note_content_field">Rating: </span><span class="note_content_value">5: Marginally below acceptance threshold</span></div><div class="note_contents"><span class="note_content_field">Confidence: </span><span class="note_content_value">4: The reviewer is confident but not absolutely certain that the evaluation is correct</span></div><div class="reply_row clearfix"></div></div><div class="children"></div></div><div class="note_with_children"><div id="note_HyeHZTbdn7" class="note panel"><div class="title_pdf_row clearfix"><h2 class="note_content_title"><a>The application of SAP seems very narrow.</a>  <button class="btn btn-xs btn-default permalink-button" title="Link to this comment" data-permalink-url="https://openreview.net/forum?id=SJMeTo09YQ&amp;noteId=HyeHZTbdn7"><span class="glyphicon glyphicon-link" aria-hidden="true"></span></button></h2></div><div class="meta_row"><span class="signatures">ICLR 2019 Conference Paper773 AnonReviewer2</span></div><div class="clearfix"><div class="meta_row pull-left"><span class="date item">01 Nov 2018 (modified: 07 Nov 2018)</span><span class="item">ICLR 2019 Conference Paper773 Official Review</span><span class="item">Readers: <span class="readers-icon glyphicon glyphicon-globe"></span> Everyone</span></div><div class="meta_row meta_actions"></div></div><div class="note_contents"><span class="note_content_field">Review: </span><span class="note_content_value">This paper proposed the concept of state-action permissibility (SAP). Given a user-defined type 1 SAP function, the algorithm learns a classifier to predict whether an action at a given state is permissible or not. Based on this prediction, the reinforcement learning (RL) algorithms can limit the exploration only to the permissible actions, and thus greatly reduce the cost of learning. The proposed algorithms are tested on two simple tasks, both of which have the same flavor of following a predefined track.

Although the results of the experiments show that SAP helps to speed up RL, I think that the application of SAC is very narrow. It is extremely difficult to define an AP1 function in general. For example, for most of the OpenAI gym environments (such as half-cheetah, ants or humanoid), it is not clear to me how to manually define an AP1 function. It would be more convincing if the paper can apply the proposed techniques to some of the benchmark OpenAI gym environments.

Even for the lane following task described in the paper, the AP1 function in eq. 5 is limited and eliminates many good solutions. It constrains that the action should not lead to more deviations to the center line in the next time step. This greedy constraint will not work in more interesting driving scenarios. For example in a sharp turn, if the curvature of the lane is too large for the car to follow, a common strategy (that can be learned by vanilla RL algorithms) is to first drive to the outer side of the lane before the turn, cut to the inner side at the turn and exit the turn to the outer side. This optimal solution to negotiate a tight turn is completely eliminated by the user-defined AP1 function (eq. 5).

The idea of AP1 is somewhat contradictory to the philosophy of reinforcement learning. AP1 is a greedy decision based on the next step while RL optimizes for the accumulated reward over many steps. RL allows taking an action that will sacrifice the immediate reward (e.g. deviate from the center line of a lane) in the next step but can accumulated more reward in the long run (successfully drive along a tight turn). In most of cases, by looking at the next state, it is just not possible to predict whether a specific action cannot lead to the optimal long-term reward (SAP).

For the above reasons, I think that the application of SAP would be very narrow, especially for reinforcement learning. I would not recommend accepting this paper at this time.</span></div><div class="note_contents"><span class="note_content_field">Rating: </span><span class="note_content_value">3: Clear rejection</span></div><div class="note_contents"><span class="note_content_field">Confidence: </span><span class="note_content_value">3: The reviewer is fairly confident that the evaluation is correct</span></div><div class="reply_row clearfix"></div></div><div class="children"><div class="note_with_children"><div id="note_Skg0LCPkRQ" class="note panel"><div class="title_pdf_row clearfix"><h2 class="note_content_title"><a>Response to AnonReviewer2</a>  <button class="btn btn-xs btn-default permalink-button" title="Link to this comment" data-permalink-url="https://openreview.net/forum?id=SJMeTo09YQ&amp;noteId=Skg0LCPkRQ"><span class="glyphicon glyphicon-link" aria-hidden="true"></span></button></h2></div><div class="meta_row"><span class="signatures">ICLR 2019 Conference Paper773 Authors</span></div><div class="clearfix"><div class="meta_row pull-left"><span class="date item">19 Nov 2018</span><span class="item">ICLR 2019 Conference Paper773 Official Comment</span><span class="item">Readers: <span class="readers-icon glyphicon glyphicon-globe"></span> Everyone</span></div><div class="meta_row meta_actions"></div></div><div class="note_contents"><span class="note_content_field">Comment: </span><span class="note_content_value">We thank you for your valuable comments. Please find our responses below.

C1: Although the results of the experiments show that SAP helps to speed up RL, I think that the application of SAP is very narrow….

R1: We agree that designing a good AP function for complex environments with too many parameters can be challenging, although not impossible. Also, note that, our work does not require user to provide the “optimal AP function” for a given problem. Several AP functions can be designed for a given problem (as we have discussed in the paper). We aim to provide a framework for existing Deep RL where if a fairly good AP function can be designed for an environment (often it’s not that difficult to come up with a fairly good AP functions for many environments), the proposed technique can result in drastic speed up. In other words, we show that the idea of SAP is useful. Designing a good AP function for a complex scenario requires more analysis and knowledge of the Application, but it certainly does not make the idea inapplicable. By no means do we say that for every task there is at least one AP function. As we stated, our goal here is to help speed up a class of problems. 

We do believe that most robot functions involving movements and navigations have the SAP property, which is not a small application domain. We can often design a fairly good AP function for robot navigation using just common sense. Thus, by learning an AP predictor, we can quickly cut-off unnecessary action exploration and speed up the learning. We believe that is what we humans do as we smartly choose permissible actions rather than blindly try everything.

Most of mentioned benchmarks in gym are for multidimensional action space. In this work, we only deal with one dimensional discrete/continuous action space. We leave the multidimensional case as our future work (also mentioned in footnote 1). Hence, these environments are not suitable for our experiments.

C2: Even for the lane following task described in the paper, the AP1 function in eq. 5 is limited and eliminates many good solutions…

R2: As an RL problem can have multiple reward functions, it can have multiple AP functions as well, depending on the goal of the task in hand.  It’s true that our proposed AP function (eqn. 5) will not work in more complex driving scenarios but we do not aim to do that in this paper at the moment. Solving the complete autonomous driving problem is out of the scope of this work. Rather, we use a specific task (lane keeping) as our test bed to prove our hypothesis -  the idea of SAP is useful to speed up RL. Thus, we don't focus on designing a complex AP function (to cover all cases). 

Regarding the suggested strategy of “driving to the outer side of the lane before the turn, cut to the inner side at the turn,” we have a different opinion. We believe that is a speed control problem, which we do not study in this paper. In real life, this happens normally because we did not slow down enough at the turn which forces us to go to the outer lane. At least, that is the case for me. If the speed is also controlled by the RL, this scenario should be avoided because it is quite dangerous unless there is no car in the outer lane. Thus, in the RL learning phase, this kind of behavior should be penalized in speed control policy learning. This scenario could happen when the angle is so sharp that it is impossible to turn without cutting into the other lane (e.g., at some U-turn locations), but in that case, an autonomous car system normally will generate a new virtual lane for the car to follow (we have worked on real-life self-driving cars in the field). Another option is just to turn the steering wheel with the maximum angle possible. In both cases, the proposed RL framework still works. 

C3:  The idea of AP1 is somewhat contradictory to the philosophy of reinforcement learning… 

R3: The purpose of AP function is only to cut off exploration space for given a state, and enabling RL to not explore non-permissible actions in similar states again and again. In particular, it estimates a permissible action space in a given state and prioritize exploration of those actions in that state compared to the non-permissible ones. There may be multiple permissible actions in a given state to choose from. But, SAP does not tell you which one is optimal at that point. Rather, SAP only tells you which one you should definitely avoid exploring, as there is a better option (action) available in that state to explore. And, it’s the RL’s job to find out the optimal policy (optimal action) from the permissible action space in the long run. Thus, as we are not chopping off any optimal solution in AP-based guidance [note, even the RL agent always explores non-permissible actions with (1- alpha) probability], we believe the idea of SAP is not contradictory to RL. Similar to human driving, we do not try all possible options as we can predict what actions are definitely not good.</span></div><div class="reply_row clearfix"></div></div><div class="children"></div></div></div></div></div></main></div>
  </div>
</div>


    <!-- Footer -->
    <footer class="sitemap">
      <div class="container">
        <div class="row hidden-xs">
          <div class="col-sm-4">
            <ul class="list-unstyled">
              <li><a href="/" class="home">Home</a></li>
              <li><a href="/venues">All Venues</a></li>
            </ul>
          </div>
    
          <div class="col-sm-4">
            <ul class="list-unstyled">
              <li><a href="/about">About OpenReview</a></li>
              <li><a href="/contact">Contact</a></li>
              <li><a href="#" data-toggle="modal" data-target="#feedback-modal">Feedback</a></li>
            </ul>
          </div>
    
          <div class="col-sm-4">
            <ul class="list-unstyled">
              <li><a href="/terms">Terms of Service</a></li>
              <li><a href="/privacy">Privacy Policy</a></li>
            </ul>
          </div>
        </div>
    
        <div class="row visible-xs-block">
          <div class="col-xs-6">
            <ul class="list-unstyled">
              <li><a href="/" class="home">Home</a></li>
              <li><a href="/about">About OpenReview</a></li>
              <li><a href="#" data-toggle="modal" data-target="#feedback-modal">Feedback</a></li>
            </ul>
          </div>
    
          <div class="col-xs-6">
            <ul class="list-unstyled">
              <li><a href="/contact">Contact</a></li>
              <li><a href="/terms">Terms of Service</a></li>
              <li><a href="/privacy">Privacy Policy</a></li>
            </ul>
          </div>
        </div>
      </div>
    </footer>
    
    <footer class="sponsor">
      <div class="container">
        <div class="row">
          <div class="col-sm-10 col-sm-offset-1">
            <p class="text-center">
              OpenReview is created by the <a href="http://www.iesl.cs.umass.edu/" target="_blank">Information Extraction and Synthesis Laboratory</a>, College of Information and Computer Science, University of Massachusetts Amherst. We gratefully acknowledge the support of the OpenReview sponsors:  Google,  Facebook, NSF, the University of Massachusetts Amherst Center for Data Science, and Center for Intelligent Information Retrieval, as well as the Google Cloud Platform for donating the computing and networking services on which OpenReview.net runs.
            </p>
          </div>
        </div>
      </div>
    </footer>

  <div id="feedback-modal" class="modal fade" tabindex="-1" role="dialog">
    <div class="modal-dialog">
      <div class="modal-content">
        <div class="modal-header">
          <button type="button" class="close" data-dismiss="modal" aria-label="Close"><span aria-hidden="true">×</span></button>
          <h3 class="modal-title">Send Feedback</h3>
        </div>
        <div class="modal-body">
          <p>Enter your feedback below and we'll get back to you as soon as possible.</p>
          <form action="/feedback" method="POST">
            <div class="form-group">
              <input type="email" name="from" class="form-control" placeholder="Email" />
            </div>
            <div class="form-group">
              <input type="text" name="subject" class="form-control" placeholder="Subject" />
            </div>
            <div class="form-group">
              <textarea name="message" class="form-control feedback-input" rows="5" placeholder="Message" required=""></textarea>
            </div>
          <ul id="ui-id-2" tabindex="0" class="ui-menu ui-widget ui-widget-content" style="display: none;"></ul></form>
        </div>
        <div class="modal-footer">
          <button type="button" class="btn btn-default" data-dismiss="modal">Cancel</button>
          <button type="button" class="btn btn-primary">Send</button>
        </div>
      </div>
    </div>
  </div>

  <script type="text/javascript" async="" src="https://www.google-analytics.com/analytics.js"></script><script src="//cdnjs.cloudflare.com/ajax/libs/jquery/2.2.4/jquery.min.js"></script>
  <script>window.jQuery || document.write('&lt;script src="/static/js/vendor/jquery-2.2.4.min.js"&gt;&lt;\/script&gt;')</script>

  <script src="/static/dist/vendor.min.js"></script>
  <script src="/static/dist/templates.min.js"></script>
  <script src="/static/dist/openreview.min.js"></script>

    <script>window.legacyScripts = true;</script>


    <script async="" src="https://www.googletagmanager.com/gtag/js?id=UA-108703919-1"></script>
    <script>
      window.dataLayer = window.dataLayer || [];
      function gtag() { dataLayer.push(arguments); }
      gtag('js', new Date());
      gtag('config', 'UA-108703919-1');
    </script>


<div role="status" aria-live="assertive" aria-relevant="additions" class="ui-helper-hidden-accessible"></div></body></html>