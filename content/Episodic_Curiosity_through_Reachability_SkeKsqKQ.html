<!DOCTYPE html><html xmlns="http://www.w3.org/1999/xhtml" class="no-js"><head>
  <meta charset="utf-8" />
  <meta http-equiv="X-UA-Compatible" content="IE=edge,chrome=1" />
  <meta name="viewport" content="width=device-width, initial-scale=1" />

  <title>Episodic Curiosity through Reachability | OpenReview</title>
  <meta name="description" content="" />

      <meta name="citation_title" content="Episodic Curiosity through Reachability" />
        <meta name="citation_author" content="Anonymous" />
      <meta name="citation_publication_date" content="2018/09/27" />
      <meta name="citation_online_date" content="2018/09/27" />
      <meta name="citation_pdf_url" content="https://openreview.net/pdf?id=SkeK3s0qKQ" />
      <meta name="twitter:card" content="summary" />
      <meta name="twitter:site" content="@openreviewnet" />
      <meta name="og:title" content="Episodic Curiosity through Reachability" />
      <meta name="og:description" content="Rewards are sparse in the real world and most today's reinforcement learning algorithms struggle with such sparsity. One solution to this problem is to allow the agent to create rewards for itself..." />
      <meta name="og:image" content="https://openreview.net/static/images/openreview_logo_512.png" />

  <link rel="stylesheet" href="https://fonts.googleapis.com/css?family=Noto+Sans:400,400i,700,700i" />

  <link rel="stylesheet" href="/static/css/bootstrap.min.css" />
  <link rel="stylesheet" href="/static/css/jquery-ui.min.css" />
  <link rel="stylesheet" href="/static/css/main.min.css" />
</head>

<body class="forum">
  <nav class="navbar navbar-inverse navbar-fixed-top" role="navigation">
    <div class="container">
  
      <div class="navbar-header">
        <button type="button" class="navbar-toggle collapsed" data-toggle="collapse" data-target="#navbar" aria-expanded="false" aria-controls="navbar">
          <span class="sr-only">Toggle navigation</span>
          <span class="icon-bar"></span>
          <span class="icon-bar"></span>
          <span class="icon-bar"></span>
        </button>
        <a class="navbar-brand home push-link" href="/"><strong>OpenReview</strong>.net</a>
      </div>
  
      <div id="navbar" class="navbar-collapse collapse">
        <form class="navbar-form navbar-left profile-search" role="search">
          <div class="form-group has-feedback">
            <input id="search_input" type="text" class="form-control ui-autocomplete-input" placeholder="Search ICLR 2019 Conference" autocomplete="off" />
            <span class="glyphicon glyphicon-search form-control-feedback" aria-hidden="true"></span>
          </div>
  
          <input id="search_group" type="hidden" value="ICLR.cc/2019/Conference" />
          <input id="search_content" type="hidden" value="all" />
        <ul id="ui-id-1" tabindex="0" class="ui-menu ui-widget ui-widget-content ui-autocomplete ui-front" style="display: none;"></ul></form>
  
        <ul class="nav navbar-nav navbar-right">
        
            <li id="user-menu"><a href="/login">Login</a></li>
        </ul>
      </div>
  
    </div>
  </nav>

  <div id="or-banner" class="banner" style="">
  <div class="container">
    <div class="row">
      <div class="col-xs-12"><a href="/group?id=ICLR.cc/2019/Conference" title="Venue Homepage"><img class="icon" src="/static/images/arrow_left.svg" /> Go to <strong>ICLR 2019 Conference</strong> homepage</a></div>
    </div>
  </div>
</div>
<div id="flash-message-container" class="alert alert-danger" role="alert" style="display: none;">
  <div class="container">
    <div class="row">
      <div class="col-xs-12">
        <div class="alert-content">
          <button type="button" class="close" aria-label="Close"><span aria-hidden="true">×</span></button>
        </div>
      </div>
    </div>
  </div>
</div>

<div class="container">
  <div class="row">
    <div class="col-xs-12">
      <main id="content" class="clearfix openbanner-visible legacy-styles"><div id="note_SkeK3s0qKQ" class="note panel"><div class="title_pdf_row clearfix"><h2 class="note_content_title"><a>Episodic Curiosity through Reachability</a> <a class="note_content_pdf" href="/pdf?id=SkeK3s0qKQ" title="Download PDF" target="_blank"><img src="/static/images/pdf_icon_blue.svg" /></a> </h2></div><div class="meta_row"><span class="signatures">Anonymous</span></div><div class="clearfix"><div class="meta_row pull-left"><span class="date item">28 Sep 2018 (modified: 11 Oct 2018)</span><span class="item">ICLR 2019 Conference Blind Submission</span><span class="item">Readers: <span class="readers-icon glyphicon glyphicon-globe"></span> Everyone</span><span class="item"><a class="action-bibtex-modal" data-bibtex="@inproceedings{    &#10;anonymous2019episodic,    &#10;title={Episodic Curiosity through Reachability},    &#10;author={Anonymous},    &#10;booktitle={Submitted to International Conference on Learning Representations},    &#10;year={2019},    &#10;url={https://openreview.net/forum?id=SkeK3s0qKQ},    &#10;note={under review}    &#10;}">Show Bibtex</a></span></div><div class="meta_row meta_actions"></div></div><div class="note_contents"><span class="note_content_field">Abstract: </span><span class="note_content_value">Rewards are sparse in the real world and most today's reinforcement learning algorithms struggle with such sparsity. One solution to this problem is to allow the agent to create rewards for itself --- thus making rewards dense and more suitable for learning. In particular, inspired by curious behaviour in animals, observing something novel could be rewarded with a bonus. Such bonus is summed up with the real task reward --- making it possible for RL algorithms to learn from the combined reward. We propose a new curiosity method which uses episodic memory to form the novelty bonus. To determine the bonus, the current observation is compared with the observations in memory. Crucially, the comparison is done based on how many environment steps it takes to reach the current observation from those in memory --- which incorporates rich information about environment dynamics. This allows us to overcome the known "couch-potato" issues of prior work --- when the agent finds a way to instantly gratify itself by exploiting actions which lead to unpredictable consequences. We test our approach in visually rich 3D environments in ViZDoom and DMLab. In ViZDoom, our agent learns to successfully navigate to a distant goal at least 2 times faster than the state-of-the-art curiosity method ICM. In DMLab, our agent generalizes well to new procedurally generated levels of the game --- reaching the goal at least 2 times more frequently than ICM on test mazes with very sparse reward.</span></div><div class="note_contents"><span class="note_content_field">Keywords: </span><span class="note_content_value">deep learning, reinforcement learning, curiosity, exploration, episodic memory</span></div><div class="note_contents"><span class="note_content_field">TL;DR: </span><span class="note_content_value">We propose a novel model of curiosity based on episodic memory and the ideas of reachability which allows us to overcome the known "couch-potato" issues of prior work.</span></div><div class="reply_row clearfix"><div class="item" id="reply_count">13 Replies</div></div></div><hr class="small" /><div id="note_children"><div class="note_with_children"><div id="note_SygAOUV4pm" class="note panel"><div class="title_pdf_row clearfix"><h2 class="note_content_title"><a>Response to the reviewers</a>  <button class="btn btn-xs btn-default permalink-button" title="Link to this comment" data-permalink-url="https://openreview.net/forum?id=SkeK3s0qKQ&amp;noteId=SygAOUV4pm"><span class="glyphicon glyphicon-link" aria-hidden="true"></span></button></h2></div><div class="meta_row"><span class="signatures">ICLR 2019 Conference Paper731 Authors</span></div><div class="clearfix"><div class="meta_row pull-left"><span class="date item">10 Nov 2018 (modified: 13 Nov 2018)</span><span class="item">ICLR 2019 Conference Paper731 Official Comment</span><span class="item">Readers: <span class="readers-icon glyphicon glyphicon-globe"></span> Everyone</span></div><div class="meta_row meta_actions"></div></div><div class="note_contents"><span class="note_content_field">Comment: </span><span class="note_content_value">We thank the reviewers for their work and their valuable comments. We are happy that the reviewers find our method interesting and innovative (AR1, AR2), note that the paper is well-written and easy to understand (AR1, AR3), and mention that the experiments in our work are well-executed (AR1, AR2).

As for the reviewers’ questions, we would like to highlight two key points (including new interesting results):
1. Pretraining of R-network and its generalization: we further extended our work with online training, which is stable and gives significantly better results with respect to the pre-trained version. Moreover, we have evidence that the R-network generalizes well beyond the areas explored during training. It even generalizes between environments. This is because it "simply" needs to learn to meaningfully compare observations, not "recognize" observations. Please see our reply to AR2 for details.
2. Environment stochasticity: while it is not the focus of our work, we experimented with adding a strong source of stochasticity to the environment, and our method is reasonably robust to it. Please see our reply to AR3 for details.

We respond in detail to each of the reviewers individually in comments to their reviews. We will work on performing the proposed experiments and updating the paper accordingly.</span></div><div class="reply_row clearfix"></div></div><div class="children"></div></div><div class="note_with_children"><div id="note_rJeSKyuo3Q" class="note panel"><div class="title_pdf_row clearfix"><h2 class="note_content_title"><a>A simple novel idea for improving exploration in DRL</a>  <button class="btn btn-xs btn-default permalink-button" title="Link to this comment" data-permalink-url="https://openreview.net/forum?id=SkeK3s0qKQ&amp;noteId=rJeSKyuo3Q"><span class="glyphicon glyphicon-link" aria-hidden="true"></span></button></h2></div><div class="meta_row"><span class="signatures">ICLR 2019 Conference Paper731 AnonReviewer2</span></div><div class="clearfix"><div class="meta_row pull-left"><span class="date item">04 Nov 2018 (modified: 07 Nov 2018)</span><span class="item">ICLR 2019 Conference Paper731 Official Review</span><span class="item">Readers: <span class="readers-icon glyphicon glyphicon-globe"></span> Everyone</span></div><div class="meta_row meta_actions"></div></div><div class="note_contents"><span class="note_content_field">Review: </span><span class="note_content_value">The main idea of this paper is to propose a heuristic method for exploration in deep reinforcement learning. The work is fairly innovative in its approach, where an episodic memory is used to store agent’s observations while rewarding the agent for reaching novel observations not yet stored in memory. The novelty here is determined by a pre-trained network that computes the within k-step-reachability of current observation to the observations stored in memory. The method is quite simple but promising and can be easily integrated with any RL algorithm.

They test their method on a pair of 3D environments, VizDoom and DMLab. The experiments are well executed and analysed. 

Positives:
-	They do a rigorous analysis of parameters, and explicitly count the pre-training interactions with the environment in their learning curves.
-	This method does not hurt when dense environmental rewards are present.
-	The memory buffer is smaller than the episode length, which avoids trivial solutions.
-	The idea of having a discriminator assess distance between states is interesting.

Questions and critics:
-	The tasks explored in this paper are all navigation based tasks, would this method also apply equally successfully to non-navigation domains such as manipulation? 
-	My main concern is that the pre-training of the embedding and comparator networks directly depends on how good the random exploration policy is that collects the data. In navigation domains it makes sense that the random policy could cover the space fairly well, however, this will not be the case for more complex tasks involving more complex dynamics.
-	It was surprising to me that the choice of k does not seem to be that important. As it implicitly defines what “novelty” means for an environment, I would have expected that its value should be calibrated better. Could that be a function of the navigation tasks considered?
-	The DMLab results are not great or comparable to the state-of-the-art methods, which may hinder interpreting how good the policies really are. This was perhaps a conscious choice given they are only interested in early training results, but that seems like a confound.
-	The architecture does not include an RNN which makes certain things very surprising even though they shouldn't (e.g. firing, or moving around a corner, are specifically surprising for ICM) as they cannot be learnt, but perhaps if they had an RNN in the architecture these would be easy to explain? Would be interesting to see what are the authors thoughts on this (apart from their computational complexity argument they mention)?
-	Having the memory contain only information about the current episode with no information transfer between episodes seems a bit strange to me, I would like to hear the motivation behind this?
-	The fact that the memory is reset between episodes, and that the buffer is small, can mean that effectively the method implements some sort of complex pseudo count over meta-states per episode? 
-	The embedding network is only trained during the pre-training phase and frozen during the RL task. This sounds a bit limiting to me: what if the agent starts exploring part of the space that was not covered during pre-training? Obviously this could lead to collapses when allowing to fine-tune it, but I feel this is rather restrictive. Again, I feel that the choice of navigation tasks did not magnify this problem, which would arise more in harder exploration tasks.
-	I think that alluding that their method is similar to babies’ behaviour in their cradle is stretched at best and not a constructive way to motivate their work…
-	In Figure 6 and 7, all individual curves from each seed run are shown, which is a bit distracting. Perhaps showing the mean and std would be a cleaner and easier-to-interpret way to report these results?

Overall, it is a simple and interesting idea and seems quite easy to implement. However, everything is highly dependent on how varying the environment is, how bad the exploration policy used for pre-training is, how good the embeddings are once frozen, and how k, action repeat and memory buffer size interact. Given that the experiments are all navigation based, it makes it hard for me to assess whether this method can work as well in other domains with harder exploration setups.
</span></div><div class="note_contents"><span class="note_content_field">Rating: </span><span class="note_content_value">6: Marginally above acceptance threshold</span></div><div class="note_contents"><span class="note_content_field">Confidence: </span><span class="note_content_value">4: The reviewer is confident but not absolutely certain that the evaluation is correct</span></div><div class="reply_row clearfix"></div></div><div class="children"><div class="note_with_children"><div id="note_S1gFJ_4Npm" class="note panel"><div class="title_pdf_row clearfix"><h2 class="note_content_title"><a>Authors' response [part 1/2]</a>  <button class="btn btn-xs btn-default permalink-button" title="Link to this comment" data-permalink-url="https://openreview.net/forum?id=SkeK3s0qKQ&amp;noteId=S1gFJ_4Npm"><span class="glyphicon glyphicon-link" aria-hidden="true"></span></button></h2></div><div class="meta_row"><span class="signatures">ICLR 2019 Conference Paper731 Authors</span></div><div class="clearfix"><div class="meta_row pull-left"><span class="date item">10 Nov 2018 (modified: 13 Nov 2018)</span><span class="item">ICLR 2019 Conference Paper731 Official Comment</span><span class="item">Readers: <span class="readers-icon glyphicon glyphicon-globe"></span> Everyone</span></div><div class="meta_row meta_actions"></div></div><div class="note_contents"><span class="note_content_field">Comment: </span><span class="note_content_value">We thank the reviewer for their work on the review.

&gt; The tasks explored in this paper are all navigation based tasks, would this method also apply equally successfully to non-navigation domains such as manipulation?

This is a great question and we work on experiments in other domains. However, we would like to point out that the tasks we already have in the paper are both non-trivial and more visually complex than in many other works in the field of sparse-reward exploration.

&gt; My main concern is that the pre-training of the embedding and comparator networks directly depends on how good the random exploration policy is that collects the data. In navigation domains it makes sense that the random policy could cover the space fairly well, however, this will not be the case for more complex tasks involving more complex dynamics.

We agree that for more complex domains randomly collected data may be insufficient and online training of the R-network may be crucial. To address the concerns of the reviewer, we have implemented a version of our algorithm which performs training of R-network online (together with the policy). Preliminary results indicate that such training is possible and does not collapse. It produces results at least as good as pre-training. Thus, we are able to demonstrate that our approach can function with online training, offering the possibility of functioning in domains where collection of random data may be insufficient.

In addition, we would like to point out that the R-network (Embedding + Comparator) can generalize beyond what was seen in the pre-training stage. We have such an experiment in the supplementary section S3 "R-network generalization study". In particular, in Table S4, R-networks trained on levels "Dense 1" and "Sparse + Doors" generalize to the "Very Sparse" environment. The visual gap is quite significant: please compare <a href="https://youtu.be/C5g10cUl7Ew" target="_blank" rel="nofollow">https://youtu.be/C5g10cUl7Ew</a> with https://youtu.be/9J4CzdOz60I, for example. All this is possible because R-network is solving a simple problem of comparing two observations given access to both observations at the same time.

Moreover, in the real world, people typically hand-design the initial exploration policy even for the standard RL methods, let alone the model-based ones (and our method could be considered partially model-based). For example, please take a look at the recent work https://ai.googleblog.com/2018/06/scalable-deep-reinforcement-learning.html (which has just received the best paper award at the Conference on Robotic Learning). Another recent work from ICLR’18 https://openreview.net/forum?id=BkisuzWRW also uses hand-crafted policy for the robotic manipulation task to collect data for training the inverse model of the environment.

&gt; It was surprising to me that the choice of k does not seem to be that important. As it implicitly defines what “novelty” means for an environment, I would have expected that its value should be calibrated better. Could that be a function of the navigation tasks considered? 

Those values of k are still rather small. What we demonstrate in this experiment is that our method is not excessively sensitive to this parameter when it is chosen within a reasonable range.

&gt; The DMLab results are not great or comparable to the state-of-the-art methods, which may hinder interpreting how good the policies really are. This was perhaps a conscious choice given they are only interested in early training results, but that seems like a confound.

As far as we know, SOTA results are achieved by Impala https://arxiv.org/abs/1802.01561 at 1B steps (250M 4-repeated steps). We haven’t yet run our experiments at this scale: we use 20M 4-repeated steps in our PPO setup with 12 actors on GPU, which already takes 2 days to complete. Furthermore, being more sample efficient is an appealing property of more effective exploration as interactions with an environment might be costly in some environments.</span></div><div class="reply_row clearfix"></div></div><div class="children"><div class="note_with_children"><div id="note_r1lgD_E46Q" class="note panel"><div class="title_pdf_row clearfix"><h2 class="note_content_title"><a>Authors' response [part 2/2]</a>  <button class="btn btn-xs btn-default permalink-button" title="Link to this comment" data-permalink-url="https://openreview.net/forum?id=SkeK3s0qKQ&amp;noteId=r1lgD_E46Q"><span class="glyphicon glyphicon-link" aria-hidden="true"></span></button></h2></div><div class="meta_row"><span class="signatures">ICLR 2019 Conference Paper731 Authors</span></div><div class="clearfix"><div class="meta_row pull-left"><span class="date item">10 Nov 2018</span><span class="item">ICLR 2019 Conference Paper731 Official Comment</span><span class="item">Readers: <span class="readers-icon glyphicon glyphicon-globe"></span> Everyone</span></div><div class="meta_row meta_actions"></div></div><div class="note_contents"><span class="note_content_field">Comment: </span><span class="note_content_value">&gt; The architecture does not include an RNN which makes certain things very surprising even though they shouldn't (e.g. firing, or moving around a corner, are specifically surprising for ICM) as they cannot be learnt, but perhaps if they had an RNN in the architecture these would be easy to explain? Would be interesting to see what are the authors thoughts on this (apart from their computational complexity argument they mention)?

We agree it would be interesting to include an RNN into the architecture. As the reviewer mentions, doing so may help with certain kinds of surprising events. This would be worth exploring, but for interpretability of results and connection with past literature we have focused on feedforward architectures. An RNN was not a part of the original ICM approach to computing the reward bonus: the next-state prediction was done based on a few recent frames. In that sense, we followed the reference implementation -- which, as we verify, reproduces the published results. In the follow-up <a href="https://pathak22.github.io/large-scale-curiosity/resources/largeScaleCuriosity2018.pdf," target="_blank" rel="nofollow">https://pathak22.github.io/large-scale-curiosity/resources/largeScaleCuriosity2018.pdf,</a> the authors didn’t use an RNN in the policy either (personal communication with the authors). 

&gt; Having the memory contain only information about the current episode with no information transfer between episodes seems a bit strange to me, I would like to hear the motivation behind this?

The typical goal of RL is to maximize the reward throughout the current episode. The information from other episodes might be coming from a completely different environment/maze (unless you make an assumption that it is the same environment in every episode). If you visited some places in one maze, how would it help you to determine novelty of the current observation in another maze?

&gt; The fact that the memory is reset between episodes, and that the buffer is small, can mean that effectively the method implements some sort of complex pseudo count over meta-states per episode?

Yes, it might be possible to understand the approach in this way. To gain a better understanding of how it works in practice, we created a visualization of the rewards, memory states and the trajectory of the agent during the episode. Please take a look here: https://youtu.be/mphIRR6VsbM. The distribution of states in memory is geometric: older memories are sparser but some are still there. This is enough to learn a reasonable exploration strategy in our environments.

&gt; The embedding network is only trained during the pre-training phase and frozen during the RL task. This sounds a bit limiting to me: what if the agent starts exploring part of the space that was not covered during pre-training? Obviously this could lead to collapses when allowing to fine-tune it, but I feel this is rather restrictive. Again, I feel that the choice of navigation tasks did not magnify this problem, which would arise more in harder exploration tasks.

Please see our comment about online training and generalization above. We did not observe collapses in our new online training experiments, nor in most of our generalization experiments.

&gt; I think that alluding that their method is similar to babies’ behaviour in their cradle is stretched at best and not a constructive way to motivate their work…

We will remove this inspiration.

&gt; In Figure 6 and 7, all individual curves from each seed run are shown, which is a bit distracting. Perhaps showing the mean and std would be a cleaner and easier-to-interpret way to report these results?

We could re-do the plots if the reviewer wishes. However, we noticed some issues with the mean+-std kind of visualization as the distribution at each step is far from looking like a gaussian. In fact, it is clearly multimodal. For example, in Figure 6 it wouldn't be clear if mean &lt; 1.0 means that the trained model doesn't always reach the goal or the training is unstable and some models reach the goal consistently while others fail consistently (the latter is actually the case for the baselines at some points during training).</span></div><div class="reply_row clearfix"></div></div><div class="children"><div class="note_with_children"><div id="note_SkefzFUvpm" class="note panel"><div class="title_pdf_row clearfix"><h2 class="note_content_title"><a>Update on online training</a>  <button class="btn btn-xs btn-default permalink-button" title="Link to this comment" data-permalink-url="https://openreview.net/forum?id=SkeK3s0qKQ&amp;noteId=SkefzFUvpm"><span class="glyphicon glyphicon-link" aria-hidden="true"></span></button></h2></div><div class="meta_row"><span class="signatures">ICLR 2019 Conference Paper731 Authors</span></div><div class="clearfix"><div class="meta_row pull-left"><span class="date item">13 Nov 2018</span><span class="item">ICLR 2019 Conference Paper731 Official Comment</span><span class="item">Readers: <span class="readers-icon glyphicon glyphicon-globe"></span> Everyone</span></div><div class="meta_row meta_actions"></div></div><div class="note_contents"><span class="note_content_field">Comment: </span><span class="note_content_value">After tuning online training of R-network, we obtained significantly improved results with respect to offline training: reward 26 -&gt; 44 on Sparse in DMLab, reward 25 -&gt; 33 on VerySparse in DMLab. Also, results look qualitatively better now: offline training bumps into the walls quite often <a href="https://youtu.be/C5g10cUl7Ew" target="_blank" rel="nofollow">https://youtu.be/C5g10cUl7Ew</a> -&gt; online training almost doesn’t bump into the walls at all https://youtu.be/d2KiaWIJgfU.

Thus, the experimental results justify that collecting data from a policy is ultimately a better way to train the R-network (probably because randomly visited states may be very unbalanced relative to what an agent actually encounters).

Although the online training experiment was on our roadmap, we would like to thank the reviewer for motivating us to do it sooner rather than later! We will include the online training experiments in the paper.</span></div><div class="reply_row clearfix"></div></div><div class="children"></div></div></div></div></div></div></div></div><div class="note_with_children"><div id="note_H1e1YGI9h7" class="note panel"><div class="title_pdf_row clearfix"><h2 class="note_content_title"><a>Not enough motivation why crustily driven approach is in interest.</a>  <button class="btn btn-xs btn-default permalink-button" title="Link to this comment" data-permalink-url="https://openreview.net/forum?id=SkeK3s0qKQ&amp;noteId=H1e1YGI9h7"><span class="glyphicon glyphicon-link" aria-hidden="true"></span></button></h2></div><div class="meta_row"><span class="signatures">ICLR 2019 Conference Paper731 AnonReviewer3</span></div><div class="clearfix"><div class="meta_row pull-left"><span class="date item">03 Nov 2018 (modified: 11 Nov 2018)</span><span class="item">ICLR 2019 Conference Paper731 Official Review</span><span class="item">Readers: <span class="readers-icon glyphicon glyphicon-globe"></span> Everyone</span><a class="note_content_pdf item" href="/revisions?id=H1e1YGI9h7" target="_blank">Show Revisions</a></div><div class="meta_row meta_actions"></div></div><div class="note_contents"><span class="note_content_field">Review: </span><span class="note_content_value">In this paper, the authors study the problem of exploration in RL when the reward process is sparse. They introduce a new curiosity based approach which considers a state novel if it was not visited before and is far from the visited states. They show that their methods perform better than two other approaches, one without curiosity-driven exploration and the second one a one-step curiosity-driven approach. 

The paper is well-written and easy to follow. The authors motivate this work by bringing an example where the state observation might be novel but important. They show that if part of the environment just changes randomly, then there is no need to explore there as much as vanilla curiosity-driven approaches to suggest. The approach in this paper partially addresses this drawback of curiosity-driven approaches. The authors miss the point why the curiosity-driven exploration approaches as in this work are in interest. 

The problem mentioned in this paper can be also solved based on efficient exploration-exploration methods where the distribution of next states is considered rather than the samples themselves. An efficient explorative/exploitative RL agent explores part of state space more if there is uncertainty in the reward and state distribution rather than not being able to predict a particular sample. In curiosity-driven approaches, if the predictability of the next state is considered, all methods are sentenced to failure in stochastic environments. The approach in this paper partially mitigate this problem but for a very specific environment setup, but still, fails if the environment is stochastic. 

</span></div><div class="note_contents"><span class="note_content_field">Rating: </span><span class="note_content_value">6: Marginally above acceptance threshold</span></div><div class="note_contents"><span class="note_content_field">Confidence: </span><span class="note_content_value">4: The reviewer is confident but not absolutely certain that the evaluation is correct</span></div><div class="reply_row clearfix"></div></div><div class="children"><div class="note_with_children"><div id="note_Skl3TfOkpQ" class="note panel"><div class="title_pdf_row clearfix"><h2 class="note_content_title"><a>Authors' response</a>  <button class="btn btn-xs btn-default permalink-button" title="Link to this comment" data-permalink-url="https://openreview.net/forum?id=SkeK3s0qKQ&amp;noteId=Skl3TfOkpQ"><span class="glyphicon glyphicon-link" aria-hidden="true"></span></button></h2></div><div class="meta_row"><span class="signatures">ICLR 2019 Conference Paper731 Authors</span></div><div class="clearfix"><div class="meta_row pull-left"><span class="date item">07 Nov 2018</span><span class="item">ICLR 2019 Conference Paper731 Official Comment</span><span class="item">Readers: <span class="readers-icon glyphicon glyphicon-globe"></span> Everyone</span></div><div class="meta_row meta_actions"></div></div><div class="note_contents"><span class="note_content_field">Comment: </span><span class="note_content_value">&gt; The authors miss the point why the curiosity-driven exploration approaches as in this work are in interest.

It would be helpful if the reviewer could be more specific here.

&gt; The problem mentioned in this paper can be also solved based on efficient exploration-exploration methods where the distribution of next states is considered rather than the samples themselves.

Could the reviewer please provide references to such methods, demonstrated on visually-rich 3D environments?

&gt; In curiosity-driven approaches, if the predictability of the next state is considered, all methods are sentenced to failure in stochastic environments. The approach in this paper partially mitigate this problem but for a very specific environment setup,

ViZDoom and DMLab are standard benchmarks. We used the standard action sets for those benchmarks. Could the reviewer please elaborate more on what is very specific about our environmental setup? 

&gt; but still, fails if the environment is stochastic.

Could the reviewer please be more specific here? That is, how does the method fail in the case that the environment is stochastic?</span></div><div class="reply_row clearfix"></div></div><div class="children"><div class="note_with_children"><div id="note_rJgv2u0kTX" class="note panel"><div class="title_pdf_row clearfix"><h2 class="note_content_title"><a>Clarification</a>  <button class="btn btn-xs btn-default permalink-button" title="Link to this comment" data-permalink-url="https://openreview.net/forum?id=SkeK3s0qKQ&amp;noteId=rJgv2u0kTX"><span class="glyphicon glyphicon-link" aria-hidden="true"></span></button></h2></div><div class="meta_row"><span class="signatures">ICLR 2019 Conference Paper731 AnonReviewer3</span></div><div class="clearfix"><div class="meta_row pull-left"><span class="date item">07 Nov 2018</span><span class="item">ICLR 2019 Conference Paper731 Official Comment</span><span class="item">Readers: <span class="readers-icon glyphicon glyphicon-globe"></span> Everyone</span></div><div class="meta_row meta_actions"></div></div><div class="note_contents"><span class="note_content_field">Comment: </span><span class="note_content_value">I agree there are quite a few papers about curiosity-driven approaches, but they are mainly heuristic approaches for deterministic settings. I would like the authors to motivate and clarify why they use this approach in the stochastic setting. The problem set up (couch-potato) to motivate the approach in this paper is not general enough. What if all the states provide stochastic next state? Then the current method breaks? The curiosity methods extend to the stochastic settings if the curiosity is derived based on distribution mismatch, if it is not, then as the authors also mentioned it results in the couch-potato problem. 

I agree that the authors put effort for their empirical study and showed improvement. But I am not sure the algorithmic idea behind this work provides sufficient contribution. I am willing to change my score if the authors can address these.</span></div><div class="reply_row clearfix"></div></div><div class="children"><div class="note_with_children"><div id="note_Skl0AOEVa7" class="note panel"><div class="title_pdf_row clearfix"><h2 class="note_content_title"><a>Authors' response</a>  <button class="btn btn-xs btn-default permalink-button" title="Link to this comment" data-permalink-url="https://openreview.net/forum?id=SkeK3s0qKQ&amp;noteId=Skl0AOEVa7"><span class="glyphicon glyphicon-link" aria-hidden="true"></span></button></h2></div><div class="meta_row"><span class="signatures">ICLR 2019 Conference Paper731 Authors</span></div><div class="clearfix"><div class="meta_row pull-left"><span class="date item">10 Nov 2018</span><span class="item">ICLR 2019 Conference Paper731 Official Comment</span><span class="item">Readers: <span class="readers-icon glyphicon glyphicon-globe"></span> Everyone</span></div><div class="meta_row meta_actions"></div></div><div class="note_contents"><span class="note_content_field">Comment: </span><span class="note_content_value">We thank the reviewer for their work on the review and their clarification answer.

First, we would like to point out that the stochastic environments are not a focus of our work. Couch-potato behaviour is not unique to stochastic environments. As we show in our experiments, perfectly normal deterministic environments could lead to such behaviour: partial observability or just hardness of future prediction can confuse the surprise-based ICM method (which was chosen as a baseline because it showed state-of-the-art results in visually-rich 3D environment ViZDoom in the prior work <a href="https://arxiv.org/pdf/1705.05363.pdf)." target="_blank" rel="nofollow">https://arxiv.org/pdf/1705.05363.pdf).</a> The randomized TV example is explicitly labeled as a "thought example" in our paper. It was chosen for the sake of illustration. We will clarify this in the paper.

Second, our method can work in an environment where "all the states provide stochastic next state". We created a version of our “VerySparse” environment with a randomized TV on the head-on display. More precisely, the lower right quadrant of the first-person view is occupied with an image from a set of 10 images. The change of the image on the TV is initiated by an additional action, provided to the agent. Using this action leads to a random image from the set to be shown on the TV. Our method still works in this setting: https://youtu.be/UhF1MmusIU4 (preliminary result computed at 7M 4-repeated steps). Additionally, we tried showing random noise on the TV screen at every step -- and our method works there as well: https://youtu.be/4B8VkPA2Mdw.</span></div><div class="reply_row clearfix"></div></div><div class="children"></div></div></div></div></div></div></div></div><div class="note_with_children"><div id="note_rJgSKGHcnm" class="note panel"><div class="title_pdf_row clearfix"><h2 class="note_content_title"><a>Review</a>  <button class="btn btn-xs btn-default permalink-button" title="Link to this comment" data-permalink-url="https://openreview.net/forum?id=SkeK3s0qKQ&amp;noteId=rJgSKGHcnm"><span class="glyphicon glyphicon-link" aria-hidden="true"></span></button></h2></div><div class="meta_row"><span class="signatures">ICLR 2019 Conference Paper731 AnonReviewer1</span></div><div class="clearfix"><div class="meta_row pull-left"><span class="date item">03 Nov 2018 (modified: 17 Nov 2018)</span><span class="item">ICLR 2019 Conference Paper731 Official Review</span><span class="item">Readers: <span class="readers-icon glyphicon glyphicon-globe"></span> Everyone</span><a class="note_content_pdf item" href="/revisions?id=rJgSKGHcnm" target="_blank">Show Revisions</a></div><div class="meta_row meta_actions"></div></div><div class="note_contents"><span class="note_content_field">Review: </span><span class="note_content_value">The authors propose an exploration bonus that is aimed to aid in sparse reward RL problems. The bonus is given by an auxillary network which tries to score whether a candidate observation is difficult to reach with respect to all previously observed novel observations which are stored in a memory buffer. The paper considers many experiments on complex 3D environments. 

The paper is well written and very well illustrated. The method can be clearly understood from the 3 figures and the examples are nice. I think the method is interesting and novel and it is evaluated on a realistic and challenging problem.

It would be good if the authors could further elaborate on the scalability of the method in terms of compute/memory requirements and related to that if the implementation is cumbersome. I didn’t understand well how the method avoids the issue of old memories leaving the buffer. It seems for a large enough environment important observations will eventually become discarded causing a poor approximation of the curiosity bonus? For the large scale experiments I would like to know more rough details of the number of the compute time needed for the method relative to the PPO baseline and the other baseline (e.g. number of nodes for example and how long they run approximately)

Are there any potential issues with adapting the method on 2D environments like Atari? this could permit direct comparisons with several other recently proposed techniques in this area.

The Grid-Oracle result is very interesting and a contribution on it’s own if similar results for complex 3D environments are not published anywhere else. It demonstrates well that exploration bonuses can help drastically in these tasks. I think if possible it would be interesting to have an idea how fast this method converges (number of training steps) and not just the final reward as reported in the tables. Indeed as a general problem the current number of training steps of any methods shown seem to indicate these techniques are too data hungry for non-simulated environments. For some applications (e.g. aimed at sim-to-real transfer) the grid-oracle approach might be a good alternative to consider. I would be interested to know if the authors had some thoughts on this.

Overall I lean towards accept, the method is shown to work on relatively very complex problems in DMLab and VizDoom while most sparse reward solutions proposed are typically evaluated on relatively simpler and unrealistic tasks. I would consider to further increase my score if the authors can address some of the comments. 
</span></div><div class="note_contents"><span class="note_content_field">Rating: </span><span class="note_content_value">8: Top 50% of accepted papers, clear accept</span></div><div class="note_contents"><span class="note_content_field">Confidence: </span><span class="note_content_value">3: The reviewer is fairly confident that the evaluation is correct</span></div><div class="reply_row clearfix"></div></div><div class="children"><div class="note_with_children"><div id="note_B1eCNi44T7" class="note panel"><div class="title_pdf_row clearfix"><h2 class="note_content_title"><a>Authors' response</a>  <button class="btn btn-xs btn-default permalink-button" title="Link to this comment" data-permalink-url="https://openreview.net/forum?id=SkeK3s0qKQ&amp;noteId=B1eCNi44T7"><span class="glyphicon glyphicon-link" aria-hidden="true"></span></button></h2></div><div class="meta_row"><span class="signatures">ICLR 2019 Conference Paper731 Authors</span></div><div class="clearfix"><div class="meta_row pull-left"><span class="date item">10 Nov 2018</span><span class="item">ICLR 2019 Conference Paper731 Official Comment</span><span class="item">Readers: <span class="readers-icon glyphicon glyphicon-globe"></span> Everyone</span></div><div class="meta_row meta_actions"></div></div><div class="note_contents"><span class="note_content_field">Comment: </span><span class="note_content_value">We thank the reviewer for their work on the review.

&gt; It would be good if the authors could further elaborate on the scalability of the method in terms of compute/memory requirements

The most computationally intensive parts of the algorithm are the memory reachability queries. Reachabilities to past memories are computed in parallel via mini-batching. We have shown the algorithm to work reasonably fast with a memory size of 200. For significantly larger memory sizes, one would need to better parallelize reachability computations -- which should be doable. Memory consumption for the stored memories is very modest (400 KB), as we only store 200 of 512-float-embeddings, not the observations.

&gt; and related to that if the implementation is cumbersome.

The implementation is relatively easy. We commit to publishing the source code if the paper is accepted -- this would make adoption even easier.

&gt; I didn’t understand well how the method avoids the issue of old memories leaving the buffer.

Forgetting is unavoidable if the storage size is limited. That said, not all old memories are erased. The distribution of memory age is geometric: so older memories are sparser than the recent ones, but still present. Please see our visualization of the memory state: <a href="https://youtu.be/mphIRR6VsbM." target="_blank" rel="nofollow">https://youtu.be/mphIRR6VsbM.</a> Please note that we denote memories by their location only for visualization purposes, the coordinates are not available to our method.

&gt; It seems for a large enough environment important observations will eventually become discarded causing a poor approximation of the curiosity bonus?

This is true: when revisiting a part of state space that the agent hasn’t been to for a long time, many of the memories from that region may have been discarded and the curiosity bonus may offer more reward for returning to these states. This should not be a problem though: the curiosity bonus would still provide some reactive incentive to move away from recent memories -- because recent memories are always well-represented. And, it is possible that it would be good to incentivise visiting states that haven’t been seen in a long time.

&gt; For the large scale experiments I would like to know more rough details of the number of the compute time needed for the method relative to the PPO baseline and the other baseline (e.g. number of nodes for example and how long they run approximately)

PPO+ICM is 1.09x slower than PPO and PPO+EC (our method) is 1.84x slower than PPO. As for the number of parameters, R-network brings 13M trainable variables, while PPO alone was 1.7M and PPO+ICM was 2M. That said, we have spent almost no effort optimizing it in terms of speed/parameters, so it is likely easy to make improvements in this respect. It’s quite likely that we do not need a Resnet-18 for the R-network -- a much simpler model may work as well. In this paper, we just followed the setup for the R-network from prior work https://arxiv.org/abs/1803.00653 because it was shown to perform well, but there is no evidence that this setup is necessary. 

&gt; Are there any potential issues with adapting the method on 2D environments like Atari? this could permit direct comparisons with several other recently proposed techniques in this area.

We haven't tried it for Atari, so it is hard to predict. That said, we try to focus on more visually complex environments. In Atari, there is always a danger that the method would exploit exact observation repeatability. One recent work https://arxiv.org/pdf/1606.04460.pdf estimated this repeatability to reach 60% in some games, and &gt; 10% in many. This creates a dangerous incentive for the exploration algorithms to brute-force this vulnerability. On the other hand, in DMLab, such repeatability was estimated by the same work as &lt; 0.1%.

&gt; The Grid-Oracle result is very interesting and a contribution on it’s own… I think if possible it would be interesting to have an idea how fast this method converges (number of training steps)

We don’t include Grid-Oracle into the plots because otherwise it is hard to see the difference between the comparable methods (note that it is not fair to compare Oracle with them). That said, Oracle converges faster than any other method -- but requires privileged information. To give specific numbers, after 5M 4-repeated steps Grid-Oracle reaches approximately reward 40 in the "Sparse" environment, reward 35 in the "Very Sparse" environment and reward 20 in the "Sparse+Doors" environment. This is way higher than any other method in our study. We will include those numbers into the manuscript.

&gt; For some applications (e.g. aimed at sim-to-real transfer) the grid-oracle approach might be a good alternative to consider.

The Oracle could be useful in situations where additional information is available about the environment. However, it is not universal, so we have not focused on the possibility of taking advantage of privileged information in the current manuscript.</span></div><div class="reply_row clearfix"></div></div><div class="children"></div></div></div></div><div class="note_with_children"><div id="note_H1leA4f127" class="note panel"><div class="title_pdf_row clearfix"><h2 class="note_content_title"><a>Mechanism Behind Generalization of Exploration?</a>  <button class="btn btn-xs btn-default permalink-button" title="Link to this comment" data-permalink-url="https://openreview.net/forum?id=SkeK3s0qKQ&amp;noteId=H1leA4f127"><span class="glyphicon glyphicon-link" aria-hidden="true"></span></button></h2></div><div class="meta_row"><span class="signatures">(anonymous)</span></div><div class="clearfix"><div class="meta_row pull-left"><span class="date item">25 Oct 2018</span><span class="item">ICLR 2019 Conference Paper731 Public Comment</span><span class="item">Readers: <span class="readers-icon glyphicon glyphicon-globe"></span> Everyone</span></div><div class="meta_row meta_actions"></div></div><div class="note_contents"><span class="note_content_field">Comment: </span><span class="note_content_value">Excellent work! I was wondering if the authors would be able to provide some intuition for a phenomenon that I find a bit puzzling in this paper. I noticed that this algorithm is tested on a set of hold-out levels that are not seen during training. Given my understanding of the approach, I am not exactly sure why this would work at all. My understanding is that the proposed algorithm encourages exploration by rewarding the visitation of states that are distant from the ones that have been seen during each episode in training. It is sensible that this would encourage policies to learn to explore in the training environments, since they are receiving rewards for seeing rare states and learning to move towards those, but it is not clear why this would work in unseen environments. Why would policies that move towards "rare" states in the training set, work at all on a separate testing set? I suspect that perhaps the policies are learning some sort of generalized exploration behavior due to the wide variety of environments seen during training? It would be great if the authors could shed more light on this.</span></div><div class="reply_row clearfix"></div></div><div class="children"><div class="note_with_children"><div id="note_rkxvwZU1nX" class="note panel"><div class="title_pdf_row clearfix"><h2 class="note_content_title"><a>Authors' response</a>  <button class="btn btn-xs btn-default permalink-button" title="Link to this comment" data-permalink-url="https://openreview.net/forum?id=SkeK3s0qKQ&amp;noteId=rkxvwZU1nX"><span class="glyphicon glyphicon-link" aria-hidden="true"></span></button></h2></div><div class="meta_row"><span class="signatures">ICLR 2019 Conference Paper731 Authors</span></div><div class="clearfix"><div class="meta_row pull-left"><span class="date item">25 Oct 2018</span><span class="item">ICLR 2019 Conference Paper731 Official Comment</span><span class="item">Readers: <span class="readers-icon glyphicon glyphicon-globe"></span> Everyone</span></div><div class="meta_row meta_actions"></div></div><div class="note_contents"><span class="note_content_field">Comment: </span><span class="note_content_value">Thank you for your interest! Indeed, the policy learns generalized exploration behaviour, because it has seen approximately 11000 unique environments (differing in layouts and sets of textures) during 20M steps of training. All those environments are generated procedurally by the DMLab engine. Moreover, DMLab has a mechanism to ensure uniqueness and disjoint train/validation/test splits. However, please keep in mind that all those environments are still generated from some distribution implemented inside the DMLab engine.</span></div><div class="reply_row clearfix"></div></div><div class="children"></div></div></div></div></div></main></div>
  </div>
</div>


    <!-- Footer -->
    <footer class="sitemap">
      <div class="container">
        <div class="row hidden-xs">
          <div class="col-sm-4">
            <ul class="list-unstyled">
              <li><a href="/" class="home">Home</a></li>
              <li><a href="/venues">All Venues</a></li>
            </ul>
          </div>
    
          <div class="col-sm-4">
            <ul class="list-unstyled">
              <li><a href="/about">About OpenReview</a></li>
              <li><a href="/contact">Contact</a></li>
              <li><a href="#" data-toggle="modal" data-target="#feedback-modal">Feedback</a></li>
            </ul>
          </div>
    
          <div class="col-sm-4">
            <ul class="list-unstyled">
              <li><a href="/terms">Terms of Service</a></li>
              <li><a href="/privacy">Privacy Policy</a></li>
            </ul>
          </div>
        </div>
    
        <div class="row visible-xs-block">
          <div class="col-xs-6">
            <ul class="list-unstyled">
              <li><a href="/" class="home">Home</a></li>
              <li><a href="/about">About OpenReview</a></li>
              <li><a href="#" data-toggle="modal" data-target="#feedback-modal">Feedback</a></li>
            </ul>
          </div>
    
          <div class="col-xs-6">
            <ul class="list-unstyled">
              <li><a href="/contact">Contact</a></li>
              <li><a href="/terms">Terms of Service</a></li>
              <li><a href="/privacy">Privacy Policy</a></li>
            </ul>
          </div>
        </div>
      </div>
    </footer>
    
    <footer class="sponsor">
      <div class="container">
        <div class="row">
          <div class="col-sm-10 col-sm-offset-1">
            <p class="text-center">
              OpenReview is created by the <a href="http://www.iesl.cs.umass.edu/" target="_blank">Information Extraction and Synthesis Laboratory</a>, College of Information and Computer Science, University of Massachusetts Amherst. We gratefully acknowledge the support of the OpenReview sponsors:  Google,  Facebook, NSF, the University of Massachusetts Amherst Center for Data Science, and Center for Intelligent Information Retrieval, as well as the Google Cloud Platform for donating the computing and networking services on which OpenReview.net runs.
            </p>
          </div>
        </div>
      </div>
    </footer>

  <div id="feedback-modal" class="modal fade" tabindex="-1" role="dialog">
    <div class="modal-dialog">
      <div class="modal-content">
        <div class="modal-header">
          <button type="button" class="close" data-dismiss="modal" aria-label="Close"><span aria-hidden="true">×</span></button>
          <h3 class="modal-title">Send Feedback</h3>
        </div>
        <div class="modal-body">
          <p>Enter your feedback below and we'll get back to you as soon as possible.</p>
          <form action="/feedback" method="POST">
            <div class="form-group">
              <input type="email" name="from" class="form-control" placeholder="Email" />
            </div>
            <div class="form-group">
              <input type="text" name="subject" class="form-control" placeholder="Subject" />
            </div>
            <div class="form-group">
              <textarea name="message" class="form-control feedback-input" rows="5" placeholder="Message" required=""></textarea>
            </div>
          <ul id="ui-id-2" tabindex="0" class="ui-menu ui-widget ui-widget-content" style="display: none;"></ul></form>
        </div>
        <div class="modal-footer">
          <button type="button" class="btn btn-default" data-dismiss="modal">Cancel</button>
          <button type="button" class="btn btn-primary">Send</button>
        </div>
      </div>
    </div>
  </div>

  <script type="text/javascript" async="" src="https://www.google-analytics.com/analytics.js"></script><script src="//cdnjs.cloudflare.com/ajax/libs/jquery/2.2.4/jquery.min.js"></script>
  <script>window.jQuery || document.write('&lt;script src="/static/js/vendor/jquery-2.2.4.min.js"&gt;&lt;\/script&gt;')</script>

  <script src="/static/dist/vendor.min.js"></script>
  <script src="/static/dist/templates.min.js"></script>
  <script src="/static/dist/openreview.min.js"></script>

    <script>window.legacyScripts = true;</script>


    <script async="" src="https://www.googletagmanager.com/gtag/js?id=UA-108703919-1"></script>
    <script>
      window.dataLayer = window.dataLayer || [];
      function gtag() { dataLayer.push(arguments); }
      gtag('js', new Date());
      gtag('config', 'UA-108703919-1');
    </script>


<div role="status" aria-live="assertive" aria-relevant="additions" class="ui-helper-hidden-accessible"></div></body></html>