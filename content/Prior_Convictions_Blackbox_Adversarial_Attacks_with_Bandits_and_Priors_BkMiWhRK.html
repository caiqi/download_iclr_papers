<!DOCTYPE html><html xmlns="http://www.w3.org/1999/xhtml" class="no-js"><head>
  <meta charset="utf-8" />
  <meta http-equiv="X-UA-Compatible" content="IE=edge,chrome=1" />
  <meta name="viewport" content="width=device-width, initial-scale=1" />

  <title>Prior Convictions: Black-box Adversarial Attacks with Bandits and Priors | OpenReview</title>
  <meta name="description" content="" />

      <meta name="citation_title" content="Prior Convictions: Black-box Adversarial Attacks with Bandits and Priors" />
        <meta name="citation_author" content="Anonymous" />
      <meta name="citation_publication_date" content="2018/09/27" />
      <meta name="citation_online_date" content="2018/09/27" />
      <meta name="citation_pdf_url" content="https://openreview.net/pdf?id=BkMiWhR5K7" />
      <meta name="twitter:card" content="summary" />
      <meta name="twitter:site" content="@openreviewnet" />
      <meta name="og:title" content="Prior Convictions: Black-box Adversarial Attacks with Bandits and..." />
      <meta name="og:description" content="We study the problem of generating adversarial examples in a black-box setting in which only loss-oracle access to a model is available. We introduce a framework that conceptually unifies much of..." />
      <meta name="og:image" content="https://openreview.net/static/images/openreview_logo_512.png" />

  <link rel="stylesheet" href="https://fonts.googleapis.com/css?family=Noto+Sans:400,400i,700,700i" />

  <link rel="stylesheet" href="/static/css/bootstrap.min.css" />
  <link rel="stylesheet" href="/static/css/jquery-ui.min.css" />
  <link rel="stylesheet" href="/static/css/main.min.css" />
</head>

<body class="forum">
  <nav class="navbar navbar-inverse navbar-fixed-top" role="navigation">
    <div class="container">
  
      <div class="navbar-header">
        <button type="button" class="navbar-toggle collapsed" data-toggle="collapse" data-target="#navbar" aria-expanded="false" aria-controls="navbar">
          <span class="sr-only">Toggle navigation</span>
          <span class="icon-bar"></span>
          <span class="icon-bar"></span>
          <span class="icon-bar"></span>
        </button>
        <a class="navbar-brand home push-link" href="/"><strong>OpenReview</strong>.net</a>
      </div>
  
      <div id="navbar" class="navbar-collapse collapse">
        <form class="navbar-form navbar-left profile-search" role="search">
          <div class="form-group has-feedback">
            <input id="search_input" type="text" class="form-control ui-autocomplete-input" placeholder="Search ICLR 2019 Conference" autocomplete="off" />
            <span class="glyphicon glyphicon-search form-control-feedback" aria-hidden="true"></span>
          </div>
  
          <input id="search_group" type="hidden" value="ICLR.cc/2019/Conference" />
          <input id="search_content" type="hidden" value="all" />
        <ul id="ui-id-1" tabindex="0" class="ui-menu ui-widget ui-widget-content ui-autocomplete ui-front" style="display: none;"></ul></form>
  
        <ul class="nav navbar-nav navbar-right">
        
            <li id="user-menu"><a href="/login">Login</a></li>
        </ul>
      </div>
  
    </div>
  </nav>

  <div id="or-banner" class="banner" style="">
  <div class="container">
    <div class="row">
      <div class="col-xs-12"><a href="/group?id=ICLR.cc/2019/Conference" title="Venue Homepage"><img class="icon" src="/static/images/arrow_left.svg" /> Go to <strong>ICLR 2019 Conference</strong> homepage</a></div>
    </div>
  </div>
</div>
<div id="flash-message-container" class="alert alert-danger" role="alert" style="display: none;">
  <div class="container">
    <div class="row">
      <div class="col-xs-12">
        <div class="alert-content">
          <button type="button" class="close" aria-label="Close"><span aria-hidden="true">Ã—</span></button>
        </div>
      </div>
    </div>
  </div>
</div>

<div class="container">
  <div class="row">
    <div class="col-xs-12">
      <main id="content" class="clearfix openbanner-visible legacy-styles"><div id="note_BkMiWhR5K7" class="note panel"><div class="title_pdf_row clearfix"><h2 class="note_content_title"><a>Prior Convictions: Black-box Adversarial Attacks with Bandits and Priors</a> <a class="note_content_pdf" href="/pdf?id=BkMiWhR5K7" title="Download PDF" target="_blank"><img src="/static/images/pdf_icon_blue.svg" /></a> </h2></div><div class="meta_row"><span class="signatures">Anonymous</span></div><div class="clearfix"><div class="meta_row pull-left"><span class="date item">28 Sep 2018 (modified: 11 Oct 2018)</span><span class="item">ICLR 2019 Conference Blind Submission</span><span class="item">Readers: <span class="readers-icon glyphicon glyphicon-globe"></span> Everyone</span><span class="item"><a class="action-bibtex-modal" data-bibtex="@inproceedings{    &#10;anonymous2019prior,    &#10;title={Prior Convictions: Black-box Adversarial Attacks with Bandits and Priors},    &#10;author={Anonymous},    &#10;booktitle={Submitted to International Conference on Learning Representations},    &#10;year={2019},    &#10;url={https://openreview.net/forum?id=BkMiWhR5K7},    &#10;note={under review}    &#10;}">Show Bibtex</a></span></div><div class="meta_row meta_actions"></div></div><div class="note_contents"><span class="note_content_field">Abstract: </span><span class="note_content_value">We study the problem of generating adversarial examples in a black-box setting in which only loss-oracle access to a model is available. We introduce a framework that conceptually unifies much of the existing work on black-box attacks, and demonstrate that the current state-of-the-art methods are optimal in a natural sense. Despite this optimality, we show how to improve black-box attacks by bringing a new element into the problem: gradient priors. We give a bandit optimization-based algorithm that allows us to seamlessly integrate any such priors, and we explicitly identify and incorporate two examples. The resulting methods use two to four times fewer queries and fail two to six times less than the current state-of-the-art. The code for reproducing our work is available at <a href="https://git.io/fAjOJ." target="_blank" rel="nofollow">https://git.io/fAjOJ.</a></span></div><div class="note_contents"><span class="note_content_field">Keywords: </span><span class="note_content_value">adversarial examples, gradient estimation, black-box attacks, model-based optimization, bandit optimization</span></div><div class="note_contents"><span class="note_content_field">TL;DR: </span><span class="note_content_value">We present a unifying view on black-box adversarial attacks as a gradient estimation problem, and then present a framework (based on bandits optimization) to integrate priors into gradient estimation, leading to significantly increased performance.</span></div><div class="reply_row clearfix"><div class="item" id="reply_count">7 Replies</div></div></div><hr class="small" /><div id="note_children"><div class="note_with_children"><div id="note_BJx1HlJJpQ" class="note panel"><div class="title_pdf_row clearfix"><h2 class="note_content_title"><a>conditionally accept</a>  <button class="btn btn-xs btn-default permalink-button" title="Link to this comment" data-permalink-url="https://openreview.net/forum?id=BkMiWhR5K7&amp;noteId=BJx1HlJJpQ"><span class="glyphicon glyphicon-link" aria-hidden="true"></span></button></h2></div><div class="meta_row"><span class="signatures">ICLR 2019 Conference Paper1206 AnonReviewer1</span></div><div class="clearfix"><div class="meta_row pull-left"><span class="date item">06 Nov 2018 (modified: 07 Nov 2018)</span><span class="item">ICLR 2019 Conference Paper1206 Official Review</span><span class="item">Readers: <span class="readers-icon glyphicon glyphicon-globe"></span> Everyone</span></div><div class="meta_row meta_actions"></div></div><div class="note_contents"><span class="note_content_field">Review: </span><span class="note_content_value">This paper formulates the black-box adversarial attack as a gradient estimation
problem, and provide some theoretical analysis to show the optimality of an
existing gradient estimation method (Neural Evolution Strategies) for black-box
attacks.

This paper also proposes two additional methods to reduce the number of queries
in black-box attack, by exploiting the spacial and temporal correlations in
gradients. They consider these techniques as priors to gradients, and a bandit
optimization based method is proposed to update these priors. 

The ideas used in this paper are not entirely new. For example, the main
gradient estimation method is the same as NES (Ilyas et al. 2017);
data-dependent priors using spatially local similarities was used in Chen et
al. 2017.  But I have no concern with this and the nice thing of this paper is 
to put these tricks under an unified theoretical framework, which I really 
appreciate.

Experiments on black-box attacks to Inception-v3 model show that the proposed
bandit based attack can significantly reduces the number of queries (2-4 times
fewer) when compared with NES. 

Overall, the paper is well written and ideas are well presented.
I have a few concerns:

1) In Figure 2, the authors show that there are strong correlations between the
gradients of current and previous steps. Such correlation heavily depends on
the selection of step size.  Imagine that the step size is sufficiently large,
such that when we arrive at a new point for the next iteration, the
optimization landscape is sufficiently changed and the new gradient is vastly
different than the previous one. On the other hand, when using a very small
step-size close to 0, gradients between consecutive steps will be almost the
same. By changing step-size I can show any degree of correlation.  I am not
sure if the improvement of Bandit_T comes from a specific selection of
step-size. More empirical evidence on this need to be shown - for example, run
Bandit_T and NES with different step sizes and observe the number of queries
required.

2) This paper did not compare with many other recent works which claim to
reduce query numbers significantly in black-box attack. For example, [1]
proposes "random feature grouping" and use PCA for reducing queries, and [2]
uses a good gradient estimator with autoencoder. I believe the proposed method
can beat them, but the authors should include at least one more baseline to 
convince the readers that the proposed method is indeed a state-of-the-art.

3) Additionally, the results in this paper are only shown on a single model
(Inception-v3), and it is hard to compare the results directly with many other
recent works. I suggest adding at least two more models for comparison (most
black-box attack papers also include MNIST and CIFAR, which should be easy to
add quickly). These numbers can be put in appendix.

Overall, this is a great paper, offering good insights on black-box adversarial
attack and provide some interesting theoretical analysis. However currently it
is still missing some important experimental results as mentioned above, and
not ready to be published as a high quality conference paper. I conditionally
accept this paper as long as sufficient experiments can be added during the
discussion period.


[1] Exploring the Space of Black-box Attacks on Deep Neural Networks, by Arjun
Nitin Bhagoji, Warren He, Bo Li and Dawn Song, <a href="https://arxiv.org/abs/1712.09491" target="_blank" rel="nofollow">https://arxiv.org/abs/1712.09491</a>
(conference version accepted by ECCV 2018)

[2] AutoZOOM: Autoencoder-based Zeroth Order Optimization Method for Attacking
Black-box Neural Networks, by Chun-Chen Tu, Paishun Ting, Pin-Yu Chen, Sijia
Liu, Huan Zhang, Jinfeng Yi, Cho-Jui Hsieh, Shin-Ming Cheng,
https://arxiv.org/abs/1805.11770
</span></div><div class="note_contents"><span class="note_content_field">Rating: </span><span class="note_content_value">5: Marginally below acceptance threshold</span></div><div class="note_contents"><span class="note_content_field">Confidence: </span><span class="note_content_value">5: The reviewer is absolutely certain that the evaluation is correct and very familiar with the relevant literature</span></div><div class="reply_row clearfix"></div></div><div class="children"><div class="note_with_children"><div id="note_B1ead6lupQ" class="note panel"><div class="title_pdf_row clearfix"><h2 class="note_content_title"><a>Response</a>  <button class="btn btn-xs btn-default permalink-button" title="Link to this comment" data-permalink-url="https://openreview.net/forum?id=BkMiWhR5K7&amp;noteId=B1ead6lupQ"><span class="glyphicon glyphicon-link" aria-hidden="true"></span></button></h2></div><div class="meta_row"><span class="signatures">ICLR 2019 Conference Paper1206 Authors</span></div><div class="clearfix"><div class="meta_row pull-left"><span class="date item">13 Nov 2018</span><span class="item">ICLR 2019 Conference Paper1206 Official Comment</span><span class="item">Readers: <span class="readers-icon glyphicon glyphicon-globe"></span> Everyone</span></div><div class="meta_row meta_actions"></div></div><div class="note_contents"><span class="note_content_field">Comment: </span><span class="note_content_value">Thank you for the detailed comments, we will be sure to make these changes in the final version of the paper. As the reviewer correctly identifies, we consider the theoretical framework of online optimization as a basis for all black-box attacks to be one of our most profound contributions. That said, in order to improve the quality of the experimental results, we have addressed and added each suggested experiment. Specifically:

1) We thank the reviewer for raising this---we initially only used the default NES step size (from Ilyas et al) to evaluate the temporal correlation. To give a fuller picture on how this temporal correlation relates with the step size, we have added a new plot in the appendix, which shows the average correlation on a trajectory as a function of the step size. 

2) To address this, we have added a table (in the Appendix) which compares our query-efficiency against that of [1] and [2]. It should also be noted, however, that both [1] and [2] can be integrated as "priors" on the gradient; in particular, that the gradient lays in some low-dimensional subspace. Our framework gives us a way to formalize these assumptions, and measure how empirically valid they are in order to find better and better black-box attacks.

3) We have also added results on ResNet-50 and VGG-16 on ImageNet, and have also benchmarked our attack on all three classifiers (Inceptionv3, ResNet-50, VGG-16) on CIFAR as well.

We will be sure to comment again with a revision when the experiments are complete and integrated into the paper. We thank the reviewer again for the valuable suggestions.

</span></div><div class="reply_row clearfix"></div></div><div class="children"></div></div></div></div><div class="note_with_children"><div id="note_B1lDhBt52X" class="note panel"><div class="title_pdf_row clearfix"><h2 class="note_content_title"><a>Good paper, low confidence.</a>  <button class="btn btn-xs btn-default permalink-button" title="Link to this comment" data-permalink-url="https://openreview.net/forum?id=BkMiWhR5K7&amp;noteId=B1lDhBt52X"><span class="glyphicon glyphicon-link" aria-hidden="true"></span></button></h2></div><div class="meta_row"><span class="signatures">ICLR 2019 Conference Paper1206 AnonReviewer3</span></div><div class="clearfix"><div class="meta_row pull-left"><span class="date item">03 Nov 2018 (modified: 07 Nov 2018)</span><span class="item">ICLR 2019 Conference Paper1206 Official Review</span><span class="item">Readers: <span class="readers-icon glyphicon glyphicon-globe"></span> Everyone</span></div><div class="meta_row meta_actions"></div></div><div class="note_contents"><span class="note_content_field">Review: </span><span class="note_content_value">Paper formalizes the gradient estimation problem in a black-box setting, and provs the equivalence of least Squares with NES. It then improves on state of the art by using priors coupled with a bandit optimization technique.

The paper is well written. The idea of using priors to improve adversarial gradient attacks is an enticing idea. The results seem convincing.

Comments:
- I missed how data dependent prior is factored into the algorithms 1-3. Is it by the choice of d? I suggest a clearer explanation.
- In fig 4, I was confused that the loss of the methods is increasing. it took me a minute to realize this is the maximized adversarial loss, and thus higher is better. you may want to spell this out for clarity. I typically associate lower loss with better algorithms.
- I am confused by Fig 4c. If I am comparing g to g*, I do expect a high cosine similarity. cos = 1 is the best. Why is correlation so small? and why is it 0 for NES? You may also want to offer additional insight in the text explaining 4c. 

Minor comments:
- Is table one misplaced?
- The symbol for "boundary of set U" may be confused with a partial derivative symbol
- first paragraph of 2.4: "our estimator a sufficiently". something missing?
- "It is the actions g_t (equal to v_t) which..." refering to g_t as actions is confusing. Although may be technically correct in bandit setting
- Further explain the need for the projection of algorithm 3, line 7.
- Fig 4: refer to true gradient as g*

Caveat: Although I am well versed in bandits, I am not familiar with adversarial training and neural network literature. There is a chance I may have misevaluated central concepts of the paper.</span></div><div class="note_contents"><span class="note_content_field">Rating: </span><span class="note_content_value">8: Top 50% of accepted papers, clear accept</span></div><div class="note_contents"><span class="note_content_field">Confidence: </span><span class="note_content_value">2: The reviewer is willing to defend the evaluation, but it is quite likely that the reviewer did not understand central parts of the paper</span></div><div class="reply_row clearfix"></div></div><div class="children"><div class="note_with_children"><div id="note_rJgK3ebOpX" class="note panel"><div class="title_pdf_row clearfix"><h2 class="note_content_title"><a>Response</a>  <button class="btn btn-xs btn-default permalink-button" title="Link to this comment" data-permalink-url="https://openreview.net/forum?id=BkMiWhR5K7&amp;noteId=rJgK3ebOpX"><span class="glyphicon glyphicon-link" aria-hidden="true"></span></button></h2></div><div class="meta_row"><span class="signatures">ICLR 2019 Conference Paper1206 Authors</span></div><div class="clearfix"><div class="meta_row pull-left"><span class="date item">13 Nov 2018</span><span class="item">ICLR 2019 Conference Paper1206 Official Comment</span><span class="item">Readers: <span class="readers-icon glyphicon glyphicon-globe"></span> Everyone</span></div><div class="meta_row meta_actions"></div></div><div class="note_contents"><span class="note_content_field">Comment: </span><span class="note_content_value">We thank the reviewer for the comments!

We address the main points below:

&gt; Data dependent prior in pseudocode: Yes it is in fact by choice of d, but we agree this can be made clearer in the pseudocode. We will make sure to describe this more clearly in our final paper.

&gt; Figure 4: We will make sure to update this and be more explicit.

&gt; Figure 4c (low cosine similarity): Remarkably, for black-box attacks, though higher cosine similarity is better, the threshold for a successful adversarial attack (in terms of cosine similarity) is extremely low. In particular, for NES, the cosine similarity (as you mentioned) is almost 0, but the gradient estimates *still* result in a successful attack! We show that using our method leads to significantly better estimates of the gradient, though as one would expect in such a query-deficient domain (100s of queries vs 3*10e5 dimensional images), still pretty poor.

We will also be sure to address all of the minor comments in our final paper. We thank the reviewer again for the useful comments and suggestions.
</span></div><div class="reply_row clearfix"></div></div><div class="children"></div></div></div></div><div class="note_with_children"><div id="note_B1xIAu-qhm" class="note panel"><div class="title_pdf_row clearfix"><h2 class="note_content_title"><a>A decent paper</a>  <button class="btn btn-xs btn-default permalink-button" title="Link to this comment" data-permalink-url="https://openreview.net/forum?id=BkMiWhR5K7&amp;noteId=B1xIAu-qhm"><span class="glyphicon glyphicon-link" aria-hidden="true"></span></button></h2></div><div class="meta_row"><span class="signatures">ICLR 2019 Conference Paper1206 AnonReviewer2</span></div><div class="clearfix"><div class="meta_row pull-left"><span class="date item">03 Nov 2018 (modified: 07 Nov 2018)</span><span class="item">ICLR 2019 Conference Paper1206 Official Review</span><span class="item">Readers: <span class="readers-icon glyphicon glyphicon-globe"></span> Everyone</span></div><div class="meta_row meta_actions"></div></div><div class="note_contents"><span class="note_content_field">Review: </span><span class="note_content_value">This paper proposes to employ the bandit optimization based approach for the generation of adversarial examples under the loss accessible black-box situation. The authors examine the feasibility of using the step and spatial dependence of the image gradients as the prior information for the estimation of true gradients. The experimental results show that the proposed method out-performs the Natural evolution strategies method with a large margin.

Although I think this paper is a decent paper that deserves an acceptance, there are several concernings:

1. Since the bound given in Theorem 1 is related to the square root of k/d, I wonder if the right-hand side could become "vanishingly small", in the case such as k=10000 and d=268203. I wish the authors could explain more about the significance of this Theorem, or provide numerical results (which could be hard).

2. Indeed I am not sure if Section 2.4 is closely related to the main topic of this paper, these theoretical results seem to be not helpful in convincing the readers about the idea of gradient priors. Also, the length of the paper is one of the reasons for the rating.

3. In the experimental results, what is the difference between one "query" and one "iteration"? It looks like in one iteration, the Algorithm 2 queries twice?</span></div><div class="note_contents"><span class="note_content_field">Rating: </span><span class="note_content_value">5: Marginally below acceptance threshold</span></div><div class="note_contents"><span class="note_content_field">Confidence: </span><span class="note_content_value">3: The reviewer is fairly confident that the evaluation is correct</span></div><div class="reply_row clearfix"></div></div><div class="children"><div class="note_with_children"><div id="note_rke9oWZuam" class="note panel"><div class="title_pdf_row clearfix"><h2 class="note_content_title"><a>Response</a>  <button class="btn btn-xs btn-default permalink-button" title="Link to this comment" data-permalink-url="https://openreview.net/forum?id=BkMiWhR5K7&amp;noteId=rke9oWZuam"><span class="glyphicon glyphicon-link" aria-hidden="true"></span></button></h2></div><div class="meta_row"><span class="signatures">ICLR 2019 Conference Paper1206 Authors</span></div><div class="clearfix"><div class="meta_row pull-left"><span class="date item">13 Nov 2018</span><span class="item">ICLR 2019 Conference Paper1206 Official Comment</span><span class="item">Readers: <span class="readers-icon glyphicon glyphicon-globe"></span> Everyone</span></div><div class="meta_row meta_actions"></div></div><div class="note_contents"><span class="note_content_field">Comment: </span><span class="note_content_value">We thank the reviewer for the detailed comments on the paper. We address the main points below:

1. Typically black-box adversarial attacks are executed in a multi-step fashion, i.e. by using small numbers of queries per gradient estimates, and taking several gradient estimate steps (Ilyas et al, the NES-based attack, for example, uses 50 queries per gradient estimate). While it may be possible to prove tighter bounds, in the 50-query regime with d=268203, the bound is actually rather tight. (Furthermore, during our own preliminary experimentation, least-squares attacks usually performed identically to NES).

2. Section 2.4 is meant to illustrate that without priors, we have essentially hit the limit of query-efficiency in black-box attacks. In particular, NES, which we found to be the current state-of-the-art, actually turns out to be approximately optimal, even from a theoretical perspective. This motivates us to take a new look on adversarial example generation, breaking through this optimality by introducing new information into the problem.

Without the proof in Section 2.4, one could reasonably hope that there are simply better gradient estimators that we can use as a drop-in replacement for NES. The theorems we prove there instead motivate our bandit optimization-based view. 

3. One iteration constitutes two queries (which are used for a variance-reduced gradient estimate via antithetic sampling). In general, the query count refers to queries of the classifier, whereas iteration counts the number of times that we take an estimated gradient step.

We hope the above points clarify the reviewer's concerns, and thank the reviewer again for the detailed feedback.</span></div><div class="reply_row clearfix"></div></div><div class="children"></div></div></div></div></div></main></div>
  </div>
</div>


    <!-- Footer -->
    <footer class="sitemap">
      <div class="container">
        <div class="row hidden-xs">
          <div class="col-sm-4">
            <ul class="list-unstyled">
              <li><a href="/" class="home">Home</a></li>
              <li><a href="/venues">All Venues</a></li>
            </ul>
          </div>
    
          <div class="col-sm-4">
            <ul class="list-unstyled">
              <li><a href="/about">About OpenReview</a></li>
              <li><a href="/contact">Contact</a></li>
              <li><a href="#" data-toggle="modal" data-target="#feedback-modal">Feedback</a></li>
            </ul>
          </div>
    
          <div class="col-sm-4">
            <ul class="list-unstyled">
              <li><a href="/terms">Terms of Service</a></li>
              <li><a href="/privacy">Privacy Policy</a></li>
            </ul>
          </div>
        </div>
    
        <div class="row visible-xs-block">
          <div class="col-xs-6">
            <ul class="list-unstyled">
              <li><a href="/" class="home">Home</a></li>
              <li><a href="/about">About OpenReview</a></li>
              <li><a href="#" data-toggle="modal" data-target="#feedback-modal">Feedback</a></li>
            </ul>
          </div>
    
          <div class="col-xs-6">
            <ul class="list-unstyled">
              <li><a href="/contact">Contact</a></li>
              <li><a href="/terms">Terms of Service</a></li>
              <li><a href="/privacy">Privacy Policy</a></li>
            </ul>
          </div>
        </div>
      </div>
    </footer>
    
    <footer class="sponsor">
      <div class="container">
        <div class="row">
          <div class="col-sm-10 col-sm-offset-1">
            <p class="text-center">
              OpenReview is created by the <a href="http://www.iesl.cs.umass.edu/" target="_blank">Information Extraction and Synthesis Laboratory</a>, College of Information and Computer Science, University of Massachusetts Amherst. We gratefully acknowledge the support of the OpenReview sponsors:  Google,  Facebook, NSF, the University of Massachusetts Amherst Center for Data Science, and Center for Intelligent Information Retrieval, as well as the Google Cloud Platform for donating the computing and networking services on which OpenReview.net runs.
            </p>
          </div>
        </div>
      </div>
    </footer>

  <div id="feedback-modal" class="modal fade" tabindex="-1" role="dialog">
    <div class="modal-dialog">
      <div class="modal-content">
        <div class="modal-header">
          <button type="button" class="close" data-dismiss="modal" aria-label="Close"><span aria-hidden="true">Ã—</span></button>
          <h3 class="modal-title">Send Feedback</h3>
        </div>
        <div class="modal-body">
          <p>Enter your feedback below and we'll get back to you as soon as possible.</p>
          <form action="/feedback" method="POST">
            <div class="form-group">
              <input type="email" name="from" class="form-control" placeholder="Email" />
            </div>
            <div class="form-group">
              <input type="text" name="subject" class="form-control" placeholder="Subject" />
            </div>
            <div class="form-group">
              <textarea name="message" class="form-control feedback-input" rows="5" placeholder="Message" required=""></textarea>
            </div>
          <ul id="ui-id-2" tabindex="0" class="ui-menu ui-widget ui-widget-content" style="display: none;"></ul></form>
        </div>
        <div class="modal-footer">
          <button type="button" class="btn btn-default" data-dismiss="modal">Cancel</button>
          <button type="button" class="btn btn-primary">Send</button>
        </div>
      </div>
    </div>
  </div>

  <script type="text/javascript" async="" src="https://www.google-analytics.com/analytics.js"></script><script src="//cdnjs.cloudflare.com/ajax/libs/jquery/2.2.4/jquery.min.js"></script>
  <script>window.jQuery || document.write('&lt;script src="/static/js/vendor/jquery-2.2.4.min.js"&gt;&lt;\/script&gt;')</script>

  <script src="/static/dist/vendor.min.js"></script>
  <script src="/static/dist/templates.min.js"></script>
  <script src="/static/dist/openreview.min.js"></script>

    <script>window.legacyScripts = true;</script>


    <script async="" src="https://www.googletagmanager.com/gtag/js?id=UA-108703919-1"></script>
    <script>
      window.dataLayer = window.dataLayer || [];
      function gtag() { dataLayer.push(arguments); }
      gtag('js', new Date());
      gtag('config', 'UA-108703919-1');
    </script>


<div role="status" aria-live="assertive" aria-relevant="additions" class="ui-helper-hidden-accessible"></div></body></html>