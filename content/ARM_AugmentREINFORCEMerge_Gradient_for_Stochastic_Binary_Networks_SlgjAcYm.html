<!DOCTYPE html><html xmlns="http://www.w3.org/1999/xhtml" class="no-js"><head>
  <meta charset="utf-8" />
  <meta http-equiv="X-UA-Compatible" content="IE=edge,chrome=1" />
  <meta name="viewport" content="width=device-width, initial-scale=1" />

  <title>ARM: Augment-REINFORCE-Merge Gradient for Stochastic Binary Networks | OpenReview</title>
  <meta name="description" content="" />

      <meta name="citation_title" content="ARM: Augment-REINFORCE-Merge Gradient for Stochastic Binary Networks" />
        <meta name="citation_author" content="Anonymous" />
      <meta name="citation_publication_date" content="2018/09/27" />
      <meta name="citation_online_date" content="2018/09/27" />
      <meta name="citation_pdf_url" content="https://openreview.net/pdf?id=S1lg0jAcYm" />
      <meta name="twitter:card" content="summary" />
      <meta name="twitter:site" content="@openreviewnet" />
      <meta name="og:title" content="ARM: Augment-REINFORCE-Merge Gradient for Stochastic Binary Networks" />
      <meta name="og:description" content="To backpropagate the gradients through stochastic binary layers, we propose the augment-REINFORCE-merge (ARM) estimator that is unbiased and has low variance. Exploiting data augmentation..." />
      <meta name="og:image" content="https://openreview.net/static/images/openreview_logo_512.png" />

  <link rel="stylesheet" href="https://fonts.googleapis.com/css?family=Noto+Sans:400,400i,700,700i" />

  <link rel="stylesheet" href="/static/css/bootstrap.min.css" />
  <link rel="stylesheet" href="/static/css/jquery-ui.min.css" />
  <link rel="stylesheet" href="/static/css/main.min.css" />
</head>

<body class="forum">
  <nav class="navbar navbar-inverse navbar-fixed-top" role="navigation">
    <div class="container">
  
      <div class="navbar-header">
        <button type="button" class="navbar-toggle collapsed" data-toggle="collapse" data-target="#navbar" aria-expanded="false" aria-controls="navbar">
          <span class="sr-only">Toggle navigation</span>
          <span class="icon-bar"></span>
          <span class="icon-bar"></span>
          <span class="icon-bar"></span>
        </button>
        <a class="navbar-brand home push-link" href="/"><strong>OpenReview</strong>.net</a>
      </div>
  
      <div id="navbar" class="navbar-collapse collapse">
        <form class="navbar-form navbar-left profile-search" role="search">
          <div class="form-group has-feedback">
            <input id="search_input" type="text" class="form-control ui-autocomplete-input" placeholder="Search ICLR 2019 Conference" autocomplete="off" />
            <span class="glyphicon glyphicon-search form-control-feedback" aria-hidden="true"></span>
          </div>
  
          <input id="search_group" type="hidden" value="ICLR.cc/2019/Conference" />
          <input id="search_content" type="hidden" value="all" />
        <ul id="ui-id-1" tabindex="0" class="ui-menu ui-widget ui-widget-content ui-autocomplete ui-front" style="display: none;"></ul></form>
  
        <ul class="nav navbar-nav navbar-right">
        
            <li id="user-menu"><a href="/login">Login</a></li>
        </ul>
      </div>
  
    </div>
  </nav>

  <div id="or-banner" class="banner" style="">
  <div class="container">
    <div class="row">
      <div class="col-xs-12"><a href="/group?id=ICLR.cc/2019/Conference" title="Venue Homepage"><img class="icon" src="/static/images/arrow_left.svg" /> Go to <strong>ICLR 2019 Conference</strong> homepage</a></div>
    </div>
  </div>
</div>
<div id="flash-message-container" class="alert alert-danger" role="alert" style="display: none;">
  <div class="container">
    <div class="row">
      <div class="col-xs-12">
        <div class="alert-content">
          <button type="button" class="close" aria-label="Close"><span aria-hidden="true">×</span></button>
        </div>
      </div>
    </div>
  </div>
</div>

<div class="container">
  <div class="row">
    <div class="col-xs-12">
      <main id="content" class="clearfix openbanner-visible legacy-styles"><div id="note_S1lg0jAcYm" class="note panel"><div class="title_pdf_row clearfix"><h2 class="note_content_title"><a>ARM: Augment-REINFORCE-Merge Gradient for Stochastic Binary Networks</a> <a class="note_content_pdf" href="/pdf?id=S1lg0jAcYm" title="Download PDF" target="_blank"><img src="/static/images/pdf_icon_blue.svg" /></a> </h2></div><div class="meta_row"><span class="signatures">Anonymous</span></div><div class="clearfix"><div class="meta_row pull-left"><span class="date item">28 Sep 2018 (modified: 11 Oct 2018)</span><span class="item">ICLR 2019 Conference Blind Submission</span><span class="item">Readers: <span class="readers-icon glyphicon glyphicon-globe"></span> Everyone</span><span class="item"><a class="action-bibtex-modal" data-bibtex="@inproceedings{    &#10;anonymous2019arm:,    &#10;title={ARM: Augment-REINFORCE-Merge Gradient for Stochastic Binary Networks},    &#10;author={Anonymous},    &#10;booktitle={Submitted to International Conference on Learning Representations},    &#10;year={2019},    &#10;url={https://openreview.net/forum?id=S1lg0jAcYm},    &#10;note={under review}    &#10;}">Show Bibtex</a></span></div><div class="meta_row meta_actions"></div></div><div class="note_contents"><span class="note_content_field">Abstract: </span><span class="note_content_value">To backpropagate the gradients through stochastic binary layers, we propose the augment-REINFORCE-merge (ARM) estimator that is unbiased and has low variance. Exploiting data augmentation, REINFORCE, and reparameterization, the ARM estimator achieves adaptive variance reduction for Monte Carlo integration by merging two expectations via common random numbers. The variance-reduction mechanism of the ARM estimator can also be attributed to antithetic sampling in an augmented space. Experimental results show the ARM estimator provides state-of-the-art performance in auto-encoding variational Bayes and maximum likelihood inference, for discrete latent variable models with one or multiple stochastic binary layers. Python code is available at <a href="https://github.com/ABC-anonymous-1." target="_blank" rel="nofollow">https://github.com/ABC-anonymous-1.</a></span></div><div class="note_contents"><span class="note_content_field">Keywords: </span><span class="note_content_value">Antithetic sampling, data augmentation, deep discrete latent variable models, variance reduction, variational auto-encoder</span></div><div class="reply_row clearfix"><div class="item" id="reply_count">8 Replies</div></div></div><hr class="small" /><div id="note_children"><div class="note_with_children"><div id="note_rkl9cK9Uam" class="note panel"><div class="title_pdf_row clearfix"><h2 class="note_content_title"><a>An interesting paper with the potential to inspire many possible extensions, but overly complicated presentation.</a>  <button class="btn btn-xs btn-default permalink-button" title="Link to this comment" data-permalink-url="https://openreview.net/forum?id=S1lg0jAcYm&amp;noteId=rkl9cK9Uam"><span class="glyphicon glyphicon-link" aria-hidden="true"></span></button></h2></div><div class="meta_row"><span class="signatures">ICLR 2019 Conference Paper862 AnonReviewer3</span></div><div class="clearfix"><div class="meta_row pull-left"><span class="date item">12 Nov 2018</span><span class="item">ICLR 2019 Conference Paper862 Official Review</span><span class="item">Readers: <span class="readers-icon glyphicon glyphicon-globe"></span> Everyone</span></div><div class="meta_row meta_actions"></div></div><div class="note_contents"><span class="note_content_field">Review: </span><span class="note_content_value">In this paper the authors propose a new variance-reduction technique to use when computing an expected loss gradient where the expectation is with respect to independent binary random variables, e.g. for training VAEs with a discrete latent space. The paper is interesting, highly relevant, simple to implement, suggests many possible extensions, and shows good results on the experiments performed. However the exposition leaves a lot to be desired.

Major comments:

The authors devote several pages of fairly dense mathematics to deriving the ARM estimate in section 2 (up to section 2.5). However I found it relatively easy to derive (15) directly, using elementary results such as the law of total expectation and a single 1-dimensional integral, in about 10 lines of equations. As the authors note, deriving (4) from (15) requires an extra line or two. In my opinion it would greatly improve the clarity of the paper to use a more direct and straightforward derivation (perhaps with the interesting historical account of how the authors first derived this result given in an appendix). I could understand the more lengthy derivation being helpful if it gave insight into the source of variance reduction, but I don't see this personally, and the current discussion of variance reduction does not refer to the derivation of (15) at all.

The analysis of variance in section 2.6 leaves a lot to be desired. The central claim of the paper is that this method reduces variance, so it is an important section! Firstly, the variance of ARM vs AR is interesting, but the variance of ARM vs REINFORCE seems also highly relevant. Secondly, it seems like it would be very informative to look at the ratio of stdev to the mean for the ARM gradient estimate, since the true gradient is multiplied by sigmoid(phi) sigmoid(-phi) and so is very small if the probability of z = 1 is close to 0 or 1, exactly in the same regime where ARM has an advantage in variance reduction over AR. For example, it may be that learning in this regime is very difficult due to the weak gradient even if the estimate is extremely low variance. Thirdly and somewhat relatedly, in this same regime (z = 1 close to 0 or 1) the ARM gradient estimate is very often 0, meaning no learning takes place, so it seems a bit strange to argue that the new method is fantastic in the regime where it's almost always not learning! Of course, not learning is better than adding lots of spurious variance as reinforce would, but perhaps this could be made clearer. Finally, the theoretical analysis involving correlation gives very little insight and is extremely hand-wavy. A short worked example in the 1D or 2D case explicitly computing the variance of REINFORCE, AR and ARM seems like it would be highly informative.

Minor comments:

In the introduction, "*approximately* maximizing the marginal likelihood" might be more accurate, since as given in (28) the exact marginal likelihood is not optimized in practice, and the exact marginal likelihood is not of the form (1) but is rather the logarithm of something of the form (1).

I wasn't clear why "equal in distribution" was used a few things for things that are simply equal, such as just above (5).

In section 2.3, I don't see any real reason the estimates in (9) and (11) "could be highly positively correlated", other than an argument along the lines of the simple one given in section 2.6 that they're often equal and so zero.

As an aside, in section 3.1, it is great not to assume conditional independence of the binary latent variables across layers, but assuming conditional independence within each layer is still very restrictive. It is reasonable for the generative distribution to have this property, since the resulting net can still be essentially "universal" by stacking enough layers, but assuming this factorization in the variational distribution is highly restrictive with hard-to-reason-about consequences for the learned generative model. I realize this is a commonly used assumption and the authors are interested in the variance reduction properties of their approach rather than the training itself, but I just mention that it would be great to see extensions of the current work that can cope tractably with correlated latent variables within each layer.

In section 3.2, according to my understanding of standard terminology, "maximum likelihood inference" is a misnomer and would normally be "maximum likelihood estimation", since maximum likelihood is a method for estimating parameters whereas inference is about inferring latent variable values given parameters.

In section 4, it would be great to see some plots of explicit variance estimates of the different methods, given the overall goal of the paper (unless I just missed this?), even though figure 1 gives some insight into the variance characteristics.

In section 4.2, the expression log 1/K \sum_k Bernoulli... differs in the placement of log from Jang et al (2017). Which is the standard convention for this task?</span></div><div class="note_contents"><span class="note_content_field">Rating: </span><span class="note_content_value">8: Top 50% of accepted papers, clear accept</span></div><div class="note_contents"><span class="note_content_field">Confidence: </span><span class="note_content_value">4: The reviewer is confident but not absolutely certain that the evaluation is correct</span></div><div class="reply_row clearfix"></div></div><div class="children"><div class="note_with_children"><div id="note_r1gn81e_aX" class="note panel"><div class="title_pdf_row clearfix"><h2 class="note_content_title"><a>Reverse-engineering the ARM estimator is leading to clearly simplified derivation and improved presentation</a>  <button class="btn btn-xs btn-default permalink-button" title="Link to this comment" data-permalink-url="https://openreview.net/forum?id=S1lg0jAcYm&amp;noteId=r1gn81e_aX"><span class="glyphicon glyphicon-link" aria-hidden="true"></span></button></h2></div><div class="meta_row"><span class="signatures">ICLR 2019 Conference Paper862 Authors</span></div><div class="clearfix"><div class="meta_row pull-left"><span class="date item">13 Nov 2018</span><span class="item">ICLR 2019 Conference Paper862 Official Comment</span><span class="item">Readers: <span class="readers-icon glyphicon glyphicon-globe"></span> Everyone</span></div><div class="meta_row meta_actions"></div></div><div class="note_contents"><span class="note_content_field">Comment: </span><span class="note_content_value">We thank Reviewer 3 for his/her insightful and constructive comments and suggestions. 

Below please find a concise response. We are preparing a detailed point-by-point response along with the revision. 

To Major Comment 1: we totally agree with you that it would be better to derive the univariate AR and ARM estimators from the analytic gradient via one dimensional integrations, and generalize them to multivariate ones using the law of total expectation. Motivated by your suggestion, we are reverse-engineering the proposed ARM estimator to considerably simplify its derivation. We will show we can derive the univariate AR (ARM) estimator with as few as two (three) equations. We will present the simplified derivation in the main body of the paper, with the original more complicated and longer derivation deferred to the Appendix. 

To Major Comment 2: we are adding all suggested analyses. 

To minor comments: we will carefully address all of them. 
</span></div><div class="reply_row clearfix"></div></div><div class="children"></div></div></div></div><div class="note_with_children"><div id="note_rygqdW892X" class="note panel"><div class="title_pdf_row clearfix"><h2 class="note_content_title"><a>ARM algorithm is an interesting approach, possibly good, but the paper is hindered by experimental and methodological problems</a>  <button class="btn btn-xs btn-default permalink-button" title="Link to this comment" data-permalink-url="https://openreview.net/forum?id=S1lg0jAcYm&amp;noteId=rygqdW892X"><span class="glyphicon glyphicon-link" aria-hidden="true"></span></button></h2></div><div class="meta_row"><span class="signatures">ICLR 2019 Conference Paper862 AnonReviewer2</span></div><div class="clearfix"><div class="meta_row pull-left"><span class="date item">03 Nov 2018 (modified: 07 Nov 2018)</span><span class="item">ICLR 2019 Conference Paper862 Official Review</span><span class="item">Readers: <span class="readers-icon glyphicon glyphicon-globe"></span> Everyone</span></div><div class="meta_row meta_actions"></div></div><div class="note_contents"><span class="note_content_field">Review: </span><span class="note_content_value">Overview.
The authors present an algorithm for lowering the variance of the score-function gradient estimator in the special case of stochastic binary networks. The algorithm, called Augment-REINFORCE-merge proceeds by augmenting binary random variables. ARM combines Rao-Blackwellization and common random numbers (equivalent to antithetic sampling in this case, due to symmetry) to produce what the authors claim to be a lower variance gradient estimator. The approach is somewhat novel. I have not seen other authors attempt to apply REINFORCE in an augmented space and with antithetic samples / common random numbers, and Rao-Blackwellization. This combination of techniques may be a good idea in the case of Bernoulli random variables. However, due to a number of issues discussed below, this claim is not possible to evaluate from the paper.

Issues/Concerns
- I assess the paper in its current form as too far below the acceptable standard in writing and in clarity of presentation, setting aside other conceptual issues which I discuss below. The paper contains many typos and a few run-on sentences that span 5-7 lines. This hinders understanding substantially. A number key terms are not explained, irregularly. Although the paper assumes that readers do not know the mean and a variance of a Bernoulli random variable, or theof  definition of an indicator function, it does not explain what random variable augmentation means. The one sentence that comes close to explaining it seems to have a typo: "From (5) it becomes clear that the Bernoulli random variable z ∼ Bernoulli(σ(φ)) can be reparameterized by racing two augmented exponential random variables ...". It is not clear what is meant by "racing," here, and I do not find it clear from equation (5) what is going on. Unfortunately, in the abstract, the paper claims that variance reduction is achieved by "data augmentation," which has a very specific meaning in machine learning unrelated to augmented random variables, further obfuscating meaning. Similarly, the term "merge" is not explained, despite the subheading 2.3.
- Computational issues are not addressed in the paper. Whether or not this method is useful in practice depends on computational complexity
- No effort is made to diagnose the source of the variance reduction, other than in the special case of analytically comparing with the Augment-REINFORCE estimator, which does not appear in any of the experiments. 
- No effort is made to empirically characterize the variance of the gradient estimator, unlike Tucker et al (2017) and Grathwohl et al. (2018).
- The algorithm presented in the appendix appears to only address single-layer stochastic binary networks, which are uninteresting in practice.
- Figure 2 (d), (e), and (f) all show that ARM was stopped early. Given that RELAX and REBAR overfit, this is a little troubling. Overal, these results are not very convincing that ARM is better, particularly in the absence of variance analysis (empirically, or other than w.r.t. the same algorithm without the merge step). All algorithms should be run for the same number of steps, particularly in cases where they may be prone to overfitting.
- Figure 1 I believe contains an error for the REINFORCE figure. In my own research I have run these experiments myself, with a value of p close to the one used by the authors. REBAR and RELAX both reduce to a REINFORCE gradient estimator with a control variate that is differentiably reparametrizable, and so the erratic behaviour of the REINFORCE estimator in this case is likely wrong.
- There is a mysterious sentence on page 6 that refers to ARM adjusting the "frequencies, amplitudes, and signs of its gradient estimates with larger and more frequent spikes for larger true gradients"
-The value to the community of another gradient estimator for binary random variables is low, given the plethora of other methods available. Given the questions remaining about this methodology and its experiments, I recommend against publication on this basis also.
- Table 2 compares results that mix widely different architectures against each other, some taken directly from papers, others possibly retrained. This is not a valid comparison to make when evaluating a new gradient estimator, where the model must be fixed. 


</span></div><div class="note_contents"><span class="note_content_field">Rating: </span><span class="note_content_value">3: Clear rejection</span></div><div class="note_contents"><span class="note_content_field">Confidence: </span><span class="note_content_value">4: The reviewer is confident but not absolutely certain that the evaluation is correct</span></div><div class="reply_row clearfix"></div></div><div class="children"><div class="note_with_children"><div id="note_r1eg1BCR3X" class="note panel"><div class="title_pdf_row clearfix"><h2 class="note_content_title"><a>Point-by-point response to address the raised issues/concerns of reviewer 2</a>  <button class="btn btn-xs btn-default permalink-button" title="Link to this comment" data-permalink-url="https://openreview.net/forum?id=S1lg0jAcYm&amp;noteId=r1eg1BCR3X"><span class="glyphicon glyphicon-link" aria-hidden="true"></span></button></h2></div><div class="meta_row"><span class="signatures">ICLR 2019 Conference Paper862 Authors</span></div><div class="clearfix"><div class="meta_row pull-left"><span class="date item">06 Nov 2018 (modified: 07 Nov 2018)</span><span class="item">ICLR 2019 Conference Paper862 Official Comment</span><span class="item">Readers: <span class="readers-icon glyphicon glyphicon-globe"></span> Everyone</span></div><div class="meta_row meta_actions"></div></div><div class="note_contents"><span class="note_content_field">Comment: </span><span class="note_content_value">
To Reviewer 2:

Comments: The authors present ... The approach is somewhat novel. I have not seen other authors attempt to apply REINFORCE in an augmented space and with antithetic samples / common random numbers, and Rao-Blackwellization. This combination of techniques may be a good idea in the case of Bernoulli random variables. However, due to a number of issues discussed below, this claim is not possible to evaluate from the paper.

Response: We thank reviewer 2 for his/her detailed comments. It appears that the reviewer has a good understanding about the technical novelty of the paper, but is not convinced by the claim of the paper. Below please find our point-by-point response, which we believe will be able to address all the raised issues/concerns. We are making revisions accordingly and will update the paper soon. 

Q1: I assess the paper in its current form as too far below the acceptable standard in writing and in clarity of presentation, setting aside other conceptual issues which I discuss below. 

A1: We have tried very hard to make the paper easy to follow. We appreciate if you could help more specifically point out where to improve writing and presentation. We will gladly incorporate your specific comments to improve the paper. 

Q2: The paper contains many typos and a few run-on sentences that span 5-7 lines. This hinders understanding substantially. 

A2: We sincerely apologize for possible typos and we appreciate if Reviewer 2 could help point them out. We will try to identify long sentences that need revision. In the meantime, we appreciate if the reviewer could point them out directly. 

Q3: A number key terms are not explained, irregularly. Although the paper assumes that readers do not know the mean and a variance of a Bernoulli random variable, or theof  definition of an indicator function, it does not explain what random variable augmentation means. 

A3: We consider that variable augmentation is a well-known concept to readers familiar with statistical models and inference algorithms (such as the EM algorithm). Even if it is unknown, we thought Eq (6) is self-explanatory given the provided background information about exponential random variables and Eq (5). Nevertheless, we will cite classical papers and add comments about variable augmentation. 

Q4: The one sentence that comes close to explaining it seems to have a typo: "From (5) it becomes clear that the Bernoulli random variable z ~ Bernoulli(σ(φ)) can be reparameterized by racing two augmented exponential random variables ...". It is not clear what is meant by "racing," here, and I do not find it clear from equation (5) what is going on. 

A4:  "Racing," which is related to the well-known "Exponential Race Problem," is not a typo. We apologize if we did not make the analogy between "racing two exponential random variables" and "treating the smaller one of two exponential random variables as the winner" clear. We will clarify it in our revision. 

Q5: Unfortunately, in the abstract, the paper claims that variance reduction is achieved by "data augmentation," which has a very specific meaning in machine learning unrelated to augmented random variables, further obfuscating meaning. 

A5: As a compromise, we are willing to change "data augmentation" to "variable augmentation" to reduce confusion. The reason we used "data augmentation" was because it is very widely used in both statistics and machine learning literature. Its origin is often attributed to the following highly cited paper:

M. A. Tanner and W. H. Wong, The Calculation of Posterior Distributions by Data Augmentation (Discussion Article), Journal of the American Statistical Association, June 1987.

Below please find several additional examples that can help justify our use of "data augmentation:"

D. A. van Dyk and X.-L. Meng, The Art of Data Augmentation (Discussion Article), Journal of Computational and Graphical Statistics, Mar., 2001.

M. A. Tanner and W. H. Wong, From EM to Data Augmentation: The Emergence of MCMC Bayesian Computation in the 1980s, Statistical Science, 2010.

N. G. Polson and S. L. Scott, Data Augmentation for Support Vector Machines, Bayesian Analysis, 2011.

K. P. Murphy, Machine Learning: A Probabilistic Perspective (Chapter 24.2.7, Page 847), 2012.

M. Xu, J. Zhu, and B. Zhang, Fast Max-Margin Matrix Factorization with Data Augmentation, ICML 2013.

Z. Gan, R. Henao, D. Carlson, and L. Carin, Learning Deep Sigmoid Belief Networks with Data Augmentation, AISTATS 2015.

Again, we are willing to change "data augmentation" to "variable augmentation" to avoid possible confusions.

Q6: Similarly, the term "merge" is not explained, despite the subheading 2.3.

A6: We thought we had clearly defined "merge" as "sharing the same set of standard exponential random variables for Monte Carlo integration, ..., More specifically, simply taking the average of (9) and (11) leads to (12)." We will try to revise it to provide better explanation. 
</span></div><div class="reply_row clearfix"></div></div><div class="children"><div class="note_with_children"><div id="note_rkgyPSRRhX" class="note panel"><div class="title_pdf_row clearfix"><h2 class="note_content_title"><a>Point-by-point response, part 2</a>  <button class="btn btn-xs btn-default permalink-button" title="Link to this comment" data-permalink-url="https://openreview.net/forum?id=S1lg0jAcYm&amp;noteId=rkgyPSRRhX"><span class="glyphicon glyphicon-link" aria-hidden="true"></span></button></h2></div><div class="meta_row"><span class="signatures">ICLR 2019 Conference Paper862 Authors</span></div><div class="clearfix"><div class="meta_row pull-left"><span class="date item">06 Nov 2018 (modified: 07 Nov 2018)</span><span class="item">ICLR 2019 Conference Paper862 Official Comment</span><span class="item">Readers: <span class="readers-icon glyphicon glyphicon-globe"></span> Everyone</span></div><div class="meta_row meta_actions"></div></div><div class="note_contents"><span class="note_content_field">Comment: </span><span class="note_content_value">Q7: Computational issues are not addressed in the paper. Whether or not this method is useful in practice depends on computational complexity

A7: We totally agree that "Whether or not this method is useful in practice depends on computational complexity" but we respectively disagree "Computational issues are not addressed in the paper." We'd like to emphasize that in Figures 2, 5, and 6, we plot the calculated training and validation ELBOs against the number of processed mini-batches (steps) in the first row, and replot the same ELBOs against the computational time in the second row. These Figures suggest ARM takes clearly shorter time to finish the same (or more) number of iterations. We will try to emphasize that and make it easier to fully understand these Figures.  

Q8: No effort is made to diagnose the source of the variance reduction, other than in the special case of analytically comparing with the Augment-REINFORCE estimator, which does not appear in any of the experiments. 

A8: This is a good point. The Augment-REINFORCE estimator performs similarly as the REINFORCE estimator does and hence was not included for comparison. We will clarify that the amount of variance reduction of ARM over Augment-REINFORCE is comparable to that of ARM over REINFORCE. The -log p(x) for the Augment-REINFORCE gradient on MNIST is 164.1, 114.6, and 162.2 for the “Linear,” “Nonlinear,” and “Two layers” networks, respectively, which are close to the results of REINFORCE gradient. We will add them to Table 2. 

Q9: No effort is made to empirically characterize the variance of the gradient estimator, unlike Tucker et al (2017) and Grathwohl et al. (2018).

A9: This is a good point. We will follow Tucker et al (2017) and Grathwohl et al. (2018) to add these empirical variance of ARM, and compare it to that of REBAR and RELAX.

Q10: The algorithm presented in the appendix appears to only address single-layer stochastic binary networks, which are uninteresting in practice.

A10: Algorithm 1 in Appendix is describing a generic  ARM algorithm (please note we had the following clarification in Appendix A: "For stochastic transforms, the implementation of ARM gradient is discussed in Section 3."). Describing the ARM algorithm for a multi-stochastic-layer network is the sole purpose of Section 3. For a network with multiple stochastic hidden layers, the ARM algorithm is described in Proposition 2 if variational auto-encoder is used, and in Proposition 3 if maximum-likelihood is used. In the revision, we will present these two Proportions into Algorithms 2 and 3 in the Appendix to make them clearer.  

Q11: Figure 2 (d), (e), and (f) all show that ARM was stopped early. Given that RELAX and REBAR overfit, this is a little troubling. 

A11: ARM is running the same (or more) number of iterations as the others. In Figure 2 (d)-(f), We did not stop ARM early intentionally; it takes clearly less time for the same number of iterations than REBAR/RELAX does because it is much faster. We will more carefully describe that to avoid confusion. Please also see our response in A7. We will also add results where we run ARM as long as REBAR/RELAX (and hence many more iterations since ARM is much faster per iteration) and show that ARM does not have the overfitting issue that REBAR/RELAX has (we did these experiments but did not include their results for brevity).

Q12: Overal, these results are not very convincing that ARM is better, particularly in the absence of variance analysis (empirically, or other than w.r.t. the same algorithm without the merge step). 

A12: As commented in A8 and A9, we will add variance analysis (both empirically and w.r.t the Augment-REINFORCE estimator).

Q13: All algorithms should be run for the same number of steps, particularly in cases where they may be prone to overfitting.

A13: We totally agree with you and this was actually what we did (in fact, we had tried running ARM with more number of steps to see whether it would overfit eventually; we did not observe overfitting even with many more iterations). Please also see our response in A11.</span></div><div class="reply_row clearfix"></div></div><div class="children"><div class="note_with_children"><div id="note_HJgQoB0An7" class="note panel"><div class="title_pdf_row clearfix"><h2 class="note_content_title"><a>Point-by-point response, part 3</a>  <button class="btn btn-xs btn-default permalink-button" title="Link to this comment" data-permalink-url="https://openreview.net/forum?id=S1lg0jAcYm&amp;noteId=HJgQoB0An7"><span class="glyphicon glyphicon-link" aria-hidden="true"></span></button></h2></div><div class="meta_row"><span class="signatures">ICLR 2019 Conference Paper862 Authors</span></div><div class="clearfix"><div class="meta_row pull-left"><span class="date item">06 Nov 2018</span><span class="item">ICLR 2019 Conference Paper862 Official Comment</span><span class="item">Readers: <span class="readers-icon glyphicon glyphicon-globe"></span> Everyone</span></div><div class="meta_row meta_actions"></div></div><div class="note_contents"><span class="note_content_field">Comment: </span><span class="note_content_value">Q14: Figure 1 I believe contains an error for the REINFORCE figure. In my own research I have run these experiments myself, with a value of p close to the one used by the authors. REBAR and RELAX both reduce to a REINFORCE gradient estimator with a control variate that is differentiably reparametrizable, and so the erratic behaviour of the REINFORCE estimator in this case is likely wrong.

A14: REINFORCE without an appropriate control variate may have huge variance and hence has erratic behavior at certain iterations (e.g., diverge) even if the step-size is set to be small. Note we set the step-size as one here. We will add the results of REINFORCE with a smaller step-size for further clarification. 

We appreciate if the reviewer could try our demo code "ARM_toy.py" in the provided anonymous GitHub code repository if he/she still believes there is an error for the REINFORCE figure. Since the reviewer also mentioned he/she had run these experiments before, we believe he/she will be impressed by both the performance and speed of ARM after running our code by himself/herself. 

Q15: There is a mysterious sentence on page 6 that refers to ARM adjusting the "frequencies, amplitudes, and signs of its gradient estimates with larger and more frequent spikes for larger true gradients"

A15: We will provide more explanation about that sentence. Briefly speaking, this is the expected behavior in theory for the ARM gradient on a univariate Bernoulli latent variable, as also shown in the third subplot of the first row of Figure 1. 

Q16: The value to the community of another gradient estimator for binary random variables is low, given the plethora of other methods available. 

A16: ARM runs much faster, delivers similar or higher testing log-likelihood and ELBOs, and is extremely simple to implement (almost as simple as REINFORCE). We believe its value will be appreciated by the community and it can be potentially plugged into many other research tasks, such as the ones mentioned in Conclusions. In fact, we have recently found a paper submitted to ICLR this year that had independently verified the correctness and excellent performance of ARM in its experiments (to preserve anonymity, we cannot reveal the name of that paper).

Q17: Given the questions remaining about this methodology and its experiments, I recommend against publication on this basis also.

A17: We believe we have addressed all your questions and we appreciate if you could take another look at our paper.

Q18: Table 2 compares results that mix widely different architectures against each other, some taken directly from papers, others possibly retrained. This is not a valid comparison to make when evaluating a new gradient estimator, where the model must be fixed. 

A18: For the results taken from literature, we have tried our best to ensure that the models are the same as the ones we use, i.e., only the gradient estimator is different (we had communicated with some authors of these papers to double check); for the models which are different from the original papers, we modify the author provided code with the same network. All efforts had been made to ensure a fair and meaningful comparison.  Please also see our response A4 to Reviewer 1.</span></div><div class="reply_row clearfix"></div></div><div class="children"></div></div></div></div></div></div></div></div><div class="note_with_children"><div id="note_S1xuO4yqn7" class="note panel"><div class="title_pdf_row clearfix"><h2 class="note_content_title"><a>This paper is very good but needs improvement</a>  <button class="btn btn-xs btn-default permalink-button" title="Link to this comment" data-permalink-url="https://openreview.net/forum?id=S1lg0jAcYm&amp;noteId=S1xuO4yqn7"><span class="glyphicon glyphicon-link" aria-hidden="true"></span></button></h2></div><div class="meta_row"><span class="signatures">ICLR 2019 Conference Paper862 AnonReviewer1</span></div><div class="clearfix"><div class="meta_row pull-left"><span class="date item">02 Nov 2018 (modified: 07 Nov 2018)</span><span class="item">ICLR 2019 Conference Paper862 Official Review</span><span class="item">Readers: <span class="readers-icon glyphicon glyphicon-globe"></span> Everyone</span></div><div class="meta_row meta_actions"></div></div><div class="note_contents"><span class="note_content_field">Review: </span><span class="note_content_value">For binary layers, how to calculate and backpropagate gradients is a big problem, particularly for the binary neural networks. To solve the problem, this paper proposes an unbiased and low variance augment-REINFORCE-merge (ARM) estimator. With the help of an appropriate reparameterization, the antithetic sampling in an augmented space can be used to drive a variance-reduction mechanism. The experimental results show that ARM estimator converges fast, has low computational complexity, and provides advanced prediction performance.

This paper is well-organized. The motivation of the proposed model is well-driven and algorithm is articulated clearly. Meanwhile, the derivations and analysis of the proposed algorithm are correct. The experimental results show that the proposed model is better than the other existing methods.

A few minor revision are list below.
1) In figure 1, it seems difficult to decide which one is better from the trace plots of the true/estimated gradients. Also, why the author choose to compare the REINFORCE instead of REBAR and RELAX, since REBAR and RELAX improve on REINFORCE by introducing stochastically estimated control variates. Also, about trace plots of the loss functions, I am curious why REINFORCE has a big vibration during 1500~2000 iterations. 
2) About Table 2, are all compared methods in the same experimental settings?
</span></div><div class="note_contents"><span class="note_content_field">Rating: </span><span class="note_content_value">7: Good paper, accept</span></div><div class="note_contents"><span class="note_content_field">Confidence: </span><span class="note_content_value">3: The reviewer is fairly confident that the evaluation is correct</span></div><div class="reply_row clearfix"></div></div><div class="children"><div class="note_with_children"><div id="note_rJxEaPoRn7" class="note panel"><div class="title_pdf_row clearfix"><h2 class="note_content_title"><a>We are making all the suggested minor revisions</a>  <button class="btn btn-xs btn-default permalink-button" title="Link to this comment" data-permalink-url="https://openreview.net/forum?id=S1lg0jAcYm&amp;noteId=rJxEaPoRn7"><span class="glyphicon glyphicon-link" aria-hidden="true"></span></button></h2></div><div class="meta_row"><span class="signatures">ICLR 2019 Conference Paper862 Authors</span></div><div class="clearfix"><div class="meta_row pull-left"><span class="date item">06 Nov 2018</span><span class="item">ICLR 2019 Conference Paper862 Official Comment</span><span class="item">Readers: <span class="readers-icon glyphicon glyphicon-globe"></span> Everyone</span></div><div class="meta_row meta_actions"></div></div><div class="note_contents"><span class="note_content_field">Comment: </span><span class="note_content_value">Thank you very much for your positive feedback. We are making these suggested minor revisions and will update the paper soon. Please see our point-by-point response to your comments below.

Q1: 1) In figure 1, it seems difficult to decide which one is better from the trace plots of the true/estimated gradients. 

A1: For this univariate binary toy example, the trace plots of the true gradients of $\phi$, shown in the first subplot of the top row, seem very different from these of the estimated gradients with ARM, shown in the third subplot of the top row. However, the trace plots of the Bernoulli probability parameter $\sigma(\phi)$ updated with the true gradient, shown in the first subplot of the bottom row, are almost indistinguishable from these updated with the ARM gradient, shown in the third subplot of the bottom row. Given the same step-size of one, it is hard to tell whether the ARM or true gradient is better for updating $\phi$, which is showing how surprisingly well ARM works!

Q2: Also, why the author choose to compare the REINFORCE instead of REBAR and RELAX, since REBAR and RELAX improve on REINFORCE by introducing stochastically estimated control variates. 

A2: We compared with REBAR and RELAX for all settings. We only showed the comparison between all five algorithms in the right subplot of Figure 1 for brevity. In the revised paper, we will add the trace plots of the estimated gradients, Bernoulli probability parameters, and loss functions for both REINFORCE and REBAR under all settings.

Q3: Also, about trace plots of the loss functions, I am curious why REINFORCE has a big vibration during 1500~2000 iterations. 

A3: The REINFORCE has large variance, which sometimes leads to divergence. We have also tried to reduce the step-size for REINFORCE. We find that the volatility can be clearly reduced but the objective could sometime still converge to the wrong point as the learning progresses. We will add the results of REINFORCE with a smaller step-size to better illustrate its behavior. 

If you are interested in playing with this toy examples by yourself, you may run ARM_toy.py provided in the anonymous Github code repository. 

Q4: 2) About Table 2, are all compared methods in the same experimental settings?

A4: We tried our best to make a comparison that is as fair as possible: 1) We have ensured that we are using the same version of banarized MNIST for all algorithms. 2) We have ensured all methods use the same network size; if the original paper used different ones, we have modified and run the author provided code (eg. LeGrad) to ensure the comparability.  3) We have run five independent trials to add error bars to make the comparison more meaningful (many previous work did not report error bars).
</span></div><div class="reply_row clearfix"></div></div><div class="children"></div></div></div></div></div></main></div>
  </div>
</div>


    <!-- Footer -->
    <footer class="sitemap">
      <div class="container">
        <div class="row hidden-xs">
          <div class="col-sm-4">
            <ul class="list-unstyled">
              <li><a href="/" class="home">Home</a></li>
              <li><a href="/venues">All Venues</a></li>
            </ul>
          </div>
    
          <div class="col-sm-4">
            <ul class="list-unstyled">
              <li><a href="/about">About OpenReview</a></li>
              <li><a href="/contact">Contact</a></li>
              <li><a href="#" data-toggle="modal" data-target="#feedback-modal">Feedback</a></li>
            </ul>
          </div>
    
          <div class="col-sm-4">
            <ul class="list-unstyled">
              <li><a href="/terms">Terms of Service</a></li>
              <li><a href="/privacy">Privacy Policy</a></li>
            </ul>
          </div>
        </div>
    
        <div class="row visible-xs-block">
          <div class="col-xs-6">
            <ul class="list-unstyled">
              <li><a href="/" class="home">Home</a></li>
              <li><a href="/about">About OpenReview</a></li>
              <li><a href="#" data-toggle="modal" data-target="#feedback-modal">Feedback</a></li>
            </ul>
          </div>
    
          <div class="col-xs-6">
            <ul class="list-unstyled">
              <li><a href="/contact">Contact</a></li>
              <li><a href="/terms">Terms of Service</a></li>
              <li><a href="/privacy">Privacy Policy</a></li>
            </ul>
          </div>
        </div>
      </div>
    </footer>
    
    <footer class="sponsor">
      <div class="container">
        <div class="row">
          <div class="col-sm-10 col-sm-offset-1">
            <p class="text-center">
              OpenReview is created by the <a href="http://www.iesl.cs.umass.edu/" target="_blank">Information Extraction and Synthesis Laboratory</a>, College of Information and Computer Science, University of Massachusetts Amherst. We gratefully acknowledge the support of the OpenReview sponsors:  Google,  Facebook, NSF, the University of Massachusetts Amherst Center for Data Science, and Center for Intelligent Information Retrieval, as well as the Google Cloud Platform for donating the computing and networking services on which OpenReview.net runs.
            </p>
          </div>
        </div>
      </div>
    </footer>

  <div id="feedback-modal" class="modal fade" tabindex="-1" role="dialog">
    <div class="modal-dialog">
      <div class="modal-content">
        <div class="modal-header">
          <button type="button" class="close" data-dismiss="modal" aria-label="Close"><span aria-hidden="true">×</span></button>
          <h3 class="modal-title">Send Feedback</h3>
        </div>
        <div class="modal-body">
          <p>Enter your feedback below and we'll get back to you as soon as possible.</p>
          <form action="/feedback" method="POST">
            <div class="form-group">
              <input type="email" name="from" class="form-control" placeholder="Email" />
            </div>
            <div class="form-group">
              <input type="text" name="subject" class="form-control" placeholder="Subject" />
            </div>
            <div class="form-group">
              <textarea name="message" class="form-control feedback-input" rows="5" placeholder="Message" required=""></textarea>
            </div>
          <ul id="ui-id-2" tabindex="0" class="ui-menu ui-widget ui-widget-content" style="display: none;"></ul></form>
        </div>
        <div class="modal-footer">
          <button type="button" class="btn btn-default" data-dismiss="modal">Cancel</button>
          <button type="button" class="btn btn-primary">Send</button>
        </div>
      </div>
    </div>
  </div>

  <script type="text/javascript" async="" src="https://www.google-analytics.com/analytics.js"></script><script src="//cdnjs.cloudflare.com/ajax/libs/jquery/2.2.4/jquery.min.js"></script>
  <script>window.jQuery || document.write('&lt;script src="/static/js/vendor/jquery-2.2.4.min.js"&gt;&lt;\/script&gt;')</script>

  <script src="/static/dist/vendor.min.js"></script>
  <script src="/static/dist/templates.min.js"></script>
  <script src="/static/dist/openreview.min.js"></script>

    <script>window.legacyScripts = true;</script>


    <script async="" src="https://www.googletagmanager.com/gtag/js?id=UA-108703919-1"></script>
    <script>
      window.dataLayer = window.dataLayer || [];
      function gtag() { dataLayer.push(arguments); }
      gtag('js', new Date());
      gtag('config', 'UA-108703919-1');
    </script>


<div role="status" aria-live="assertive" aria-relevant="additions" class="ui-helper-hidden-accessible"></div></body></html>