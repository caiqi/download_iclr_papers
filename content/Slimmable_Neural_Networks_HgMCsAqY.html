<!DOCTYPE html><html xmlns="http://www.w3.org/1999/xhtml" class="no-js"><head>
  <meta charset="utf-8" />
  <meta http-equiv="X-UA-Compatible" content="IE=edge,chrome=1" />
  <meta name="viewport" content="width=device-width, initial-scale=1" />

  <title>Slimmable Neural Networks | OpenReview</title>
  <meta name="description" content="" />

      <meta name="citation_title" content="Slimmable Neural Networks" />
        <meta name="citation_author" content="Anonymous" />
      <meta name="citation_publication_date" content="2018/09/27" />
      <meta name="citation_online_date" content="2018/09/27" />
      <meta name="citation_pdf_url" content="https://openreview.net/pdf?id=H1gMCsAqY7" />
      <meta name="twitter:card" content="summary" />
      <meta name="twitter:site" content="@openreviewnet" />
      <meta name="og:title" content="Slimmable Neural Networks" />
      <meta name="og:description" content="We present a simple and general method to train a single neural network executable at different widths (number of channels in a layer), permitting instant and adaptive accuracy-efficiency..." />
      <meta name="og:image" content="https://openreview.net/static/images/openreview_logo_512.png" />

  <link rel="stylesheet" href="https://fonts.googleapis.com/css?family=Noto+Sans:400,400i,700,700i" />

  <link rel="stylesheet" href="/static/css/bootstrap.min.css" />
  <link rel="stylesheet" href="/static/css/jquery-ui.min.css" />
  <link rel="stylesheet" href="/static/css/main.min.css" />
</head>

<body class="forum">
  <nav class="navbar navbar-inverse navbar-fixed-top" role="navigation">
    <div class="container">
  
      <div class="navbar-header">
        <button type="button" class="navbar-toggle collapsed" data-toggle="collapse" data-target="#navbar" aria-expanded="false" aria-controls="navbar">
          <span class="sr-only">Toggle navigation</span>
          <span class="icon-bar"></span>
          <span class="icon-bar"></span>
          <span class="icon-bar"></span>
        </button>
        <a class="navbar-brand home push-link" href="/"><strong>OpenReview</strong>.net</a>
      </div>
  
      <div id="navbar" class="navbar-collapse collapse">
        <form class="navbar-form navbar-left profile-search" role="search">
          <div class="form-group has-feedback">
            <input id="search_input" type="text" class="form-control ui-autocomplete-input" placeholder="Search ICLR 2019 Conference" autocomplete="off" />
            <span class="glyphicon glyphicon-search form-control-feedback" aria-hidden="true"></span>
          </div>
  
          <input id="search_group" type="hidden" value="ICLR.cc/2019/Conference" />
          <input id="search_content" type="hidden" value="all" />
        <ul id="ui-id-1" tabindex="0" class="ui-menu ui-widget ui-widget-content ui-autocomplete ui-front" style="display: none;"></ul></form>
  
        <ul class="nav navbar-nav navbar-right">
        
            <li id="user-menu"><a href="/login">Login</a></li>
        </ul>
      </div>
  
    </div>
  </nav>

  <div id="or-banner" class="banner" style="">
  <div class="container">
    <div class="row">
      <div class="col-xs-12"><a href="/group?id=ICLR.cc/2019/Conference" title="Venue Homepage"><img class="icon" src="/static/images/arrow_left.svg" /> Go to <strong>ICLR 2019 Conference</strong> homepage</a></div>
    </div>
  </div>
</div>
<div id="flash-message-container" class="alert alert-danger" role="alert" style="display: none;">
  <div class="container">
    <div class="row">
      <div class="col-xs-12">
        <div class="alert-content">
          <button type="button" class="close" aria-label="Close"><span aria-hidden="true">×</span></button>
        </div>
      </div>
    </div>
  </div>
</div>

<div class="container">
  <div class="row">
    <div class="col-xs-12">
      <main id="content" class="clearfix openbanner-visible legacy-styles"><div id="note_H1gMCsAqY7" class="note panel"><div class="title_pdf_row clearfix"><h2 class="note_content_title"><a>Slimmable Neural Networks</a> <a class="note_content_pdf" href="/pdf?id=H1gMCsAqY7" title="Download PDF" target="_blank"><img src="/static/images/pdf_icon_blue.svg" /></a> </h2></div><div class="meta_row"><span class="signatures">Anonymous</span></div><div class="clearfix"><div class="meta_row pull-left"><span class="date item">28 Sep 2018 (modified: 18 Nov 2018)</span><span class="item">ICLR 2019 Conference Blind Submission</span><span class="item">Readers: <span class="readers-icon glyphicon glyphicon-globe"></span> Everyone</span><span class="item"><a class="action-bibtex-modal" data-bibtex="@inproceedings{    &#10;anonymous2019slimmable,    &#10;title={Slimmable Neural Networks},    &#10;author={Anonymous},    &#10;booktitle={Submitted to International Conference on Learning Representations},    &#10;year={2019},    &#10;url={https://openreview.net/forum?id=H1gMCsAqY7},    &#10;note={under review}    &#10;}">Show Bibtex</a></span><a class="note_content_pdf item" href="/revisions?id=H1gMCsAqY7" target="_blank">Show Revisions</a></div><div class="meta_row meta_actions"></div></div><div class="note_contents"><span class="note_content_field">Abstract: </span><span class="note_content_value">We present a simple and general method to train a single neural network executable at different widths (number of channels in a layer), permitting instant and adaptive accuracy-efficiency trade-offs at runtime. Instead of training individual networks with different width multipliers, we train a shared network with switchable batch normalization. At runtime, the network can adjust its width on the fly according to on-device benchmarks and resource constraints, rather than downloading and offloading different models. Our trained networks, named slimmable neural networks, achieve similar (and in many cases better) ImageNet classification accuracy than individually trained models of MobileNet v1, MobileNet v2, ShuffleNet and ResNet-50 at different widths respectively. We also demonstrate better performance of slimmable models compared with individual ones across a wide range of applications including COCO bounding-box object detection, instance segmentation and person keypoint detection without tuning hyper-parameters. Lastly we visualize and discuss the learned features of slimmable networks. Code and models will be released.</span></div><div class="note_contents"><span class="note_content_field">Keywords: </span><span class="note_content_value">Slimmable neural networks, mobile deep learning, accuracy-efficiency trade-offs</span></div><div class="note_contents"><span class="note_content_field">TL;DR: </span><span class="note_content_value">We present a simple and general method to train a single neural network executable at different widths (number of channels in a layer), permitting instant and adaptive accuracy-efficiency trade-offs at runtime.</span></div><div class="reply_row clearfix"><div class="item" id="reply_count">12 Replies</div></div></div><hr class="small" /><div id="note_children"><div class="note_with_children"><div id="note_ryl2OdD8p7" class="note panel"><div class="title_pdf_row clearfix"><h2 class="note_content_title"><a>"reducing depth cannot reduce memory footprint"?</a>  <button class="btn btn-xs btn-default permalink-button" title="Link to this comment" data-permalink-url="https://openreview.net/forum?id=H1gMCsAqY7&amp;noteId=ryl2OdD8p7"><span class="glyphicon glyphicon-link" aria-hidden="true"></span></button></h2></div><div class="meta_row"><span class="signatures">(anonymous)</span></div><div class="clearfix"><div class="meta_row pull-left"><span class="date item">12 Nov 2018</span><span class="item">ICLR 2019 Conference Paper874 Public Comment</span><span class="item">Readers: <span class="readers-icon glyphicon glyphicon-globe"></span> Everyone</span></div><div class="meta_row meta_actions"></div></div><div class="note_contents"><span class="note_content_field">Comment: </span><span class="note_content_value">It is claimed in the 3rd paragraph in introduction that, 

 "Nevertheless, in contrast to width (number of channels), reducing depth cannot reduce memory footprint which is commonly constrained during runtime."

However, in my understanding, the momory reduces linearly when reducing depth for deep neural network. Could you please explain more on this?

</span></div><div class="reply_row clearfix"></div></div><div class="children"><div class="note_with_children"><div id="note_rkgg7sI0TQ" class="note panel"><div class="title_pdf_row clearfix"><h2 class="note_content_title"><a>Authors' Reply to Comment</a>  <button class="btn btn-xs btn-default permalink-button" title="Link to this comment" data-permalink-url="https://openreview.net/forum?id=H1gMCsAqY7&amp;noteId=rkgg7sI0TQ"><span class="glyphicon glyphicon-link" aria-hidden="true"></span></button></h2></div><div class="meta_row"><span class="signatures">ICLR 2019 Conference Paper874 Authors</span></div><div class="clearfix"><div class="meta_row pull-left"><span class="date item">18 Nov 2018</span><span class="item">ICLR 2019 Conference Paper874 Official Comment</span><span class="item">Readers: <span class="readers-icon glyphicon glyphicon-globe"></span> Everyone</span></div><div class="meta_row meta_actions"></div></div><div class="note_contents"><span class="note_content_field">Comment: </span><span class="note_content_value">Thanks for your interest in our work. Our claim is correct: at runtime reducing depth cannot reduce memory footprint.

For a simple example, consider a layer-by-layer network stacking same convolution layers, the output of layer N can always be placed into the memory of its input after computation, and feed into next layer (N+1). Because at runtime, there is no need to store feature of previous layers generally (in training, they are required for gradient computation).

A good reference is MobileNet v2 paper [1], section 5.1 memory efficient inference. It shows that the memory footprint can be simplified to: M = max_{layer_i \in all layers} (memory_input of layer_i + memory_output of layer_i).

The memory footprint M is a MAX operation over all layers, instead of SUM, during inference.


[1] Sandler, Mark, et al. “MobileNet v2: Inverted residuals and linear bottlenecks: Mobile networks for classification, detection and segmentation." arXiv preprint arXiv:1801.04381 (2018).</span></div><div class="reply_row clearfix"></div></div><div class="children"></div></div></div></div><div class="note_with_children"><div id="note_r1gelBGgTm" class="note panel"><div class="title_pdf_row clearfix"><h2 class="note_content_title"><a>Good motivation but not convincing results</a>  <button class="btn btn-xs btn-default permalink-button" title="Link to this comment" data-permalink-url="https://openreview.net/forum?id=H1gMCsAqY7&amp;noteId=r1gelBGgTm"><span class="glyphicon glyphicon-link" aria-hidden="true"></span></button></h2></div><div class="meta_row"><span class="signatures">(anonymous)</span></div><div class="clearfix"><div class="meta_row pull-left"><span class="date item">07 Nov 2018</span><span class="item">ICLR 2019 Conference Paper874 Public Comment</span><span class="item">Readers: <span class="readers-icon glyphicon glyphicon-globe"></span> Everyone</span></div><div class="meta_row meta_actions"></div></div><div class="note_contents"><span class="note_content_field">Comment: </span><span class="note_content_value">The motivation to train one model end deploy in multiple devices is quite interesting.  However, the experimental results are not convincing. 

In Table 3, most of the S-networks reduce performance compared to their individual counterparts. It's not cumbersome to train individual slimmed model that has higher accuracy in portable device and the same FLOPs as the S-model, since training runtime is not the key problem with increasing amount of computational powers.

In Table 5, the baselines of R-50-FPN-1× are much lower than those reported in the original paper of Faster R-CNN and Mask R-CNN.  In previous work, the box and mask AP of Mask+R-50-FPN-1× are 37.3 and 33.7, while box AP for Faster+R-50-FPN-1× is 36.4. These results are already comparable and even better than the S-networks. The same problem applies to the keypoints. Therefore, it is unclear that S-model would still bring performance gain when the standard baselines are employed.

Another concern is that S-model seems to degenerate performance in ImageNet, as the paper mentioned "a slimmable network is expected to have lower performance than individually trained ones intuitively". But it turns out that the pretrained S-model in ImageNet has large improvement when finetuned in detection and segmentation. This violates common sense.</span></div><div class="reply_row clearfix"></div></div><div class="children"><div class="note_with_children"><div id="note_HyeH_nUAaX" class="note panel"><div class="title_pdf_row clearfix"><h2 class="note_content_title"><a>Authors' Reply to Comment</a>  <button class="btn btn-xs btn-default permalink-button" title="Link to this comment" data-permalink-url="https://openreview.net/forum?id=H1gMCsAqY7&amp;noteId=HyeH_nUAaX"><span class="glyphicon glyphicon-link" aria-hidden="true"></span></button></h2></div><div class="meta_row"><span class="signatures">ICLR 2019 Conference Paper874 Authors</span></div><div class="clearfix"><div class="meta_row pull-left"><span class="date item">18 Nov 2018</span><span class="item">ICLR 2019 Conference Paper874 Official Comment</span><span class="item">Readers: <span class="readers-icon glyphicon glyphicon-globe"></span> Everyone</span></div><div class="meta_row meta_actions"></div></div><div class="note_contents"><span class="note_content_field">Comment: </span><span class="note_content_value">Thanks for your interest in our work! However, we cannot agree with your comments. We have addressed your questions and concerns below:

1. As introduced in Sec. 1 and concluded by all reviewers, this work aims to "train a single neural network executable at different widths for different devices" 
We never claim "training runtime is the key problem". And our focus is not on "training a single network" but on "a single network executable at different widths". The testing runtime and flexible accuracy-efficiency trade-offs are what we care. 
2. In Table 3 for ImageNet classification, the top-1 accuracy is actually improved by 0.5 in total.

3. Although all experiments are conducted with same settings for both individual and slimmable models, we also noticed that the reproduced performance of individual models was lower than original papers. A potential reason is included in Appendix B of the first submitted version (original *-RCNN papers use ResNet-50 with strides on 1x1 convolution, while we follow PyTorch official implemented ResNet-50 with strides on 3x3). After submission, we found a recently released detection framework MMDetection [1] that has settings for pytorch-style ResNet-50. Thus we have conducted another set of detection experiments and included the results in Appendix C (same mAP is reproduced, for example, Faster-R-50-FPN-1x with 36.4 mAP).
And our conclusion still holds: on detection tasks, slimmable models have better performance than individually trained models, especially for small models. Specifically, for 0.25x models, slimmable network has 2.0+ mAP, which is indeed significant. For 1.0x models, slimmable also have 0.4+ mAP, 0.7+ mAP for Faster-RCNN and Mask-RCNN. We will fully release our code (both training and testing) and pre-trained models on both ImageNet classification and COCO detection sets. 

4. Image classification trains models from scratch, while COCO detection fine-tunes pre-trained ImageNet models. The improvement on detection may due to better learned representation of slimmable models on ImageNet when transfer to COCO tasks. We have also mentioned in our submission that it is probably due to implicit distillation and richer supervision. The reason behind the improvements is beyond the motivation of this submission and requires future investigation. We try to avoid strong claims of any deep reason because none of them is strictly proved by us yet.

We sincerely thank you for posting these concerns and we will always try our best to address them. Please let us know if you have further question or concern. Thanks!


[1] <a href="https://github.com/open-mmlab/mmdetection" target="_blank" rel="nofollow">https://github.com/open-mmlab/mmdetection</a></span></div><div class="reply_row clearfix"></div></div><div class="children"></div></div></div></div><div class="note_with_children"><div id="note_rklrgxdT37" class="note panel"><div class="title_pdf_row clearfix"><h2 class="note_content_title"><a>The paper proposes an idea of combining different size models together into one shared net. And the performance is claimed to be slightly worse for classification and much better for detection.</a>  <button class="btn btn-xs btn-default permalink-button" title="Link to this comment" data-permalink-url="https://openreview.net/forum?id=H1gMCsAqY7&amp;noteId=rklrgxdT37"><span class="glyphicon glyphicon-link" aria-hidden="true"></span></button></h2></div><div class="meta_row"><span class="signatures">ICLR 2019 Conference Paper874 AnonReviewer2</span></div><div class="clearfix"><div class="meta_row pull-left"><span class="date item">05 Nov 2018 (modified: 07 Nov 2018)</span><span class="item">ICLR 2019 Conference Paper874 Official Review</span><span class="item">Readers: <span class="readers-icon glyphicon glyphicon-globe"></span> Everyone</span></div><div class="meta_row meta_actions"></div></div><div class="note_contents"><span class="note_content_field">Review: </span><span class="note_content_value">The idea is really interesting. One only need to train and maintain one single model, but use it in different platforms of different computational power.

And according to the experiment results of COCO detection, the S-version models are much better than original versions (eg. faster-0.25x, from 24.6 to 30.0) . The improvement is huge to me. However the authors do not explain any deep reasons.

And for classification, there are slightly performance drop instead of a large improvement which is also hard to understand. 

For detection, experiments on depth-wise convolution based models (such as mobilenet and shufflenet) are suggested to make this work more solid and meaningful.

</span></div><div class="note_contents"><span class="note_content_field">Rating: </span><span class="note_content_value">8: Top 50% of accepted papers, clear accept</span></div><div class="note_contents"><span class="note_content_field">Confidence: </span><span class="note_content_value">4: The reviewer is confident but not absolutely certain that the evaluation is correct</span></div><div class="reply_row clearfix"></div></div><div class="children"><div class="note_with_children"><div id="note_rylzSkDA6X" class="note panel"><div class="title_pdf_row clearfix"><h2 class="note_content_title"><a>Authors' Reply to Review</a>  <button class="btn btn-xs btn-default permalink-button" title="Link to this comment" data-permalink-url="https://openreview.net/forum?id=H1gMCsAqY7&amp;noteId=rylzSkDA6X"><span class="glyphicon glyphicon-link" aria-hidden="true"></span></button></h2></div><div class="meta_row"><span class="signatures">ICLR 2019 Conference Paper874 Authors</span></div><div class="clearfix"><div class="meta_row pull-left"><span class="date item">18 Nov 2018</span><span class="item">ICLR 2019 Conference Paper874 Official Comment</span><span class="item">Readers: <span class="readers-icon glyphicon glyphicon-globe"></span> Everyone</span></div><div class="meta_row meta_actions"></div></div><div class="note_contents"><span class="note_content_field">Comment: </span><span class="note_content_value">Thanks for your review efforts! We have addressed all questions below:

1. We aim to train single neural network executable at different widths. We find slimmable networks achieve better results especially for small models (e.g., 0.25x) on detection tasks. We have mentioned that it is probably due to implicit distillation, richer supervision and better learned representation (since detection results are based on pre-trained ImageNet learned representation). We try to avoid strong claims of any deep reason because none of them is strictly proved by us yet. Explaining deep reasons for improvements are not the motivation or the focus of this paper. But we are actively exploring on these questions!

2. In fact, on average the image classification results are also improved (0.5 better top-1 accuracy in total), especially for small models. After submission, we have improved accuracy of S-ShuffleNet due to an additional ReLU layer (our implementation bug) between depthwise convolution and group convolution (Figure 2 of ShuffleNet [3]). Our models will be released.

3. Thanks for the good suggestion! Currently we conduct detection experiments mainly on Detectron [1] and MMDetection [2] framework where ResNet-50 is among the most efficient models. We do value this suggestion and will try to implement mobilenet-based detectors. Besides, all code (including classification and detection) and pre-trained models will be released soon and we warmly welcome the community to work on together.

Thanks!


[1] <a href="https://github.com/facebookresearch/Detectron" target="_blank" rel="nofollow">https://github.com/facebookresearch/Detectron</a>
[2] https://github.com/open-mmlab/mmdetection
[3] Zhang et al. Shufflenet: An extremely efficientconvolutional neural network for mobile devices.arXiv preprint arXiv:1707.01083, 2017.</span></div><div class="reply_row clearfix"></div></div><div class="children"></div></div></div></div><div class="note_with_children"><div id="note_BJez1UBa27" class="note panel"><div class="title_pdf_row clearfix"><h2 class="note_content_title"><a>Very exciting work</a>  <button class="btn btn-xs btn-default permalink-button" title="Link to this comment" data-permalink-url="https://openreview.net/forum?id=H1gMCsAqY7&amp;noteId=BJez1UBa27"><span class="glyphicon glyphicon-link" aria-hidden="true"></span></button></h2></div><div class="meta_row"><span class="signatures">ICLR 2019 Conference Paper874 AnonReviewer1</span></div><div class="clearfix"><div class="meta_row pull-left"><span class="date item">05 Nov 2018 (modified: 07 Nov 2018)</span><span class="item">ICLR 2019 Conference Paper874 Official Review</span><span class="item">Readers: <span class="readers-icon glyphicon glyphicon-globe"></span> Everyone</span></div><div class="meta_row meta_actions"></div></div><div class="note_contents"><span class="note_content_field">Review: </span><span class="note_content_value">This paper presents a straightforward looking approach for creating a neural networks that can run under different resource constraints, e.g. less computation but lower quality solution and expensive high quality solution, while all the networks are having the same filters. The idea is to share the filters of the cheapest network with those of the larger more expensive networksa and train all those networks jointly with weight sharing. One important practical observation is that the batch-normalization parameters should not be shared between those filters in order to get good results. However, the most interesting surprising observation, that is the main novelty of the work that even the highest quality vision network get substantially better by this training methodology as compared to be training alone without any weight sharing with the smaller networks, when trained for object detection and segmentation purposes (but not for recognition). This is a highly unexpected result and provides a new unanticipated way of training better segmentation models. It is especially nice that the paper does not pretend that this phenomenon is well understood but leaves its proper explanation for future work. I think a lot of interesting work is to be expected along these lines.</span></div><div class="note_contents"><span class="note_content_field">Rating: </span><span class="note_content_value">9: Top 15% of accepted papers, strong accept</span></div><div class="note_contents"><span class="note_content_field">Confidence: </span><span class="note_content_value">5: The reviewer is absolutely certain that the evaluation is correct and very familiar with the relevant literature</span></div><div class="reply_row clearfix"></div></div><div class="children"><div class="note_with_children"><div id="note_HJe6pnUA6m" class="note panel"><div class="title_pdf_row clearfix"><h2 class="note_content_title"><a>Authors' Reply to Review</a>  <button class="btn btn-xs btn-default permalink-button" title="Link to this comment" data-permalink-url="https://openreview.net/forum?id=H1gMCsAqY7&amp;noteId=HJe6pnUA6m"><span class="glyphicon glyphicon-link" aria-hidden="true"></span></button></h2></div><div class="meta_row"><span class="signatures">ICLR 2019 Conference Paper874 Authors</span></div><div class="clearfix"><div class="meta_row pull-left"><span class="date item">18 Nov 2018</span><span class="item">ICLR 2019 Conference Paper874 Official Comment</span><span class="item">Readers: <span class="readers-icon glyphicon glyphicon-globe"></span> Everyone</span></div><div class="meta_row meta_actions"></div></div><div class="note_contents"><span class="note_content_field">Comment: </span><span class="note_content_value">Thanks for your positive review and encouragements! We also believe the discovery of slimmable network opens up the possibility to many related fields including model distillation, network compression and better representation learning. We are actively exploring on these topics and hope this submission may contribute to ICLR community.</span></div><div class="reply_row clearfix"></div></div><div class="children"></div></div></div></div><div class="note_with_children"><div id="note_H1lytDzo2Q" class="note panel"><div class="title_pdf_row clearfix"><h2 class="note_content_title"><a>algo details and numbers</a>  <button class="btn btn-xs btn-default permalink-button" title="Link to this comment" data-permalink-url="https://openreview.net/forum?id=H1gMCsAqY7&amp;noteId=H1lytDzo2Q"><span class="glyphicon glyphicon-link" aria-hidden="true"></span></button></h2></div><div class="meta_row"><span class="signatures">ICLR 2019 Conference Paper874 AnonReviewer3</span></div><div class="clearfix"><div class="meta_row pull-left"><span class="date item">03 Nov 2018 (modified: 07 Nov 2018)</span><span class="item">ICLR 2019 Conference Paper874 Official Review</span><span class="item">Readers: <span class="readers-icon glyphicon glyphicon-globe"></span> Everyone</span></div><div class="meta_row meta_actions"></div></div><div class="note_contents"><span class="note_content_field">Review: </span><span class="note_content_value">This paper trains a single network executable at different widths. This is implemented by maintaining separate BN parameter and statistics for different width. The problem is well-motivated and the proposed method can be very helpful for deployment of deep models to devices with varying capacity and computational ability.
 
This paper is well-written and the experiments are performed on various structures. Still I have several concerns regarding the algorithm.
1. In algo 1, while gradients for convolutional and fully-connected layers are accumulated for all switches before update, how are the parameters for different switches updated?
2. In algo 1, the gradients of all switches are accumulated before the update. This may result in implicit unbalanced gradient information, e.g. the connections in 0.25x model in Figure 1 has gradient flows on all four different switches,  while the right-most 0.25x connections in 1.0x model has only one gradient flow from the 1.0x switch, will this unbalanced gradient information increase optimization difficulty and how is it solved?
3.  In the original ResNet paper, <a href="https://arxiv.org/pdf/1512.03385.pdf," target="_blank" rel="nofollow">https://arxiv.org/pdf/1512.03385.pdf,</a> the top-1 error of RestNet-50 is &lt;21% in Table 4. The number reported in this paper (Table 3) is 23.9. Where does the difference come from? </span></div><div class="note_contents"><span class="note_content_field">Rating: </span><span class="note_content_value">7: Good paper, accept</span></div><div class="note_contents"><span class="note_content_field">Confidence: </span><span class="note_content_value">4: The reviewer is confident but not absolutely certain that the evaluation is correct</span></div><div class="reply_row clearfix"></div></div><div class="children"><div class="note_with_children"><div id="note_r1xT_6ICTX" class="note panel"><div class="title_pdf_row clearfix"><h2 class="note_content_title"><a>Authors' Reply to Review</a>  <button class="btn btn-xs btn-default permalink-button" title="Link to this comment" data-permalink-url="https://openreview.net/forum?id=H1gMCsAqY7&amp;noteId=r1xT_6ICTX"><span class="glyphicon glyphicon-link" aria-hidden="true"></span></button></h2></div><div class="meta_row"><span class="signatures">ICLR 2019 Conference Paper874 Authors</span></div><div class="clearfix"><div class="meta_row pull-left"><span class="date item">18 Nov 2018</span><span class="item">ICLR 2019 Conference Paper874 Official Comment</span><span class="item">Readers: <span class="readers-icon glyphicon glyphicon-globe"></span> Everyone</span></div><div class="meta_row meta_actions"></div></div><div class="note_contents"><span class="note_content_field">Comment: </span><span class="note_content_value">Thanks for your review efforts! We have addressed all three questions below:

1. As mentioned in Section 3.3, the only modification is to accumulate all gradients from different switches. It means that the optimizer (SGD for image recognition tasks) is exactly the same as training individual models (same momentum, etc.). The only difference is the value of gradient for each parameter. In Algorithm 1, we follow pytorch-style API and use optimizer.step() to indicate applying gradients. We have not observed any difficulty in optimization of slimmable networks using default optimizer in Algorithm 1.

2. There is no "unbalanced gradient" problem in training slimmable networks (it may seem like so). The parameters of 0.25x seem to have "more gradients", but in the forward view, these parameters of 0.25x are also used four times in Net 0.25x, 0.5x, 0.75x and 1.0x. It means the parameters in 0.25x are more important for the overall performance of slimmable networks. In fact, back-propagation is strictly based on forward feature propagation. In the forward view, as mentioned in Section 3.3, our primary objective to train a slimmable network is to optimize its accuracy averaged from all switches.

3. Our reported ResNet-50 accuracy is correct (23.9 top-1 error). We evaluate single-crop testing accuracy instead of 10-crop following all our baselines. The ResNet-50 single-crop testing accuracy is publicly reported in ResNeXt paper (Table 3, 1st row) [1], released code [2] and many other publications. Our ResNet-50 has same implementation with PyTorch official pre-trained model zoo [3] where the top-1 error is also 23.9 instead of &lt;21% (in fact ResNet-152 still has &gt; 21% single-crop top-1 error rate).

We sincerely hope the rating can be reconsidered if it was affected by above questions. Thanks for your time and review efforts!


[1] Xie, Saining, et al. "Aggregated residual transformations for deep neural networks." Computer Vision and Pattern Recognition (CVPR), 2017 IEEE Conference on. IEEE, 2017.
[2] <a href="https://github.com/facebookresearch/ResNeXt" target="_blank" rel="nofollow">https://github.com/facebookresearch/ResNeXt</a>
[3] https://pytorch.org/docs/stable/torchvision/models.html</span></div><div class="reply_row clearfix"></div></div><div class="children"></div></div></div></div><div class="note_with_children"><div id="note_Byg-61r4n7" class="note panel"><div class="title_pdf_row clearfix"><h2 class="note_content_title"><a>A related work</a>  <button class="btn btn-xs btn-default permalink-button" title="Link to this comment" data-permalink-url="https://openreview.net/forum?id=H1gMCsAqY7&amp;noteId=Byg-61r4n7"><span class="glyphicon glyphicon-link" aria-hidden="true"></span></button></h2></div><div class="meta_row"><span class="signatures"><a href="/profile?id=~Jason_Kuen1" class="profile-link">Jason Kuen</a></span></div><div class="clearfix"><div class="meta_row pull-left"><span class="date item">29 Oct 2018</span><span class="item">ICLR 2019 Conference Paper874 Public Comment</span><span class="item">Readers: <span class="readers-icon glyphicon glyphicon-globe"></span> Everyone</span></div><div class="meta_row meta_actions"></div></div><div class="note_contents"><span class="note_content_field">Comment: </span><span class="note_content_value">Nice work! I had a paper published at CVPR 2018 on training convolutional networks that support instant and adaptive accuracy-efficiency trade-offs at runtime, via early downsampling rather than networking slimming. My paper also includes a similar technique of using independent BatchNorm parameters (just means and stds in my paper, whereas you "unshare" all of BatchNorm parameters) for different trade-off configurations. 

I'd appreciate if you would include a reference to it - "Stochastic Downsampling for Cost-Adjustable Inference and Improved Regularization in Convolutional Networks". Thanks.
</span></div><div class="reply_row clearfix"></div></div><div class="children"><div class="note_with_children"><div id="note_rkxP-cICpm" class="note panel"><div class="title_pdf_row clearfix"><h2 class="note_content_title"><a>Authors' Reply to Comment</a>  <button class="btn btn-xs btn-default permalink-button" title="Link to this comment" data-permalink-url="https://openreview.net/forum?id=H1gMCsAqY7&amp;noteId=rkxP-cICpm"><span class="glyphicon glyphicon-link" aria-hidden="true"></span></button></h2></div><div class="meta_row"><span class="signatures">ICLR 2019 Conference Paper874 Authors</span></div><div class="clearfix"><div class="meta_row pull-left"><span class="date item">18 Nov 2018</span><span class="item">ICLR 2019 Conference Paper874 Official Comment</span><span class="item">Readers: <span class="readers-icon glyphicon glyphicon-globe"></span> Everyone</span></div><div class="meta_row meta_actions"></div></div><div class="note_contents"><span class="note_content_field">Comment: </span><span class="note_content_value">Thanks for your interest in our work! We have added the citation.</span></div><div class="reply_row clearfix"></div></div><div class="children"></div></div></div></div></div></main></div>
  </div>
</div>


    <!-- Footer -->
    <footer class="sitemap">
      <div class="container">
        <div class="row hidden-xs">
          <div class="col-sm-4">
            <ul class="list-unstyled">
              <li><a href="/" class="home">Home</a></li>
              <li><a href="/venues">All Venues</a></li>
            </ul>
          </div>
    
          <div class="col-sm-4">
            <ul class="list-unstyled">
              <li><a href="/about">About OpenReview</a></li>
              <li><a href="/contact">Contact</a></li>
              <li><a href="#" data-toggle="modal" data-target="#feedback-modal">Feedback</a></li>
            </ul>
          </div>
    
          <div class="col-sm-4">
            <ul class="list-unstyled">
              <li><a href="/terms">Terms of Service</a></li>
              <li><a href="/privacy">Privacy Policy</a></li>
            </ul>
          </div>
        </div>
    
        <div class="row visible-xs-block">
          <div class="col-xs-6">
            <ul class="list-unstyled">
              <li><a href="/" class="home">Home</a></li>
              <li><a href="/about">About OpenReview</a></li>
              <li><a href="#" data-toggle="modal" data-target="#feedback-modal">Feedback</a></li>
            </ul>
          </div>
    
          <div class="col-xs-6">
            <ul class="list-unstyled">
              <li><a href="/contact">Contact</a></li>
              <li><a href="/terms">Terms of Service</a></li>
              <li><a href="/privacy">Privacy Policy</a></li>
            </ul>
          </div>
        </div>
      </div>
    </footer>
    
    <footer class="sponsor">
      <div class="container">
        <div class="row">
          <div class="col-sm-10 col-sm-offset-1">
            <p class="text-center">
              OpenReview is created by the <a href="http://www.iesl.cs.umass.edu/" target="_blank">Information Extraction and Synthesis Laboratory</a>, College of Information and Computer Science, University of Massachusetts Amherst. We gratefully acknowledge the support of the OpenReview sponsors:  Google,  Facebook, NSF, the University of Massachusetts Amherst Center for Data Science, and Center for Intelligent Information Retrieval, as well as the Google Cloud Platform for donating the computing and networking services on which OpenReview.net runs.
            </p>
          </div>
        </div>
      </div>
    </footer>

  <div id="feedback-modal" class="modal fade" tabindex="-1" role="dialog">
    <div class="modal-dialog">
      <div class="modal-content">
        <div class="modal-header">
          <button type="button" class="close" data-dismiss="modal" aria-label="Close"><span aria-hidden="true">×</span></button>
          <h3 class="modal-title">Send Feedback</h3>
        </div>
        <div class="modal-body">
          <p>Enter your feedback below and we'll get back to you as soon as possible.</p>
          <form action="/feedback" method="POST">
            <div class="form-group">
              <input type="email" name="from" class="form-control" placeholder="Email" />
            </div>
            <div class="form-group">
              <input type="text" name="subject" class="form-control" placeholder="Subject" />
            </div>
            <div class="form-group">
              <textarea name="message" class="form-control feedback-input" rows="5" placeholder="Message" required=""></textarea>
            </div>
          <ul id="ui-id-2" tabindex="0" class="ui-menu ui-widget ui-widget-content" style="display: none;"></ul></form>
        </div>
        <div class="modal-footer">
          <button type="button" class="btn btn-default" data-dismiss="modal">Cancel</button>
          <button type="button" class="btn btn-primary">Send</button>
        </div>
      </div>
    </div>
  </div>

  <script type="text/javascript" async="" src="https://www.google-analytics.com/analytics.js"></script><script src="//cdnjs.cloudflare.com/ajax/libs/jquery/2.2.4/jquery.min.js"></script>
  <script>window.jQuery || document.write('&lt;script src="/static/js/vendor/jquery-2.2.4.min.js"&gt;&lt;\/script&gt;')</script>

  <script src="/static/dist/vendor.min.js"></script>
  <script src="/static/dist/templates.min.js"></script>
  <script src="/static/dist/openreview.min.js"></script>

    <script>window.legacyScripts = true;</script>


    <script async="" src="https://www.googletagmanager.com/gtag/js?id=UA-108703919-1"></script>
    <script>
      window.dataLayer = window.dataLayer || [];
      function gtag() { dataLayer.push(arguments); }
      gtag('js', new Date());
      gtag('config', 'UA-108703919-1');
    </script>


<div role="status" aria-live="assertive" aria-relevant="additions" class="ui-helper-hidden-accessible"></div></body></html>