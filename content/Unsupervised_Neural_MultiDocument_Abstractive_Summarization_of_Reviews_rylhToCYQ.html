<!DOCTYPE html><html xmlns="http://www.w3.org/1999/xhtml" class="no-js"><head>
  <meta charset="utf-8" />
  <meta http-equiv="X-UA-Compatible" content="IE=edge,chrome=1" />
  <meta name="viewport" content="width=device-width, initial-scale=1" />

  <title>Unsupervised Neural Multi-Document Abstractive Summarization of Reviews | OpenReview</title>
  <meta name="description" content="" />

      <meta name="citation_title" content="Unsupervised Neural Multi-Document Abstractive Summarization of Reviews" />
        <meta name="citation_author" content="Anonymous" />
      <meta name="citation_publication_date" content="2018/09/27" />
      <meta name="citation_online_date" content="2018/09/27" />
      <meta name="citation_pdf_url" content="https://openreview.net/pdf?id=rylhToC5YQ" />
      <meta name="twitter:card" content="summary" />
      <meta name="twitter:site" content="@openreviewnet" />
      <meta name="og:title" content="Unsupervised Neural Multi-Document Abstractive Summarization of..." />
      <meta name="og:description" content="Abstractive summarization has been studied using neural sequence transduction methods with datasets of large, paired document-summary examples. However, such datasets are rare and the models..." />
      <meta name="og:image" content="https://openreview.net/static/images/openreview_logo_512.png" />

  <link rel="stylesheet" href="https://fonts.googleapis.com/css?family=Noto+Sans:400,400i,700,700i" />

  <link rel="stylesheet" href="/static/css/bootstrap.min.css" />
  <link rel="stylesheet" href="/static/css/jquery-ui.min.css" />
  <link rel="stylesheet" href="/static/css/main.min.css" />
</head>

<body class="forum">
  <nav class="navbar navbar-inverse navbar-fixed-top" role="navigation">
    <div class="container">
  
      <div class="navbar-header">
        <button type="button" class="navbar-toggle collapsed" data-toggle="collapse" data-target="#navbar" aria-expanded="false" aria-controls="navbar">
          <span class="sr-only">Toggle navigation</span>
          <span class="icon-bar"></span>
          <span class="icon-bar"></span>
          <span class="icon-bar"></span>
        </button>
        <a class="navbar-brand home push-link" href="/"><strong>OpenReview</strong>.net</a>
      </div>
  
      <div id="navbar" class="navbar-collapse collapse">
        <form class="navbar-form navbar-left profile-search" role="search">
          <div class="form-group has-feedback">
            <input id="search_input" type="text" class="form-control ui-autocomplete-input" placeholder="Search ICLR 2019 Conference" autocomplete="off" />
            <span class="glyphicon glyphicon-search form-control-feedback" aria-hidden="true"></span>
          </div>
  
          <input id="search_group" type="hidden" value="ICLR.cc/2019/Conference" />
          <input id="search_content" type="hidden" value="all" />
        <ul id="ui-id-1" tabindex="0" class="ui-menu ui-widget ui-widget-content ui-autocomplete ui-front" style="display: none;"></ul></form>
  
        <ul class="nav navbar-nav navbar-right">
        
            <li id="user-menu"><a href="/login">Login</a></li>
        </ul>
      </div>
  
    </div>
  </nav>

  <div id="or-banner" class="banner" style="">
  <div class="container">
    <div class="row">
      <div class="col-xs-12"><a href="/group?id=ICLR.cc/2019/Conference" title="Venue Homepage"><img class="icon" src="/static/images/arrow_left.svg" /> Go to <strong>ICLR 2019 Conference</strong> homepage</a></div>
    </div>
  </div>
</div>
<div id="flash-message-container" class="alert alert-danger" role="alert" style="display: none;">
  <div class="container">
    <div class="row">
      <div class="col-xs-12">
        <div class="alert-content">
          <button type="button" class="close" aria-label="Close"><span aria-hidden="true">×</span></button>
        </div>
      </div>
    </div>
  </div>
</div>

<div class="container">
  <div class="row">
    <div class="col-xs-12">
      <main id="content" class="clearfix openbanner-visible legacy-styles"><div id="note_rylhToC5YQ" class="note panel"><div class="title_pdf_row clearfix"><h2 class="note_content_title"><a>Unsupervised Neural Multi-Document Abstractive Summarization of Reviews</a> <a class="note_content_pdf" href="/pdf?id=rylhToC5YQ" title="Download PDF" target="_blank"><img src="/static/images/pdf_icon_blue.svg" /></a> </h2></div><div class="meta_row"><span class="signatures">Anonymous</span></div><div class="clearfix"><div class="meta_row pull-left"><span class="date item">28 Sep 2018 (modified: 13 Nov 2018)</span><span class="item">ICLR 2019 Conference Blind Submission</span><span class="item">Readers: <span class="readers-icon glyphicon glyphicon-globe"></span> Everyone</span><span class="item"><a class="action-bibtex-modal" data-bibtex="@inproceedings{    &#10;anonymous2019unsupervised,    &#10;title={Unsupervised Neural Multi-Document Abstractive Summarization},    &#10;author={Anonymous},    &#10;booktitle={Submitted to International Conference on Learning Representations},    &#10;year={2019},    &#10;url={https://openreview.net/forum?id=rylhToC5YQ},    &#10;note={under review}    &#10;}">Show Bibtex</a></span><a class="note_content_pdf item" href="/revisions?id=rylhToC5YQ" target="_blank">Show Revisions</a></div><div class="meta_row meta_actions"></div></div><div class="note_contents"><span class="note_content_field">Abstract: </span><span class="note_content_value">Abstractive summarization has been studied using neural sequence transduction methods with datasets of large, paired document-summary examples. However, such datasets are rare and the models trained from them do not generalize to other domains. Recently, some progress has been made in learning sequence-to-sequence mappings with only unpaired examples. In our work, we consider the setting where there are only documents (product or business reviews) with no summaries provided, and propose an end-to-end, neural model architecture to perform unsupervised abstractive summarization. Our proposed model consists of an auto-encoder trained so that the mean of the representations of the input reviews decodes to a reasonable summary-review.  We consider variants of the proposed architecture and perform an ablation study to show the importance of specific components.  We show through metrics and human evaluation that the generated summaries are highly abstractive, fluent, relevant, and representative of the average sentiment of the input reviews.</span></div><div class="note_contents"><span class="note_content_field">Keywords: </span><span class="note_content_value">unsupervised learning, abstractive summarization, reviews, text generation</span></div><div class="note_contents"><span class="note_content_field">TL;DR: </span><span class="note_content_value">We propose an end-to-end neural model for unsupervised multi-document abstractive summarization, applying it to business and product reviews.</span></div><div class="reply_row clearfix"><div class="item" id="reply_count">8 Replies</div></div></div><hr class="small" /><div id="note_children"><div class="note_with_children"><div id="note_SylKksg_pm" class="note panel"><div class="title_pdf_row clearfix"><h2 class="note_content_title"><a>Summary of changes, Nov 12, 2018.</a>  <button class="btn btn-xs btn-default permalink-button" title="Link to this comment" data-permalink-url="https://openreview.net/forum?id=rylhToC5YQ&amp;noteId=SylKksg_pm"><span class="glyphicon glyphicon-link" aria-hidden="true"></span></button></h2></div><div class="meta_row"><span class="signatures">ICLR 2019 Conference Paper841 Authors</span></div><div class="clearfix"><div class="meta_row pull-left"><span class="date item">13 Nov 2018</span><span class="item">ICLR 2019 Conference Paper841 Official Comment</span><span class="item">Readers: <span class="readers-icon glyphicon glyphicon-globe"></span> Everyone</span></div><div class="meta_row meta_actions"></div></div><div class="note_contents"><span class="note_content_field">Comment: </span><span class="note_content_value">We thank all 3 reviewers for their feedback, which we have used to improve the paper. We summarize the changes here and replied individually as well to each reviewer below. 

While the proxy metrics defined are useful in model development, the gold standard for evaluating summaries, human evaluation, was missing and we have now added it to validate our model. These results agree (rank order) with the automatic metrics and show that the abstractive model has comparable sentiment agreement, information agreement, and fluency with the extractive method.

We made many changes to clarify the problem more formally and described the models in more detail. We also clarified limitations of the model and metrics. 

While we believe the architecture proposed can be applied to data other than reviews, we added “... of Reviews” to the title since we only showed results on reviews.
</span></div><div class="reply_row clearfix"></div></div><div class="children"></div></div><div class="note_with_children"><div id="note_BygCDbac37" class="note panel"><div class="title_pdf_row clearfix"><h2 class="note_content_title"><a>Promising unsupervised approach, but clarity issues</a>  <button class="btn btn-xs btn-default permalink-button" title="Link to this comment" data-permalink-url="https://openreview.net/forum?id=rylhToC5YQ&amp;noteId=BygCDbac37"><span class="glyphicon glyphicon-link" aria-hidden="true"></span></button></h2></div><div class="meta_row"><span class="signatures">ICLR 2019 Conference Paper841 AnonReviewer2</span></div><div class="clearfix"><div class="meta_row pull-left"><span class="date item">03 Nov 2018 (modified: 07 Nov 2018)</span><span class="item">ICLR 2019 Conference Paper841 Official Review</span><span class="item">Readers: <span class="readers-icon glyphicon glyphicon-globe"></span> Everyone</span></div><div class="meta_row meta_actions"></div></div><div class="note_contents"><span class="note_content_field">Review: </span><span class="note_content_value">Overall and positives:

The paper investigates the problem of multidocument summarization
without paired documents to summary data, thus using an unsupervised
approach. The main model is constructed using a pair of locked
autoencoders and decoders. The model is trained to optimize the
combination of 1. Loss between reconstructions of the original reviews
(from the encoded reviews) and original the reviews, 2. And the
average similarity of the encoded version of the docs with the encoded
representation of the summary, generated from the mean representation
of the given documents.

By comparing with a few simple baseline models, the authors were able
to demonstrate the potential of the design against several naive
approaches (on real datasets, YELP and AMAZON reviews). 
The necessity of several model components is demonstrated
through ablation studies. The paper is relatively well structured and
complete. The topic of the paper fits well with ICLR. The paper
provides decent technical contributions with some novel ideas about
multi-doc summary learning models without a (supervised) paired
data set.

Comments / Issues

[ issue 6 is most important ]

1.  Problem presentation. The problem was not properly introduced and
elaborated. In fact, there is not a formal and mathematical
introduction of the problem, input, output, dataset and model
parameters. The notations used are not very clearly defined and are
quite handwavy, (e.g. what is V, dimensions of inputs x_i was not
mentioned until much later in the paper). The authors should make
these more precise. Similar problem with presentations of the models,
parameters, and hyperparameters.

3.  How does non-equal weighted linear combinations of l_rec and l_sim
change the results? Other variation of the overall loss function? How
do we see the loss function interaction in the training, validation
and test data? With the proposed model, these could be interesting to
observe.

4.  In equation two, the decoder seems to be very directly affecting
the quality of the output summary. Teacher forcing was used to train
the decoder in part (1) of the model, but without ground truth, I
would expect more discussions and experiments on how the Gumbel
softmax trick affect or help the performance of the output.

5.  Baseline models and metrics

(1) There should be more details on how the language model is trained,
some examples, and how the reviews are generated from the language
model as a base model (in supplement?).

(2). It is difficult to get a sense of how these metrics corresponds
to the actual perceived quality of the summary from the
presentation. (see next)

(3). It will be more relevant to evaluate the proposed design
vs. other neural models, and/or more tested and proved methods.

6. The rating classifier (CLF) is intriguing, but it's not clearly
explained and its effect on the evaluation of the performance is not
clear: One of the key metrics used in the evaluation relies on the
output rating of a classifier, CLF, that predicts reader ratings on
reviews (eg on YELP).  The classifier is said to have 72%
accuracy. First, the accuracy is not clearly defined, and the details
of the classifier and its training is not explained (what features are
its input, is the output ordinal regression).  Equation 4 is not
explained clearly: what does 'comparing' in 'by comparing the
predicted rating given the summary rating..' mean?  The classifier may
have good performance, but it's unclear how this accuracy should
affect the results of the model comparisons.

The CLF is used to evaluate the rating of output
reviews from various models. There is no justification these outputs
are in the same space or generally the same type of document with the
training sample (assuming real Yelp reviews).  That is probably
particularly true for concatenation of the reviews, and the CLF classifier
scores the concatenation very high (or  eq 4 somehow leads to highest value
for the concatenation of reviews )... It's not clear whether such a classifier is 
beneficial in this context.

7. Summary vs Reviews. It seems that the model is built on an implicit
assumption that the output summary of the multi-doc should be
sufficiently similar with the individual input docs.  This may be not
true in many cases, which affects whether the approach generalizes.
Doc inputs could be covering different aspects of the review subject
(heterogeneity among the input docs, including topics, sentiment etc),
or they could have very different writing styles or length compared to
a summary.  The evaluation metrics may not work well in such
scenarios.  Maybe some pre-classification or clustering of the inputs,
and then doing summarization for each, would  help?  In the conclusions section, the
authors do mention summarizing negative and positive reviews
separately.





</span></div><div class="note_contents"><span class="note_content_field">Rating: </span><span class="note_content_value">5: Marginally below acceptance threshold</span></div><div class="note_contents"><span class="note_content_field">Confidence: </span><span class="note_content_value">4: The reviewer is confident but not absolutely certain that the evaluation is correct</span></div><div class="reply_row clearfix"></div></div><div class="children"><div class="note_with_children"><div id="note_SJgmRtgdT7" class="note panel"><div class="title_pdf_row clearfix"><h2 class="note_content_title"><a>Added human evaluation; additions to improve clarity of paper</a>  <button class="btn btn-xs btn-default permalink-button" title="Link to this comment" data-permalink-url="https://openreview.net/forum?id=rylhToC5YQ&amp;noteId=SJgmRtgdT7"><span class="glyphicon glyphicon-link" aria-hidden="true"></span></button></h2></div><div class="meta_row"><span class="signatures">ICLR 2019 Conference Paper841 Authors</span></div><div class="clearfix"><div class="meta_row pull-left"><span class="date item">13 Nov 2018</span><span class="item">ICLR 2019 Conference Paper841 Official Comment</span><span class="item">Readers: <span class="readers-icon glyphicon glyphicon-globe"></span> Everyone</span></div><div class="meta_row meta_actions"></div></div><div class="note_contents"><span class="note_content_field">Comment: </span><span class="note_content_value">Thank you for your review and the very helpful, comprehensive feedback, which we strove to address. We agree that the biggest previous issue was uncertainty around the evaluation. As a result we added results of a human evaluation that directly assesses various aspects of summarization quality and showing similar results as our proxy metrics. Regarding your specific points:

1. We’ve added a more formal, mathematical description of the problem setup. Overall, we’ve tried to be clearer and and consistent with our notation.

3. Good question. We did try a larger weight on l_sim (as intuitively, this loss helps the model produce outputs that actually summarize the original review), but we did not find meaningful differences and pointed this out in the paper.

4. Although there are no ground-truth summaries for Equation 2, there are ground truth reconstructions in Equation 1. As shown in the ablations, it is crucial that the decoders are tied in this architecture. In one of our experiments, the “Early cosine loss” (also shown schematically in Appendix A), we did not need to use the Gumbel-softmax estimator and simply decoded auto-regressively from the mean vector. That experiment shows that the decoding the summary as part of training significantly improves results.

5. (1) We’ve added details about how the language model was trained in the Experimental Setup section, as well as how the reviews were generated using the language model in the “No Training” baseline in the Baselines section. The reviews in the “No training” model are generated in the same fashion as the proposed model. The purpose of this baseline is to show that optimizing the proposed loss improves the output over simply using pre-trained language models.

(2) Great point. We’ve added human evaluation experiments on Mechanical Turk regarding the quality of the summaries to assess the validity of our metrics. Briefly, the results show that our metrics guided us to a good model -- the extractive and abstractive models obtain comparable results on how well they summarize information and sentiment, and the abstractive summaries are similarly fluent.

(3) There are no known neural, end-to-end models for this problem setup and this being the first is one of our main contributions. We hoped that reasonable model variations and ablations would probe into the efficacy of various aspects of our model. 

6. We’ve modified the Rating accuracy description to hopefully make clear that the classifier is trained by taking as input a review x (sequence of tokens) and producing probabilities over the 5 possible ratings (i.e. it is a classification problem and not a regression problem). There are no hand-engineered features. The rating with the highest probability is the predicted rating. This is then compared to the average rating of the original reviews (rounded to the nearest 1-5 star rating).

In general, we agree that the classifier baseline should be applied carefully. We’ve removed the concatenation baseline because we believe it’s outside the input space of the classifier. However, we believe the rating accuracy still applies to the other models and is a useful metric. For instance, the summaries produced by our model are constrained to the review space due to the tying of the decoders. Our human evaluation experiments also agree with the trends provided by our rating accuracy metric.

7. The assumption we make is the summary should be in some sense the “centroid” of the documents it is summarizing. If there are some positive reviews, but they are mostly negative, the summary will be mostly negative which is representative. If a priori we have a notion of review importance, we could weight some reviews higher in Equation (3) rather than equally. Or as you suggest we could summarize different clusters; in this case the most natural clustering is by review rating. In Figure 3, we show how multiple reviews could be generated for the same business, but pre-clustered by rating. We also clarify that our model architecture produces summaries in the form of a single review.
</span></div><div class="reply_row clearfix"></div></div><div class="children"></div></div></div></div><div class="note_with_children"><div id="note_B1gpdP5K37" class="note panel"><div class="title_pdf_row clearfix"><h2 class="note_content_title"><a>Evaluation methodology and measures are questionable and should not be adopted by the community</a>  <button class="btn btn-xs btn-default permalink-button" title="Link to this comment" data-permalink-url="https://openreview.net/forum?id=rylhToC5YQ&amp;noteId=B1gpdP5K37"><span class="glyphicon glyphicon-link" aria-hidden="true"></span></button></h2></div><div class="meta_row"><span class="signatures">ICLR 2019 Conference Paper841 AnonReviewer1</span></div><div class="clearfix"><div class="meta_row pull-left"><span class="date item">02 Nov 2018 (modified: 07 Nov 2018)</span><span class="item">ICLR 2019 Conference Paper841 Official Review</span><span class="item">Readers: <span class="readers-icon glyphicon glyphicon-globe"></span> Everyone</span></div><div class="meta_row meta_actions"></div></div><div class="note_contents"><span class="note_content_field">Review: </span><span class="note_content_value">This paper proposes a method for multi-document abstractive summarization. The model has two main components, one part is an autoencoder used to help learn encoded document representations which can be used to reconstruct the original documents, and a second component for the summarization step which also aims to ensure that the summary is similar to the original document. 

The biggest problem with this paper is in its evaluation methodology. I don't really know what any of the three evaluation measures are actually measuring, and there is no human subject evaluation back them up.
- Rating Accuracy seems to depend on the choice of CLF used, and at best says whether the summary conveys the same average opinion as the original reviews. This captures a small amount about the actual contents of the reviews. For example, it does not capture the distribution of opinions, or the actual contents that are conveyed.
- Word Overlap with the original documents does not seem to be a good measure of quality for abstractive systems, as there could easily be abstractive summaries with low overlap that are nevertheless very good exactly because they aggregate information and generalize. It is certainly not appropriate to use to compare between extractive and abstractive systems.
-There are many well-known problems with using log likelihood as a measure of fluency and grammaticality, such as biases around length, and frequency of the words.
It also seems that these evaluation measures would interact with the length of the summary being evaluated in ways which systems could game.

Other points:
- Multi-Lead-1: The lead baseline works very well in single-document news summarization. Since this model is being applied in a multi-document setting to something that is not news, it is hard to see how this baseline is justified.

- Despite the fact that the model is only applied to product reviews, and there seem to be modelling decisions tailored to this domain, the paper title does not specify so, which in my opinion is a type of over-claiming.

Having a paper with poor evaluation measure may set a precedent that causes damage to an entire line of research. For this reason, I am not comfortable with recommending an accept.</span></div><div class="note_contents"><span class="note_content_field">Rating: </span><span class="note_content_value">3: Clear rejection</span></div><div class="note_contents"><span class="note_content_field">Confidence: </span><span class="note_content_value">4: The reviewer is confident but not absolutely certain that the evaluation is correct</span></div><div class="reply_row clearfix"></div></div><div class="children"><div class="note_with_children"><div id="note_B1eUccxupQ" class="note panel"><div class="title_pdf_row clearfix"><h2 class="note_content_title"><a>Added human evaluation (that corresponds to our metrics)</a>  <button class="btn btn-xs btn-default permalink-button" title="Link to this comment" data-permalink-url="https://openreview.net/forum?id=rylhToC5YQ&amp;noteId=B1eUccxupQ"><span class="glyphicon glyphicon-link" aria-hidden="true"></span></button></h2></div><div class="meta_row"><span class="signatures">ICLR 2019 Conference Paper841 Authors</span></div><div class="clearfix"><div class="meta_row pull-left"><span class="date item">13 Nov 2018</span><span class="item">ICLR 2019 Conference Paper841 Official Comment</span><span class="item">Readers: <span class="readers-icon glyphicon glyphicon-globe"></span> Everyone</span></div><div class="meta_row meta_actions"></div></div><div class="note_contents"><span class="note_content_field">Comment: </span><span class="note_content_value">Thank you for your feedback, which we have incorporated, and resulting in, we believe, a much stronger paper. We agree the lack of human evaluation to validate our methods was a glaring omission in the original paper. As a result, we added Table 2 with human evaluation results of the summaries on multiple dimensions, showing our model is competitive with the extractive baseline with respect to representing the overall sentiment and information in the input reviews and also the fluency. We clarified that the automatic metrics (which rank order the methods similarly) are useful for guiding model development, but the only gold standard here is human evaluation.

Regarding the points about the metrics:
Rating accuracy: The sentiment of a good summary should be reflective of the overall sentiment of the reviews. We approximate this overall segment by the average rating. We clarify that this captures a necessary aspect of the summary, but by itself is not sufficient, which is why we have other things we look at, including now human evaluation.   The “actual contents that are conveyed” is meant to be covered by the word overlap score and one of our human eval questions.
Word overlap: we agree with your point that abstractive systems could have lower overlap because they aggregate information and generalize. We clarified in the paper that this word overlap score is included as a sanity check: it’s possible to get a high rating accuracy while talking about something completely unrelated, and a very low word-overlap would suggest something pathological. That said, it appears to rank-order similarly as our human evaluation question.
Negative log-likelihood: In this paper now we only use this metric to compare abstractive variants. It’s true that using it to compare to the “Concatenation” baseline (now removed) was inappropriate. The gold standard for measuring fluency would be a human evaluation which we added.

Other points:
Multi-lead 1: Having proved to be a strong baseline in other summarization tasks, we sought to create an analog of Multi-lead 1 in our multi-document setting. This proved to be a reasonably strong baseline. In any case, this is simply one of several baselines which we compare against.
We don’t want to overclaim, and we’ve modified the title of our paper to include “of Reviews” and added clarification of limitations in the Conclusion.</span></div><div class="reply_row clearfix"></div></div><div class="children"></div></div></div></div><div class="note_with_children"><div id="note_BJgnUKPvhX" class="note panel"><div class="title_pdf_row clearfix"><h2 class="note_content_title"><a>Novel work breaking ground on abstractive unsupervised multi-document summarization</a>  <button class="btn btn-xs btn-default permalink-button" title="Link to this comment" data-permalink-url="https://openreview.net/forum?id=rylhToC5YQ&amp;noteId=BJgnUKPvhX"><span class="glyphicon glyphicon-link" aria-hidden="true"></span></button></h2></div><div class="meta_row"><span class="signatures">ICLR 2019 Conference Paper841 AnonReviewer3</span></div><div class="clearfix"><div class="meta_row pull-left"><span class="date item">01 Nov 2018 (modified: 07 Nov 2018)</span><span class="item">ICLR 2019 Conference Paper841 Official Review</span><span class="item">Readers: <span class="readers-icon glyphicon glyphicon-globe"></span> Everyone</span></div><div class="meta_row meta_actions"></div></div><div class="note_contents"><span class="note_content_field">Review: </span><span class="note_content_value"># Positive aspects of this submission

- This submission presents a really novel, creative, and useful way to achieve unsupervised abstractive multi-document summarization, which is quite an impressive feat.

- The alternative metrics in the absence of ground-truth summaries seem really useful and can be reused for other summarization problems where ground-truth summaries are missing. In particular, the prediction of review/summary score as a summarization metric is very well thought of.

- The model variations and experiments clearly demonstrate the usefulness of every aspect of the proposed model.

# Criticism

- The proposed model assumes that the output summary is similar in writing style and length to each of the inputs, which is not the case for most summarization tasks. This makes the proposed model hard to compare to the majority of previous works in supervised multi-document summarization like the ones evaluated on the DUC 2004 dataset.

- The lack of applicability to existing supervised summarization use cases leaves unanswered the question of how much correlation there is between the proposed unsupervised metrics and existing metrics like the ROUGE score, even if they seem intuitively correlated.

- This model suffers from the usual symptoms of other abstractive summarization models (fluency errors, factual inaccuracies). But this shouldn't overshadow the bigger contributions of this paper, since dealing with these specific issues is still an open research problem.</span></div><div class="note_contents"><span class="note_content_field">Rating: </span><span class="note_content_value">9: Top 15% of accepted papers, strong accept</span></div><div class="note_contents"><span class="note_content_field">Confidence: </span><span class="note_content_value">4: The reviewer is confident but not absolutely certain that the evaluation is correct</span></div><div class="reply_row clearfix"></div></div><div class="children"><div class="note_with_children"><div id="note_SJeQ9oU_6X" class="note panel"><div class="title_pdf_row clearfix"><h2 class="note_content_title"><a>Added human evaluation and further discussed limitations. </a>  <button class="btn btn-xs btn-default permalink-button" title="Link to this comment" data-permalink-url="https://openreview.net/forum?id=rylhToC5YQ&amp;noteId=SJeQ9oU_6X"><span class="glyphicon glyphicon-link" aria-hidden="true"></span></button></h2></div><div class="meta_row"><span class="signatures">ICLR 2019 Conference Paper841 Authors</span></div><div class="clearfix"><div class="meta_row pull-left"><span class="date item">13 Nov 2018</span><span class="item">ICLR 2019 Conference Paper841 Official Comment</span><span class="item">Readers: <span class="readers-icon glyphicon glyphicon-globe"></span> Everyone</span></div><div class="meta_row meta_actions"></div></div><div class="note_contents"><span class="note_content_field">Comment: </span><span class="note_content_value">Thank you for your feedback. 

-We clarified in the paper that a limitation here is the summary is assumed to be in the form of a review with similar stylistic characteristics as the input reviews.
-Although ROUGE is often used in the summarization literature, it attempts to approximate human evaluation of summaries, which is the gold standard. We have shown that the metrics we used in model development have guided us to good human evaluations.
- Indeed the symptoms discussed in the error analysis affect all current neural text generation models, and we included them to ensure we didn't claim to have solved it.</span></div><div class="reply_row clearfix"></div></div><div class="children"></div></div></div></div><div class="note_with_children"><div id="note_rJgGTrfa5Q" class="note panel"><div class="title_pdf_row clearfix"><h2 class="note_content_title"><a>Further research about factual inaccuracies</a>  <button class="btn btn-xs btn-default permalink-button" title="Link to this comment" data-permalink-url="https://openreview.net/forum?id=rylhToC5YQ&amp;noteId=rJgGTrfa5Q"><span class="glyphicon glyphicon-link" aria-hidden="true"></span></button></h2></div><div class="meta_row"><span class="signatures">(anonymous)</span></div><div class="clearfix"><div class="meta_row pull-left"><span class="date item">12 Oct 2018</span><span class="item">ICLR 2019 Conference Paper841 Public Comment</span><span class="item">Readers: <span class="readers-icon glyphicon glyphicon-globe"></span> Everyone</span></div><div class="meta_row meta_actions"></div></div><div class="note_contents"><span class="note_content_field">Comment: </span><span class="note_content_value">In Section 5.5, you mention that "Factual
accuracy is an ongoing area of research in the summarization field". Do you have some recent papers in mind that analyze or address this specific issue in generated summaries?

By the way, this is a very interesting and thought-provoking submission!</span></div><div class="reply_row clearfix"></div></div><div class="children"><div class="note_with_children"><div id="note_BJe1LOl_aX" class="note panel trashed"><div class="title_pdf_row clearfix"><h2 class="note_content_title">  <button class="btn btn-xs btn-default permalink-button" title="Link to this comment" data-permalink-url="https://openreview.net/forum?id=rylhToC5YQ&amp;noteId=BJe1LOl_aX"><span class="glyphicon glyphicon-link" aria-hidden="true"></span></button></h2></div><div class="meta_row"><span class="signatures">[Deleted]</span></div><div class="clearfix"><div class="meta_row pull-left"><span class="item">ICLR 2019 Conference Paper841 Official Comment</span><span class="item">Readers: <span class="readers-icon glyphicon glyphicon-globe"></span> Everyone</span></div><div class="meta_row meta_actions"></div></div><div class="reply_row clearfix"></div></div><div class="children"></div></div></div></div></div></main></div>
  </div>
</div>


    <!-- Footer -->
    <footer class="sitemap">
      <div class="container">
        <div class="row hidden-xs">
          <div class="col-sm-4">
            <ul class="list-unstyled">
              <li><a href="/" class="home">Home</a></li>
              <li><a href="/venues">All Venues</a></li>
            </ul>
          </div>
    
          <div class="col-sm-4">
            <ul class="list-unstyled">
              <li><a href="/about">About OpenReview</a></li>
              <li><a href="/contact">Contact</a></li>
              <li><a href="#" data-toggle="modal" data-target="#feedback-modal">Feedback</a></li>
            </ul>
          </div>
    
          <div class="col-sm-4">
            <ul class="list-unstyled">
              <li><a href="/terms">Terms of Service</a></li>
              <li><a href="/privacy">Privacy Policy</a></li>
            </ul>
          </div>
        </div>
    
        <div class="row visible-xs-block">
          <div class="col-xs-6">
            <ul class="list-unstyled">
              <li><a href="/" class="home">Home</a></li>
              <li><a href="/about">About OpenReview</a></li>
              <li><a href="#" data-toggle="modal" data-target="#feedback-modal">Feedback</a></li>
            </ul>
          </div>
    
          <div class="col-xs-6">
            <ul class="list-unstyled">
              <li><a href="/contact">Contact</a></li>
              <li><a href="/terms">Terms of Service</a></li>
              <li><a href="/privacy">Privacy Policy</a></li>
            </ul>
          </div>
        </div>
      </div>
    </footer>
    
    <footer class="sponsor">
      <div class="container">
        <div class="row">
          <div class="col-sm-10 col-sm-offset-1">
            <p class="text-center">
              OpenReview is created by the <a href="http://www.iesl.cs.umass.edu/" target="_blank">Information Extraction and Synthesis Laboratory</a>, College of Information and Computer Science, University of Massachusetts Amherst. We gratefully acknowledge the support of the OpenReview sponsors:  Google,  Facebook, NSF, the University of Massachusetts Amherst Center for Data Science, and Center for Intelligent Information Retrieval, as well as the Google Cloud Platform for donating the computing and networking services on which OpenReview.net runs.
            </p>
          </div>
        </div>
      </div>
    </footer>

  <div id="feedback-modal" class="modal fade" tabindex="-1" role="dialog">
    <div class="modal-dialog">
      <div class="modal-content">
        <div class="modal-header">
          <button type="button" class="close" data-dismiss="modal" aria-label="Close"><span aria-hidden="true">×</span></button>
          <h3 class="modal-title">Send Feedback</h3>
        </div>
        <div class="modal-body">
          <p>Enter your feedback below and we'll get back to you as soon as possible.</p>
          <form action="/feedback" method="POST">
            <div class="form-group">
              <input type="email" name="from" class="form-control" placeholder="Email" />
            </div>
            <div class="form-group">
              <input type="text" name="subject" class="form-control" placeholder="Subject" />
            </div>
            <div class="form-group">
              <textarea name="message" class="form-control feedback-input" rows="5" placeholder="Message" required=""></textarea>
            </div>
          <ul id="ui-id-2" tabindex="0" class="ui-menu ui-widget ui-widget-content" style="display: none;"></ul></form>
        </div>
        <div class="modal-footer">
          <button type="button" class="btn btn-default" data-dismiss="modal">Cancel</button>
          <button type="button" class="btn btn-primary">Send</button>
        </div>
      </div>
    </div>
  </div>

  <script type="text/javascript" async="" src="https://www.google-analytics.com/analytics.js"></script><script src="//cdnjs.cloudflare.com/ajax/libs/jquery/2.2.4/jquery.min.js"></script>
  <script>window.jQuery || document.write('&lt;script src="/static/js/vendor/jquery-2.2.4.min.js"&gt;&lt;\/script&gt;')</script>

  <script src="/static/dist/vendor.min.js"></script>
  <script src="/static/dist/templates.min.js"></script>
  <script src="/static/dist/openreview.min.js"></script>

    <script>window.legacyScripts = true;</script>


    <script async="" src="https://www.googletagmanager.com/gtag/js?id=UA-108703919-1"></script>
    <script>
      window.dataLayer = window.dataLayer || [];
      function gtag() { dataLayer.push(arguments); }
      gtag('js', new Date());
      gtag('config', 'UA-108703919-1');
    </script>


<div role="status" aria-live="assertive" aria-relevant="additions" class="ui-helper-hidden-accessible"></div></body></html>