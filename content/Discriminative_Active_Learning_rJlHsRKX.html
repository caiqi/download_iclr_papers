<!DOCTYPE html><html xmlns="http://www.w3.org/1999/xhtml" class="no-js"><head>
  <meta charset="utf-8" />
  <meta http-equiv="X-UA-Compatible" content="IE=edge,chrome=1" />
  <meta name="viewport" content="width=device-width, initial-scale=1" />

  <title>Discriminative Active Learning | OpenReview</title>
  <meta name="description" content="" />

      <meta name="citation_title" content="Discriminative Active Learning" />
        <meta name="citation_author" content="Anonymous" />
      <meta name="citation_publication_date" content="2018/09/27" />
      <meta name="citation_online_date" content="2018/09/27" />
      <meta name="citation_pdf_url" content="https://openreview.net/pdf?id=rJl-HsR9KX" />
      <meta name="twitter:card" content="summary" />
      <meta name="twitter:site" content="@openreviewnet" />
      <meta name="og:title" content="Discriminative Active Learning" />
      <meta name="og:description" content="We propose a new batch mode active learning algorithm designed for neural networks and large query batch sizes. The method, Discriminative Active Learning (DAL), poses active learning as a binary..." />
      <meta name="og:image" content="https://openreview.net/static/images/openreview_logo_512.png" />

  <link rel="stylesheet" href="https://fonts.googleapis.com/css?family=Noto+Sans:400,400i,700,700i" />

  <link rel="stylesheet" href="/static/css/bootstrap.min.css" />
  <link rel="stylesheet" href="/static/css/jquery-ui.min.css" />
  <link rel="stylesheet" href="/static/css/main.min.css" />
</head>

<body class="forum">
  <nav class="navbar navbar-inverse navbar-fixed-top" role="navigation">
    <div class="container">
  
      <div class="navbar-header">
        <button type="button" class="navbar-toggle collapsed" data-toggle="collapse" data-target="#navbar" aria-expanded="false" aria-controls="navbar">
          <span class="sr-only">Toggle navigation</span>
          <span class="icon-bar"></span>
          <span class="icon-bar"></span>
          <span class="icon-bar"></span>
        </button>
        <a class="navbar-brand home push-link" href="/"><strong>OpenReview</strong>.net</a>
      </div>
  
      <div id="navbar" class="navbar-collapse collapse">
        <form class="navbar-form navbar-left profile-search" role="search">
          <div class="form-group has-feedback">
            <input id="search_input" type="text" class="form-control ui-autocomplete-input" placeholder="Search ICLR 2019 Conference" autocomplete="off" />
            <span class="glyphicon glyphicon-search form-control-feedback" aria-hidden="true"></span>
          </div>
  
          <input id="search_group" type="hidden" value="ICLR.cc/2019/Conference" />
          <input id="search_content" type="hidden" value="all" />
        <ul id="ui-id-1" tabindex="0" class="ui-menu ui-widget ui-widget-content ui-autocomplete ui-front" style="display: none;"></ul></form>
  
        <ul class="nav navbar-nav navbar-right">
        
            <li id="user-menu"><a href="/login">Login</a></li>
        </ul>
      </div>
  
    </div>
  </nav>

  <div id="or-banner" class="banner" style="">
  <div class="container">
    <div class="row">
      <div class="col-xs-12"><a href="/group?id=ICLR.cc/2019/Conference" title="Venue Homepage"><img class="icon" src="/static/images/arrow_left.svg" /> Go to <strong>ICLR 2019 Conference</strong> homepage</a></div>
    </div>
  </div>
</div>
<div id="flash-message-container" class="alert alert-danger" role="alert" style="display: none;">
  <div class="container">
    <div class="row">
      <div class="col-xs-12">
        <div class="alert-content">
          <button type="button" class="close" aria-label="Close"><span aria-hidden="true">×</span></button>
        </div>
      </div>
    </div>
  </div>
</div>

<div class="container">
  <div class="row">
    <div class="col-xs-12">
      <main id="content" class="clearfix openbanner-visible legacy-styles"><div id="note_rJl-HsR9KX" class="note panel"><div class="title_pdf_row clearfix"><h2 class="note_content_title"><a>Discriminative Active Learning</a> <a class="note_content_pdf" href="/pdf?id=rJl-HsR9KX" title="Download PDF" target="_blank"><img src="/static/images/pdf_icon_blue.svg" /></a> </h2></div><div class="meta_row"><span class="signatures">Anonymous</span></div><div class="clearfix"><div class="meta_row pull-left"><span class="date item">28 Sep 2018 (modified: 08 Nov 2018)</span><span class="item">ICLR 2019 Conference Blind Submission</span><span class="item">Readers: <span class="readers-icon glyphicon glyphicon-globe"></span> Everyone</span><span class="item"><a class="action-bibtex-modal" data-bibtex="@inproceedings{    &#10;anonymous2019discriminative,    &#10;title={Discriminative Active Learning},    &#10;author={Anonymous},    &#10;booktitle={Submitted to International Conference on Learning Representations},    &#10;year={2019},    &#10;url={https://openreview.net/forum?id=rJl-HsR9KX},    &#10;note={under review}    &#10;}">Show Bibtex</a></span><a class="note_content_pdf item" href="/revisions?id=rJl-HsR9KX" target="_blank">Show Revisions</a></div><div class="meta_row meta_actions"></div></div><div class="note_contents"><span class="note_content_field">Abstract: </span><span class="note_content_value">We propose a new batch mode active learning algorithm designed for neural networks and large query batch sizes. The method, Discriminative Active Learning (DAL), poses active learning as a binary classification task, attempting to choose examples to label in such a way as to make the labeled set and the unlabeled pool indistinguishable. Experimenting on image classification tasks, we empirically show our method to be on par with state of the art methods in medium and large query batch sizes, while being simple to implement and also extend to other domains besides classification tasks. Our experiments also show that none of the state of the art methods of today are clearly better than uncertainty sampling, negating some of the reported results in the recent literature.</span></div><div class="note_contents"><span class="note_content_field">TL;DR: </span><span class="note_content_value">A new active learning algorithm for the batch mode setting using neural networks</span></div><div class="note_contents"><span class="note_content_field">Keywords: </span><span class="note_content_value">Active Learning, Neural Networks</span></div><div class="reply_row clearfix"><div class="item" id="reply_count">12 Replies</div></div></div><hr class="small" /><div id="note_children"><div class="note_with_children"><div id="note_BkeIeafd6m" class="note panel"><div class="title_pdf_row clearfix"><h2 class="note_content_title"><a>DFAL's implementation</a>  <button class="btn btn-xs btn-default permalink-button" title="Link to this comment" data-permalink-url="https://openreview.net/forum?id=rJl-HsR9KX&amp;noteId=BkeIeafd6m"><span class="glyphicon glyphicon-link" aria-hidden="true"></span></button></h2></div><div class="meta_row"><span class="signatures"><a href="/profile?id=~M%C3%A9lanie_Ducoffe1" class="profile-link">Mélanie Ducoffe</a></span></div><div class="clearfix"><div class="meta_row pull-left"><span class="date item">13 Nov 2018</span><span class="item">ICLR 2019 Conference Paper65 Public Comment</span><span class="item">Readers: <span class="readers-icon glyphicon glyphicon-globe"></span> Everyone</span></div><div class="meta_row meta_actions"></div></div><div class="note_contents"><span class="note_content_field">Comment: </span><span class="note_content_value">Tahnk you for this paper. I do agree that domain adaptation is highly relevant for the active learning community, and thus your method opens up promising results for the field.

As one of the main author of DFAL, I would like to point out a misinterpretation of our method in your implementation: DFAL queries the data with the smallest adversarial perturbation, but adds also the corresponding adversarial examples to the labeled training set, using twice the same label. In our experiments (check our arxiv version), we noticed that adding the adversarial samples in the labeled training set do increase the test accuracy.

On the other hand, I disagree with your overall statement “none of the state-of-the-art methods of today are clearly better than uncertainty sampling”. DFAL is a top score approach, thus it does not take into account the correlations of the samples, as any batch mode active learning methods do. This is why we conduct our experiments on small query batch sizes (10 in most of our experiments). Indeed, in Figure 3 of your paper, the highest ranking samples of DBAL/DFAL are not correlated to the most uncertainty samples. Then by using a smaller size of queries, you should notice some improvements of those methods.
For fairness sake, it would be better either to run your experiments with smaller query batch sizes or use a diversity-based criterion with DFAL, and other top score approaches such as DBAL or EGL. For example, [Submodularity in Data Subset Selection and Active Learning, Wei et al. 2015] has been combined with uncertainty selection on deep neural networks.

In case the authors wish to extend their experiments to shallow classifiers, I would highly suggest a comparison with [Querying Representative And Discriminative Batch Mode Active Learning, Wang et al. 2013], that combines uncertainty selection and MMD discrepancy to cover at best the distribution.

I just intend this as feedbacks for revision, I don't intend for this to have any bearing on whether the paper is accepted or rejected.</span></div><div class="reply_row clearfix"></div></div><div class="children"><div class="note_with_children"><div id="note_rJeyyMi567" class="note panel"><div class="title_pdf_row clearfix"><h2 class="note_content_title"><a>Response</a>  <button class="btn btn-xs btn-default permalink-button" title="Link to this comment" data-permalink-url="https://openreview.net/forum?id=rJl-HsR9KX&amp;noteId=rJeyyMi567"><span class="glyphicon glyphicon-link" aria-hidden="true"></span></button></h2></div><div class="meta_row"><span class="signatures">ICLR 2019 Conference Paper65 Authors</span></div><div class="clearfix"><div class="meta_row pull-left"><span class="date item">15 Nov 2018</span><span class="item">ICLR 2019 Conference Paper65 Official Comment</span><span class="item">Readers: <span class="readers-icon glyphicon glyphicon-globe"></span> Everyone</span></div><div class="meta_row meta_actions"></div></div><div class="note_contents"><span class="note_content_field">Comment: </span><span class="note_content_value">Thank you for your comments and for your original DFAL paper, which we found to be very compelling.

We apologize for misinterpreting your method in that we did not add the adversarial examples to our labeled set along with the original samples, which could have increased performance as you wrote. However, this sort of adversarial training is known to be a good regularizer for the training process in general and so we would say that any active learning method can benefit from this sort of data augmentation. If we understand your method correctly, the choice of example to label and the adversarial data augmentation can be decoupled, and this way all methods may benefit from it. So, adding the adversarial examples to the training set only for one AL method could be seen as an unfair advantage to that method.

Regarding our statement that no method clearly beats uncertainty sampling, we would clarify that this is definitely true for large batch sizes (100 and more), which we claim are the only viable query sizes for deep learning applications. This means that a deep learning practitioner when faced with the need to choose an active learning method will have a hard time convincing himself to use a method that is more complicated than uncertainty sampling, given the current empirical evidence (we unfortunately have to include DAL in this assessment). We feel this is an important point to make, as it contradicts the results of the core set paper, for example.
Even though we feel the smaller query batch size domain isn't as relevant to deep learning applications, we reran our experiments again on the MNIST dataset with an initial labeled set of size 100 and query batch size of 10 as you suggested. These experiments do show a slight advantage to DFAL over the other methods, but one that is small compared to the one presented in your paper. The graphs look quite similar to the query size of 100 in our paper - DFAL wins initially (up to ~400 labeled samples), then DBAL and uncertainty sampling begin to catch up to it. The clear difference between the DAL and Core Set methods and DFAL, DBAL and US is present as in the query size 100 experiments in our paper, but surprisingly we see that DAL clearly beats the Core Set approach, which turns out not to outperform random sampling. This is different from your results for Core Set (which are close to DFAL), but we would need more time to understand what we did differently and so we reserve judgement about Core Set at this point.
Given these results, we would still say that there needs to be more empirical evidence in order to choose DFAL or any other method over uncertainty sampling in small query sizes.</span></div><div class="reply_row clearfix"></div></div><div class="children"></div></div></div></div><div class="note_with_children"><div id="note_HJeEuH3b67" class="note panel"><div class="title_pdf_row clearfix"><h2 class="note_content_title"><a>Summary of changes to the revised version of the paper</a>  <button class="btn btn-xs btn-default permalink-button" title="Link to this comment" data-permalink-url="https://openreview.net/forum?id=rJl-HsR9KX&amp;noteId=HJeEuH3b67"><span class="glyphicon glyphicon-link" aria-hidden="true"></span></button></h2></div><div class="meta_row"><span class="signatures">ICLR 2019 Conference Paper65 Authors</span></div><div class="clearfix"><div class="meta_row pull-left"><span class="date item">08 Nov 2018</span><span class="item">ICLR 2019 Conference Paper65 Official Comment</span><span class="item">Readers: <span class="readers-icon glyphicon glyphicon-globe"></span> Everyone</span></div><div class="meta_row meta_actions"></div></div><div class="note_contents"><span class="note_content_field">Comment: </span><span class="note_content_value">We thank the reviewers and others for the effort they spent in reviewing our paper. We hope our responses help answer some of the reviewer concerns. We have uploaded a revised version of our paper to answer some of these concerns. We tried to be concise as to not exceed the 8 page restriction too much. The changes are as following:
1. We added a discussion comparing our work to the unsupervised domain adaptation paper mentioned by reviewer #2. This appears section 3.1.1.
2. We clarified section 4.3 which deals with handling overfitting issues. We explain that the 98% accuracy we mention is relevant for an unlabeled pool that is ten times the size of the labeled set.
3. We changed the citations to be /citep instead of /citet and added commas between consecutive citations.</span></div><div class="reply_row clearfix"></div></div><div class="children"></div></div><div class="note_with_children"><div id="note_rJgQJxcgpQ" class="note panel"><div class="title_pdf_row clearfix"><h2 class="note_content_title"><a>A question on the domain adaptation bound</a>  <button class="btn btn-xs btn-default permalink-button" title="Link to this comment" data-permalink-url="https://openreview.net/forum?id=rJl-HsR9KX&amp;noteId=rJgQJxcgpQ"><span class="glyphicon glyphicon-link" aria-hidden="true"></span></button></h2></div><div class="meta_row"><span class="signatures">(anonymous)</span></div><div class="clearfix"><div class="meta_row pull-left"><span class="date item">08 Nov 2018</span><span class="item">ICLR 2019 Conference Paper65 Public Comment</span><span class="item">Readers: <span class="readers-icon glyphicon glyphicon-globe"></span> Everyone</span></div><div class="meta_row meta_actions"></div></div><div class="note_contents"><span class="note_content_field">Comment: </span><span class="note_content_value">In Lemma 3.1 of Ben-David (2010) adaptation bound, the gap between the empirical and expected diveregence depends on the  VC dim of the classifier. However, in the deep neural network such a value will be **extremely** high. Therefore when you only minimize the empirical divergence, this Lemma can not support it will have a small divergence between the two **real** distributions, since the complexity term will be much bigger than the empirical divergence.




  </span></div><div class="reply_row clearfix"></div></div><div class="children"><div class="note_with_children"><div id="note_Ske5lz2b6X" class="note panel"><div class="title_pdf_row clearfix"><h2 class="note_content_title"><a>True - we only use the bound as motivation</a>  <button class="btn btn-xs btn-default permalink-button" title="Link to this comment" data-permalink-url="https://openreview.net/forum?id=rJl-HsR9KX&amp;noteId=Ske5lz2b6X"><span class="glyphicon glyphicon-link" aria-hidden="true"></span></button></h2></div><div class="meta_row"><span class="signatures">ICLR 2019 Conference Paper65 Authors</span></div><div class="clearfix"><div class="meta_row pull-left"><span class="date item">08 Nov 2018</span><span class="item">ICLR 2019 Conference Paper65 Official Comment</span><span class="item">Readers: <span class="readers-icon glyphicon glyphicon-globe"></span> Everyone</span></div><div class="meta_row meta_actions"></div></div><div class="note_contents"><span class="note_content_field">Comment: </span><span class="note_content_value">Thank you for your interest and question regarding our paper.
You are correct in that the bound we present is very far from being tight when the hypothesis class in question is a family of neural networks. Unfortunately, this problem exists in all generalization bounds that use a capacity measure of the hypothesis class to derive the bound.
For this reason, we treat the theoretical bound from domain adaptation as merely a motivation for our algorithm.</span></div><div class="reply_row clearfix"></div></div><div class="children"></div></div></div></div><div class="note_with_children"><div id="note_S1lASPI9nm" class="note panel"><div class="title_pdf_row clearfix"><h2 class="note_content_title"><a>very nice paper with neat idea, good theoretical intuition/justification, clear transparent code/algorithm, and good experimental results</a>  <button class="btn btn-xs btn-default permalink-button" title="Link to this comment" data-permalink-url="https://openreview.net/forum?id=rJl-HsR9KX&amp;noteId=S1lASPI9nm"><span class="glyphicon glyphicon-link" aria-hidden="true"></span></button></h2></div><div class="meta_row"><span class="signatures">ICLR 2019 Conference Paper65 AnonReviewer1</span></div><div class="clearfix"><div class="meta_row pull-left"><span class="date item">03 Nov 2018 (modified: 07 Nov 2018)</span><span class="item">ICLR 2019 Conference Paper65 Official Review</span><span class="item">Readers: <span class="readers-icon glyphicon glyphicon-globe"></span> Everyone</span></div><div class="meta_row meta_actions"></div></div><div class="note_contents"><span class="note_content_field">Review: </span><span class="note_content_value">Thank you for this enjoyable paper. 

Summary: The authors propose a novel approach to active learning as follows. At each iteration they develop a classifier that can discriminate between the samples in the labeled and unlabeled sets; they select the top few samples that are most likely to be from the unlabeled set as per this classifier, and request the oracle to provide labels for this batch next. This simple idea is shown to have a principled basis and theoretical background, related to GANs and to previous results from the literature. They provide clear algorithms and open source code for easy verification, and public testing. They provide good experimental verification on CIFAR-10 and MNIST benchmarks. I personally look at new papers more for novel ideas and good intuition/theoretical justification than an immediate improvement in benchmark results, so I enjoyed this paper thoroughly. 

Results: Among other things they show that their algorithms ranks the samples to be next labeled quite differently than uncertainty sampling based approaches; that their method is at least as accurate/sample-efficient as the state of the art ; and that some previously published experimental results are incorrect(!). As the authors will probably agree I am not convinced the proposed method is better than previous algorithms in any statistically significant way, but the novel idea itself is worth publishing even if it is just as good as the state of the art. 

Novelty: I liked the paper very much because it provides quite an innovative new approach to look at active learning, which resembles GANs and Core set ideas in some ways, yet differs in significant ways that are critical for active learning. I've been working and publishing in related areas for a long time so I genuinely found your central idea refreshing and new.

Relevance: The paper is very relevant to the ICLR community and addresses critical questions. 

Question:
My intuition as a Bayesian is that we most need to find labels that maximize the mutual information I(y,w) where w are the weights of the neural net. In practice this corresponds to the samples x which have the maximum class uncertainty, but for which the parameters under the posterior disagree about the outcome the most, eg see discussion below equation 2 for  Bayesian Active Learning by Disagreement (BALD) in this paper <a href="https://arxiv.org/pdf/1112.5745.pdf" target="_blank" rel="nofollow">https://arxiv.org/pdf/1112.5745.pdf</a> . In essence: The above means that the labels that provide most information about the classification model are most valuable for active learning. 

However, your approach intuitively ignores the conditional distribution(ie py(|x)), and instead tries to make the original unconditional distribution p(x) between the labeled and unlabeled sets similar. Yet, it works beautifully. So: Why does this work? What is the intuition?
</span></div><div class="note_contents"><span class="note_content_field">Rating: </span><span class="note_content_value">8: Top 50% of accepted papers, clear accept</span></div><div class="note_contents"><span class="note_content_field">Confidence: </span><span class="note_content_value">4: The reviewer is confident but not absolutely certain that the evaluation is correct</span></div><div class="reply_row clearfix"></div></div><div class="children"><div class="note_with_children"><div id="note_HJgzFz2bpQ" class="note panel"><div class="title_pdf_row clearfix"><h2 class="note_content_title"><a>Some intuition for ignoring P(y|x)</a>  <button class="btn btn-xs btn-default permalink-button" title="Link to this comment" data-permalink-url="https://openreview.net/forum?id=rJl-HsR9KX&amp;noteId=HJgzFz2bpQ"><span class="glyphicon glyphicon-link" aria-hidden="true"></span></button></h2></div><div class="meta_row"><span class="signatures">ICLR 2019 Conference Paper65 Authors</span></div><div class="clearfix"><div class="meta_row pull-left"><span class="date item">08 Nov 2018</span><span class="item">ICLR 2019 Conference Paper65 Official Comment</span><span class="item">Readers: <span class="readers-icon glyphicon glyphicon-globe"></span> Everyone</span></div><div class="meta_row meta_actions"></div></div><div class="note_contents"><span class="note_content_field">Comment: </span><span class="note_content_value"> We appreciate the time and effort of reviewing our paper, and thank the reviewer for the kind words.

As for an intuition for why our method should work while ignoring the conditional probability P(y|x), we would not view the problem we are trying to solve as one which is tailored only to classification, and so we do not need access to the probability over the labels. As you say, we are trying to find a subset of the distribution which represents the true distribution as much as possible. Under the assumption that the distribution of labels is strongly dependent on the inputs (a reasonable assumption in our view), then getting a labeled set which correctly captures P(x) should reasonably capture P(x,y) as well, which is what we need for classification. A similar notion can be found in the Core-Set approach, where Lipschitz assumptions on the labeling function allows for a bound on the test loss of the classifier, and so the objective becomes to cover the representation of the data (which ignores the labels as well).
We would also note that while we do ignore P(y|x) in DAL, there is still information from the labels that is used during the entire active learning process, since we are running DAL on a representation learned by a classifier that uses the labels in the labeled set.
Having said all of that, it is quite reasonable to think that a method which combines DAL with other methods which do use the label information could improve on our results.</span></div><div class="reply_row clearfix"></div></div><div class="children"></div></div></div></div><div class="note_with_children"><div id="note_SJgxZFHc2m" class="note panel"><div class="title_pdf_row clearfix"><h2 class="note_content_title"><a>Interesting but counter-intuitive idea which works well in practice. It needs a better motivation/explanation.</a>  <button class="btn btn-xs btn-default permalink-button" title="Link to this comment" data-permalink-url="https://openreview.net/forum?id=rJl-HsR9KX&amp;noteId=SJgxZFHc2m"><span class="glyphicon glyphicon-link" aria-hidden="true"></span></button></h2></div><div class="meta_row"><span class="signatures">ICLR 2019 Conference Paper65 AnonReviewer3</span></div><div class="clearfix"><div class="meta_row pull-left"><span class="date item">03 Nov 2018 (modified: 07 Nov 2018)</span><span class="item">ICLR 2019 Conference Paper65 Official Review</span><span class="item">Readers: <span class="readers-icon glyphicon glyphicon-globe"></span> Everyone</span></div><div class="meta_row meta_actions"></div></div><div class="note_contents"><span class="note_content_field">Review: </span><span class="note_content_value">The paper is proposing a distribution matching as a metric for active learning. Basic intuition is: if we can make the distribution of labelled and unlabelled examples similar to each other, training error in one will approximate the training error in the other. Hence, a model learned using labelled ones will do well in unlabelled ones. The main tool to enforce this distributional distance is using adversarial learning similar to GANs or gradient reversal network for domain adaptation.

The idea is definitely interesting. I am not sure about why should it work (I explained in detail later), but it does work well empirically. Moreover, it is very easy to implement. Given any learned or hard-coded features, learning a simple binary classifier is sufficient to implement the method. The mini-queries idea in 4.1 is especially interesting. Handling large batches in active learning is always a problem but this neat trick make it much easier.

I think the proposed method is counter intuitive as the discussion does not explain why should it work better than random sampling. Clearly if labelled samples are randomly sampled, labelled and unlabelled data is coming from the exactly same distribution. Hence, the distance (H-divergence, TV-distance etc.) between them is 0. My main question to authors is why does this method work better than random sampling? A similar question is; since they are coming from the exact same distribution, what is the meaning of minimizing empirical H-divergence? I think a more detailed study on a toy problem could potentially explain this. Authors can generate 1-D or 2D samples from a well defined distribution (eg. Gaussians with different means/variances for each class) and visualize what is the algorithm actually doing. 

Considering my point that these data points are actually coming from the same distribution, discussion in Section 3.1 is rather unjustified. Most of the entities discussed in that section are probabilistic entities (generally speaking expected values) and does not differ between labelled and unlabelled case since they have same underlying distribution. Their empirical values are different but this is beyond the study of Ben-David(2010). Therefore, I am not sure does the Section 3.1 is contributing to the paper without any explicit connection to the empirical divergence minimization. More importantly a much similar work from domain adaptation is [Unsupervised domain adaptation by backpropagation, ICML 2015] and it should also be discussed in the paper.

Some minor issues:
- Are the hyper-parameters kept fixed for all experiments. In other words, does the training size of 5k and 15k share hyperparameters? Which might be sub-optimal.
- The experiments use very large batch sizes. A smaller batch sizes might separate the algorithms better.
- References in the text have some issues. There are missing commas between references in the text. There are also some cases where \citep should have been used but \citet is used. A careful pass over them might be beneficial.

In summary, I think the paper is interesting, easy to implement and possibly useful to the large part of the community since active learning is very important problem. I think the major weakness of the paper is the fact that authors did not give a clear explain why does it actually work. I think it is crucial for authors to provide a theoretical or an empirical study which answer this question.

</span></div><div class="note_contents"><span class="note_content_field">Rating: </span><span class="note_content_value">5: Marginally below acceptance threshold</span></div><div class="note_contents"><span class="note_content_field">Confidence: </span><span class="note_content_value">4: The reviewer is confident but not absolutely certain that the evaluation is correct</span></div><div class="reply_row clearfix"></div></div><div class="children"><div class="note_with_children"><div id="note_SJl0Bm2-6Q" class="note panel"><div class="title_pdf_row clearfix"><h2 class="note_content_title"><a>Relevance of domain adaptation and clarifications </a>  <button class="btn btn-xs btn-default permalink-button" title="Link to this comment" data-permalink-url="https://openreview.net/forum?id=rJl-HsR9KX&amp;noteId=SJl0Bm2-6Q"><span class="glyphicon glyphicon-link" aria-hidden="true"></span></button></h2></div><div class="meta_row"><span class="signatures">ICLR 2019 Conference Paper65 Authors</span></div><div class="clearfix"><div class="meta_row pull-left"><span class="date item">08 Nov 2018</span><span class="item">ICLR 2019 Conference Paper65 Official Comment</span><span class="item">Readers: <span class="readers-icon glyphicon glyphicon-globe"></span> Everyone</span></div><div class="meta_row meta_actions"></div></div><div class="note_contents"><span class="note_content_field">Comment: </span><span class="note_content_value"> Thank you for your thoughtful comments regarding our paper.

We'll first address your comments about using the H-divergence as a motivation for our method and the expectation that DAL shouldn't improve upon random sampling. We agree that when considering the underlying distributions of the labeled and unlabeled sets, the TV distance between them is zero and that random sampling allows claiming that they are indeed the same distribution. That being said, we work under the assumption that the labeled set is very small compared to the unlabeled pool, and so it can be viewed as coming from a different, empirical distribution (for instance, one which contains only some of the modes of the original distribution). When represented in this way, for a fixed labeled set, we claim that we have a distribution shift that we must correct for using active learning. We could either sample an example randomly from the real distribution, which will give us in expectation a good solution (assuming we sample enough examples), or actively choose the examples which minimize this distribution drift which is caused by the small size of the labeled set. So, while in expectation over large labeled sets we should expect the domain shift to be zero, in essence when we are dealing with small labeled sets it is more accurate to treat the distributions as different and attempt to minimize their distance. This is mentioned shortly in the second paragraph of section 3.1.
A simple example to motivate our reasoning can be to look at a finite sample space with n possible values for x, and P(x) being some multinomial distribution over the values of x. We also have an estimate of the probabilities, P*(x), which comes from the relative frequency of every x in out labeled set. These two distributions are different and we want to make them more similar. Our method will choose the sample to label which maximizes P(x)/(P(x)+P*(x)), which is the one where P(x)/P*(x) is largest. This is a good heuristic as opposed to picking randomly, which might even push the distributions farther apart.
We can also motivate this in a similar way to the Core-Set approach, in which we care about representing all of the modes of the distribution (having all possible examples of x in our labeled set). If we use DAL for the above example, assuming P(x) is non zero for every x, then we will represent all of the modes after n samples. Contrary to DAL, random sampling will capture all of the modes in ~nlog(n) samples, which is a big difference if n is large and we want to have as few labeled samples as we can.
Finally, we did run experiments on toy data when trying to compare our method to the Core-Set approach. We used two Gaussians, one having a large variance and the other having a smaller one. While the Core-Set approach favored labeling samples from the large variance Gaussian, DAL labeled samples more equally from the two Gaussians. We felt this experiment, along with other ones we ran on low dimensional data, didn't add enough to justify getting into the final paper under the ICLR page constraints, and chose to let the experiments on more realistic data be the main message.

As for your comment about the similar work from domain adaptation, we thank you for bringing this paper to our attention. We discuss it briefly in section 3.1.1 of the revised submission.

As for the minor issues you raise:
The hyper-parameters are indeed kept fixed, and we agree this could be sub-optimal. Using cross validation in every iteration of the active learning experiment for every AL method for the amount of experiments we ran was computationally too expensive for us. Still, since all algorithms were tested in the same playing field with the same parameters, we think our results can be trusted.
The batch sizes were indeed quite large, as we are addressing active learning for neural networks where using small batch sizes is often impractical since we are trying to amass a relatively large dataset. We agree that in the domain of small batch sizes there could be stronger differences between the methods, specifically a bigger advantage to uncertainty based methods compared to DAL and the Core-Set approach.</span></div><div class="reply_row clearfix"></div></div><div class="children"></div></div></div></div><div class="note_with_children"><div id="note_BJx-6PRtnQ" class="note panel"><div class="title_pdf_row clearfix"><h2 class="note_content_title"><a>Method motivation and experimental results are not very convincing</a>  <button class="btn btn-xs btn-default permalink-button" title="Link to this comment" data-permalink-url="https://openreview.net/forum?id=rJl-HsR9KX&amp;noteId=BJx-6PRtnQ"><span class="glyphicon glyphicon-link" aria-hidden="true"></span></button></h2></div><div class="meta_row"><span class="signatures">ICLR 2019 Conference Paper65 AnonReviewer2</span></div><div class="clearfix"><div class="meta_row pull-left"><span class="date item">02 Nov 2018 (modified: 07 Nov 2018)</span><span class="item">ICLR 2019 Conference Paper65 Official Review</span><span class="item">Readers: <span class="readers-icon glyphicon glyphicon-globe"></span> Everyone</span></div><div class="meta_row meta_actions"></div></div><div class="note_contents"><span class="note_content_field">Review: </span><span class="note_content_value">This paper presents a new approach to an active learning problem where the idea is to train a classifier to distinguish labeled and unlabeled datapoints and select those that look the most like unlabeled.

The paper is clearly written and easy to follow. The idea is quite novel and evokes interesting thoughts. I appreciated that the authors provide links and connections to other problems. Another positive aspect is that evaluation methodology is quite sound and includes comparison to many recent algorithms for AL with neural networks. The analysis of Section 5.5 is quite interesting.
However, I have a few concerns regarding the methodology. First of all, I am not completely convinced by the fact that selecting the samples that resemble the most unlabeled data is beneficial for the classifier. It seem that in this case just the data from under-explored regions will be selected at every new iteration. If this is the purpose, some simpler methods, for example, relying on density sampling, can be used. Could you elaborate how you method would compare to them? I can see this method as a way to measure the representativeness of datapoints, but I would see it as a component of AL, not an AL alone. What would happen it is combined with Uncertainty and you use it to labeled the points that are both uncertain and resemble unlabeled data? 
Besides, the proposed approach does not take the advantage of all the information that is available to AL, in particular, it does not use at the information about labels. I believe that labels contain a lot of useful information for making an informed selection decision and ignoring it when it is available is not rational.  
Next, I have conceptual difficulties understanding what would happen to a classifier at next iteration when it is trained on the data that was determined by the previous classifier. Seems that the training data is non-iid and might cause some strange bias. In addition to this, it sounds a bit strange to use classification where overfitting is acceptable.
Finally, the results of the experimental evaluation do not demonstrate a significant advantage of the proposed method and thus it is unclear is there is a benefit of using this method in practice. 

Questions:
- Could you elaborate why DAL strategy does not end up doing just random sampling?
- Nothing restrict DAL from being applied with classifiers other than neural networks and smaller problems. How do you think DAL would work on simpler datasets and classifiers?
- How does the classifier (that distinguished between labeled and unlabeled data) deal with very unbalanced classes? I suppose that normally unlabeled set is much bigger than labeled. What does 98% accuracy mean in this case?
- How many experiments were run to produce each figure? Are error bars of most experiments so small that are almost invisible?

Small comments:
- I think in many cases citep command should be used instead of cite. 
- Can you explain more about the paragraph 3 of related work where you say that uncertainty-based approach would be different from margin-based approach if the classifier is neural network?
- Last sentence before 3.1: how do you guarantee in this case that the selected examples are not similar to each other (that was mentioned as a limitation for batch uncertainty selection, last paragraph on page 1)?
- It was hard to understand the beginning of 5.5, at first it sounds like the ranking of methods is going to be analysed.
- I am not sure "discriminative" is a good name for this algorithm. It suggested that is it opposite to "generative" (query synthesis?), but then all AL that rank datapoints with some scoring function are "discriminative".</span></div><div class="note_contents"><span class="note_content_field">Rating: </span><span class="note_content_value">4: Ok but not good enough - rejection</span></div><div class="note_contents"><span class="note_content_field">Confidence: </span><span class="note_content_value">4: The reviewer is confident but not absolutely certain that the evaluation is correct</span></div><div class="reply_row clearfix"></div></div><div class="children"><div class="note_with_children"><div id="note_Bke4oN2Z6Q" class="note panel"><div class="title_pdf_row clearfix"><h2 class="note_content_title"><a>Clarifications</a>  <button class="btn btn-xs btn-default permalink-button" title="Link to this comment" data-permalink-url="https://openreview.net/forum?id=rJl-HsR9KX&amp;noteId=Bke4oN2Z6Q"><span class="glyphicon glyphicon-link" aria-hidden="true"></span></button></h2></div><div class="meta_row"><span class="signatures">ICLR 2019 Conference Paper65 Authors</span></div><div class="clearfix"><div class="meta_row pull-left"><span class="date item">08 Nov 2018</span><span class="item">ICLR 2019 Conference Paper65 Official Comment</span><span class="item">Readers: <span class="readers-icon glyphicon glyphicon-globe"></span> Everyone</span></div><div class="meta_row meta_actions"></div></div><div class="note_contents"><span class="note_content_field">Comment: </span><span class="note_content_value"> Thank you for your detailed comments and questions regarding our paper and the time and effort spent reviewing it.

You are correct in saying that DAL in essence chooses the under-represented regions of the data and acts similarly to a density based approach (this can be seen in experiments on toy data as well). Indeed, for low-dimensional data there is no reason to choose to use DAL over classical density-based approaches which directly estimate the density. DAL is relevant for high dimensional data, where density-based approaches break down due to the difficulty of estimating probability densities in high dimensions. While estimating densities is difficult, we have great tools for binary classification in high dimensional data and so DAL remains effective in these domains. We briefly mention this in the first paragraph of section 3.
As for having DAL be supplementary to uncertainty based methods, we agree that such a combination could be beneficial and improve the results of both DAL and uncertainty separately, but combining the methods in a non-naive way isn't straightforward and was outside the scope of the current work.

Regarding the use of information about the labels, we would only comment that the labels aren't completely ignored in the entire active learning process of DAL. While it is true that DAL only uses the representation to pick the next sample to label, the representation itself is learned using a classifier which takes the labels of the labeled set into account. This is important, since running DAL on the raw data gives results which are much worse. The same can be said regarding the Core-Set approach, where it ignores the labels of the representation for choosing the examples to label, but uses a representation learned using the labels.

As for the experimental results, the point is well taken that the results aren't convincing enough to choose DAL over the other methods in practice. Even worse, our results show that for the benchmarks of MNIST and CIFAR-10, there isn't enough information for a practitioner to decide about any of the existing methods. This speaks to a general issue in benchmarking modern AL methods, which we admittedly did not address in our paper fully by exploring other, larger datasets. Exploring the problem of benchmarking AL methods is important, but tangential to a paper that aims to introduce a new method.
Still, we feel that our method is interesting and has merit even if it only performs comparably to the existing methods and does not improve upon them.</span></div><div class="reply_row clearfix"></div></div><div class="children"><div class="note_with_children"><div id="note_Sygh3VnZpm" class="note panel"><div class="title_pdf_row clearfix"><h2 class="note_content_title"><a>Further clarifications</a>  <button class="btn btn-xs btn-default permalink-button" title="Link to this comment" data-permalink-url="https://openreview.net/forum?id=rJl-HsR9KX&amp;noteId=Sygh3VnZpm"><span class="glyphicon glyphicon-link" aria-hidden="true"></span></button></h2></div><div class="meta_row"><span class="signatures">ICLR 2019 Conference Paper65 Authors</span></div><div class="clearfix"><div class="meta_row pull-left"><span class="date item">08 Nov 2018</span><span class="item">ICLR 2019 Conference Paper65 Official Comment</span><span class="item">Readers: <span class="readers-icon glyphicon glyphicon-globe"></span> Everyone</span></div><div class="meta_row meta_actions"></div></div><div class="note_contents"><span class="note_content_field">Comment: </span><span class="note_content_value">In response to your specific questions:

To explain the difference to random sampling, we refer you to our reply to reviewer #2, where we give an example using a finite sample space of size n where P(x) is multinomial. We explain that our choice of sample to label in this scenario is more reasonable than random sampling. We also show that form a mode-covering perspective, DAL needs n samples to cover all the n modes of the distribution while random sampling needs ~nlog(n).

As for simpler classifiers, we used DAL with a RBF-kernel SVM on the MNIST benchmark. This gave us results better than random, but not as impressive as the neural network results. Also, we explored DAL on different synthetic 2D datasets with linear classifiers.
In the linear classifier setting DAL has some issues which stem from the low expressiveness of the linear classifier. DAL fits a linear classifier, trying to separate the labeled set from the unlabeled set, and chooses the example which is farthest away from the linear decision boundary in the direction of P(x~U). This is problematic because the labeled and unlabeled sets quickly become difficult to separate linearly, even if the real distribution can be linearly separated according to the labels. Secondly, the fact that we choose the farthest example from the linear decision boundary means that the method is very sensitive to outliers. Neural networks don't experience these problems as much, because they are very expressive and can take into account the density between the labeled and unlabeled examples, and so are also less sensitive to outliers.

As for the unbalanced classes question, we chose to deal with it in a very simple way which was both weighting the classes in the loss function according to their proportions, sub-sampling the unlabeled set to be 10 times the size of the labeled set and using large batch sizes during training. For this specific scenario, 98% accuracy worked for us. This wasn't clear enough in the paper and so we added a short clarification to this in the revised version in section 4.3.
We agree that if one wants to incorporate the entire unlabeled set when the labeled set is small, more care needs to be given to dealing with the unbalanced class problem. In this case, we would try training the network while forcing the batches to always contain a certain percentage of examples from the labeled set.

Regarding the experiments, we ran 20 experiments for the MNIST and CIFAR-10 benchmarks, so most error-bars are indeed too small to see.

To answer you final comments:

When we say that uncertainty-based approaches and margin-based approaches differ when we move away from linear binary classifiers, this is because uncertainty-based methods use the *output* probabilities of the model to decide which example to label, while margin-based methods use the euclidean distance of the *input* from the decision boundary. When the classifier is linear there is a direct, monotonous connection between the euclidean distance from the decision boundary and the output probabilities of the linear model, and so the two approaches become one and the same (the closer you are to the decision boundary, the more the model is uncertain). On the other hand, when the model is highly non-linear, the euclidean distance between the input and the decision boundary doesn't necessarily correlate to the uncertainty in the output probabilities, and this is where the two approaches can differ in their queries.

Finally, we cannot guarantee that our queries are diverse, but we can raise the chances of that happening by using the "mini-query" trick we introduced in the paper. We simply make several successive sub-queries of smaller size, updating the binary labels for DAL after every sub-query. This makes it less likely that two similar samples will be queried in consecutive sub-queries, and makes DAL a little bit more "batch-aware". We of course pay a price in runtime the more sub-queries we use, since we need to train the binary classifier between every two sub-queries.</span></div><div class="reply_row clearfix"></div></div><div class="children"></div></div></div></div></div></div></div></main></div>
  </div>
</div>


    <!-- Footer -->
    <footer class="sitemap">
      <div class="container">
        <div class="row hidden-xs">
          <div class="col-sm-4">
            <ul class="list-unstyled">
              <li><a href="/" class="home">Home</a></li>
              <li><a href="/venues">All Venues</a></li>
            </ul>
          </div>
    
          <div class="col-sm-4">
            <ul class="list-unstyled">
              <li><a href="/about">About OpenReview</a></li>
              <li><a href="/contact">Contact</a></li>
              <li><a href="#" data-toggle="modal" data-target="#feedback-modal">Feedback</a></li>
            </ul>
          </div>
    
          <div class="col-sm-4">
            <ul class="list-unstyled">
              <li><a href="/terms">Terms of Service</a></li>
              <li><a href="/privacy">Privacy Policy</a></li>
            </ul>
          </div>
        </div>
    
        <div class="row visible-xs-block">
          <div class="col-xs-6">
            <ul class="list-unstyled">
              <li><a href="/" class="home">Home</a></li>
              <li><a href="/about">About OpenReview</a></li>
              <li><a href="#" data-toggle="modal" data-target="#feedback-modal">Feedback</a></li>
            </ul>
          </div>
    
          <div class="col-xs-6">
            <ul class="list-unstyled">
              <li><a href="/contact">Contact</a></li>
              <li><a href="/terms">Terms of Service</a></li>
              <li><a href="/privacy">Privacy Policy</a></li>
            </ul>
          </div>
        </div>
      </div>
    </footer>
    
    <footer class="sponsor">
      <div class="container">
        <div class="row">
          <div class="col-sm-10 col-sm-offset-1">
            <p class="text-center">
              OpenReview is created by the <a href="http://www.iesl.cs.umass.edu/" target="_blank">Information Extraction and Synthesis Laboratory</a>, College of Information and Computer Science, University of Massachusetts Amherst. We gratefully acknowledge the support of the OpenReview sponsors:  Google,  Facebook, NSF, the University of Massachusetts Amherst Center for Data Science, and Center for Intelligent Information Retrieval, as well as the Google Cloud Platform for donating the computing and networking services on which OpenReview.net runs.
            </p>
          </div>
        </div>
      </div>
    </footer>

  <div id="feedback-modal" class="modal fade" tabindex="-1" role="dialog">
    <div class="modal-dialog">
      <div class="modal-content">
        <div class="modal-header">
          <button type="button" class="close" data-dismiss="modal" aria-label="Close"><span aria-hidden="true">×</span></button>
          <h3 class="modal-title">Send Feedback</h3>
        </div>
        <div class="modal-body">
          <p>Enter your feedback below and we'll get back to you as soon as possible.</p>
          <form action="/feedback" method="POST">
            <div class="form-group">
              <input type="email" name="from" class="form-control" placeholder="Email" />
            </div>
            <div class="form-group">
              <input type="text" name="subject" class="form-control" placeholder="Subject" />
            </div>
            <div class="form-group">
              <textarea name="message" class="form-control feedback-input" rows="5" placeholder="Message" required=""></textarea>
            </div>
          <ul id="ui-id-2" tabindex="0" class="ui-menu ui-widget ui-widget-content" style="display: none;"></ul></form>
        </div>
        <div class="modal-footer">
          <button type="button" class="btn btn-default" data-dismiss="modal">Cancel</button>
          <button type="button" class="btn btn-primary">Send</button>
        </div>
      </div>
    </div>
  </div>

  <script type="text/javascript" async="" src="https://www.google-analytics.com/analytics.js"></script><script src="//cdnjs.cloudflare.com/ajax/libs/jquery/2.2.4/jquery.min.js"></script>
  <script>window.jQuery || document.write('&lt;script src="/static/js/vendor/jquery-2.2.4.min.js"&gt;&lt;\/script&gt;')</script>

  <script src="/static/dist/vendor.min.js"></script>
  <script src="/static/dist/templates.min.js"></script>
  <script src="/static/dist/openreview.min.js"></script>

    <script>window.legacyScripts = true;</script>


    <script async="" src="https://www.googletagmanager.com/gtag/js?id=UA-108703919-1"></script>
    <script>
      window.dataLayer = window.dataLayer || [];
      function gtag() { dataLayer.push(arguments); }
      gtag('js', new Date());
      gtag('config', 'UA-108703919-1');
    </script>


<div role="status" aria-live="assertive" aria-relevant="additions" class="ui-helper-hidden-accessible"></div></body></html>