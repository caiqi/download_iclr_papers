<!DOCTYPE html><html xmlns="http://www.w3.org/1999/xhtml" class="no-js"><head>
  <meta charset="utf-8" />
  <meta http-equiv="X-UA-Compatible" content="IE=edge,chrome=1" />
  <meta name="viewport" content="width=device-width, initial-scale=1" />

  <title>Beyond Greedy Ranking: Slate Optimization via List-CVAE | OpenReview</title>
  <meta name="description" content="" />

      <meta name="citation_title" content="Beyond Greedy Ranking: Slate Optimization via List-CVAE" />
        <meta name="citation_author" content="Anonymous" />
      <meta name="citation_publication_date" content="2018/09/27" />
      <meta name="citation_online_date" content="2018/09/27" />
      <meta name="citation_pdf_url" content="https://openreview.net/pdf?id=r1xX42R5Fm" />
      <meta name="twitter:card" content="summary" />
      <meta name="twitter:site" content="@openreviewnet" />
      <meta name="og:title" content="Beyond Greedy Ranking: Slate Optimization via List-CVAE" />
      <meta name="og:description" content="The conventional approach to solving the recommendation problem greedily ranks&#10;  individual document candidates by prediction scores. However, this method fails to&#10;  optimize the slate as a whole, and..." />
      <meta name="og:image" content="https://openreview.net/static/images/openreview_logo_512.png" />

  <link rel="stylesheet" href="https://fonts.googleapis.com/css?family=Noto+Sans:400,400i,700,700i" />

  <link rel="stylesheet" href="/static/css/bootstrap.min.css" />
  <link rel="stylesheet" href="/static/css/jquery-ui.min.css" />
  <link rel="stylesheet" href="/static/css/main.min.css" />
</head>

<body class="forum">
  <nav class="navbar navbar-inverse navbar-fixed-top" role="navigation">
    <div class="container">
  
      <div class="navbar-header">
        <button type="button" class="navbar-toggle collapsed" data-toggle="collapse" data-target="#navbar" aria-expanded="false" aria-controls="navbar">
          <span class="sr-only">Toggle navigation</span>
          <span class="icon-bar"></span>
          <span class="icon-bar"></span>
          <span class="icon-bar"></span>
        </button>
        <a class="navbar-brand home push-link" href="/"><strong>OpenReview</strong>.net</a>
      </div>
  
      <div id="navbar" class="navbar-collapse collapse">
        <form class="navbar-form navbar-left profile-search" role="search">
          <div class="form-group has-feedback">
            <input id="search_input" type="text" class="form-control ui-autocomplete-input" placeholder="Search ICLR 2019 Conference" autocomplete="off" />
            <span class="glyphicon glyphicon-search form-control-feedback" aria-hidden="true"></span>
          </div>
  
          <input id="search_group" type="hidden" value="ICLR.cc/2019/Conference" />
          <input id="search_content" type="hidden" value="all" />
        <ul id="ui-id-1" tabindex="0" class="ui-menu ui-widget ui-widget-content ui-autocomplete ui-front" style="display: none;"></ul></form>
  
        <ul class="nav navbar-nav navbar-right">
        
            <li id="user-menu"><a href="/login">Login</a></li>
        </ul>
      </div>
  
    </div>
  </nav>

  <div id="or-banner" class="banner" style="">
  <div class="container">
    <div class="row">
      <div class="col-xs-12"><a href="/group?id=ICLR.cc/2019/Conference" title="Venue Homepage"><img class="icon" src="/static/images/arrow_left.svg" /> Go to <strong>ICLR 2019 Conference</strong> homepage</a></div>
    </div>
  </div>
</div>
<div id="flash-message-container" class="alert alert-danger" role="alert" style="display: none;">
  <div class="container">
    <div class="row">
      <div class="col-xs-12">
        <div class="alert-content">
          <button type="button" class="close" aria-label="Close"><span aria-hidden="true">×</span></button>
        </div>
      </div>
    </div>
  </div>
</div>

<div class="container">
  <div class="row">
    <div class="col-xs-12">
      <main id="content" class="clearfix openbanner-visible legacy-styles"><div id="note_r1xX42R5Fm" class="note panel"><div class="title_pdf_row clearfix"><h2 class="note_content_title"><a>Beyond Greedy Ranking: Slate Optimization via List-CVAE</a> <a class="note_content_pdf" href="/pdf?id=r1xX42R5Fm" title="Download PDF" target="_blank"><img src="/static/images/pdf_icon_blue.svg" /></a> </h2></div><div class="meta_row"><span class="signatures">Anonymous</span></div><div class="clearfix"><div class="meta_row pull-left"><span class="date item">28 Sep 2018 (modified: 16 Nov 2018)</span><span class="item">ICLR 2019 Conference Blind Submission</span><span class="item">Readers: <span class="readers-icon glyphicon glyphicon-globe"></span> Everyone</span><span class="item"><a class="action-bibtex-modal" data-bibtex="@inproceedings{    &#10;anonymous2019beyond,    &#10;title={Beyond Greedy Ranking: Slate Optimization via List-CVAE},    &#10;author={Anonymous},    &#10;booktitle={Submitted to International Conference on Learning Representations},    &#10;year={2019},    &#10;url={https://openreview.net/forum?id=r1xX42R5Fm},    &#10;note={under review}    &#10;}">Show Bibtex</a></span><a class="note_content_pdf item" href="/revisions?id=r1xX42R5Fm" target="_blank">Show Revisions</a></div><div class="meta_row meta_actions"></div></div><div class="note_contents"><span class="note_content_field">Abstract: </span><span class="note_content_value">The conventional approach to solving the recommendation problem greedily ranks
individual document candidates by prediction scores. However, this method fails to
optimize the slate as a whole, and hence, often struggles to capture biases caused
by the page layout and document interdepedencies. The slate recommendation
problem aims to directly find the optimally ordered subset of documents (i.e.
slates) that best serve users’ interests. Solving this problem is hard due to the
combinatorial explosion of document candidates and their display positions on the
page. Therefore we propose a paradigm shift from the traditional viewpoint of solving a ranking problem to a direct slate generation framework. In this paper, we introduce List Conditional Variational Auto-Encoders (ListCVAE),
which learn the joint distribution of documents on the slate conditioned
on user responses, and directly generate full slates. Experiments on simulated
and real-world data show that List-CVAE outperforms greedy ranking methods
consistently on various scales of documents corpora.</span></div><div class="note_contents"><span class="note_content_field">Keywords: </span><span class="note_content_value">CVAE, VAE, recommendation system, slate optimization, whole page optimization</span></div><div class="note_contents"><span class="note_content_field">TL;DR: </span><span class="note_content_value">We used a CVAE type model structure to learn to directly generate slates/whole pages for recommendation systems.</span></div><div class="reply_row clearfix"><div class="item" id="reply_count">6 Replies</div></div></div><hr class="small" /><div id="note_children"><div class="note_with_children"><div id="note_Syx1JutAn7" class="note panel"><div class="title_pdf_row clearfix"><h2 class="note_content_title"><a>Scalable method for the slate-recommendation task </a>  <button class="btn btn-xs btn-default permalink-button" title="Link to this comment" data-permalink-url="https://openreview.net/forum?id=r1xX42R5Fm&amp;noteId=Syx1JutAn7"><span class="glyphicon glyphicon-link" aria-hidden="true"></span></button></h2></div><div class="meta_row"><span class="signatures">ICLR 2019 Conference Paper1434 AnonReviewer3</span></div><div class="clearfix"><div class="meta_row pull-left"><span class="date item">06 Nov 2018 (modified: 07 Nov 2018)</span><span class="item">ICLR 2019 Conference Paper1434 Official Review</span><span class="item">Readers: <span class="readers-icon glyphicon glyphicon-globe"></span> Everyone</span></div><div class="meta_row meta_actions"></div></div><div class="note_contents"><span class="note_content_field">Review: </span><span class="note_content_value">This paper pr poses a conditional generative model for slate-based recommendations. The idea of slate recommendation is to model an ordered-list of items instead of modelling each item independently as is usually done (e.g., for computational reasons). This provides a more natural framework for recommending lists of items (vs. recommending the items with the top scores).

To generate slates, the authors propose to learn a mapping from a utility function (value) to an actual slate of products (i.e., the model conditions on the utility).  Once fitted, recommending good slates is then achieved by conditioning on the optimal utility (which is problem dependant) and generating a slate according to that utility. This procedure which is learned in a conditional VAE framework effectively bypasses the intractable combinatorial search problem (i.e., choosing the best ordered list of k-items from the set of all items) by instead estimating a model which generates slates of a particular utility. The results demonstrate empirically that the approach outperforms several baselines.      

This idea seems promising and provides an interesting methodological development for recommender systems. Presumably this approach, given the right data, could also learn interesting concepts such as substitution, complementarity, and cannibalization.

The paper is fairly clear although the model is never formally expressed: I would suggest defining it using math and not only a figure.  The study is also interesting although the lack of publicly available datasets limits the extent of it and the strength of the results. Overall, it would be good to compare to a few published baselines even if these were not tailored to this specific problem.


A few detailed comments (in approximate decreasing order of importance):

- Baselines. The current baselines seem to focus on what may be used in industry with a specific focus on efficient methods at test time.

  For this venue, I would suggest that it is necessary to compare to other published baselines. Either baselines that use a similar setup or, at least, strong collaborative filtering baselines that frame the problem as a regression one.

  If prediction time is important then you could also compare your method to others in that respect.

- training/test mismatch. There seems to be a mismatch between the value of the conditioning information at train and at test. How do you know that your fitted model will generalize to this "new" setting?

- In Figures: If I understand correctly the figures (4--6) report test performance as a function of training steps. Is that correct?

  Could you explain why the random baseline seems to do so well? That is, for a large number of items, I would expect that it should get close to zero expected number of clicks.

- Figure 6d. It seems like that subfigure is not discussed. Why does CVAE perform worse on the hardest training set?

- The way you create slates from the yoochoose challenge seems a bit arbitrary. Perhaps I don't know this data well enough but it seems like using the temporal aspects of the observations to define a slate makes the resulting data closer to a subset selection problem than an ordered list.

- Section 3. It's currently titled "Theory" but doesn't seem to contain any theory. Perhaps consider renaming to "Method" or "Model"</span></div><div class="note_contents"><span class="note_content_field">Rating: </span><span class="note_content_value">6: Marginally above acceptance threshold</span></div><div class="note_contents"><span class="note_content_field">Confidence: </span><span class="note_content_value">4: The reviewer is confident but not absolutely certain that the evaluation is correct</span></div><div class="reply_row clearfix"></div></div><div class="children"><div class="note_with_children"><div id="note_Sklln4tiam" class="note panel"><div class="title_pdf_row clearfix"><h2 class="note_content_title"><a>Replies to the reviewer's comments [paper updated]  </a>  <button class="btn btn-xs btn-default permalink-button" title="Link to this comment" data-permalink-url="https://openreview.net/forum?id=r1xX42R5Fm&amp;noteId=Sklln4tiam"><span class="glyphicon glyphicon-link" aria-hidden="true"></span></button></h2></div><div class="meta_row"><span class="signatures">ICLR 2019 Conference Paper1434 Authors</span></div><div class="clearfix"><div class="meta_row pull-left"><span class="date item">16 Nov 2018</span><span class="item">ICLR 2019 Conference Paper1434 Official Comment</span><span class="item">Readers: <span class="readers-icon glyphicon glyphicon-globe"></span> Everyone</span></div><div class="meta_row meta_actions"></div></div><div class="note_contents"><span class="note_content_field">Comment: </span><span class="note_content_value">Thank you for the thoughtful review. We updated the paper to reflect several of the reviewer’s comments, which will be mentioned below. In this paper we mainly compared List-CVAE with popular baselines that are suitable for large-scale production settings. For non-greedy baselines, we ran experiments against auto-regressive LSTM (learning contextual/positional biases) and auto-regressive position MLP (learning positional biases) models, which are now included in the results and they performed on par with the greedy baselines. 

In terms of prediction time, all baselines (except auto-regressive methods) and List-CVAE have a run-time complexity of O(k * log(n)) where k is the slate size and n is the number of documents. To obtain logarithmic scaling, one can use kd-trees [Sproull, R.F., Refinements to nearest-neighbor searching in k-dimensional trees. Algorithmica 6(4) (1991) 579–589]. Given this, providing exact numbers becomes highly dependent on the underlying implementation details. With that said, in our experiments, all greedy baselines and the CVAE model are performing below 0.04 ms per test example on a single GPU, and their differences are very small (less than 0.01 ms). The auto-regressive LSTM is ~10 times slower as expected.

Regarding the generalization ability of the model, we performed a test in the Yoochoose dataset by masking out a percentage of the top responses at the end of Section 4.2. List-CVAE only failed to generalize to the case of training on slates with maximum 1 positive response (h=40%, Figure 6d). This result is expected since List-CVAE can not hope to learn much about the interactions between documents given only 0 or 1 positive response per slate, whereas the MLP-type models learn click probabilities of single documents in the same way as in slates with higher responses. This discussion is now added to the paper.

It is true that evaluation of our model requires choosing a c* at or near the edge of the support of P(c). However we can compromise generalization vs performance by controlling c* to some extent (in this paper, we did not need to use sub-optimal conditioning since the model readily generalizes well to the optimal condition. However in practice, depending on the datasets, one can choose close-to-optimal conditioning at test time for better generalization results). Moreover, interactions between documents are generated by similar mechanisms whether they are from the optimal or sub-optimal slates. Thus the model can learn these mechanisms from sub-optimal slates and generalize to optimal slates (as the experiment results indicate). 

The figures (4--6) do report test/eval performance as a function of training steps. Due to the setup (Eq. 5) of our simulation environments, a random slate has on average over 3 clicks. On the Yoochoose dataset, in Section 4.2, the paper explained that “we removed slates with no positive responses such that after removal they only account for 50% of the total number of slates” (in order to save training time). Therefore the random slates from the training set (a clarification of this baseline has been added to the paper) have on average slightly over 0.5 purchases. 

Given that there are few publicly available slate datasets, we had to devise slates using the temporal order of clicks/purchases within each user session. If the ordering were not important, it would have considerably weakened the performance of List-CVAE.

Other comments (e.g.: Theory -&gt; Method) have been addressed in the newly upload version of the paper.
</span></div><div class="reply_row clearfix"></div></div><div class="children"></div></div></div></div><div class="note_with_children"><div id="note_BklYOAG62m" class="note panel"><div class="title_pdf_row clearfix"><h2 class="note_content_title"><a>Using variational auto-encoders to go beyond greedy construction of slates for recommendations</a>  <button class="btn btn-xs btn-default permalink-button" title="Link to this comment" data-permalink-url="https://openreview.net/forum?id=r1xX42R5Fm&amp;noteId=BklYOAG62m"><span class="glyphicon glyphicon-link" aria-hidden="true"></span></button></h2></div><div class="meta_row"><span class="signatures">ICLR 2019 Conference Paper1434 AnonReviewer1</span></div><div class="clearfix"><div class="meta_row pull-left"><span class="date item">05 Nov 2018 (modified: 07 Nov 2018)</span><span class="item">ICLR 2019 Conference Paper1434 Official Review</span><span class="item">Readers: <span class="readers-icon glyphicon glyphicon-globe"></span> Everyone</span></div><div class="meta_row meta_actions"></div></div><div class="note_contents"><span class="note_content_field">Review: </span><span class="note_content_value">The paper proposes using a variational auto-encoder to learn how to map a user context, and a desired user response to a slate of item recommendations. During training, data collected from an existing recommender policy (user contexts, displayed slate, recorded user response) can be used to train the encoder and decoder of the auto-encoder to map from contexts to a latent variable and decode the latent variable to a slate. Once trained, we can invoke the encoder with a new user context and the desired user response, and decode the resulting latent variable to construct an optimal slate.

A basic question for such an approach is: [Fig2] Why do we expect generalization from the user responses c(r) seen in training to c(r*) that we construct for testing? At an extreme, suppose our slate recommendation policy always picks the same k items and never gets a click. We can optimize Eqn3 very well on any dataset collected from our policy; but I don't expect deploying the VAE to production with c(r*) as the desired user response will give us anything meaningful.
The generalization test on RecSys2015-medium (Fig6d) confirms this intuition. Under what conditions can we hope for reliable generalization?

The comment about ranking evaluation metrics being unsuitable (because they favor greedy approaches) needs to be justified. There are several metrics that favor diversity (e.g. BPR, ERR) where a pointwise greedy scoring function will perform very sub-optimally.  Such metrics are more transparent than a neural network trained to predict Eqn6. See comment above for why I don't expect the neural net trained to predict Eqn6 on training data will not necessarily generalize to the testing regime we care about (at the core, finding a slate recommendation policy is asking how best to change the distribution P(s), which introduces a distribution shift between training and test regimes for this neural net).

There are 2 central claims in the paper: that this approach can scale to many more candidate items (and hence, we don't need candidate generation followed by a ranking step); and that this approach can reason about interaction-effects within a slate to go beyond greedy scoring. For the second claim, there are many other approaches that go beyond greedy (one of the most recent is Ai et al, SIGIR2018 <a href="https://arxiv.org/pdf/1804.05936.pdf;" target="_blank" rel="nofollow">https://arxiv.org/pdf/1804.05936.pdf;</a> the references there should point to a long history of beyond-greedy scoring) These approaches should also be benchmarked in the synthetic and semi-synthetic experiments. At a glance, many neural rankers (DSSM-based approaches) use a nearly identical decoder to CVAE (one of the most recent is Zamani et al, CIKM2018 https://dl.acm.org/citation.cfm?id=3271800; the references there should point to many other neural rankers) These approaches should also be benchmarked in the expts.
This way, we have a more representative picture of the gain of CVAE from a more flexible (slate-level) encoder-decoder; and the gain from using item embeddings to achieve scalability.

</span></div><div class="note_contents"><span class="note_content_field">Rating: </span><span class="note_content_value">4: Ok but not good enough - rejection</span></div><div class="note_contents"><span class="note_content_field">Confidence: </span><span class="note_content_value">3: The reviewer is fairly confident that the evaluation is correct</span></div><div class="reply_row clearfix"></div></div><div class="children"><div class="note_with_children"><div id="note_SygYUHKopX" class="note panel"><div class="title_pdf_row clearfix"><h2 class="note_content_title"><a>Replies to the reviewer's comments [paper updated] </a>  <button class="btn btn-xs btn-default permalink-button" title="Link to this comment" data-permalink-url="https://openreview.net/forum?id=r1xX42R5Fm&amp;noteId=SygYUHKopX"><span class="glyphicon glyphicon-link" aria-hidden="true"></span></button></h2></div><div class="meta_row"><span class="signatures">ICLR 2019 Conference Paper1434 Authors</span></div><div class="clearfix"><div class="meta_row pull-left"><span class="date item">16 Nov 2018</span><span class="item">ICLR 2019 Conference Paper1434 Official Comment</span><span class="item">Readers: <span class="readers-icon glyphicon glyphicon-globe"></span> Everyone</span></div><div class="meta_row meta_actions"></div></div><div class="note_contents"><span class="note_content_field">Comment: </span><span class="note_content_value">Thank you for the informative review focusing on the following three aspects of the paper, generalization capacity of the model, evaluation metrics and comparison against other baselines. We incorporated the reviewer’s comments into the latest version of the paper, to add a couple of non-greedy baselines and clarify the generalization capacity and the general motivation of our work.

Regarding the generalization ability, given the binary vector format of conditions and the model design, the decoder of the List-CVAE model learns the relationship between the compression of a document and its binary response, and it picks up pairwise (or more generally, sub-slate) interactions from different sub-optimal training slates yet using all of them to construct an optimal slate at test time. We tested the generalization capacity of the model in real world experiments by masking out a percentage of the top responded slates in the Yoochoose dataset at the end of Section 4.2. List-CVAE showed strong generalization capacity and only failed to generalize in the case of training on slates with maximum 1 positive response (h=40%, Figure 6d). This result is expected since List-CVAE can not learn much about the interactions between documents given 0 or 1 positive response, whereas the MLP-type models learn click probabilities of single documents in the same way as in slates with higher responses. 

It is true that evaluation of our model requires choosing a c* at or near the edge of the support of P(c). However we can compromise generalization vs. performance by controlling c* to some extent (in this paper, we did not need to use sub-optimal conditioning since the model readily generalizes well to the optimal condition. However in practice, depending on the datasets, one can choose close-to-optimal conditioning at test time for better generalization results). Moreover, interactions between documents are generated by similar mechanisms whether they are from the optimal or sub-optimal slates. Thus the model can learn these mechanisms from sub-optimal slates and generalize to optimal slates (as the experiment results indicate). This discussion has been added to the paper.
  
On evaluation metrics, it is not always the case that a higher diversity-inclusive score gives better slates measured by user’s total responses. Even though diversity-inclusive metrics are indeed more transparent, they do not directly measure our end goal, which is to maximize the expected number of total responses on the generated slates. We added some clarification on this in the paper.

Regarding our choice of baselines, in this paper, our goal is to challenge the industry state-of-the-art benchmark two-stage ranking on both small and large scales. Therefore we mainly compared List-CVAE with popular baselines that are suitable for large-scale recommender system production settings. For non-greedy baselines, we ran experiments against auto-regressive LSTM (learning contextual/positional biases)  and auto-regressive position MLP (learning positional biases)  models, which are now included in the results and they performed on par with the greedy baselines. 

The two models proposed by the reviewer (Ai 2018, Zamani 2018) (we added Ai 2018 as a reference in the paper) are solving information retrieval problems and hence (rightfully) think about slate generation from a ranking paradigm using greedy ranking evaluation metrics such as nDCG, which assumes that “better” documents should be put into higher positions. However, while this is a natural assumption for information retrieval problems, it is the exact assumption that we avoid making since our problem setting is optimal slate generation for maximizing user engagement. One can imagine cases where leaving good quality documents towards the end of the slate encourages users to browse to later positions of the slate, and thus the effects on total user engagement may be diverse. 

Finally, we would like to emphasize that we are proposing a paradigm shift for recommender systems that aim to maximize user engagements on whole slates, departing from the traditional viewpoint of a ranking problem and to adopt a direct slate generation framework. As such, we call for new baselines and evaluation metrics that are representative of the new paradigm. 
</span></div><div class="reply_row clearfix"></div></div><div class="children"></div></div></div></div><div class="note_with_children"><div id="note_rJg80M9qhX" class="note panel"><div class="title_pdf_row clearfix"><h2 class="note_content_title"><a>Review</a>  <button class="btn btn-xs btn-default permalink-button" title="Link to this comment" data-permalink-url="https://openreview.net/forum?id=r1xX42R5Fm&amp;noteId=rJg80M9qhX"><span class="glyphicon glyphicon-link" aria-hidden="true"></span></button></h2></div><div class="meta_row"><span class="signatures">ICLR 2019 Conference Paper1434 AnonReviewer2</span></div><div class="clearfix"><div class="meta_row pull-left"><span class="date item">03 Nov 2018 (modified: 07 Nov 2018)</span><span class="item">ICLR 2019 Conference Paper1434 Official Review</span><span class="item">Readers: <span class="readers-icon glyphicon glyphicon-globe"></span> Everyone</span></div><div class="meta_row meta_actions"></div></div><div class="note_contents"><span class="note_content_field">Review: </span><span class="note_content_value">This paper proposes a List Conditional Variational Autoencoder approach for the slate recommendation problem. Particularly, it learns the joint document distribution on the slate conditioned on user responses, and directly generates full slates. The experiments show that the proposed method surpasses greedy ranking approaches.

Pros:
+ nice motivation, and the connection with industrial recommendation systems where candidate nomination and ranker is being used is engaging
+ it provides a conditional generative modeling framework for slate recommendation
+ the simulation experiments very clearly show that the expected number of clicks as obtained by the proposed List-CVAE is much higher compared to the chosen baselines. A similar story is shown for the YOOCHOOSE challenge dataset.

Cons: 
- Do the experiments explicitly compare with the nomination &amp; ranking industry standard?
- Comparison with other slate recommendation approaches besides the greedy baselines?
- Comparison with non-slate recommendation models of Figure 1?

Overall, this is a very nicely written paper, and the experiments both in the simulated and real dataset shows the promise of the proposed approach.</span></div><div class="note_contents"><span class="note_content_field">Rating: </span><span class="note_content_value">7: Good paper, accept</span></div><div class="note_contents"><span class="note_content_field">Confidence: </span><span class="note_content_value">4: The reviewer is confident but not absolutely certain that the evaluation is correct</span></div><div class="reply_row clearfix"></div></div><div class="children"><div class="note_with_children"><div id="note_ryg178Yi6Q" class="note panel"><div class="title_pdf_row clearfix"><h2 class="note_content_title"><a>Replies to the reviewer's comments [paper updated] </a>  <button class="btn btn-xs btn-default permalink-button" title="Link to this comment" data-permalink-url="https://openreview.net/forum?id=r1xX42R5Fm&amp;noteId=ryg178Yi6Q"><span class="glyphicon glyphicon-link" aria-hidden="true"></span></button></h2></div><div class="meta_row"><span class="signatures">ICLR 2019 Conference Paper1434 Authors</span></div><div class="clearfix"><div class="meta_row pull-left"><span class="date item">16 Nov 2018</span><span class="item">ICLR 2019 Conference Paper1434 Official Comment</span><span class="item">Readers: <span class="readers-icon glyphicon glyphicon-globe"></span> Everyone</span></div><div class="meta_row meta_actions"></div></div><div class="note_contents"><span class="note_content_field">Comment: </span><span class="note_content_value">Thank you for the nice review! We updated the paper to add more baselines to address the reviewer’s comments. The ranking industry standard is generally not well-defined, but in this paper, we mainly compare against the benchmark industry method of the two-stage ranking, which is a weaker performing version of the Greedy MLP baseline due to the candidate generation stage. We also compare List-CVAE with other popular greedy baselines that are suitable for large-scale production settings. For non-greedy baselines, we ran experiments against auto-regressive LSTM (learning contextual/positional biases) and auto-regressive position MLP (learning positional biases) models, which are now included in the results and they performed on par with the greedy baselines. </span></div><div class="reply_row clearfix"></div></div><div class="children"></div></div></div></div></div></main></div>
  </div>
</div>


    <!-- Footer -->
    <footer class="sitemap">
      <div class="container">
        <div class="row hidden-xs">
          <div class="col-sm-4">
            <ul class="list-unstyled">
              <li><a href="/" class="home">Home</a></li>
              <li><a href="/venues">All Venues</a></li>
            </ul>
          </div>
    
          <div class="col-sm-4">
            <ul class="list-unstyled">
              <li><a href="/about">About OpenReview</a></li>
              <li><a href="/contact">Contact</a></li>
              <li><a href="#" data-toggle="modal" data-target="#feedback-modal">Feedback</a></li>
            </ul>
          </div>
    
          <div class="col-sm-4">
            <ul class="list-unstyled">
              <li><a href="/terms">Terms of Service</a></li>
              <li><a href="/privacy">Privacy Policy</a></li>
            </ul>
          </div>
        </div>
    
        <div class="row visible-xs-block">
          <div class="col-xs-6">
            <ul class="list-unstyled">
              <li><a href="/" class="home">Home</a></li>
              <li><a href="/about">About OpenReview</a></li>
              <li><a href="#" data-toggle="modal" data-target="#feedback-modal">Feedback</a></li>
            </ul>
          </div>
    
          <div class="col-xs-6">
            <ul class="list-unstyled">
              <li><a href="/contact">Contact</a></li>
              <li><a href="/terms">Terms of Service</a></li>
              <li><a href="/privacy">Privacy Policy</a></li>
            </ul>
          </div>
        </div>
      </div>
    </footer>
    
    <footer class="sponsor">
      <div class="container">
        <div class="row">
          <div class="col-sm-10 col-sm-offset-1">
            <p class="text-center">
              OpenReview is created by the <a href="http://www.iesl.cs.umass.edu/" target="_blank">Information Extraction and Synthesis Laboratory</a>, College of Information and Computer Science, University of Massachusetts Amherst. We gratefully acknowledge the support of the OpenReview sponsors:  Google,  Facebook, NSF, the University of Massachusetts Amherst Center for Data Science, and Center for Intelligent Information Retrieval, as well as the Google Cloud Platform for donating the computing and networking services on which OpenReview.net runs.
            </p>
          </div>
        </div>
      </div>
    </footer>

  <div id="feedback-modal" class="modal fade" tabindex="-1" role="dialog">
    <div class="modal-dialog">
      <div class="modal-content">
        <div class="modal-header">
          <button type="button" class="close" data-dismiss="modal" aria-label="Close"><span aria-hidden="true">×</span></button>
          <h3 class="modal-title">Send Feedback</h3>
        </div>
        <div class="modal-body">
          <p>Enter your feedback below and we'll get back to you as soon as possible.</p>
          <form action="/feedback" method="POST">
            <div class="form-group">
              <input type="email" name="from" class="form-control" placeholder="Email" />
            </div>
            <div class="form-group">
              <input type="text" name="subject" class="form-control" placeholder="Subject" />
            </div>
            <div class="form-group">
              <textarea name="message" class="form-control feedback-input" rows="5" placeholder="Message" required=""></textarea>
            </div>
          <ul id="ui-id-2" tabindex="0" class="ui-menu ui-widget ui-widget-content" style="display: none;"></ul></form>
        </div>
        <div class="modal-footer">
          <button type="button" class="btn btn-default" data-dismiss="modal">Cancel</button>
          <button type="button" class="btn btn-primary">Send</button>
        </div>
      </div>
    </div>
  </div>

  <script type="text/javascript" async="" src="https://www.google-analytics.com/analytics.js"></script><script src="//cdnjs.cloudflare.com/ajax/libs/jquery/2.2.4/jquery.min.js"></script>
  <script>window.jQuery || document.write('&lt;script src="/static/js/vendor/jquery-2.2.4.min.js"&gt;&lt;\/script&gt;')</script>

  <script src="/static/dist/vendor.min.js"></script>
  <script src="/static/dist/templates.min.js"></script>
  <script src="/static/dist/openreview.min.js"></script>

    <script>window.legacyScripts = true;</script>


    <script async="" src="https://www.googletagmanager.com/gtag/js?id=UA-108703919-1"></script>
    <script>
      window.dataLayer = window.dataLayer || [];
      function gtag() { dataLayer.push(arguments); }
      gtag('js', new Date());
      gtag('config', 'UA-108703919-1');
    </script>


<div role="status" aria-live="assertive" aria-relevant="additions" class="ui-helper-hidden-accessible"></div></body></html>