<!DOCTYPE html><html xmlns="http://www.w3.org/1999/xhtml" class="no-js"><head>
  <meta charset="utf-8" />
  <meta http-equiv="X-UA-Compatible" content="IE=edge,chrome=1" />
  <meta name="viewport" content="width=device-width, initial-scale=1" />

  <title>Dense Morphological Network: An Universal Function Approximator | OpenReview</title>
  <meta name="description" content="" />

      <meta name="citation_title" content="Dense Morphological Network: An Universal Function Approximator" />
        <meta name="citation_author" content="Anonymous" />
      <meta name="citation_publication_date" content="2018/09/27" />
      <meta name="citation_online_date" content="2018/09/27" />
      <meta name="citation_pdf_url" content="https://openreview.net/pdf?id=SyxknjC9KQ" />
      <meta name="twitter:card" content="summary" />
      <meta name="twitter:site" content="@openreviewnet" />
      <meta name="og:title" content="Dense Morphological Network: An Universal Function Approximator" />
      <meta name="og:description" content="Artificial neural networks are built on the basic operation of linear combination and non-linear activation function. Theoretically this structure can approximate any continuous function with three..." />
      <meta name="og:image" content="https://openreview.net/static/images/openreview_logo_512.png" />

  <link rel="stylesheet" href="https://fonts.googleapis.com/css?family=Noto+Sans:400,400i,700,700i" />

  <link rel="stylesheet" href="/static/css/bootstrap.min.css" />
  <link rel="stylesheet" href="/static/css/jquery-ui.min.css" />
  <link rel="stylesheet" href="/static/css/main.min.css" />
</head>

<body class="forum">
  <nav class="navbar navbar-inverse navbar-fixed-top" role="navigation">
    <div class="container">
  
      <div class="navbar-header">
        <button type="button" class="navbar-toggle collapsed" data-toggle="collapse" data-target="#navbar" aria-expanded="false" aria-controls="navbar">
          <span class="sr-only">Toggle navigation</span>
          <span class="icon-bar"></span>
          <span class="icon-bar"></span>
          <span class="icon-bar"></span>
        </button>
        <a class="navbar-brand home push-link" href="/"><strong>OpenReview</strong>.net</a>
      </div>
  
      <div id="navbar" class="navbar-collapse collapse">
        <form class="navbar-form navbar-left profile-search" role="search">
          <div class="form-group has-feedback">
            <input id="search_input" type="text" class="form-control ui-autocomplete-input" placeholder="Search ICLR 2019 Conference" autocomplete="off" />
            <span class="glyphicon glyphicon-search form-control-feedback" aria-hidden="true"></span>
          </div>
  
          <input id="search_group" type="hidden" value="ICLR.cc/2019/Conference" />
          <input id="search_content" type="hidden" value="all" />
        <ul id="ui-id-1" tabindex="0" class="ui-menu ui-widget ui-widget-content ui-autocomplete ui-front" style="display: none;"></ul></form>
  
        <ul class="nav navbar-nav navbar-right">
        
            <li id="user-menu"><a href="/login">Login</a></li>
        </ul>
      </div>
  
    </div>
  </nav>

  <div id="or-banner" class="banner" style="">
  <div class="container">
    <div class="row">
      <div class="col-xs-12"><a href="/group?id=ICLR.cc/2019/Conference" title="Venue Homepage"><img class="icon" src="/static/images/arrow_left.svg" /> Go to <strong>ICLR 2019 Conference</strong> homepage</a></div>
    </div>
  </div>
</div>
<div id="flash-message-container" class="alert alert-danger" role="alert" style="display: none;">
  <div class="container">
    <div class="row">
      <div class="col-xs-12">
        <div class="alert-content">
          <button type="button" class="close" aria-label="Close"><span aria-hidden="true">Ã—</span></button>
        </div>
      </div>
    </div>
  </div>
</div>

<div class="container">
  <div class="row">
    <div class="col-xs-12">
      <main id="content" class="clearfix openbanner-visible legacy-styles"><div id="note_SyxknjC9KQ" class="note panel"><div class="title_pdf_row clearfix"><h2 class="note_content_title"><a>Dense Morphological Network: An Universal Function Approximator</a> <a class="note_content_pdf" href="/pdf?id=SyxknjC9KQ" title="Download PDF" target="_blank"><img src="/static/images/pdf_icon_blue.svg" /></a> </h2></div><div class="meta_row"><span class="signatures">Anonymous</span></div><div class="clearfix"><div class="meta_row pull-left"><span class="date item">28 Sep 2018 (modified: 17 Nov 2018)</span><span class="item">ICLR 2019 Conference Blind Submission</span><span class="item">Readers: <span class="readers-icon glyphicon glyphicon-globe"></span> Everyone</span><span class="item"><a class="action-bibtex-modal" data-bibtex="@inproceedings{    &#10;anonymous2019morph-net:,    &#10;title={Morph-Net: An Universal Function Approximator},    &#10;author={Anonymous},    &#10;booktitle={Submitted to International Conference on Learning Representations},    &#10;year={2019},    &#10;url={https://openreview.net/forum?id=SyxknjC9KQ},    &#10;note={under review}    &#10;}">Show Bibtex</a></span><a class="note_content_pdf item" href="/revisions?id=SyxknjC9KQ" target="_blank">Show Revisions</a></div><div class="meta_row meta_actions"></div></div><div class="note_contents"><span class="note_content_field">Abstract: </span><span class="note_content_value">Artificial neural networks are built on the basic operation of linear combination and non-linear activation function. Theoretically this structure can approximate any continuous function with three layer architecture. But in practice learning  the parameters of such network can be hard. Also the choice of activation function can greatly impact the performance of the network. In this paper we are proposing to replace the basic linear combination operation with non-linear operations that do away with the need of additional non-linear activation function. To this end we are proposing the use of elementary  morphological operations (dilation and erosion) as the basic operation in neurons. We show that these networks (Denoted as Morph-Net) with morphological operations can approximate any smooth function requiring less number of parameters than what is necessary for normal neural networks. The results show that our network perform favorably when compared with similar structured network. We have carried out our experiments on  MNIST, Fashion-MNIST, CIFAR10 and CIFAR100.</span></div><div class="note_contents"><span class="note_content_field">Keywords: </span><span class="note_content_value">Mathematical Morphology, Neural Network, Activation Function, Universal Aproximatimation.</span></div><div class="note_contents"><span class="note_content_field">TL;DR: </span><span class="note_content_value">Using mophological operation (dilation and erosion) we have defined a class of network which can approximate any continious function. </span></div><div class="reply_row clearfix"><div class="item" id="reply_count">10 Replies</div></div></div><hr class="small" /><div id="note_children"><div class="note_with_children"><div id="note_H1eSNCd6p7" class="note panel"><div class="title_pdf_row clearfix"><h2 class="note_content_title"><a>Revision</a>  <button class="btn btn-xs btn-default permalink-button" title="Link to this comment" data-permalink-url="https://openreview.net/forum?id=SyxknjC9KQ&amp;noteId=H1eSNCd6p7"><span class="glyphicon glyphicon-link" aria-hidden="true"></span></button></h2></div><div class="meta_row"><span class="signatures">ICLR 2019 Conference Paper676 Authors</span></div><div class="clearfix"><div class="meta_row pull-left"><span class="date item">17 Nov 2018</span><span class="item">ICLR 2019 Conference Paper676 Official Comment</span><span class="item">Readers: <span class="readers-icon glyphicon glyphicon-globe"></span> Everyone</span></div><div class="meta_row meta_actions"></div></div><div class="note_contents"><span class="note_content_field">Comment: </span><span class="note_content_value">- Ablation study on other data set
- Discussion on Multi-layer and gradient propagation is added.
- [1] Ref added 
- Title has been changed from "Morph-Net" to "DENSE MORPHOLOGICAL NETWORK"
- Our contribution is highlighted in introduction </span></div><div class="reply_row clearfix"></div></div><div class="children"></div></div><div class="note_with_children"><div id="note_B1gHuIGwpm" class="note panel"><div class="title_pdf_row clearfix"><h2 class="note_content_title"><a>A nice idea but weak empirical results</a>  <button class="btn btn-xs btn-default permalink-button" title="Link to this comment" data-permalink-url="https://openreview.net/forum?id=SyxknjC9KQ&amp;noteId=B1gHuIGwpm"><span class="glyphicon glyphicon-link" aria-hidden="true"></span></button></h2></div><div class="meta_row"><span class="signatures">ICLR 2019 Conference Paper676 AnonReviewer4</span></div><div class="clearfix"><div class="meta_row pull-left"><span class="date item">12 Nov 2018</span><span class="item">ICLR 2019 Conference Paper676 Official Review</span><span class="item">Readers: <span class="readers-icon glyphicon glyphicon-globe"></span> Everyone</span></div><div class="meta_row meta_actions"></div></div><div class="note_contents"><span class="note_content_field">Review: </span><span class="note_content_value">This paper introduces Morph-Net, a new architecture that intertwine morphological operator such as dilation/erosion with linear layer. Authors first show than Morph-Net are universal approximator. Morph-Net can be expressed as a sum of multi-order hinge functions which can approximate any continuous function. They then validate empirically the Morph-Net on  MNIST, FashionMNIST,  CIFAR10 and CIFAR100 datasets. In particular, authors investigate a 3 layers  fully-connected Morph-Net and shows that it can outperform its Tanh/Relu/Maxout counterparts.

The paper is a nice read also some specific point could be clarify. For instance it is not clear how the structuring elements of the dilation/erosion are learned? Are the learned simply through backpropagation? Also, it is not clear to me how Morph-Net differs from the previously proposed morphological neurons? 

Empirical evaluation of Morph-Net could be improved as well. In particular, authors focus on image classification task. While they show that Morph-Net can outperform other fully connected architecture, the results on CIFAR10/100 seems low compared to convolutional network. It raises the question of the advantages of Morph-Net over convolutional neural networks ?  Authors also limit their exploration to  3-layer networks. Why donâ€™t you explore deeper network for both baseline and Morph-Net?  Finally, if I am not mistaken, authors use the same set of hyperparameters for the baselines/Morph-Net? It is not clear to me if the hyperparameters are optimal for all the approach? They might give an unfair advantage to one of the baseline or Morph-Net?

Overall, this paper present a nice idea. Showing the Morph-Net is an universal approximator is a nice result. However, the empirical evaluation could be improved. It is not clear to me at this point if Morph-Net brings a benefit compare to convolutional net for image classification task.
</span></div><div class="note_contents"><span class="note_content_field">Rating: </span><span class="note_content_value">5: Marginally below acceptance threshold</span></div><div class="note_contents"><span class="note_content_field">Confidence: </span><span class="note_content_value">3: The reviewer is fairly confident that the evaluation is correct</span></div><div class="reply_row clearfix"></div></div><div class="children"><div class="note_with_children"><div id="note_BkeaejOa6Q" class="note panel"><div class="title_pdf_row clearfix"><h2 class="note_content_title"><a>Weak but with respect to deep CNNs.</a>  <button class="btn btn-xs btn-default permalink-button" title="Link to this comment" data-permalink-url="https://openreview.net/forum?id=SyxknjC9KQ&amp;noteId=BkeaejOa6Q"><span class="glyphicon glyphicon-link" aria-hidden="true"></span></button></h2></div><div class="meta_row"><span class="signatures">ICLR 2019 Conference Paper676 Authors</span></div><div class="clearfix"><div class="meta_row pull-left"><span class="date item">17 Nov 2018</span><span class="item">ICLR 2019 Conference Paper676 Official Comment</span><span class="item">Readers: <span class="readers-icon glyphicon glyphicon-globe"></span> Everyone</span></div><div class="meta_row meta_actions"></div></div><div class="note_contents"><span class="note_content_field">Comment: </span><span class="note_content_value">Thank you  for reviewing our paper with valuable and detailed comments. 

Q1: "The paper is a nice read also some specific point could be clarify. For instance it is not clear how the structuring elements of the dilation/erosion are learned? Are the learned simply through backpropagation? "
&gt;&gt; You are right, the structuring elements in dilation-erosion layer and the weights of linear combination layer learn through back propagation. we have added a sub section highlighting a gradient calculation and back-propagation on our network.


Q2: "Also, it is not clear to me how Morph-Net differs from the previously proposed morphological neurons?"
&gt;&gt; Morphological neurons have been defined in the literature in different ways. Although, all of them use dilation and erosion operation, this is usually followed by an additional operation (e.g. activation function [Ritter and Sussner 1996]). For our network we have defined dilation and erosion neurons that perform only dilation and erosion operation respectively. Apart from that, our network also employs an additional linear combination layer. As shown in Theorem 1, these two layers together can approximate any smooth continuous function without requiring additional activation functions. This claim cannot be made if only morphological neurons are used in the network. 


Q3: "Empirical evaluation of Morph-Net could be improved as well. In particular, authors focus on image classification task. While they show that Morph-Net can outperform other fully connected architecture, the results on CIFAR10/100 seems low compared to convolutional network.It raises the question of the advantages of Morph-Net over convolutional neural networks ?"
&gt;&gt; It is true that the convolution networks perform well for images as they are able to extract features based on spatial information. However, in this work we have defined our network for flattened input data and densely connected layers. For this reason our network does not have the advantage of conv type of operation. The main aim is to show that this type of network have capabilities similar to artificial neural networks while using less number of parameters. The advantage of this network over CNNs can possibly be shown after defining the dilation and erosion as 2D operations. 

Q4: "Authors also limit their exploration to  3-layer networks. Why donâ€™t you explore deeper network for both baseline and Morph-Net?"
&gt;&gt; We have proved that using only 3-layer (considering input, dilation-erosion and linear combination layers) network any continuous function can be approximated. That is why we have shown the results using 3-layer networks only. As for going to the multi-layer case, the layers can be stacked in two ways. 
[Type-I] Multiple dilation-erosion layer, followed by a single linear combination layer at the end.
[Type-II] A layer-unit may be defined as Dilation-Erosion layer followed by a linear combination layer. Then this layer-unit may be repeated desired number of times to realize the multi-layer dense morphological network. 
For the network of Type-I, it can be argued that the network is performing some combination of opening and closing operation, and their linear combination. As there are dilation-erosion (DE) layers back to back, the problem of gradient propagation is amplified. As a result it takes much more time to train than single layer architecture (Table 6). 
Similar explanation doesn't work for Type-II networks. From Figure 7 we see that the network has tendency to overfit.


Q5: "Finally, if I am not mistaken, authors use the same set of hyperparameters for the baselines/Morph-Net? It is not clear to me if the hyperparameters are optimal for all the approach? They might give an unfair advantage to one of the baseline or Morph-Net?"
&gt;&gt; Yes, we have used same hyperparamenters for the baseline and Morph-Net, because we want to show that our network is more expressive when using similar hyper-parameters. The hyperparameters may not be optimal for any of the network. This is done for comparison purpose only.

Q6: "Overall, this paper present a nice idea. Showing the Morph-Net is an universal approximator is a nice result. However, the empirical evaluation could be improved. It is not clear to me at this point if Morph-Net brings a benefit compare to convolutional net for image classification task"
&gt;&gt; Since we have not defined 2D Dilation/Erosion in this paper so we refrain ourselves from commenting on this issue, i.e., whether Morph-Net brings a benefit compare to Convolutional or not. However we believe, this is one of the forerunner work and it opens a many directions of future research. 

Thank you again, please let us know if there are any queries or confusion. 
</span></div><div class="reply_row clearfix"></div></div><div class="children"></div></div></div></div><div class="note_with_children"><div id="note_S1l6Toan3Q" class="note panel"><div class="title_pdf_row clearfix"><h2 class="note_content_title"><a>The proposed idea is to replace standard nonlinear activation function with an erosion/dilation operation. The authors report encouraging results but the baseline networks are not state-of-the-art. </a>  <button class="btn btn-xs btn-default permalink-button" title="Link to this comment" data-permalink-url="https://openreview.net/forum?id=SyxknjC9KQ&amp;noteId=S1l6Toan3Q"><span class="glyphicon glyphicon-link" aria-hidden="true"></span></button></h2></div><div class="meta_row"><span class="signatures">ICLR 2019 Conference Paper676 AnonReviewer3</span></div><div class="clearfix"><div class="meta_row pull-left"><span class="date item">05 Nov 2018 (modified: 07 Nov 2018)</span><span class="item">ICLR 2019 Conference Paper676 Official Review</span><span class="item">Readers: <span class="readers-icon glyphicon glyphicon-globe"></span> Everyone</span></div><div class="meta_row meta_actions"></div></div><div class="note_contents"><span class="note_content_field">Review: </span><span class="note_content_value">This paper proposes to replace the standard RELU/tanh units with a combination of dilation and erosion operations, arguing for the observation that the new operator creates more hyper-planes and therefore have more expressive power.

The paper is interesting and there are encouraging results which show a couple of percentage improvements over relu/tanh units.  This paper is also clearly written and easy to understand. However there are two issues:
1. It is somewhat unclear from the paper what  is the main novelty here (compared to existing morpho neurons), is it the learning of the structuring element s? is it the combination of the dilation+erosion operations?
2. The second issue is that presumably due to the fact that Conv layers are not used, the accuracy on cifar-10 and cifar-100 are significantly lower than state-of-the-art. It would make the paper extremely strong if the improvement translated to CNNs which are performing near the state-of-the-art. What happens if relu units in CNNs were swapped out for the proposed dilation/erosion operators?
</span></div><div class="note_contents"><span class="note_content_field">Rating: </span><span class="note_content_value">5: Marginally below acceptance threshold</span></div><div class="note_contents"><span class="note_content_field">Confidence: </span><span class="note_content_value">4: The reviewer is confident but not absolutely certain that the evaluation is correct</span></div><div class="reply_row clearfix"></div></div><div class="children"><div class="note_with_children"><div id="note_Syxd-gzDTQ" class="note panel"><div class="title_pdf_row clearfix"><h2 class="note_content_title"><a>The main idea is to replace the normal artificial neural networks by using basic morphological operations. Non-requrement of activation functions is a by-product.</a>  <button class="btn btn-xs btn-default permalink-button" title="Link to this comment" data-permalink-url="https://openreview.net/forum?id=SyxknjC9KQ&amp;noteId=Syxd-gzDTQ"><span class="glyphicon glyphicon-link" aria-hidden="true"></span></button></h2></div><div class="meta_row"><span class="signatures">ICLR 2019 Conference Paper676 Authors</span></div><div class="clearfix"><div class="meta_row pull-left"><span class="date item">12 Nov 2018 (modified: 17 Nov 2018)</span><span class="item">ICLR 2019 Conference Paper676 Official Comment</span><span class="item">Readers: <span class="readers-icon glyphicon glyphicon-globe"></span> Everyone</span></div><div class="meta_row meta_actions"></div></div><div class="note_contents"><span class="note_content_field">Comment: </span><span class="note_content_value">Thank you  for reviewing our paper with valuable and detailed comments.

Q1: "This paper proposes to replace the standard RELU/tanh units with a combination of dilation and erosion operations, arguing for the observation that the new operator creates more hyper-planes and therefore have more expressive power"
&gt;&gt; In this paper we propose to build networks with basic morphological operations. This gives us the power to build networks with similar expressive power of the normal artificial neural networks without the need of activation functions, while {\em requiring less number of parameters}. Replacing the standard nonlinear activation function is not the main goal, it may be a by-product of the dilation-erosion operation. However, thanks for pointing this out. 


Q2: "The paper is interesting and there are encouraging results which show a couple of percentage improvements over relu/tanh units.  This paper is also clearly written and easy to understand. However there are two issues:
1. It is some what unclear from the paper what is the main novelty here (compared to existing morpho neurons), is it the learning of the structuring elements? is it the combination of the dilation+erosion operations?"
&gt;&gt;The main contribution are as follows.
1. The use of linear combination operation after dilation-erosion operation. This structure, as shown in Section 3.3, can approximate any continuous function given enough dilation/erosion neurons.
2. We have shown that the networks build with such layers do not need activation functions.
3. The use of dilation-erosion layer followed by linear combination layer greatly increases number of possible decision boundaries. As a result, complex decision boundaries can be learned using small number of parameters. This is visually shown using a toy dataset in Section 4.1.

Note that, in the dilation and erosion layers, we have considered structuring elements only of same size. However, in the training process we learn the values of the structuring element pixels as well as the weights of the linear combination layer. 
However, we will add a paragraph highlighting our contribution in the revised version. 

Q3: "The second issue is that presumably due to the fact that Conv layers are not used, the accuracy on cifar-10 and cifar-100 are significantly lower than state-of-the-art. It would make the paper extremely strong if the improvement translated to CNNs which are performing near the state-of-the-art. What happens if relu units in CNNs were swapped out for the proposed dilation/erosion operators?"
&gt;&gt; It is true that the convolution layers perform well for images as they are able to extract features based on spatial information. 
However, in this work we have defined our network for flattened input data. Our network structure and the operations is totally different than classical neural network. For instance in the first layer we take addition (subtraction) with weights (i.e., values of structuring element) instead of multiplication and then take max (min) instead of addition to implement 1-D dilation (erosion) operation. In the next layer we are taking weighted combination of the output from this layer. 
So, we do not and cannot directly use convolution layer in our network, and just swapping the activation function with dilation/erosion layers will not work. For this reason, we have compared our work with neural networks containing dense layers. For harnessing the spatial information, 2D dilation-erosion layer may be defined where the structuring element is much smaller than the input (image). 

Thank you again, please let us know if there are any queries or confusion. 
</span></div><div class="reply_row clearfix"></div></div><div class="children"></div></div></div></div><div class="note_with_children"><div id="note_HJeCtix9hQ" class="note panel"><div class="title_pdf_row clearfix"><h2 class="note_content_title"><a>Interesting idea for using morphological operators but too preliminary</a>  <button class="btn btn-xs btn-default permalink-button" title="Link to this comment" data-permalink-url="https://openreview.net/forum?id=SyxknjC9KQ&amp;noteId=HJeCtix9hQ"><span class="glyphicon glyphicon-link" aria-hidden="true"></span></button></h2></div><div class="meta_row"><span class="signatures">ICLR 2019 Conference Paper676 AnonReviewer2</span></div><div class="clearfix"><div class="meta_row pull-left"><span class="date item">03 Nov 2018 (modified: 07 Nov 2018)</span><span class="item">ICLR 2019 Conference Paper676 Official Review</span><span class="item">Readers: <span class="readers-icon glyphicon glyphicon-globe"></span> Everyone</span></div><div class="meta_row meta_actions"></div></div><div class="note_contents"><span class="note_content_field">Review: </span><span class="note_content_value">The authors introduce Morph-Net, a single layer neural network where
the mapping is performed using morphological dilation and erosion.
I was expecting something applied to convolutional networks as such operators
are very popular in image processing, so the naming is a bit misleading.

It is shown that the proposed network can approximate any smooth function, 
assuming a sufficiently large number of hidden neurons, that is a nice result.

Clarity should be improved, for example it is mentioned that the structuring
element is learned but never clearly explained how and what difficulties it poses.
In the main text it is written that alpha is {-1, 1}, which would result in a
combinatorial search, but never explained how it is learned in practice.
This is shown only in the appendix but it is not clear to me that using a binarization
with the weights is not prone to degenerate solutions and/or to learn at all
if proper initialization is not used.
Did the authors experiment with smooth versions or other form of binarization with
straight-through estimator or sampling?

In the proof for theorem 1 it is not clear if the convergence of the proposed
network is faster or slower than that of a classic single layer network.

The main result of the paper is that the structuring element can be learned,
but there is no discussion on what it is learned. Also, there is no comparison
on related approaches that try to learn the structuring element in an end-to-end
fashion such as [1].

Experiments lack a more thorough comparison with state-of-the-art and at least
an ablation study to show that the proposed approach is effective and has merit.
For example, what is the relative contribution of using dilation and erosion
jointly versus either one of them.
What is the comparison with a winner-take-all unit over groups of neurons
such as max-pooling?

It seems that extending the work to multiple layers should be trivial but it is
not reported and is left to future investigations. This hints at issues with
the optimization and should be discussed, is it related to the binarization
mentioned above?

Overall the idea is interesting but the way the structuring element is learned
should be discussed in more details and exemplified visually. Experiments need
to be improved and overall applicability is uncertain at this stage.

=======
[1] Masci et al., A Learning Framework for Morphological Operators Using Counter--Harmonic Mean.
</span></div><div class="note_contents"><span class="note_content_field">Rating: </span><span class="note_content_value">5: Marginally below acceptance threshold</span></div><div class="note_contents"><span class="note_content_field">Confidence: </span><span class="note_content_value">5: The reviewer is absolutely certain that the evaluation is correct and very familiar with the relevant literature</span></div><div class="reply_row clearfix"></div></div><div class="children"><div class="note_with_children"><div id="note_r1gIG8_6pQ" class="note panel"><div class="title_pdf_row clearfix"><h2 class="note_content_title"><a>Our work is one of the forerunner work and it opens a many directions of future research[Part 1/2]</a>  <button class="btn btn-xs btn-default permalink-button" title="Link to this comment" data-permalink-url="https://openreview.net/forum?id=SyxknjC9KQ&amp;noteId=r1gIG8_6pQ"><span class="glyphicon glyphicon-link" aria-hidden="true"></span></button></h2></div><div class="meta_row"><span class="signatures">ICLR 2019 Conference Paper676 Authors</span></div><div class="clearfix"><div class="meta_row pull-left"><span class="date item">17 Nov 2018</span><span class="item">ICLR 2019 Conference Paper676 Official Comment</span><span class="item">Readers: <span class="readers-icon glyphicon glyphicon-globe"></span> Everyone</span></div><div class="meta_row meta_actions"></div></div><div class="note_contents"><span class="note_content_field">Comment: </span><span class="note_content_value">Thank you  for reviewing our paper with valuable and detailed comments. 

Q1: "The authors introduce Morph-Net, a single layer neural network where the mapping is performed using morphological dilation and erosion.I was expecting something applied to convolutional networks as such operators are very popular in image processing, so the naming is a bit misleading."
&gt;&gt; We will update the name of the paper to "Dense Morphological Network: A Universal Function Approximator" to reduce confusion. 

Q2: "It is shown that the proposed network can approximate any smooth function, assuming a sufficiently large number of hidden neurons, that is a nice result.Clarity should be improved, for example it is mentioned that the structuring element is learned but never clearly explained how and what difficulties it poses."
&gt;&gt; In our network we learn the structuring element and the weights of the linear combination layer using gradient descent method for minimizing loss. While learning structuring elements there maybe following problem.
Dilation (Erosion) operation involves max (min) operation. So it implies that, during back propagation, gradient of loss function with respect to components of structuring element is zero except the component corresponding to max (min). That means for each data only one component of structuring element may be updated. As a result, the learning  process may be slow. 
We did not notice any other difficulties these dilation-erosion operation may give rise to. Note that, in this work, We have focused only on how the network works in general settings. 

Q3: "In the main text it is written that alpha is {-1, 1}, which would result in a combinatorial search, but never explained how it is learned in practice. This is shown only in the appendix but it is not clear to me that using a binarization with the weights is not prone to degenerate solutions and/or to learn at all if proper initialization is not used. Did the authors experiment with smooth versions or other form of binarization with straight-through estimator or sampling?"
&gt;&gt; The parameter alpha is not learned in our method, not even used. It is introduced only for proving the theorems. We only learn the structuring element and the weights of the linear combination layer during the training. 


Q4: "Review: In the proof for theorem 1 it is not clear if the convergence of the proposed
network is faster or slower than that of a classic single layer network."
&gt;&gt; Theorem 1 only shows that our network can approximate any continuous function provided there are enough nodes in dilation-erosion layer. We are not claiming anything regarding the convergence rate. However, it is already mentioned that training our network may be slower at times depending on the dimension of data. </span></div><div class="reply_row clearfix"></div></div><div class="children"><div class="note_with_children"><div id="note_H1gh48_TTX" class="note panel"><div class="title_pdf_row clearfix"><h2 class="note_content_title"><a>Our work is one of the forerunner work and it opens a many directions of future research[Part 2/2] </a>  <button class="btn btn-xs btn-default permalink-button" title="Link to this comment" data-permalink-url="https://openreview.net/forum?id=SyxknjC9KQ&amp;noteId=H1gh48_TTX"><span class="glyphicon glyphicon-link" aria-hidden="true"></span></button></h2></div><div class="meta_row"><span class="signatures">ICLR 2019 Conference Paper676 Authors</span></div><div class="clearfix"><div class="meta_row pull-left"><span class="date item">17 Nov 2018</span><span class="item">ICLR 2019 Conference Paper676 Official Comment</span><span class="item">Readers: <span class="readers-icon glyphicon glyphicon-globe"></span> Everyone</span></div><div class="meta_row meta_actions"></div></div><div class="note_contents"><span class="note_content_field">Comment: </span><span class="note_content_value">Q5: " The main result of the paper is that the structuring element can be learned, but there is no discussion on what it is learned. Also, there is no comparison on related approaches that try to learn the structuring element in an end-to-end fashion such as [1].
&gt;&gt; Thank you for the reference. we are learning the parameters of the structuring elements  in dilation-erosion layer  and the  parameters of the weighted combination layer. However, our main contribution is not the learning of structuring element. The main contribution is to design a dense morphological networks with dilation-erosion and their linear combination having similar expressive power as the artificial neural networks. As we are using morphological operations, learning of structuring elements comes into picture and for our case the size of the structuring element is same as that of the input. This is not the case for [1]. In our paper we are  flattening the image (cifar-10, cifar-100)  and producing the  class label. 

Q6: "Experiments lack a more thorough comparison with state-of-the-art and at least an ablation study to show that the proposed approach is effective and has merit. For example, what is the relative contribution of using dilation and erosion jointly versus either one of them. What is the comparison with a winner-take-all unit over groups of neurons such as max-pooling?"
&gt;&gt; Thank you for the suggestion. We have shown the contribution of dilation and erosion neurons for the toy data only. We have updated the manuscript to show this relative comparison in other data sets. 

Q7: "It seems that extending the work to multiple layers should be trivial but it is not reported and is left to future investigations. This hints at issues with the optimization and should be discussed, is it related to the binarization mentioned above? "
&gt;&gt; You are right,  it is  not that trivial to extend the work for multiple layers. Since our theoretical justification is on single layer, we have not shown the results with multiple layers. 
However, based on the reviewer's suggestion, we have added some results with multiple layers in Section 5 and 
Table 6.
Extension of this network to multiple layers can be done in two ways. 
[Type-I] Multiple dilation-erosion layer, followed by a single linear combination layer at the end.
[Type-II] A layer-unit may be defined as Dilation-Erosion layer followed by a linear combination layer. Then this layer-unit may be repeated desired number of times to realize the multi-layer dense morphological network. 
For the network of Type-I, it can be argued that the network is performing some combination of opening and closing operation, and their linear combination. As there are dilation-erosion (DE) layers back to back, the problem of gradient propagation is amplified. As a result it takes much more time to train than single layer architecture (Table 6). 
Similar explanation doesn't work for Type-II networks. From Figure 7 we see that the network has tendency to over-fit. 

Q8: Overall the idea is interesting but the way the structuring element is learned should be discussed in more details and exemplified visually. Experiments need to be improved and overall applicability is uncertain at this stage. 
&gt;&gt; Some more experimental results and explanation are incorporated in the revised version. 

Thank you again, please let us know if there are any queries or confusion. 
</span></div><div class="reply_row clearfix"></div></div><div class="children"></div></div></div></div></div></div><div class="note_with_children"><div id="note_BygipUQr5Q" class="note panel"><div class="title_pdf_row clearfix"><h2 class="note_content_title"><a>Name conflict</a>  <button class="btn btn-xs btn-default permalink-button" title="Link to this comment" data-permalink-url="https://openreview.net/forum?id=SyxknjC9KQ&amp;noteId=BygipUQr5Q"><span class="glyphicon glyphicon-link" aria-hidden="true"></span></button></h2></div><div class="meta_row"><span class="signatures"><a href="/profile?id=~Elad_Eban1" class="profile-link">Elad Eban</a></span></div><div class="clearfix"><div class="meta_row pull-left"><span class="date item">06 Oct 2018</span><span class="item">ICLR 2019 Conference Paper676 Public Comment</span><span class="item">Readers: <span class="readers-icon glyphicon glyphicon-globe"></span> Everyone</span></div><div class="meta_row meta_actions"></div></div><div class="note_contents"><span class="note_content_field">Comment: </span><span class="note_content_value">Hi,

It would be nice and very useful if you consider renaming your paper, as a paper named "MorphNet: Fast &amp; Simple Resource-Constrained Structure Learning of Deep Networks", was published in CVPR 2018. I believe this is  bad name conflict as the papers topic are related enough to cause confusion. 

Respectfully,

Elad Eban

see:
<a href="https://arxiv.org/abs/1711.06798" target="_blank" rel="nofollow">https://arxiv.org/abs/1711.06798</a></span></div><div class="reply_row clearfix"></div></div><div class="children"><div class="note_with_children"><div id="note_r1lgUd5v9X" class="note panel"><div class="title_pdf_row clearfix"><h2 class="note_content_title"><a>Thank you for  pointing out</a>  <button class="btn btn-xs btn-default permalink-button" title="Link to this comment" data-permalink-url="https://openreview.net/forum?id=SyxknjC9KQ&amp;noteId=r1lgUd5v9X"><span class="glyphicon glyphicon-link" aria-hidden="true"></span></button></h2></div><div class="meta_row"><span class="signatures">ICLR 2019 Conference Paper676 Authors</span></div><div class="clearfix"><div class="meta_row pull-left"><span class="date item">07 Oct 2018</span><span class="item">ICLR 2019 Conference Paper676 Official Comment</span><span class="item">Readers: <span class="readers-icon glyphicon glyphicon-globe"></span> Everyone</span></div><div class="meta_row meta_actions"></div></div><div class="note_contents"><span class="note_content_field">Comment: </span><span class="note_content_value">Thank you for pointing out. We will change the title to something else. 

Thank you,
</span></div><div class="reply_row clearfix"></div></div><div class="children"></div></div></div></div></div></main></div>
  </div>
</div>


    <!-- Footer -->
    <footer class="sitemap">
      <div class="container">
        <div class="row hidden-xs">
          <div class="col-sm-4">
            <ul class="list-unstyled">
              <li><a href="/" class="home">Home</a></li>
              <li><a href="/venues">All Venues</a></li>
            </ul>
          </div>
    
          <div class="col-sm-4">
            <ul class="list-unstyled">
              <li><a href="/about">About OpenReview</a></li>
              <li><a href="/contact">Contact</a></li>
              <li><a href="#" data-toggle="modal" data-target="#feedback-modal">Feedback</a></li>
            </ul>
          </div>
    
          <div class="col-sm-4">
            <ul class="list-unstyled">
              <li><a href="/terms">Terms of Service</a></li>
              <li><a href="/privacy">Privacy Policy</a></li>
            </ul>
          </div>
        </div>
    
        <div class="row visible-xs-block">
          <div class="col-xs-6">
            <ul class="list-unstyled">
              <li><a href="/" class="home">Home</a></li>
              <li><a href="/about">About OpenReview</a></li>
              <li><a href="#" data-toggle="modal" data-target="#feedback-modal">Feedback</a></li>
            </ul>
          </div>
    
          <div class="col-xs-6">
            <ul class="list-unstyled">
              <li><a href="/contact">Contact</a></li>
              <li><a href="/terms">Terms of Service</a></li>
              <li><a href="/privacy">Privacy Policy</a></li>
            </ul>
          </div>
        </div>
      </div>
    </footer>
    
    <footer class="sponsor">
      <div class="container">
        <div class="row">
          <div class="col-sm-10 col-sm-offset-1">
            <p class="text-center">
              OpenReview is created by the <a href="http://www.iesl.cs.umass.edu/" target="_blank">Information Extraction and Synthesis Laboratory</a>, College of Information and Computer Science, University of Massachusetts Amherst. We gratefully acknowledge the support of the OpenReview sponsors:  Google,  Facebook, NSF, the University of Massachusetts Amherst Center for Data Science, and Center for Intelligent Information Retrieval, as well as the Google Cloud Platform for donating the computing and networking services on which OpenReview.net runs.
            </p>
          </div>
        </div>
      </div>
    </footer>

  <div id="feedback-modal" class="modal fade" tabindex="-1" role="dialog">
    <div class="modal-dialog">
      <div class="modal-content">
        <div class="modal-header">
          <button type="button" class="close" data-dismiss="modal" aria-label="Close"><span aria-hidden="true">Ã—</span></button>
          <h3 class="modal-title">Send Feedback</h3>
        </div>
        <div class="modal-body">
          <p>Enter your feedback below and we'll get back to you as soon as possible.</p>
          <form action="/feedback" method="POST">
            <div class="form-group">
              <input type="email" name="from" class="form-control" placeholder="Email" />
            </div>
            <div class="form-group">
              <input type="text" name="subject" class="form-control" placeholder="Subject" />
            </div>
            <div class="form-group">
              <textarea name="message" class="form-control feedback-input" rows="5" placeholder="Message" required=""></textarea>
            </div>
          <ul id="ui-id-2" tabindex="0" class="ui-menu ui-widget ui-widget-content" style="display: none;"></ul></form>
        </div>
        <div class="modal-footer">
          <button type="button" class="btn btn-default" data-dismiss="modal">Cancel</button>
          <button type="button" class="btn btn-primary">Send</button>
        </div>
      </div>
    </div>
  </div>

  <script type="text/javascript" async="" src="https://www.google-analytics.com/analytics.js"></script><script src="//cdnjs.cloudflare.com/ajax/libs/jquery/2.2.4/jquery.min.js"></script>
  <script>window.jQuery || document.write('&lt;script src="/static/js/vendor/jquery-2.2.4.min.js"&gt;&lt;\/script&gt;')</script>

  <script src="/static/dist/vendor.min.js"></script>
  <script src="/static/dist/templates.min.js"></script>
  <script src="/static/dist/openreview.min.js"></script>

    <script>window.legacyScripts = true;</script>


    <script async="" src="https://www.googletagmanager.com/gtag/js?id=UA-108703919-1"></script>
    <script>
      window.dataLayer = window.dataLayer || [];
      function gtag() { dataLayer.push(arguments); }
      gtag('js', new Date());
      gtag('config', 'UA-108703919-1');
    </script>


<div role="status" aria-live="assertive" aria-relevant="additions" class="ui-helper-hidden-accessible"></div></body></html>