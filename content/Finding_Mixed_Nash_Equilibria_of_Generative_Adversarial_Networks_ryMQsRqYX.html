<!DOCTYPE html><html xmlns="http://www.w3.org/1999/xhtml" class="no-js"><head>
  <meta charset="utf-8" />
  <meta http-equiv="X-UA-Compatible" content="IE=edge,chrome=1" />
  <meta name="viewport" content="width=device-width, initial-scale=1" />

  <title>Finding Mixed Nash Equilibria of Generative Adversarial Networks | OpenReview</title>
  <meta name="description" content="" />

      <meta name="citation_title" content="Finding Mixed Nash Equilibria of Generative Adversarial Networks" />
        <meta name="citation_author" content="Anonymous" />
      <meta name="citation_publication_date" content="2018/09/27" />
      <meta name="citation_online_date" content="2018/09/27" />
      <meta name="citation_pdf_url" content="https://openreview.net/pdf?id=ryMQ5sRqYX" />
      <meta name="twitter:card" content="summary" />
      <meta name="twitter:site" content="@openreviewnet" />
      <meta name="og:title" content="Finding Mixed Nash Equilibria of Generative Adversarial Networks" />
      <meta name="og:description" content="We reconsider the training objective of Generative Adversarial Networks (GANs) from the mixed Nash Equilibria (NE) perspective. Inspired by the classical prox methods, we develop a novel..." />
      <meta name="og:image" content="https://openreview.net/static/images/openreview_logo_512.png" />

  <link rel="stylesheet" href="https://fonts.googleapis.com/css?family=Noto+Sans:400,400i,700,700i" />

  <link rel="stylesheet" href="/static/css/bootstrap.min.css" />
  <link rel="stylesheet" href="/static/css/jquery-ui.min.css" />
  <link rel="stylesheet" href="/static/css/main.min.css" />
</head>

<body class="forum">
  <nav class="navbar navbar-inverse navbar-fixed-top" role="navigation">
    <div class="container">
  
      <div class="navbar-header">
        <button type="button" class="navbar-toggle collapsed" data-toggle="collapse" data-target="#navbar" aria-expanded="false" aria-controls="navbar">
          <span class="sr-only">Toggle navigation</span>
          <span class="icon-bar"></span>
          <span class="icon-bar"></span>
          <span class="icon-bar"></span>
        </button>
        <a class="navbar-brand home push-link" href="/"><strong>OpenReview</strong>.net</a>
      </div>
  
      <div id="navbar" class="navbar-collapse collapse">
        <form class="navbar-form navbar-left profile-search" role="search">
          <div class="form-group has-feedback">
            <input id="search_input" type="text" class="form-control ui-autocomplete-input" placeholder="Search ICLR 2019 Conference" autocomplete="off" />
            <span class="glyphicon glyphicon-search form-control-feedback" aria-hidden="true"></span>
          </div>
  
          <input id="search_group" type="hidden" value="ICLR.cc/2019/Conference" />
          <input id="search_content" type="hidden" value="all" />
        <ul id="ui-id-1" tabindex="0" class="ui-menu ui-widget ui-widget-content ui-autocomplete ui-front" style="display: none;"></ul></form>
  
        <ul class="nav navbar-nav navbar-right">
        
            <li id="user-menu"><a href="/login">Login</a></li>
        </ul>
      </div>
  
    </div>
  </nav>

  <div id="or-banner" class="banner" style="">
  <div class="container">
    <div class="row">
      <div class="col-xs-12"><a href="/group?id=ICLR.cc/2019/Conference" title="Venue Homepage"><img class="icon" src="/static/images/arrow_left.svg" /> Go to <strong>ICLR 2019 Conference</strong> homepage</a></div>
    </div>
  </div>
</div>
<div id="flash-message-container" class="alert alert-danger" role="alert" style="display: none;">
  <div class="container">
    <div class="row">
      <div class="col-xs-12">
        <div class="alert-content">
          <button type="button" class="close" aria-label="Close"><span aria-hidden="true">×</span></button>
        </div>
      </div>
    </div>
  </div>
</div>

<div class="container">
  <div class="row">
    <div class="col-xs-12">
      <main id="content" class="clearfix openbanner-visible legacy-styles"><div id="note_ryMQ5sRqYX" class="note panel"><div class="title_pdf_row clearfix"><h2 class="note_content_title"><a>Finding Mixed Nash Equilibria of Generative Adversarial Networks</a> <a class="note_content_pdf" href="/pdf?id=ryMQ5sRqYX" title="Download PDF" target="_blank"><img src="/static/images/pdf_icon_blue.svg" /></a> </h2></div><div class="meta_row"><span class="signatures">Anonymous</span></div><div class="clearfix"><div class="meta_row pull-left"><span class="date item">28 Sep 2018 (modified: 14 Nov 2018)</span><span class="item">ICLR 2019 Conference Blind Submission</span><span class="item">Readers: <span class="readers-icon glyphicon glyphicon-globe"></span> Everyone</span><span class="item"><a class="action-bibtex-modal" data-bibtex="@inproceedings{    &#10;anonymous2019finding,    &#10;title={Finding Mixed Nash Equilibria of Generative Adversarial Networks},    &#10;author={Anonymous},    &#10;booktitle={Submitted to International Conference on Learning Representations},    &#10;year={2019},    &#10;url={https://openreview.net/forum?id=ryMQ5sRqYX},    &#10;note={under review}    &#10;}">Show Bibtex</a></span><a class="note_content_pdf item" href="/revisions?id=ryMQ5sRqYX" target="_blank">Show Revisions</a></div><div class="meta_row meta_actions"></div></div><div class="note_contents"><span class="note_content_field">Abstract: </span><span class="note_content_value">We reconsider the training objective of Generative Adversarial Networks (GANs) from the mixed Nash Equilibria (NE) perspective. Inspired by the classical prox methods, we develop a novel algorithmic framework for GANs via an infinite-dimensional two-player game and prove rigorous convergence rates to the mixed NE. We then propose a principled procedure to reduce our novel prox methods to simple sampling routines, leading to practically efficient algorithms. Finally, we provide experimental evidence that our approach outperforms methods that seek pure strategy equilibria, such as SGD, Adam, and RMSProp, both in speed and quality.</span></div><div class="note_contents"><span class="note_content_field">Keywords: </span><span class="note_content_value">GANs, mixed Nash equilibrium, mirror descent, sampling</span></div><div class="reply_row clearfix"><div class="item" id="reply_count">9 Replies</div></div></div><hr class="small" /><div id="note_children"><div class="note_with_children"><div id="note_rygbA2hua7" class="note panel"><div class="title_pdf_row clearfix"><h2 class="note_content_title"><a>General response: ****New experiments****</a>  <button class="btn btn-xs btn-default permalink-button" title="Link to this comment" data-permalink-url="https://openreview.net/forum?id=ryMQ5sRqYX&amp;noteId=rygbA2hua7"><span class="glyphicon glyphicon-link" aria-hidden="true"></span></button></h2></div><div class="meta_row"><span class="signatures">ICLR 2019 Conference Paper521 Authors</span></div><div class="clearfix"><div class="meta_row pull-left"><span class="date item">14 Nov 2018</span><span class="item">ICLR 2019 Conference Paper521 Official Comment</span><span class="item">Readers: <span class="readers-icon glyphicon glyphicon-globe"></span> Everyone</span></div><div class="meta_row meta_actions"></div></div><div class="note_contents"><span class="note_content_field">Comment: </span><span class="note_content_value">As Reviewer2 suggested, we performed additional numerical comparisons to modern methods. The revision includes two contemporary algorithms, including the simultaneous and alternated Extra-Adam, from the concurrent ICLR submission

[2] A Variational Inequality Perspective on Generative Adversarial Networks.

While the theory of [2] results in an algorithm for simultaneous updates, as in our method, the authors in the numerical evidence use an alternating update scheme, whose convergence guarantee is unknown. Moreover, note also that the theorems in [2] apply only to the simultaneous update algorithm for the (strongly) convex-concave objectives and not the GAN objective. Hence, the alternating update algorithm is mostly motivated by the empirical evidence. 

In the new Figure 2, one can see that the simultaneous Extra-Adam fails in our experiment, while the performance of alternated Extra-Adam is comparable to Mirror-GAN. Currently it is unclear with one dataset whether our approach yields better empirical result than [2]. However, we highlight that our algorithm is motivated by a clear mixed NE theoretical model and approximations that directly apply to this model. 

Finally, the alternated Extra-Adam slightly outperformed our Mirror-GAN on the LSUN dataset in terms of Inception Score, whereas the slight improvement in the numerics, in our opinion, did not translate into the quality of the generated images; see Table 2. We plan on further testing our method on other datasets as well. 
</span></div><div class="reply_row clearfix"></div></div><div class="children"></div></div><div class="note_with_children"><div id="note_rJgX333d6m" class="note panel"><div class="title_pdf_row clearfix"><h2 class="note_content_title"><a>General response: ****Literature****</a>  <button class="btn btn-xs btn-default permalink-button" title="Link to this comment" data-permalink-url="https://openreview.net/forum?id=ryMQ5sRqYX&amp;noteId=rJgX333d6m"><span class="glyphicon glyphicon-link" aria-hidden="true"></span></button></h2></div><div class="meta_row"><span class="signatures">ICLR 2019 Conference Paper521 Authors</span></div><div class="clearfix"><div class="meta_row pull-left"><span class="date item">14 Nov 2018</span><span class="item">ICLR 2019 Conference Paper521 Official Comment</span><span class="item">Readers: <span class="readers-icon glyphicon glyphicon-globe"></span> Everyone</span></div><div class="meta_row meta_actions"></div></div><div class="note_contents"><span class="note_content_field">Comment: </span><span class="note_content_value">As the reviewers are keen on pointing out subtleties in contributions, we would like to highlight that we were very careful with our citations. 

First of all, we stress that the paper, which we were aware of at the point of submission, 

[1] Mirror Descent Learning in Continuous Games by Zhou et. al. at CDC 2017

is of little relevance to our study. Since [1] assumes a CONCAVE reward, ordinary finite-dimensional mirror descent already solves the PURE strategy equilibrium. 

In particular, there is no notion of mixed Nash Equilibrium (mixed NE) in [1], and all the optimization variables are in subsets of R^d, and hence, it is completely different from our setting. When applied to training GANs, one necessarily faces the problem of non-concavity, and no result in [1] can hold. This problem was studied by (Grnarova et al. 2018) in our submission, which we have cited and compared to in detail. 

We also stress that we have already cited the most relevant literature (Balandat et al., 2016) on convex games in Banach spaces, and the relevance in other related works (such as the ones provided by Reviewer3) are contained in (Balandat et al., 2016). 

Specifically, our framework is algorithmic, and we study convergence of a specific, implementable, algorithm, which has the same theme as (Balandat et al., 2016). 

On the other hand, the second and third paper provided by Reviewer3 concern existence of mirror maps and do not provide any algorithm. Moreover, the results in both papers do not apply to our problem. 

On one hand, they rely on theory of martingale type which gives nontrivial bounds only for L^p spaces with 1&lt;p&lt;\infty, whereas the special case of p=1 is essential to our interest. 

Moreover, the third paper assumes the optimization constraint to be centrally-symmetric, which does not hold for the set of all probability distributions.

Please let us know if we have missed other references.
</span></div><div class="reply_row clearfix"></div></div><div class="children"></div></div><div class="note_with_children"><div id="note_HJxCtnhOaX" class="note panel"><div class="title_pdf_row clearfix"><h2 class="note_content_title"><a>General response: ****Novelty and significance****</a>  <button class="btn btn-xs btn-default permalink-button" title="Link to this comment" data-permalink-url="https://openreview.net/forum?id=ryMQ5sRqYX&amp;noteId=HJxCtnhOaX"><span class="glyphicon glyphicon-link" aria-hidden="true"></span></button></h2></div><div class="meta_row"><span class="signatures">ICLR 2019 Conference Paper521 Authors</span></div><div class="clearfix"><div class="meta_row pull-left"><span class="date item">14 Nov 2018</span><span class="item">ICLR 2019 Conference Paper521 Official Comment</span><span class="item">Readers: <span class="readers-icon glyphicon glyphicon-globe"></span> Everyone</span></div><div class="meta_row meta_actions"></div></div><div class="note_contents"><span class="note_content_field">Comment: </span><span class="note_content_value">Reviewer2 and 3 criticized our work from lacking sufficient technicality on infinite-dimensional mirror descent (inf-dim MD); we argue that this is an oversight of our contribution. It is also not a fair evaluation. 

Simply put, prior to our work, the literature treats the GAN formulations as if they are (semi) convex-concave games. Then, you see familiar optimization methods from the deep, earlier literature in game theory and online learning applied to the GAN problem, basically ignoring the non-convexity of neural nets (except for the very few literature that we cited). 

Our work proposes a new insight, which is obvious in retrospect for theorists: We seek distributions over GAN parameters and show that the underlying learning problem is a well-posed, bi-affine convex game, albeit infinite dimensional. 

We build on this insight by showing how the infinite-dimensional entropic MD applies to it with rates while the iterations of this algorithm can be obtained via Langevin dynamics. We also take care of the end-to-end engineering aspects of this proposal and integrate an algorithm that has excellent practical performance. 

There is no such work in the literature, which encouraged us to state that we resolved an open problem. 

We contend that none of the elements of our approach has theory dressing or obfuscation. We do not use an existing algorithm that has guarantees with a different set of assumptions and yet still apply to GANs. All the elements in our approach are necessary. If you take any one of them out, whether it is the infinite dimensional bi-affine game reformulation via Riesz representation, or Langevin dynamics-based iterations, the framework falls down. 

We also know that people technically skilled can do the infinite-dimensional MD extensions, but since there is nothing to cite currently, we put the derivation for completeness. 

In fact, Marc Teboulle mentioned to us in private communication in October 2018 that he had the infinite-dimensional characterization of the entropic MD done but did not publish. We are also personally aware that Arkadi Nemirovskii is more than capable of doing such a derivation, along with many other optimization experts.  

That being said, however, we would appreciate if the reviewers can point out to us any literature that has the derivation written down and how it connects with Langevin dynamics for sampling. The reason for this terse remark is that if we do not provide the derivation, it is a point for criticism. If we do it for completeness, as in the current paper, it should not be a reason for omitting our real contributions and over-emphasizing as if it is the main contribution of this work in an overly harsh manner. 

Finally, we stress that our theory allows to make the strongest theoretical claim for training GANs to date: Assuming the success of sampling from a single neural net, which was empirically justified in previous work, we can show non-asymptotic convergence rates to the saddle points. In contrast, even if we assume that SGD/Adam/etc. globally optimize a single neural net, can we say anything about the convergence to the saddle points of classical GANs? To our knowledge, the answer is currently none.
</span></div><div class="reply_row clearfix"></div></div><div class="children"></div></div><div class="note_with_children"><div id="note_rJew8qeCn7" class="note panel"><div class="title_pdf_row clearfix"><h2 class="note_content_title"><a>Generalize mirror descent to infinite dimensional spaces. No really new theory or practice insight.</a>  <button class="btn btn-xs btn-default permalink-button" title="Link to this comment" data-permalink-url="https://openreview.net/forum?id=ryMQ5sRqYX&amp;noteId=rJew8qeCn7"><span class="glyphicon glyphicon-link" aria-hidden="true"></span></button></h2></div><div class="meta_row"><span class="signatures">ICLR 2019 Conference Paper521 AnonReviewer2</span></div><div class="clearfix"><div class="meta_row pull-left"><span class="date item">06 Nov 2018 (modified: 07 Nov 2018)</span><span class="item">ICLR 2019 Conference Paper521 Official Review</span><span class="item">Readers: <span class="readers-icon glyphicon glyphicon-globe"></span> Everyone</span></div><div class="meta_row meta_actions"></div></div><div class="note_contents"><span class="note_content_field">Review: </span><span class="note_content_value">This paper proposes to consider the mixed equilibrium objective function for GANS. The authors generalize the mirror descent/mirror prox to handle continuous games. The technical challenge is to write those algorithms in infinite dimensional spaces. This reviewer finds this however to be a mere technicality, and there seems to be no conceptual obstruction. In fact other paper have already written this, see for example ``Mirror Descent Learning in Continuous Games" by Zhou et al. at CDC 2017 (I'm sure there are other references too).
While the theory part is not particularly exciting, the paper could be saved by the experiments. However as far I can tell the authors are only able to reproduce the results obtained with more classical approaches.
</span></div><div class="note_contents"><span class="note_content_field">Rating: </span><span class="note_content_value">4: Ok but not good enough - rejection</span></div><div class="note_contents"><span class="note_content_field">Confidence: </span><span class="note_content_value">4: The reviewer is confident but not absolutely certain that the evaluation is correct</span></div><div class="reply_row clearfix"></div></div><div class="children"><div class="note_with_children"><div id="note_Skl0WT3Op7" class="note panel"><div class="title_pdf_row clearfix"><h2 class="note_content_title"><a>Thank you the your comments</a>  <button class="btn btn-xs btn-default permalink-button" title="Link to this comment" data-permalink-url="https://openreview.net/forum?id=ryMQ5sRqYX&amp;noteId=Skl0WT3Op7"><span class="glyphicon glyphicon-link" aria-hidden="true"></span></button></h2></div><div class="meta_row"><span class="signatures">ICLR 2019 Conference Paper521 Authors</span></div><div class="clearfix"><div class="meta_row pull-left"><span class="date item">14 Nov 2018</span><span class="item">ICLR 2019 Conference Paper521 Official Comment</span><span class="item">Readers: <span class="readers-icon glyphicon glyphicon-globe"></span> Everyone</span></div><div class="meta_row meta_actions"></div></div><div class="note_contents"><span class="note_content_field">Comment: </span><span class="note_content_value">We have addressed all of your concerns and we look forward to further correspondence with you.

“ The technical challenge is to write those algorithms in infinite dimensional spaces. This reviewer finds this however to be a mere technicality, and there seems to be no conceptual obstruction.”

Please see the ****Novelty and significance**** part of the general response.

The conceptual leap here is not the infinite dimensional algorithm; rather, it is the infinite dimensional re-formulation of the GAN problem via the Riesz representation. 

We do know the entropic mirror descent solves the problem in finite dimension, however it is surprising to see that the mirror-prox along with Langevin dynamics handle the GAN problem in a simple fashion, which this paper brings to the table. Our results are not some theory dressing to motivate a method that already works in practice. 

Our formulation, the algorithm (a fusion of entropic mirror descent and Langevin dynamics), and the practical results, to our knowledge, is a solid contribution to the literature. Without the infinite dimensional characterization, we felt like we would be criticized for lack of rigor. 

“ In fact other paper have already written this, see for example ``Mirror Descent Learning in Continuous Games’’ by Zhou et al. at CDC 2017 (I'm sure there are other references too).”

The paper, as explained in the general response, is not relevant to this work. Regarding algorithms for solving infinite-dimensional games, the reference (Balandat et al., 2016) is the most appropriate to our knowledge. We would appreciate if you can point out some references that are closer to our work than (Grnarova et al. 2018) or (Balandat et al., 2016), which we might have possibly missed.


“While the theory part is not particularly exciting, the paper could be saved by the experiments. However as far I can tell the authors are only able to reproduce the results obtained with more classical approaches.”

We now have a comparison to the most contemporary algorithm; please see the general response above and the revision.
</span></div><div class="reply_row clearfix"></div></div><div class="children"></div></div></div></div><div class="note_with_children"><div id="note_r1eNSuXihm" class="note panel"><div class="title_pdf_row clearfix"><h2 class="note_content_title"><a>interesting extension of mirror-prox with some important missing pieces</a>  <button class="btn btn-xs btn-default permalink-button" title="Link to this comment" data-permalink-url="https://openreview.net/forum?id=ryMQ5sRqYX&amp;noteId=r1eNSuXihm"><span class="glyphicon glyphicon-link" aria-hidden="true"></span></button></h2></div><div class="meta_row"><span class="signatures">ICLR 2019 Conference Paper521 AnonReviewer3</span></div><div class="clearfix"><div class="meta_row pull-left"><span class="date item">03 Nov 2018 (modified: 07 Nov 2018)</span><span class="item">ICLR 2019 Conference Paper521 Official Review</span><span class="item">Readers: <span class="readers-icon glyphicon glyphicon-globe"></span> Everyone</span></div><div class="meta_row meta_actions"></div></div><div class="note_contents"><span class="note_content_field">Review: </span><span class="note_content_value">This paper extends the mirror-descent and mirror-prox algorithms to infinite dimensional Banach spaces so that they can be applied to solve the mixed Nash equilibrium of the popular generative adversarial networks. The main technical results appear to be formal but straightforward extensions of existing techniques in finite dimensional spaces. A sample-based practical algorithm is proposed so that the infinite dimensional algorithms can still be computed. Experiments are a bit disappointing as the authors only used visual appeal as an evaluation criterion. (I understand why the authors chose to do so but as an algorithmic paper, resorting to an evaluation based on visual appeal is almost always unsatisfactory.)

Quality: The quality of this work is moderate. Quite strangely, the authors made a fundamental mistake at the very beginning: their definition of approximate mixed equilibrium  (page 2, Notation) is bizarre and different from those in previous work (such as Nemirovski's MP paper). Fortunately, this is perhaps only an oversight on the definition; the algorithms and theorems are for the correct definition anyways. Example: consider min_{-1&lt;= x &lt;= 1} max_{-1&lt;=y&lt;=1} xy. Should we call (x, 0) an (approximate) NE for any x??

Another major issue with this work is its relaxation into mixed NE. The "bilinarization" trick in Eq (5) goes back to Kantorovich (who perhaps deserves to be mentioned), and is a relaxation in general: we now have to use a mixture of generators. Since MD/MP is not sparse, in the end we must use a large number of mixtures of generators. This certainly will create some computational issues, and make comparison to pure NE methods unfair.

Clarity: The writing of this work is mostly easily to follow. However, the presentation of the technical results suffers from a real dilemma: On one hand, the authors completely ignored the technical difference between infinite dimensional Banach spaces and finite dimensional spaces. In fact, the authors never even formally defined the underlying Banach spaces. Another example, is the mapping G on page 3 continuous? wrt what topology? without such discussion what do you mean by Frechet derivative on page 4? when is the entropy function well-defined? when is the integral of exponential well-defined? Part of me totally understand that these technicalities are daunting and perhaps should not appear in the main text. On the other hand, aren't these technicalities the only "interesting and nontrivial" part of the extension to infinite dimensional spaces? If we do not care about such technicalities and can safely "assume they can be taken care of," then why is this work nontrivial? I do not see a way to resolve this dilemma here but suggest the authors consider maybe a different venue for such type of results.

Originality: The novelty of this work is limited. The extension of MD/MP to infinite dimensional spaces is mostly formal but straightforward. In fact I believe previous authors such as Nemirovski deliberately restrict to finite dimensional spaces not because of technical incapability but to avoid uninspiring technicalities. Some very related previous works were not mentioned at all:
-- Mirror Descent Learning in Continuous Games
-- Convex Games in Banach Spaces
-- On the Universality of Online Mirror Descent

The sample based algorithms are more interesting because they make the infinite dimensional extensions implementable. However, one can not say much about their convergence behavior at the moment.

Significance: The main results, although not difficult to obtain, can potentially be very useful in broadening our arsenal of tools for training GANs. The claim "resolving the longstanding problem that no provably convergent algorithm exists for general GAN" in the Abstract is disturbing, because the authors changed the definition of GAN and because the technical contributions of this work do not live up to that strong claim. 
</span></div><div class="note_contents"><span class="note_content_field">Rating: </span><span class="note_content_value">5: Marginally below acceptance threshold</span></div><div class="note_contents"><span class="note_content_field">Confidence: </span><span class="note_content_value">5: The reviewer is absolutely certain that the evaluation is correct and very familiar with the relevant literature</span></div><div class="reply_row clearfix"></div></div><div class="children"><div class="note_with_children"><div id="note_Ske0Vphua7" class="note panel"><div class="title_pdf_row clearfix"><h2 class="note_content_title"><a>We thank you for the knowledgeable review</a>  <button class="btn btn-xs btn-default permalink-button" title="Link to this comment" data-permalink-url="https://openreview.net/forum?id=ryMQ5sRqYX&amp;noteId=Ske0Vphua7"><span class="glyphicon glyphicon-link" aria-hidden="true"></span></button></h2></div><div class="meta_row"><span class="signatures">ICLR 2019 Conference Paper521 Authors</span></div><div class="clearfix"><div class="meta_row pull-left"><span class="date item">14 Nov 2018</span><span class="item">ICLR 2019 Conference Paper521 Official Comment</span><span class="item">Readers: <span class="readers-icon glyphicon glyphicon-globe"></span> Everyone</span></div><div class="meta_row meta_actions"></div></div><div class="note_contents"><span class="note_content_field">Comment: </span><span class="note_content_value">Comments that were already addressed in the general response are omitted. We are more than happy to engage in any further discussion regarding below.

“... the authors made a fundamental mistake …”

Thank you for pointing this out; we have fixed it.

“The "bilinarization" trick in Eq (5) goes back to Kantorovich…”

You might be referring to the Kantorovich’s Duality (KD) for relaxed Monge’s problem. It seems that Kantorovich himself never used any such trick when he derived KD in the classic “On the translocation of masses 1942”. Instead, he directly started with the relaxed problem and used a limit argument. Another potentially related paper is “On a problem of Monge 1948”, which we can only find the Russian version and therefore are unable to check. As a result, we would appreciate if the reviewer can further clarify upon this comment.

In any case, we agree with the reviewer that it is worth mentioning KD; see the new first paragraph in Section 2.2.

“Since MD/MP is not sparse, in the end we must use a large number of mixtures of generators.” 

This is precisely why we resort to the averaging scheme; see Section 4.2.


“This certainly will create some computational issues, and make comparison to pure NE methods unfair.”

With our averaging scheme, there are no computational issues.

Our comparison to pure NE methods is entirely fair since we are comparing the final output images under similar computational resources, which is ultimately what people care about for GANs. 


“Clarity: The writing of this work …”

This paragraph warrants a sentence-by-sentence reply; we have addressed all your concerns in the revision.

“”On one hand, the authors completely ignored the technical...”” 

This comment is perhaps unfair, as we have spent a full section in Appendix A on technical details of inf-dim MD. Even though the presentation in the main text is heuristic, we did refer the readers to Appendix A for precise statements. We are sure that the purpose is well-understood by the reviewer.

“In fact, the authors never even formally defined the underlying Banach spaces.”

We purposely avoided the term “Banach space” for general audience, but an expert should find no difficulty in checking the details of Appendix A. It is easier to start with Banach spaces, but would make the paper less accessible. We are willing to change the presentation if the reviewer feels that the term is necessary.

“”is the mapping G on page 3 continuous ... when is the integral of exponential well-defined?””

Thanks for pointing out these missing pieces. We have incorporated them all with two new paragraphs in Appendix A.

“”Part of me totally understand ... If we do not care about such technicalities and can safely "assume they can be taken care of," then why is this work nontrivial?””

As explained in the general response, we have never claimed that the inf-dim MD is the main contribution. We also handled all the technical details in Appendix A which, thanks to your review, have become more complete in the revision (we also fixed an issue in the definition of our function class).


“Originality: The novelty of this work is limited... In fact I believe previous authors such as Nemirovski deliberately restrict to finite dimensional spaces ... ”

Let us reiterate that the technicality is not the major goal of our paper, and we knew that inf-dim MD is folklore among experts; see general response.

“The sample based algorithms are more interesting ... However, one can not say much about their convergence behavior at the moment.”

Please see the general response above; our framework provides, to our knowledge, the strongest theoretical claim for training GANs. For the efficient algorithms with averaging, we did admit that it is heuristic, but it works well in practice and is derived on the guidance provided by the theoretical foundations.

“The claim ... is disturbing, because the authors changed the definition of GAN ...”

We agree that the sentence can be misleading and we have removed it. But we argue that the GAN framework is not married to the classical pure strategy formulation so it is perfectly fine to change the definition of GAN.
</span></div><div class="reply_row clearfix"></div></div><div class="children"></div></div></div></div><div class="note_with_children"><div id="note_SygwzvX9nm" class="note panel"><div class="title_pdf_row clearfix"><h2 class="note_content_title"><a>An interesting mixed strategy perspective to train GANs</a>  <button class="btn btn-xs btn-default permalink-button" title="Link to this comment" data-permalink-url="https://openreview.net/forum?id=ryMQ5sRqYX&amp;noteId=SygwzvX9nm"><span class="glyphicon glyphicon-link" aria-hidden="true"></span></button></h2></div><div class="meta_row"><span class="signatures">ICLR 2019 Conference Paper521 AnonReviewer1</span></div><div class="clearfix"><div class="meta_row pull-left"><span class="date item">03 Nov 2018 (modified: 07 Nov 2018)</span><span class="item">ICLR 2019 Conference Paper521 Official Review</span><span class="item">Readers: <span class="readers-icon glyphicon glyphicon-globe"></span> Everyone</span></div><div class="meta_row meta_actions"></div></div><div class="note_contents"><span class="note_content_field">Review: </span><span class="note_content_value">This paper uses a mixed strategy perspective for GANs. With this formulation the non-convex game formulation of GANs can be transformed into a infinite dimensional problem analog to a finite dimensional bilinear problem.  

I really like this approach, that tries to find methods that converge globally to (mixed) Nash equilibriums. However I have some concerns. 

- I'm concerned about the definition of a $O(T^{-1})-NE$. Actually, this merit function is not standard for game. It can be 0 even if $x_t,y_t$ is far from the equilibrium (for instance for the problem $\min_{x \in \Delta_d}\max_{y \in \Delta_d} x^\top y$ with $x_t = (1,0,\ldots,0)$ and $y_t= (F(x_{NE},y_{NE}),1-F(x_{NE},y_{NE}),0,\ldots,0)$ we have $F(x_t,y_t) = F(x_{NE},y_{NE})$ but $x_{NE} = y_{NE} =(1/d,\ldots,1/d)$). One merit function that could be considered is $\max_{y} F(x,y_t) - \min_{y} F(x_t,y)$. 

- There is a gap between the theory and the practical method that could be bridged. Actually Theorem 2 assume that the stochastic derivatives are unbiased but since your Langevin dynamics gives you an *approximate* of the next distribution an analysis taking into account this bias would provide much stronger results. More precisely, it would be interesting to have a result similar as Theorem 2 with conditions on $\epsilon_t$ and $K_t$. For instance, if the theoretical $K_t$ is too large it would reduce the interest of your algorithm. I think this analysis is key since it allows to claim that you can properly approximate the distributions of interest.

If you are able to ease these concerns I'm eager to increase my grade.


- "(5) is exactly the infinite-dimensional analogue of (1):" Actually it is not exactly the analogue since $&lt;.,.&gt;$ is not a scalar product anymore (particularly, $&lt;g,\mu&gt;$ is not defined) but the canonical pairing between a space and its dual (we are loosing something going to infinite dimension).
I think it should be clarified somewhere. 

Minor comments: 
- on the updates rules of $\theta$ and $\omega$ (Page 6) the Gaussian noises are missing. 
- On algorithm 3,4,5 and 6 the Gaussian noise is too wide and causes an Overfull.
</span></div><div class="note_contents"><span class="note_content_field">Rating: </span><span class="note_content_value">5: Marginally below acceptance threshold</span></div><div class="note_contents"><span class="note_content_field">Confidence: </span><span class="note_content_value">4: The reviewer is confident but not absolutely certain that the evaluation is correct</span></div><div class="reply_row clearfix"></div></div><div class="children"><div class="note_with_children"><div id="note_r1lqOanOaQ" class="note panel"><div class="title_pdf_row clearfix"><h2 class="note_content_title"><a>We thank you for the insightful comments, in particular for suggesting the bias analysis</a>  <button class="btn btn-xs btn-default permalink-button" title="Link to this comment" data-permalink-url="https://openreview.net/forum?id=ryMQ5sRqYX&amp;noteId=r1lqOanOaQ"><span class="glyphicon glyphicon-link" aria-hidden="true"></span></button></h2></div><div class="meta_row"><span class="signatures">ICLR 2019 Conference Paper521 Authors</span></div><div class="clearfix"><div class="meta_row pull-left"><span class="date item">14 Nov 2018</span><span class="item">ICLR 2019 Conference Paper521 Official Comment</span><span class="item">Readers: <span class="readers-icon glyphicon glyphicon-globe"></span> Everyone</span></div><div class="meta_row meta_actions"></div></div><div class="note_contents"><span class="note_content_field">Comment: </span><span class="note_content_value">Minor comments are incorporated in the revision.

“ I'm concerned about the definition of a $O(T^{-1})-NE$...”

Thank you for noting this; we have fixed the definition. As Reviewer3 mentioned, This was an oversight of the definition, and our convergence guarantees are for the correct definition.


“...an analysis taking into account this bias would provide much stronger results.”

Thank you for the very illuminating idea; we now have bounds that explicitly take the bias into account in Theorem 2.

However, we note that quantifying conditions on eps_t and K_t is equivalent to proving non-asymptotic bounds for sampling from distributions over neural nets, a task that is more difficult than proving convergence to global optima, and hence is well beyond the scope of our work. However, under fairly mild conditions, it is known that at least asymptotically, SGLD converges at rate t^{-1/3}; see [*].

“... Actually it is not exactly the analogue since $&lt;.,.&gt;$ is not a scalar product…

Thanks for pointing this out. We have added a footnote to clarify this point.

[*] Consistency and fluctuations for stochastic gradient Langevin dynamics by Teh et al., JMLR 2016. 
</span></div><div class="reply_row clearfix"></div></div><div class="children"></div></div></div></div></div></main></div>
  </div>
</div>


    <!-- Footer -->
    <footer class="sitemap">
      <div class="container">
        <div class="row hidden-xs">
          <div class="col-sm-4">
            <ul class="list-unstyled">
              <li><a href="/" class="home">Home</a></li>
              <li><a href="/venues">All Venues</a></li>
            </ul>
          </div>
    
          <div class="col-sm-4">
            <ul class="list-unstyled">
              <li><a href="/about">About OpenReview</a></li>
              <li><a href="/contact">Contact</a></li>
              <li><a href="#" data-toggle="modal" data-target="#feedback-modal">Feedback</a></li>
            </ul>
          </div>
    
          <div class="col-sm-4">
            <ul class="list-unstyled">
              <li><a href="/terms">Terms of Service</a></li>
              <li><a href="/privacy">Privacy Policy</a></li>
            </ul>
          </div>
        </div>
    
        <div class="row visible-xs-block">
          <div class="col-xs-6">
            <ul class="list-unstyled">
              <li><a href="/" class="home">Home</a></li>
              <li><a href="/about">About OpenReview</a></li>
              <li><a href="#" data-toggle="modal" data-target="#feedback-modal">Feedback</a></li>
            </ul>
          </div>
    
          <div class="col-xs-6">
            <ul class="list-unstyled">
              <li><a href="/contact">Contact</a></li>
              <li><a href="/terms">Terms of Service</a></li>
              <li><a href="/privacy">Privacy Policy</a></li>
            </ul>
          </div>
        </div>
      </div>
    </footer>
    
    <footer class="sponsor">
      <div class="container">
        <div class="row">
          <div class="col-sm-10 col-sm-offset-1">
            <p class="text-center">
              OpenReview is created by the <a href="http://www.iesl.cs.umass.edu/" target="_blank">Information Extraction and Synthesis Laboratory</a>, College of Information and Computer Science, University of Massachusetts Amherst. We gratefully acknowledge the support of the OpenReview sponsors:  Google,  Facebook, NSF, the University of Massachusetts Amherst Center for Data Science, and Center for Intelligent Information Retrieval, as well as the Google Cloud Platform for donating the computing and networking services on which OpenReview.net runs.
            </p>
          </div>
        </div>
      </div>
    </footer>

  <div id="feedback-modal" class="modal fade" tabindex="-1" role="dialog">
    <div class="modal-dialog">
      <div class="modal-content">
        <div class="modal-header">
          <button type="button" class="close" data-dismiss="modal" aria-label="Close"><span aria-hidden="true">×</span></button>
          <h3 class="modal-title">Send Feedback</h3>
        </div>
        <div class="modal-body">
          <p>Enter your feedback below and we'll get back to you as soon as possible.</p>
          <form action="/feedback" method="POST">
            <div class="form-group">
              <input type="email" name="from" class="form-control" placeholder="Email" />
            </div>
            <div class="form-group">
              <input type="text" name="subject" class="form-control" placeholder="Subject" />
            </div>
            <div class="form-group">
              <textarea name="message" class="form-control feedback-input" rows="5" placeholder="Message" required=""></textarea>
            </div>
          <ul id="ui-id-2" tabindex="0" class="ui-menu ui-widget ui-widget-content" style="display: none;"></ul></form>
        </div>
        <div class="modal-footer">
          <button type="button" class="btn btn-default" data-dismiss="modal">Cancel</button>
          <button type="button" class="btn btn-primary">Send</button>
        </div>
      </div>
    </div>
  </div>

  <script type="text/javascript" async="" src="https://www.google-analytics.com/analytics.js"></script><script src="//cdnjs.cloudflare.com/ajax/libs/jquery/2.2.4/jquery.min.js"></script>
  <script>window.jQuery || document.write('&lt;script src="/static/js/vendor/jquery-2.2.4.min.js"&gt;&lt;\/script&gt;')</script>

  <script src="/static/dist/vendor.min.js"></script>
  <script src="/static/dist/templates.min.js"></script>
  <script src="/static/dist/openreview.min.js"></script>

    <script>window.legacyScripts = true;</script>


    <script async="" src="https://www.googletagmanager.com/gtag/js?id=UA-108703919-1"></script>
    <script>
      window.dataLayer = window.dataLayer || [];
      function gtag() { dataLayer.push(arguments); }
      gtag('js', new Date());
      gtag('config', 'UA-108703919-1');
    </script>


<div role="status" aria-live="assertive" aria-relevant="additions" class="ui-helper-hidden-accessible"></div></body></html>