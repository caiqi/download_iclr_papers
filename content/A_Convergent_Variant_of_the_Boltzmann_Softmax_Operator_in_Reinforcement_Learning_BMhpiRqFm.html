<!DOCTYPE html><html xmlns="http://www.w3.org/1999/xhtml" class="no-js"><head>
  <meta charset="utf-8" />
  <meta http-equiv="X-UA-Compatible" content="IE=edge,chrome=1" />
  <meta name="viewport" content="width=device-width, initial-scale=1" />

  <title>A Convergent Variant of the Boltzmann Softmax Operator in Reinforcement Learning | OpenReview</title>
  <meta name="description" content="" />

      <meta name="citation_title" content="A Convergent Variant of the Boltzmann Softmax Operator in Reinforcement Learning" />
        <meta name="citation_author" content="Anonymous" />
      <meta name="citation_publication_date" content="2018/09/27" />
      <meta name="citation_online_date" content="2018/09/27" />
      <meta name="citation_pdf_url" content="https://openreview.net/pdf?id=B1MhpiRqFm" />
      <meta name="twitter:card" content="summary" />
      <meta name="twitter:site" content="@openreviewnet" />
      <meta name="og:title" content="A Convergent Variant of the Boltzmann Softmax Operator in..." />
      <meta name="og:description" content="The Boltzmann softmax operator can trade-off well between exploration and exploitation according to current estimation in an exponential weighting scheme, which is a promising way to address the..." />
      <meta name="og:image" content="https://openreview.net/static/images/openreview_logo_512.png" />

  <link rel="stylesheet" href="https://fonts.googleapis.com/css?family=Noto+Sans:400,400i,700,700i" />

  <link rel="stylesheet" href="/static/css/bootstrap.min.css" />
  <link rel="stylesheet" href="/static/css/jquery-ui.min.css" />
  <link rel="stylesheet" href="/static/css/main.min.css" />
</head>

<body class="forum">
  <nav class="navbar navbar-inverse navbar-fixed-top" role="navigation">
    <div class="container">
  
      <div class="navbar-header">
        <button type="button" class="navbar-toggle collapsed" data-toggle="collapse" data-target="#navbar" aria-expanded="false" aria-controls="navbar">
          <span class="sr-only">Toggle navigation</span>
          <span class="icon-bar"></span>
          <span class="icon-bar"></span>
          <span class="icon-bar"></span>
        </button>
        <a class="navbar-brand home push-link" href="/"><strong>OpenReview</strong>.net</a>
      </div>
  
      <div id="navbar" class="navbar-collapse collapse">
        <form class="navbar-form navbar-left profile-search" role="search">
          <div class="form-group has-feedback">
            <input id="search_input" type="text" class="form-control ui-autocomplete-input" placeholder="Search ICLR 2019 Conference" autocomplete="off" />
            <span class="glyphicon glyphicon-search form-control-feedback" aria-hidden="true"></span>
          </div>
  
          <input id="search_group" type="hidden" value="ICLR.cc/2019/Conference" />
          <input id="search_content" type="hidden" value="all" />
        <ul id="ui-id-1" tabindex="0" class="ui-menu ui-widget ui-widget-content ui-autocomplete ui-front" style="display: none;"></ul></form>
  
        <ul class="nav navbar-nav navbar-right">
        
            <li id="user-menu"><a href="/login">Login</a></li>
        </ul>
      </div>
  
    </div>
  </nav>

  <div id="or-banner" class="banner" style="">
  <div class="container">
    <div class="row">
      <div class="col-xs-12"><a href="/group?id=ICLR.cc/2019/Conference" title="Venue Homepage"><img class="icon" src="/static/images/arrow_left.svg" /> Go to <strong>ICLR 2019 Conference</strong> homepage</a></div>
    </div>
  </div>
</div>
<div id="flash-message-container" class="alert alert-danger" role="alert" style="display: none;">
  <div class="container">
    <div class="row">
      <div class="col-xs-12">
        <div class="alert-content">
          <button type="button" class="close" aria-label="Close"><span aria-hidden="true">×</span></button>
        </div>
      </div>
    </div>
  </div>
</div>

<div class="container">
  <div class="row">
    <div class="col-xs-12">
      <main id="content" class="clearfix openbanner-visible legacy-styles"><div id="note_B1MhpiRqFm" class="note panel"><div class="title_pdf_row clearfix"><h2 class="note_content_title"><a>A Convergent Variant of the Boltzmann Softmax Operator in Reinforcement Learning</a> <a class="note_content_pdf" href="/pdf?id=B1MhpiRqFm" title="Download PDF" target="_blank"><img src="/static/images/pdf_icon_blue.svg" /></a> </h2></div><div class="meta_row"><span class="signatures">Anonymous</span></div><div class="clearfix"><div class="meta_row pull-left"><span class="date item">28 Sep 2018 (modified: 18 Nov 2018)</span><span class="item">ICLR 2019 Conference Blind Submission</span><span class="item">Readers: <span class="readers-icon glyphicon glyphicon-globe"></span> Everyone</span><span class="item"><a class="action-bibtex-modal" data-bibtex="@inproceedings{    &#10;anonymous2019boltzmann,    &#10;title={Boltzmann Weighting Done Right in Reinforcement Learning},    &#10;author={Anonymous},    &#10;booktitle={Submitted to International Conference on Learning Representations},    &#10;year={2019},    &#10;url={https://openreview.net/forum?id=B1MhpiRqFm},    &#10;note={under review}    &#10;}">Show Bibtex</a></span><a class="note_content_pdf item" href="/revisions?id=B1MhpiRqFm" target="_blank">Show Revisions</a></div><div class="meta_row meta_actions"></div></div><div class="note_contents"><span class="note_content_field">Abstract: </span><span class="note_content_value">The Boltzmann softmax operator can trade-off well between exploration and exploitation according to current estimation in an exponential weighting scheme, which is a promising way to address the exploration-exploitation dilemma in reinforcement learning. Unfortunately, the Boltzmann softmax operator is not a non-expansion, which may lead to unstable or even divergent learning behavior when used in estimating the value function. The non-expansion is a vital and widely-used sufficient condition to guarantee the convergence of value iteration. However, how to characterize the effect of such non-expansive operators in value iteration remains an open problem. In this paper, we propose a new technique to analyze the error bound of value iteration with the the Boltzmann softmax operator. We then propose the dynamic Boltzmann softmax(DBS) operator to enable the convergence to the optimal value function in value iteration.  We also present convergence rate analysis of the algorithm.
Using Q-learning as an application, we show that the DBS operator can be applied in a model-free reinforcement learning algorithm. Finally, we demonstrate the effectiveness of the DBS operator in a toy problem called GridWorld and a suite of Atari games. Experimental results show that outperforms DQN substantially in benchmark games.</span></div><div class="note_contents"><span class="note_content_field">Keywords: </span><span class="note_content_value">Reinforcement Learning, Boltzmann Softmax Operator, Value Function Estimation</span></div><div class="reply_row clearfix"><div class="item" id="reply_count">7 Replies</div></div></div><hr class="small" /><div id="note_children"><div class="note_with_children"><div id="note_rkx-AhYRaX" class="note panel"><div class="title_pdf_row clearfix"><h2 class="note_content_title"><a>Summary of the updated version</a>  <button class="btn btn-xs btn-default permalink-button" title="Link to this comment" data-permalink-url="https://openreview.net/forum?id=B1MhpiRqFm&amp;noteId=rkx-AhYRaX"><span class="glyphicon glyphicon-link" aria-hidden="true"></span></button></h2></div><div class="meta_row"><span class="signatures">ICLR 2019 Conference Paper839 Authors</span></div><div class="clearfix"><div class="meta_row pull-left"><span class="date item">18 Nov 2018</span><span class="item">ICLR 2019 Conference Paper839 Official Comment</span><span class="item">Readers: <span class="readers-icon glyphicon glyphicon-globe"></span> Everyone</span></div><div class="meta_row meta_actions"></div></div><div class="note_contents"><span class="note_content_field">Comment: </span><span class="note_content_value">We thank the reviewers for their careful reading and thoughtful reviews. We have updated the submission accordingly and the main changes in the updated version of the paper include:

+ we elaborate more about the exploration-exploitation dilemma in value function optimization
+ we add empirical analysis of the exploration-exploitation dilemma
+ we compare with G-learning in the GridWorld
+ we discuss more related papers
+ we refine experimental results of Atari
+ we elaborate the details for the proof of Theorem 1</span></div><div class="reply_row clearfix"></div></div><div class="children"></div></div><div class="note_with_children"><div id="note_Hklc614o37" class="note panel"><div class="title_pdf_row clearfix"><h2 class="note_content_title"><a>Okay paper but relatively thin novelty</a>  <button class="btn btn-xs btn-default permalink-button" title="Link to this comment" data-permalink-url="https://openreview.net/forum?id=B1MhpiRqFm&amp;noteId=Hklc614o37"><span class="glyphicon glyphicon-link" aria-hidden="true"></span></button></h2></div><div class="meta_row"><span class="signatures">ICLR 2019 Conference Paper839 AnonReviewer3</span></div><div class="clearfix"><div class="meta_row pull-left"><span class="date item">03 Nov 2018 (modified: 07 Nov 2018)</span><span class="item">ICLR 2019 Conference Paper839 Official Review</span><span class="item">Readers: <span class="readers-icon glyphicon glyphicon-globe"></span> Everyone</span></div><div class="meta_row meta_actions"></div></div><div class="note_contents"><span class="note_content_field">Review: </span><span class="note_content_value">Summary: This work demonstrates that, although the Boltzmann softmax operator is not a non-expansion, a proposed dynamic Boltzmann operator (DBS) can be used in conjunction with value iteration and Q-learning to achieve convergence to V* and Q*, respectively. This time-varying operator replaces the traditional max operator. The authors show empirical performance gains of DBS+Q-learning over Q-learning in a gridworld and DBS+DQN over DQN on Atari games.

Novelty: (1) The error bound of value iteration with the Boltzmann softmax operator and convergence &amp; convergence rate results in this setting seem novel. (2) The novelty of the dynamic Boltzmann operator is somewhat thin, as (Singh et al. 2000) show that a dynamic weighting of the Boltzmann operator achieves convergence to the optimal value function in SARSA(0). In that work, the weighting is state-dependent, so the main algorithmic novelty in this paper is removing the dependence on state visitation for the beta parameter by making it solely dependent on time. A question for the authors: How does the proof in this work relate to / differ from the convergence proofs in (Singh et al. 2000)?

Clarity: In the DBS Q-learning algorithm, it is unclear under which policy actions are selected, e.g. using epsilon-greedy/epsilon-Boltzmann versus using the Boltzmann distribution applied to the Q(s, a) values. If the Boltzmann distribution is used then the algorithm that is presented is in fact expected SARSA and not Q-learning. The paper would benefit from making this clear.

Soundness: (1) The proof of Theorem 4 implicitly assumes that all states are visited infinitely often, which is not necessarily true with the given algorithm (if the policy used to select actions is the Boltzmann policy). (2) The proof of Theorem 1 uses the fact that |L(Q) - max(Q)| &lt;= log(|A|) / beta, which is not immediately clear from the result cited in McKay (2003). (3) The paper claims in the introduction that “the non-expansive property is vital to guarantee … the convergence of the learning algorithm.” This is not necessarily the case -- see Bellemare et al., Increasing the Action Gap: New Operators for Reinforcement Learning, 2016. 

Quality: (1) I appreciate that the authors evaluated their method on the suite of 49 Atari games. This said, the increase in median performance is relatively small, the delta being about half that of the increase due to double DQN. The improvement in mean score in great part stems from a large improvement occurs on Atlantis.

There are also a number of experimental details that are missing. Is the only change from DQN the change in update rule, while keeping the epsilon-greedy rule? In this case, I find a disconnect between the stated goal (to trade off exploration and exploitation) and the results. Why would we expect the Boltzmann softmax to work better when combined to epsilon-greedy? If not, can you give more details e.g. how beta was annealed over time, etc.?

Finally, can you briefly compare your algorithm to the temperature scheduling method described in Fox et al., Taming the Noise in Reinforcement Learning via Soft Updates, 2016?

Additional Comments:
(1) It would be helpful to have Atari results provided in raw game scores in addition to the human-normalized scores (Figure 5). (2) The human normalized scores listed in Figure 5 for DQN are different than the ones listed in the Double DQN paper (Van Hasselt et al, 2016). (3) For the DBS-DQN algorithm, the authors set beta_t = ct^2 - how is the value of c determined? (4) Text in legends and axes of Figure 1 and Figure 2 plots is very small. (5) Typo: citation for MacKay - Information Theory, Inference and Learning Algorithms - author name listed twice.

Similarly, if the main contribution is DBS, it would be interesting to have a more in-depth empirical analysis of the method -- how does performance (in Atari or otherwise) vary with the temperature schedule, how exploration is affected, etc.?</span></div><div class="note_contents"><span class="note_content_field">Rating: </span><span class="note_content_value">4: Ok but not good enough - rejection</span></div><div class="note_contents"><span class="note_content_field">Confidence: </span><span class="note_content_value">5: The reviewer is absolutely certain that the evaluation is correct and very familiar with the relevant literature</span></div><div class="reply_row clearfix"></div></div><div class="children"><div class="note_with_children"><div id="note_r1xsbcFA6Q" class="note panel"><div class="title_pdf_row clearfix"><h2 class="note_content_title"><a>To Reviewer3</a>  <button class="btn btn-xs btn-default permalink-button" title="Link to this comment" data-permalink-url="https://openreview.net/forum?id=B1MhpiRqFm&amp;noteId=r1xsbcFA6Q"><span class="glyphicon glyphicon-link" aria-hidden="true"></span></button></h2></div><div class="meta_row"><span class="signatures">ICLR 2019 Conference Paper839 Authors</span></div><div class="clearfix"><div class="meta_row pull-left"><span class="date item">18 Nov 2018</span><span class="item">ICLR 2019 Conference Paper839 Official Comment</span><span class="item">Readers: <span class="readers-icon glyphicon glyphicon-globe"></span> Everyone</span></div><div class="meta_row meta_actions"></div></div><div class="note_contents"><span class="note_content_field">Comment: </span><span class="note_content_value">Thank you for the comments. Please find our responses as below, especially for the novelty of the work.

Q1: The novelty of the DBS operator.
A1: First of all, thank you for viewing our analysis for DBS novel. As we mentioned in the paper and showed by the corresponding title, we mainly aim to enable the convergence of the widely-used Boltzmann operator by a better exploration-exploitation trade-off, which is dispensable for reinforcement learning. As far as we know, it is the first time that we find a variant of the Boltzmann operator with good convergence rate.

Although the state-dependent weighting of Boltzmann operator is proposed in (Singh et al. 2000), our DBS operator is state-independent and can scale to high-dimensional state space, which is crucial for RL algorithms. Furthermore, their operator is for on-policy RL algorithm, i.e. SARSA, while our DBS is for value iteration (a basic algorithm to solve the MDP) and Q-learning (a more popular off-policy RL algorithm). Therefore, our Q-learning algorithm with DBS is novel.

Due to the difference of our algorithms and that in (Singh et al. 2000), we develop new techniques to prove the convergence. Specifically, for value iteration, we propose a novel analysis to characterize the error bound of value iteration with the Boltzmann operator, prove the convergence and present convergence rate analysis; for Q-learning, we leverage the stochastic approximation lemma (SA Lemma) presented in (Singh et al. 2000), which is an extension of the classic stochastic approximation theorem proven in (Jaakkola et al. 1994), to relate the process to the well-defined stochastic process in SA Lemma and then we quantify the additional term using similar techniques in our Theorem 1. Our results of value iteration have little relation with (Singh et al. 2000) and are mainly based on our own analysis (Proposition 1, Theorem 1, Theorem 2, and Theorem).

Q2: What is the action selection policy? The states should be visited infinitely.
A2: In our DBS Q-learning algorithm, the action selection policy is epsilon-greedy. Thus, states will be visited infinitely often. We make it clearer in the updated version.

Please note that, the exploration-exploitation dilemma here is related to value function optimization (Asadi et al. 2017), rather than the traditional view of exploring the environment and exploiting the action during the action selection process. In stochastic environments, the max operator updates the value estimator in a ‘hard’ way by greedily summarizing action-value functions according to current estimation. However, this may not be accurate due to noise in the environment. Even in deterministic environments, this may not be accurate either. This is because the estimate for the value is not correct in the early stage of the learning process. We elaborate the effect of exploration and added empirical study in the updated version, please refer to Section 5.1.

Q3: |L(Q) - max(Q)| &lt;= log(A||) / beta is not immediately clear.
A3: We give more details of the proof in the updated version, please refer to Appendix B.

Q4: Non-expansion is not necessary for convergence.
A4: Yes, non-expansion is an important and widely-used sufficient condition to guarantee the convergence of the learning problem (Littman 1996, Asadi et al. 2017). In this understanding, we say non-expansion is ‘vital’ for convergence. (Bellemare et al. 2016) proposed an alternative sufficient condition different from the non-expansion property. However, the condition is still not enough to cover common operators violating non-expansion such as the Boltzmann softmax operator. 

Q5: Detailed comments for the experiment. 
A5: Here are our quick feedbacks.
1) We compare with G-learning and analyze the effect in the updated version (Section 5.1).
2) We change the score to raw game scores in the updated version (Appendix H). 
3) Please note that our score listed is exactly the same with ‘Dueling Network Architectures for Deep Reinforcement Learning’ and ‘Rainbow’, where the (original) scores for DQN are raw scores. 
4) In our experiments, c is in [0, 1], and we have tuned the value of c in some of the games. This is because different games have different features and should have different values of c.
5) We have redrawn the plots to make it more reader-friendly and corrected some typos in the updated version.</span></div><div class="reply_row clearfix"></div></div><div class="children"></div></div></div></div><div class="note_with_children"><div id="note_SJlRd9sYnQ" class="note panel"><div class="title_pdf_row clearfix"><h2 class="note_content_title"><a>I don't think the theoretical results represent a significant advance</a>  <button class="btn btn-xs btn-default permalink-button" title="Link to this comment" data-permalink-url="https://openreview.net/forum?id=B1MhpiRqFm&amp;noteId=SJlRd9sYnQ"><span class="glyphicon glyphicon-link" aria-hidden="true"></span></button></h2></div><div class="meta_row"><span class="signatures">ICLR 2019 Conference Paper839 AnonReviewer1</span></div><div class="clearfix"><div class="meta_row pull-left"><span class="date item">02 Nov 2018 (modified: 07 Nov 2018)</span><span class="item">ICLR 2019 Conference Paper839 Official Review</span><span class="item">Readers: <span class="readers-icon glyphicon glyphicon-globe"></span> Everyone</span></div><div class="meta_row meta_actions"></div></div><div class="note_contents"><span class="note_content_field">Review: </span><span class="note_content_value">The writing and organization of the paper are clear.  Theorem 1 seems fine but is straightforward to anyone who has studied this topic and knows the literature.  Corollary one may be technically wrong (or at least it doesn't follow from the theorem), though this can be fixed by replacing the lim with a limsup.  Theorem 4 seems to be the main result all the work is leading up to, but I think this is wrong.  Stronger conditions are required on the sequence \beta_t, along the lines discussed in the paragraph on Boltzmann exploration in Section 2.2 of Singh et al 2000.  The proof provided by the authors relies on a "Lemma 2" which I can't find in the paper.  The computational results are potentially interesting but call for further scrutiny.  Given the issues with the theoretical results, I think its hard to justify accepting the paper.</span></div><div class="note_contents"><span class="note_content_field">Rating: </span><span class="note_content_value">4: Ok but not good enough - rejection</span></div><div class="note_contents"><span class="note_content_field">Confidence: </span><span class="note_content_value">4: The reviewer is confident but not absolutely certain that the evaluation is correct</span></div><div class="reply_row clearfix"></div></div><div class="children"><div class="note_with_children"><div id="note_Skx9B9YRa7" class="note panel"><div class="title_pdf_row clearfix"><h2 class="note_content_title"><a>Clarification to Reviewer1</a>  <button class="btn btn-xs btn-default permalink-button" title="Link to this comment" data-permalink-url="https://openreview.net/forum?id=B1MhpiRqFm&amp;noteId=Skx9B9YRa7"><span class="glyphicon glyphicon-link" aria-hidden="true"></span></button></h2></div><div class="meta_row"><span class="signatures">ICLR 2019 Conference Paper839 Authors</span></div><div class="clearfix"><div class="meta_row pull-left"><span class="date item">18 Nov 2018</span><span class="item">ICLR 2019 Conference Paper839 Official Comment</span><span class="item">Readers: <span class="readers-icon glyphicon glyphicon-globe"></span> Everyone</span></div><div class="meta_row meta_actions"></div></div><div class="note_contents"><span class="note_content_field">Comment: </span><span class="note_content_value">Thank you for the comments. We are afraid that you have some misunderstandings for our work.

Q1: Theorem 1 is straightforward.
A1: The effect of operators which are not non-expansion when applied in value iteration is an open problem and worth studying (Algorithms for Sequential Decision Making, Littman, 1996). Although error bounds of value iteration with the traditional max operator is well-established, there’s no results for the Boltzmann softmax operator which violates the property of non-expansion. 

In Theorem 1, we propose a novel analysis to characterize the error bound of the Boltzmann operator when applied in value iteration. Please note that this is the first time that the analysis is presented, and it is of vital importance as value iteration is the basis for RL algorithms.
	
Q2: Corollary 1 may be technically wrong.
A2: Please note that ||·||_{\infty} denotes the L-\infty norm, and ||V_0 - V^*||_{\infty}, \log{|A|}, \beta, and \gamma are all constants which will not change by taking the limit of t. Corollary 1 is derived by taking the limit of t in both sides of Inequality (6) in Theorem 1. 

Q3: Theorem 4 may be wrong. Stronger conditions are required.
A3: Theorem 4 is correct. In our DBS Q-learning algorithm, the action selection policy is epsilon-greedy. Thus, states will be visited infinitely often. In addition, different from (Singh et al., 2000), where they study on-policy reinforcement learning algorithm (Sarsa), \beta is state-independent here and thus is more flexible. Please also note that the main result of the paper is the characterization of the (dynamic) Boltzmann softmax operator in value iteration (Theorem 1, Theorem 2, and Theorem 3). We then apply the DBS operator in a well-known off-policy reinforcement learning algorithm, i.e Q-learning, and Theorem 4 is to guarantee the convergence of the resulting DBS Q-learning algorithm. 

Q4: Cannot find Lemma 2.
A4: Lemma 2 refers to the stochastic approximation lemma (Lemma 1) in Section 3.1 of (Singh et al., 2000).</span></div><div class="reply_row clearfix"></div></div><div class="children"></div></div></div></div><div class="note_with_children"><div id="note_Syer4w_d2m" class="note panel"><div class="title_pdf_row clearfix"><h2 class="note_content_title"><a>Boltzmann Weighting Done Right in Reinforcement Learning</a>  <button class="btn btn-xs btn-default permalink-button" title="Link to this comment" data-permalink-url="https://openreview.net/forum?id=B1MhpiRqFm&amp;noteId=Syer4w_d2m"><span class="glyphicon glyphicon-link" aria-hidden="true"></span></button></h2></div><div class="meta_row"><span class="signatures">ICLR 2019 Conference Paper839 AnonReviewer2</span></div><div class="clearfix"><div class="meta_row pull-left"><span class="date item">01 Nov 2018 (modified: 07 Nov 2018)</span><span class="item">ICLR 2019 Conference Paper839 Official Review</span><span class="item">Readers: <span class="readers-icon glyphicon glyphicon-globe"></span> Everyone</span></div><div class="meta_row meta_actions"></div></div><div class="note_contents"><span class="note_content_field">Review: </span><span class="note_content_value">I liked this paper overall, though I feel that the way it is pitched to the reader is misguided. The looseness with which this paper uses 'exploration-exploitation tradeoff' is worrying. This paper does not attack that tradeoff at all really, since the tradeoff in RL concerns exploitation of understood knowledge vs deep-directed exploration, rather than just annealing between the max action and the mean over all actions (which does not incorporate any notion of uncertainty). Though I do recognize that the field overall is loose in this respect,  I do think this paper needs to rewrite its claims significantly. In fact it can be shown that Boltzmann exploration that incorporates a particular annealing schedule (but no notion of uncertainty) can be forced to suffer essentially linear regret even in the simple bandit case (O(T^(1-eps)) for any eps &gt; 0) which of course means that it doesn't explore efficiently at all (see Singh 2000, Cesa-Bianchi 2017). Theorem 4 does not imply efficient exploration, since it requires very strong conditions on the alphas, and note that the same proof applies to vanilla Q-learning, which we know does not explore well.

I presume the title of this paper is a homage to the recent 'Boltzmann Exploration Done Right' paper, however, though the paper is cited, it is not discussed at all. That paper proved a strong regret bound for Boltzmann-like exploration in the bandit case, which this paper actually does not for the RL case, so in some sense the homage is misplaced. Another recent paper that actually does prove a regret bound for a Boltzmann policy for RL is 'Variational Bayesian Reinforcement Learning with Regret Bounds', which also anneals the temperature, this should be mentioned.

All this is not to say that the paper is without merit, just that the main claims about exploration are not valid and consequently it needs to be repositioned. If the authors do that then I can revise my review.

Algorithm 2 has two typos related to s' and a'.</span></div><div class="note_contents"><span class="note_content_field">Rating: </span><span class="note_content_value">5: Marginally below acceptance threshold</span></div><div class="note_contents"><span class="note_content_field">Confidence: </span><span class="note_content_value">4: The reviewer is confident but not absolutely certain that the evaluation is correct</span></div><div class="reply_row clearfix"></div></div><div class="children"><div class="note_with_children"><div id="note_BylNDqY0T7" class="note panel"><div class="title_pdf_row clearfix"><h2 class="note_content_title"><a>To Reviewer2</a>  <button class="btn btn-xs btn-default permalink-button" title="Link to this comment" data-permalink-url="https://openreview.net/forum?id=B1MhpiRqFm&amp;noteId=BylNDqY0T7"><span class="glyphicon glyphicon-link" aria-hidden="true"></span></button></h2></div><div class="meta_row"><span class="signatures">ICLR 2019 Conference Paper839 Authors</span></div><div class="clearfix"><div class="meta_row pull-left"><span class="date item">18 Nov 2018</span><span class="item">ICLR 2019 Conference Paper839 Official Comment</span><span class="item">Readers: <span class="readers-icon glyphicon glyphicon-globe"></span> Everyone</span></div><div class="meta_row meta_actions"></div></div><div class="note_contents"><span class="note_content_field">Comment: </span><span class="note_content_value">Thank you very much for the thoughtful reviews, especially for the exploration-exploitation trade-off.

In this paper, we aim to make the Boltzmann softmax operator converge from the view of trade-off between exploration and exploitation in value function optimization, instead of the traditional understanding in the action selection process. To be specific, in stochastic environments, the max operator updates the value estimator in a ‘hard’ way by greedily summarizing action-value functions according to current estimation. However, this may not be accurate due to noise in the environment. Even in deterministic environments, this may not be correct either. This is because the estimate for the value is not correct in the early phase of the learning process. We elaborate this and distinguish it from the exploration-exploitation trade-off in the updated version in Section 2.2 and Section 5.1.

Considering the title would be misleading, we change it accordingly.

Thank you for pointing out the reference paper. We cite and discuss the paper in the updated version in Section 6 (Related Work).</span></div><div class="reply_row clearfix"></div></div><div class="children"></div></div></div></div></div></main></div>
  </div>
</div>


    <!-- Footer -->
    <footer class="sitemap">
      <div class="container">
        <div class="row hidden-xs">
          <div class="col-sm-4">
            <ul class="list-unstyled">
              <li><a href="/" class="home">Home</a></li>
              <li><a href="/venues">All Venues</a></li>
            </ul>
          </div>
    
          <div class="col-sm-4">
            <ul class="list-unstyled">
              <li><a href="/about">About OpenReview</a></li>
              <li><a href="/contact">Contact</a></li>
              <li><a href="#" data-toggle="modal" data-target="#feedback-modal">Feedback</a></li>
            </ul>
          </div>
    
          <div class="col-sm-4">
            <ul class="list-unstyled">
              <li><a href="/terms">Terms of Service</a></li>
              <li><a href="/privacy">Privacy Policy</a></li>
            </ul>
          </div>
        </div>
    
        <div class="row visible-xs-block">
          <div class="col-xs-6">
            <ul class="list-unstyled">
              <li><a href="/" class="home">Home</a></li>
              <li><a href="/about">About OpenReview</a></li>
              <li><a href="#" data-toggle="modal" data-target="#feedback-modal">Feedback</a></li>
            </ul>
          </div>
    
          <div class="col-xs-6">
            <ul class="list-unstyled">
              <li><a href="/contact">Contact</a></li>
              <li><a href="/terms">Terms of Service</a></li>
              <li><a href="/privacy">Privacy Policy</a></li>
            </ul>
          </div>
        </div>
      </div>
    </footer>
    
    <footer class="sponsor">
      <div class="container">
        <div class="row">
          <div class="col-sm-10 col-sm-offset-1">
            <p class="text-center">
              OpenReview is created by the <a href="http://www.iesl.cs.umass.edu/" target="_blank">Information Extraction and Synthesis Laboratory</a>, College of Information and Computer Science, University of Massachusetts Amherst. We gratefully acknowledge the support of the OpenReview sponsors:  Google,  Facebook, NSF, the University of Massachusetts Amherst Center for Data Science, and Center for Intelligent Information Retrieval, as well as the Google Cloud Platform for donating the computing and networking services on which OpenReview.net runs.
            </p>
          </div>
        </div>
      </div>
    </footer>

  <div id="feedback-modal" class="modal fade" tabindex="-1" role="dialog">
    <div class="modal-dialog">
      <div class="modal-content">
        <div class="modal-header">
          <button type="button" class="close" data-dismiss="modal" aria-label="Close"><span aria-hidden="true">×</span></button>
          <h3 class="modal-title">Send Feedback</h3>
        </div>
        <div class="modal-body">
          <p>Enter your feedback below and we'll get back to you as soon as possible.</p>
          <form action="/feedback" method="POST">
            <div class="form-group">
              <input type="email" name="from" class="form-control" placeholder="Email" />
            </div>
            <div class="form-group">
              <input type="text" name="subject" class="form-control" placeholder="Subject" />
            </div>
            <div class="form-group">
              <textarea name="message" class="form-control feedback-input" rows="5" placeholder="Message" required=""></textarea>
            </div>
          <ul id="ui-id-2" tabindex="0" class="ui-menu ui-widget ui-widget-content" style="display: none;"></ul></form>
        </div>
        <div class="modal-footer">
          <button type="button" class="btn btn-default" data-dismiss="modal">Cancel</button>
          <button type="button" class="btn btn-primary">Send</button>
        </div>
      </div>
    </div>
  </div>

  <script type="text/javascript" async="" src="https://www.google-analytics.com/analytics.js"></script><script src="//cdnjs.cloudflare.com/ajax/libs/jquery/2.2.4/jquery.min.js"></script>
  <script>window.jQuery || document.write('&lt;script src="/static/js/vendor/jquery-2.2.4.min.js"&gt;&lt;\/script&gt;')</script>

  <script src="/static/dist/vendor.min.js"></script>
  <script src="/static/dist/templates.min.js"></script>
  <script src="/static/dist/openreview.min.js"></script>

    <script>window.legacyScripts = true;</script>


    <script async="" src="https://www.googletagmanager.com/gtag/js?id=UA-108703919-1"></script>
    <script>
      window.dataLayer = window.dataLayer || [];
      function gtag() { dataLayer.push(arguments); }
      gtag('js', new Date());
      gtag('config', 'UA-108703919-1');
    </script>


<div role="status" aria-live="assertive" aria-relevant="additions" class="ui-helper-hidden-accessible"></div></body></html>