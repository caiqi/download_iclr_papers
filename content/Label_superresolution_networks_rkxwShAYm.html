<!DOCTYPE html><html xmlns="http://www.w3.org/1999/xhtml" class="no-js"><head>
  <meta charset="utf-8" />
  <meta http-equiv="X-UA-Compatible" content="IE=edge,chrome=1" />
  <meta name="viewport" content="width=device-width, initial-scale=1" />

  <title>Label super-resolution networks | OpenReview</title>
  <meta name="description" content="" />

      <meta name="citation_title" content="Label super-resolution networks" />
        <meta name="citation_author" content="Anonymous" />
      <meta name="citation_publication_date" content="2018/09/27" />
      <meta name="citation_online_date" content="2018/09/27" />
      <meta name="citation_pdf_url" content="https://openreview.net/pdf?id=rkxwShA9Ym" />
      <meta name="twitter:card" content="summary" />
      <meta name="twitter:site" content="@openreviewnet" />
      <meta name="og:title" content="Label super-resolution networks" />
      <meta name="og:description" content="We present a deep learning-based method for super-resolving coarse (low-resolution) labels assigned to groups of image pixels into pixel-level (high-resolution) labels, given the joint distribution..." />
      <meta name="og:image" content="https://openreview.net/static/images/openreview_logo_512.png" />

  <link rel="stylesheet" href="https://fonts.googleapis.com/css?family=Noto+Sans:400,400i,700,700i" />

  <link rel="stylesheet" href="/static/css/bootstrap.min.css" />
  <link rel="stylesheet" href="/static/css/jquery-ui.min.css" />
  <link rel="stylesheet" href="/static/css/main.min.css" />
</head>

<body class="forum">
  <nav class="navbar navbar-inverse navbar-fixed-top" role="navigation">
    <div class="container">
  
      <div class="navbar-header">
        <button type="button" class="navbar-toggle collapsed" data-toggle="collapse" data-target="#navbar" aria-expanded="false" aria-controls="navbar">
          <span class="sr-only">Toggle navigation</span>
          <span class="icon-bar"></span>
          <span class="icon-bar"></span>
          <span class="icon-bar"></span>
        </button>
        <a class="navbar-brand home push-link" href="/"><strong>OpenReview</strong>.net</a>
      </div>
  
      <div id="navbar" class="navbar-collapse collapse">
        <form class="navbar-form navbar-left profile-search" role="search">
          <div class="form-group has-feedback">
            <input id="search_input" type="text" class="form-control ui-autocomplete-input" placeholder="Search ICLR 2019 Conference" autocomplete="off" />
            <span class="glyphicon glyphicon-search form-control-feedback" aria-hidden="true"></span>
          </div>
  
          <input id="search_group" type="hidden" value="ICLR.cc/2019/Conference" />
          <input id="search_content" type="hidden" value="all" />
        <ul id="ui-id-1" tabindex="0" class="ui-menu ui-widget ui-widget-content ui-autocomplete ui-front" style="display: none;"></ul></form>
  
        <ul class="nav navbar-nav navbar-right">
        
            <li id="user-menu"><a href="/login">Login</a></li>
        </ul>
      </div>
  
    </div>
  </nav>

  <div id="or-banner" class="banner" style="">
  <div class="container">
    <div class="row">
      <div class="col-xs-12"><a href="/group?id=ICLR.cc/2019/Conference" title="Venue Homepage"><img class="icon" src="/static/images/arrow_left.svg" /> Go to <strong>ICLR 2019 Conference</strong> homepage</a></div>
    </div>
  </div>
</div>
<div id="flash-message-container" class="alert alert-danger" role="alert" style="display: none;">
  <div class="container">
    <div class="row">
      <div class="col-xs-12">
        <div class="alert-content">
          <button type="button" class="close" aria-label="Close"><span aria-hidden="true">×</span></button>
        </div>
      </div>
    </div>
  </div>
</div>

<div class="container">
  <div class="row">
    <div class="col-xs-12">
      <main id="content" class="clearfix openbanner-visible legacy-styles"><div id="note_rkxwShA9Ym" class="note panel"><div class="title_pdf_row clearfix"><h2 class="note_content_title"><a>Label super-resolution networks</a> <a class="note_content_pdf" href="/pdf?id=rkxwShA9Ym" title="Download PDF" target="_blank"><img src="/static/images/pdf_icon_blue.svg" /></a> </h2></div><div class="meta_row"><span class="signatures">Anonymous</span></div><div class="clearfix"><div class="meta_row pull-left"><span class="date item">28 Sep 2018 (modified: 20 Nov 2018)</span><span class="item">ICLR 2019 Conference Blind Submission</span><span class="item">Readers: <span class="readers-icon glyphicon glyphicon-globe"></span> Everyone</span><span class="item"><a class="action-bibtex-modal" data-bibtex="@inproceedings{    &#10;anonymous2019label,    &#10;title={Label super-resolution networks},    &#10;author={Anonymous},    &#10;booktitle={Submitted to International Conference on Learning Representations},    &#10;year={2019},    &#10;url={https://openreview.net/forum?id=rkxwShA9Ym},    &#10;note={under review}    &#10;}">Show Bibtex</a></span><a class="note_content_pdf item" href="/revisions?id=rkxwShA9Ym" target="_blank">Show Revisions</a></div><div class="meta_row meta_actions"></div></div><div class="note_contents"><span class="note_content_field">Abstract: </span><span class="note_content_value">We present a deep learning-based method for super-resolving coarse (low-resolution) labels assigned to groups of image pixels into pixel-level (high-resolution) labels, given the joint distribution between those low- and high-resolution labels. This method involves a novel loss function that minimizes the distance between a distribution determined by a set of model outputs and the corresponding distribution given by low-resolution labels over the same set of outputs. This setup does not require that the high-resolution classes match the low-resolution classes and can be used in high-resolution semantic segmentation tasks where high-resolution labeled data is not available. Furthermore, our proposed method is able to utilize both data with low-resolution labels and any available high-resolution labels, which we show improves performance compared to a network trained only with the same amount of high-resolution data.
We test our proposed algorithm in a challenging land cover mapping task to super-resolve labels at a 30m resolution to a separate set of labels at a 1m resolution. We compare our algorithm with models that are trained on high-resolution data and show that 1) we can achieve similar performance using only low-resolution data; and 2) we can achieve better performance when we incorporate a small amount of high-resolution data in our training. We also test our approach on a medical imaging problem, resolving low-resolution probability maps into high-resolution segmentation of lymphocytes with accuracy equal to that of fully supervised models.</span></div><div class="note_contents"><span class="note_content_field">Keywords: </span><span class="note_content_value">weakly supervised segmentation, land cover mapping, medical imaging</span></div><div class="note_contents"><span class="note_content_field">TL;DR: </span><span class="note_content_value">Super-resolving coarse labels into pixel-level labels, applied to aerial imagery and medical scans.</span></div><div class="reply_row clearfix"><div class="item" id="reply_count">7 Replies</div></div></div><hr class="small" /><div id="note_children"><div class="note_with_children"><div id="note_SJeTefhtn7" class="note panel"><div class="title_pdf_row clearfix"><h2 class="note_content_title"><a>A very well written paper with substantial and well organized experimental content, but overall a bit too narrow in scope and technical contribution</a>  <button class="btn btn-xs btn-default permalink-button" title="Link to this comment" data-permalink-url="https://openreview.net/forum?id=rkxwShA9Ym&amp;noteId=SJeTefhtn7"><span class="glyphicon glyphicon-link" aria-hidden="true"></span></button></h2></div><div class="meta_row"><span class="signatures">ICLR 2019 Conference Paper1552 AnonReviewer3</span></div><div class="clearfix"><div class="meta_row pull-left"><span class="date item">02 Nov 2018 (modified: 07 Nov 2018)</span><span class="item">ICLR 2019 Conference Paper1552 Official Review</span><span class="item">Readers: <span class="readers-icon glyphicon glyphicon-globe"></span> Everyone</span></div><div class="meta_row meta_actions"></div></div><div class="note_contents"><span class="note_content_field">Review: </span><span class="note_content_value">The authors present a technique to exploit low resolution labels from a space Z to provide weak supervision to a semantic segmentation network which predicts high resolution labels from a different space Y, assuming that a joint distribution of Z, Y is known a-priori.

The paper is very well written and easy to follow, the main contribution is clearly and rigorously explained in the technical section.
The technical contribution is somehow limited, but it is substantially validated by a very well organized and convincing experimental evaluation.
Overall, I have three main points of criticism (detailed in the following), which however aren't enough to not recommend this paper for acceptance.

Main cons:

1) At points, the paper reads more like a technical report about solving specific problems in land cover estimation and lymphocyte segmentation than a machine learning paper.
Many paragraphs are devoted to describe the specifics of these two problems and to design methods to overcome them.
The main technical contribution of the paper seems to be specifically tailored to solve the particular setup encountered in land cover estimation, i.e. two different sets of labels with different resolution on the same segmentation data, which ties to the next point.

2) One important limitation lies in the fact that the distribution p(c|z) needs to be known a-priori and somehow derived from additional problem-specific knowledge.
This is not an issue in the two tasks considered in the paper, but in my opinion it could severely limit the applicability of the proposed approach.
I think the paper would benefit from the inclusion of some discussion about how this limitation could be overcome.

3) It's not very clear to me why the gaussian approximation with the specific mean and variance values defined in eq.4 would be a good approximation for p_net(c_lk|X).
Could the authors expand on this?</span></div><div class="note_contents"><span class="note_content_field">Rating: </span><span class="note_content_value">7: Good paper, accept</span></div><div class="note_contents"><span class="note_content_field">Confidence: </span><span class="note_content_value">4: The reviewer is confident but not absolutely certain that the evaluation is correct</span></div><div class="reply_row clearfix"></div></div><div class="children"><div class="note_with_children"><div id="note_HJg9xHKi6X" class="note panel"><div class="title_pdf_row clearfix"><h2 class="note_content_title"><a>Authors' response, part 2</a>  <button class="btn btn-xs btn-default permalink-button" title="Link to this comment" data-permalink-url="https://openreview.net/forum?id=rkxwShA9Ym&amp;noteId=HJg9xHKi6X"><span class="glyphicon glyphicon-link" aria-hidden="true"></span></button></h2></div><div class="meta_row"><span class="signatures">ICLR 2019 Conference Paper1552 Authors</span></div><div class="clearfix"><div class="meta_row pull-left"><span class="date item">16 Nov 2018</span><span class="item">ICLR 2019 Conference Paper1552 Official Comment</span><span class="item">Readers: <span class="readers-icon glyphicon glyphicon-globe"></span> Everyone</span></div><div class="meta_row meta_actions"></div></div><div class="note_contents"><span class="note_content_field">Comment: </span><span class="note_content_value">In response to (3): 

We view the output of the core network as a generative model of (hard) segmentations, where the label at a given pixel is drawn from the distribution given by the model’s output. A version of the central limit theorem implies that if one samples the label at each pixel, the count of pixels of a class c within a block, appropriately normalized, will follow an approximately Gaussian distribution whose mean and variance are the average mean and variance of the distributions at individual pixels (eq. 4).

This point of view also leads naturally to the proposed loss function (eq. 7). Here we are maximizing the probability of the model producing the set of labels with the highest log-likelihood under both the network output and the known joint distribution of high-res and low-res labels. (In other words, we independently draw counts from p_net and p_coarse and choose the optimal counts conditioned on the two counts being equal.) On the other hand, the KL divergence mentioned in the footnote measures the expected log-likelihood under p_coarse of a sample count drawn from p_net. 

Please see the response to Reviewer2 below for more discussion of the statistics and loss functions.</span></div><div class="reply_row clearfix"></div></div><div class="children"></div></div><div class="note_with_children"><div id="note_SJlqORuoTX" class="note panel"><div class="title_pdf_row clearfix"><h2 class="note_content_title"><a>Authors' response, part 1</a>  <button class="btn btn-xs btn-default permalink-button" title="Link to this comment" data-permalink-url="https://openreview.net/forum?id=rkxwShA9Ym&amp;noteId=SJlqORuoTX"><span class="glyphicon glyphicon-link" aria-hidden="true"></span></button></h2></div><div class="meta_row"><span class="signatures">ICLR 2019 Conference Paper1552 Authors</span></div><div class="clearfix"><div class="meta_row pull-left"><span class="date item">16 Nov 2018 (modified: 20 Nov 2018)</span><span class="item">ICLR 2019 Conference Paper1552 Official Comment</span><span class="item">Readers: <span class="readers-icon glyphicon glyphicon-globe"></span> Everyone</span></div><div class="meta_row meta_actions"></div></div><div class="note_contents"><span class="note_content_field">Comment: </span><span class="note_content_value">[Modified Nov. 11 to reflect changes in the text.] 

Thank you for your thoughtful comments and questions.

In response to (1) and (2):

Certainly, knowing the distributions p(c|z) is a prerequisite to using our method. In the problem we are considering, where high-resolution and low-resolution classes may not match one-to-one, one must establish at least a weak correspondence between the two kinds of classes -- else, one does not know anything about the meaning of the target (high-res) classes. 
 
In our main example, land cover mapping, high-resolution data is expensive and difficult to collect, but plenty of low-resolution data exists. However, our method is more general, as there are different potential sources of this distribution:
 
- Labels given in coarse blocks with a known distribution (as the NLCD data in our land cover example). In fact, these need not be derived from any high-resolution data. For example, we could set the target distributions based on the descriptions in the NLCD specification (Table 3). Indeed, we found that this gave similar results, although more noise was seen in classes like "Water" and Evergreen Forest" where the specification allows for a wide interval (e.g., [0.75,1], translated into mu=0.875 and sigma=0.25/sqrt(12)) but the true mean is much closer to 1 (Table 4). 
Furthermore, this distribution can be tuned by hand (forcing the "Water" class to have higher proportion of water than what was in the NLCD description, for example). If there are only a handful of coarse and fine-grained labels, then such experimentation is not unreasonable.
 
- Quantized density estimates from another model (as the coarse predictor output in our lymphocyte example).
 
- A coarse segmentation provided by another model. We mock this by blurring the ground truth distribution in the Cityscapes pedestrians example, but this may come from the output of a coarser segmentation model, a class activation map coming from a classification model, etc. (as Reviewer1 seems to be suggesting).
 
We have added a small extension to the Cityscapes pedestrians example, showing how we can super-resolve coarse segmentations. We have updated Appendix D [now Appendix F] with these results and revised the text to emphasize the applicability of our approach to different kinds of problems.
 
We think that more general approaches to overcoming this limitation would be an interesting subject for future work. Potential directions are: (a) beginning with only rough priors on the distributions, estimate them by iteratively updating them with the label counts currently being predicted in blocks of each low-resolution class during training; (b) combine this with an unsupervised segmentation method to infer high-resolution classes, knowing they are distributed similarly in blocks of any given low-resolution class. In other words, we could estimate the joint distribution with EM. However, in most applications some knowledge of the relationship between classes is available, and, as discussed above, even weak or hand-tuned priors p(c|z) are often sufficient (depending, of course, on the capacity of the core model and the ability of the gradient descent to vastly overtrain).

We do think the method will be of interest to wide readership as there are many ways to adapt it to new applications. (We do see now that our focus on the two applications that most need this method may create impression that the idea is limited to these couple of applications, and we will address that in the writing.)
</span></div><div class="reply_row clearfix"></div></div><div class="children"></div></div></div></div><div class="note_with_children"><div id="note_ByguYDKY3X" class="note panel"><div class="title_pdf_row clearfix"><h2 class="note_content_title"><a>Interesting approach, and unique use cases</a>  <button class="btn btn-xs btn-default permalink-button" title="Link to this comment" data-permalink-url="https://openreview.net/forum?id=rkxwShA9Ym&amp;noteId=ByguYDKY3X"><span class="glyphicon glyphicon-link" aria-hidden="true"></span></button></h2></div><div class="meta_row"><span class="signatures">ICLR 2019 Conference Paper1552 AnonReviewer1</span></div><div class="clearfix"><div class="meta_row pull-left"><span class="date item">02 Nov 2018 (modified: 07 Nov 2018)</span><span class="item">ICLR 2019 Conference Paper1552 Official Review</span><span class="item">Readers: <span class="readers-icon glyphicon glyphicon-globe"></span> Everyone</span></div><div class="meta_row meta_actions"></div></div><div class="note_contents"><span class="note_content_field">Review: </span><span class="note_content_value">This paper presents a method to super-resolve coarse low-res segmentation labels, if the joint distribution of low-res and high-res labels are known. The problem formulation and the proposed solution are valid, given the examples of land cover super-resolution and lymphocyte segmentation.
I like the paper in general, with the following concerns/thoughts:
1. While matching the divergence of low-res and high-res segmentations, will the model simply collapse and predict noisy boundaries? Or is it already the case, as can be seen in Figure 8 of Appendix? It seems possible that the model is learning high resolution noises. I suggest the authors to do more careful analysis on this.
2. I am curious to see if the proposed technique can be used in other aspects, like super-resolving the boundary of semantic segmentations.</span></div><div class="note_contents"><span class="note_content_field">Rating: </span><span class="note_content_value">6: Marginally above acceptance threshold</span></div><div class="note_contents"><span class="note_content_field">Confidence: </span><span class="note_content_value">4: The reviewer is confident but not absolutely certain that the evaluation is correct</span></div><div class="reply_row clearfix"></div></div><div class="children"><div class="note_with_children"><div id="note_SygsHEtspm" class="note panel"><div class="title_pdf_row clearfix"><h2 class="note_content_title"><a>Authors' response</a>  <button class="btn btn-xs btn-default permalink-button" title="Link to this comment" data-permalink-url="https://openreview.net/forum?id=rkxwShA9Ym&amp;noteId=SygsHEtspm"><span class="glyphicon glyphicon-link" aria-hidden="true"></span></button></h2></div><div class="meta_row"><span class="signatures">ICLR 2019 Conference Paper1552 Authors</span></div><div class="clearfix"><div class="meta_row pull-left"><span class="date item">16 Nov 2018 (modified: 20 Nov 2018)</span><span class="item">ICLR 2019 Conference Paper1552 Official Comment</span><span class="item">Readers: <span class="readers-icon glyphicon glyphicon-globe"></span> Everyone</span></div><div class="meta_row meta_actions"></div></div><div class="note_contents"><span class="note_content_field">Comment: </span><span class="note_content_value">[Modified Nov. 11 to reflect changes in the text.]

Thank you for your thoughtful comments and questions.

(1) In Figure 8 [now Figure 9; see also Figures 10-12], we see qualitatively that both the high-resolution and low-resolution models are sensitive to small-scale input features, and the low-resolution model indeed has little punishment for small-scale *label* errors when data is given at a scale of 8 numbers per image. Yet, our results demonstrate that a model that sees no high-resolution data can learn to (a) be sensitive to shape and (b) make highly certain predictions around boundaries (cf. Fig. 4). 

This also depends on the capacity of the core segmentation model. In principle, if it is highly expressive, it could learn to recognize the blocks and fill in the labels inside the block to fit the frequencies without regard to the input features. This did not happen in our experiments, partly because the neural networks are difficult to overtrain.

(2) Please see the response to Reviewer3 above regarding the uses of our method beyond the setup of the land cover example, as she or he raised closely related questions. We have updated Appendix D [now Appendix F] with an example of super-resolving coarse segmentations and discussion of other approaches to obtaining coarse labels.
</span></div><div class="reply_row clearfix"></div></div><div class="children"></div></div></div></div><div class="note_with_children"><div id="note_rkln9EjE3X" class="note panel"><div class="title_pdf_row clearfix"><h2 class="note_content_title"><a>Fun, useful, and well presented idea. Experimental results are convincing, too. </a>  <button class="btn btn-xs btn-default permalink-button" title="Link to this comment" data-permalink-url="https://openreview.net/forum?id=rkxwShA9Ym&amp;noteId=rkln9EjE3X"><span class="glyphicon glyphicon-link" aria-hidden="true"></span></button></h2></div><div class="meta_row"><span class="signatures">ICLR 2019 Conference Paper1552 AnonReviewer2</span></div><div class="clearfix"><div class="meta_row pull-left"><span class="date item">29 Oct 2018 (modified: 07 Nov 2018)</span><span class="item">ICLR 2019 Conference Paper1552 Official Review</span><span class="item">Readers: <span class="readers-icon glyphicon glyphicon-globe"></span> Everyone</span></div><div class="meta_row meta_actions"></div></div><div class="note_contents"><span class="note_content_field">Review: </span><span class="note_content_value">Paper summary: 

This paper presents a deep-learning based method for super-resolving low-resolution labels into high-resolution labels given the joint distribution between those low- and high- resolution labels. This is useful for many semantic segmentation tasks where high-resolution ground truth data is hard and expensive to collect. Its main contribution is a novel loss function that allows to minimize the distance between the distribution determined by a set of model outputs and the corresponding distribution given by low-resolution label over the same set of outputs. The paper also thoroughly evaluates the proposed method for two main tasks, the first being a land cover mapping task and the second being a medical imaging problem.

For the land cover application, adding low-resolution data to high-resolution data worsens the results when evaluating on the geographic area from which the high-resolution data was taken. However, when testing the model on new geographic areas and only adding the low-resolution data from this new area in training makes significant improvements.

Generally the paper is very well written, well structured, all explanations are clear, examples and figures are presented when needed and convey helpful information for the reader. The overall idea is fun, original, useful (especially in remote sensing) and is presented in a a convincing way. All major claims are supported by experimental evaluation. There are nevertheless a few concerns:

Major Concerns:

On a conceptual level, the main concern is that the paper assumes we are given a joint distribution of low and high resolution labels, “where we are given the joint distribution P(Y,Z)”, which seems the main limitation of this method. In fact, to correctly estimat this joint distribution either requires additional knowledge about low-resolution data such as the example presented on the NCLD data : “For instance, the “Developed, Medium Intensity” class [...] of the coarse classes”, or it requires actual high-resolution labelled data to correctly estimate this joint distribution. I think the paper would greatly benefit from including a section that discusses the impact of this limitation.

Another point is footnote 3 on page 5. This argument is valid but it would be more convincing to give a thorough explanation on why the choice of the presented loss function is better compared to the KL divergence based loss function or at least some evidence that the two perform similarly when evaluating the method.

Minor Concerns: 

-	“such as CRFs or iterative evaluation” I would include a citation on this type of work.
-	Format of some references in the text need to be corrected, e.g. “into different land cover classes Demir et al. (2018); Kuo et al. (2018); Davydow et al. (2018); Tian et al. (2018).” 

</span></div><div class="note_contents"><span class="note_content_field">Rating: </span><span class="note_content_value">9: Top 15% of accepted papers, strong accept</span></div><div class="note_contents"><span class="note_content_field">Confidence: </span><span class="note_content_value">4: The reviewer is confident but not absolutely certain that the evaluation is correct</span></div><div class="reply_row clearfix"></div></div><div class="children"><div class="note_with_children"><div id="note_B1xSzvYip7" class="note panel"><div class="title_pdf_row clearfix"><h2 class="note_content_title"><a>Authors' response</a>  <button class="btn btn-xs btn-default permalink-button" title="Link to this comment" data-permalink-url="https://openreview.net/forum?id=rkxwShA9Ym&amp;noteId=B1xSzvYip7"><span class="glyphicon glyphicon-link" aria-hidden="true"></span></button></h2></div><div class="meta_row"><span class="signatures">ICLR 2019 Conference Paper1552 Authors</span></div><div class="clearfix"><div class="meta_row pull-left"><span class="date item">16 Nov 2018 (modified: 20 Nov 2018)</span><span class="item">ICLR 2019 Conference Paper1552 Official Comment</span><span class="item">Readers: <span class="readers-icon glyphicon glyphicon-globe"></span> Everyone</span></div><div class="meta_row meta_actions"></div></div><div class="note_contents"><span class="note_content_field">Comment: </span><span class="note_content_value">[Modified Nov. 11 to reflect changes in the text.]

Thank you for your thoughtful comments and questions. We have taken account of the Minor Concerns you raised.

In response to the Major Concerns:

We agree with your comment on P(Y,Z) and will incorporate discussion of both the generality and limitations of fixing a joint distribution. Please see the response to Reviewer3, part 1, above, as well as the updated Appendix F [formerly Appendix D], for discussion on this. In short, the estimated joint distribution need not be derived from high-resolution data: it could be specified a priori, tuned manually, derived from the output of another model, etc.

Loss functions: We summarize the intuitive motivation in our response to Reviewer3, part 2, above.

Qualitatively, the two loss functions have a similar form. They minimize:
(first term) the L2 distance between observed and expected counts normalized by the expected (resp. expected+observed) variance in (7) (resp. KL);
(second term) the variance at individual pixels.
In (7), the added term in the denominator reduces the weight of the L2 distance when the model is uncertain (sigma is large). When the block size is large, the difference between the two functions becomes insignificant. However, when the block size is small, (7) punishes the model for predictions that are very certain but incorrect, so it must balance between high certainty (second term) and low certainty on unlikely predictions (first term).
 
Quantitatively, if there are c classes, the maximum possible sigma2 occurs when all outputs are uniform over the classes and is equal to
1/Bk * 1/c * (1-1/c),
In the land cover experiment, where Bk=900, and c=4, sigma2 is approximately 0.0002. If rho=0.03, as it may be for classes very unlikely to occur in given blocks (see Table 4), then rho2=0.0009, on the same order as sigma2. Thus we punish more for predictions that are certain but predict a class that is unlikely to occur. As predictions become more certain during training, sigma2 becomes insignificant.
 
Indeed, in early experiments we found that beginning SR-only training with the KL distance sometimes led to pathological local minima, such as a single class always being predicted with high certainty. In contrast, it seems that using (7) in early training -- favoring uncertainty in unlikely predictions -- enables the behavior in Figure 7. However, if training is initialized with a well-performing model, the two criteria give similar results.
</span></div><div class="reply_row clearfix"></div></div><div class="children"></div></div></div></div></div></main></div>
  </div>
</div>


    <!-- Footer -->
    <footer class="sitemap">
      <div class="container">
        <div class="row hidden-xs">
          <div class="col-sm-4">
            <ul class="list-unstyled">
              <li><a href="/" class="home">Home</a></li>
              <li><a href="/venues">All Venues</a></li>
            </ul>
          </div>
    
          <div class="col-sm-4">
            <ul class="list-unstyled">
              <li><a href="/about">About OpenReview</a></li>
              <li><a href="/contact">Contact</a></li>
              <li><a href="#" data-toggle="modal" data-target="#feedback-modal">Feedback</a></li>
            </ul>
          </div>
    
          <div class="col-sm-4">
            <ul class="list-unstyled">
              <li><a href="/terms">Terms of Service</a></li>
              <li><a href="/privacy">Privacy Policy</a></li>
            </ul>
          </div>
        </div>
    
        <div class="row visible-xs-block">
          <div class="col-xs-6">
            <ul class="list-unstyled">
              <li><a href="/" class="home">Home</a></li>
              <li><a href="/about">About OpenReview</a></li>
              <li><a href="#" data-toggle="modal" data-target="#feedback-modal">Feedback</a></li>
            </ul>
          </div>
    
          <div class="col-xs-6">
            <ul class="list-unstyled">
              <li><a href="/contact">Contact</a></li>
              <li><a href="/terms">Terms of Service</a></li>
              <li><a href="/privacy">Privacy Policy</a></li>
            </ul>
          </div>
        </div>
      </div>
    </footer>
    
    <footer class="sponsor">
      <div class="container">
        <div class="row">
          <div class="col-sm-10 col-sm-offset-1">
            <p class="text-center">
              OpenReview is created by the <a href="http://www.iesl.cs.umass.edu/" target="_blank">Information Extraction and Synthesis Laboratory</a>, College of Information and Computer Science, University of Massachusetts Amherst. We gratefully acknowledge the support of the OpenReview sponsors:  Google,  Facebook, NSF, the University of Massachusetts Amherst Center for Data Science, and Center for Intelligent Information Retrieval, as well as the Google Cloud Platform for donating the computing and networking services on which OpenReview.net runs.
            </p>
          </div>
        </div>
      </div>
    </footer>

  <div id="feedback-modal" class="modal fade" tabindex="-1" role="dialog">
    <div class="modal-dialog">
      <div class="modal-content">
        <div class="modal-header">
          <button type="button" class="close" data-dismiss="modal" aria-label="Close"><span aria-hidden="true">×</span></button>
          <h3 class="modal-title">Send Feedback</h3>
        </div>
        <div class="modal-body">
          <p>Enter your feedback below and we'll get back to you as soon as possible.</p>
          <form action="/feedback" method="POST">
            <div class="form-group">
              <input type="email" name="from" class="form-control" placeholder="Email" />
            </div>
            <div class="form-group">
              <input type="text" name="subject" class="form-control" placeholder="Subject" />
            </div>
            <div class="form-group">
              <textarea name="message" class="form-control feedback-input" rows="5" placeholder="Message" required=""></textarea>
            </div>
          <ul id="ui-id-2" tabindex="0" class="ui-menu ui-widget ui-widget-content" style="display: none;"></ul></form>
        </div>
        <div class="modal-footer">
          <button type="button" class="btn btn-default" data-dismiss="modal">Cancel</button>
          <button type="button" class="btn btn-primary">Send</button>
        </div>
      </div>
    </div>
  </div>

  <script type="text/javascript" async="" src="https://www.google-analytics.com/analytics.js"></script><script src="//cdnjs.cloudflare.com/ajax/libs/jquery/2.2.4/jquery.min.js"></script>
  <script>window.jQuery || document.write('&lt;script src="/static/js/vendor/jquery-2.2.4.min.js"&gt;&lt;\/script&gt;')</script>

  <script src="/static/dist/vendor.min.js"></script>
  <script src="/static/dist/templates.min.js"></script>
  <script src="/static/dist/openreview.min.js"></script>

    <script>window.legacyScripts = true;</script>


    <script async="" src="https://www.googletagmanager.com/gtag/js?id=UA-108703919-1"></script>
    <script>
      window.dataLayer = window.dataLayer || [];
      function gtag() { dataLayer.push(arguments); }
      gtag('js', new Date());
      gtag('config', 'UA-108703919-1');
    </script>


<div role="status" aria-live="assertive" aria-relevant="additions" class="ui-helper-hidden-accessible"></div></body></html>