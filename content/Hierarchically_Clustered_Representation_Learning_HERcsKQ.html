<!DOCTYPE html><html xmlns="http://www.w3.org/1999/xhtml" class="no-js"><head>
  <meta charset="utf-8" />
  <meta http-equiv="X-UA-Compatible" content="IE=edge,chrome=1" />
  <meta name="viewport" content="width=device-width, initial-scale=1" />

  <title>Hierarchically Clustered Representation Learning | OpenReview</title>
  <meta name="description" content="" />

      <meta name="citation_title" content="Hierarchically Clustered Representation Learning" />
        <meta name="citation_author" content="Anonymous" />
      <meta name="citation_publication_date" content="2018/09/27" />
      <meta name="citation_online_date" content="2018/09/27" />
      <meta name="citation_pdf_url" content="https://openreview.net/pdf?id=H1ERcs09KQ" />
      <meta name="twitter:card" content="summary" />
      <meta name="twitter:site" content="@openreviewnet" />
      <meta name="og:title" content="Hierarchically Clustered Representation Learning" />
      <meta name="og:description" content="The joint optimization of representation learning and clustering in the embedding space has experienced a breakthrough in recent years. In spite of the advance, clustering with representation..." />
      <meta name="og:image" content="https://openreview.net/static/images/openreview_logo_512.png" />

  <link rel="stylesheet" href="https://fonts.googleapis.com/css?family=Noto+Sans:400,400i,700,700i" />

  <link rel="stylesheet" href="/static/css/bootstrap.min.css" />
  <link rel="stylesheet" href="/static/css/jquery-ui.min.css" />
  <link rel="stylesheet" href="/static/css/main.min.css" />
</head>

<body class="forum">
  <nav class="navbar navbar-inverse navbar-fixed-top" role="navigation">
    <div class="container">
  
      <div class="navbar-header">
        <button type="button" class="navbar-toggle collapsed" data-toggle="collapse" data-target="#navbar" aria-expanded="false" aria-controls="navbar">
          <span class="sr-only">Toggle navigation</span>
          <span class="icon-bar"></span>
          <span class="icon-bar"></span>
          <span class="icon-bar"></span>
        </button>
        <a class="navbar-brand home push-link" href="/"><strong>OpenReview</strong>.net</a>
      </div>
  
      <div id="navbar" class="navbar-collapse collapse">
        <form class="navbar-form navbar-left profile-search" role="search">
          <div class="form-group has-feedback">
            <input id="search_input" type="text" class="form-control ui-autocomplete-input" placeholder="Search ICLR 2019 Conference" autocomplete="off" />
            <span class="glyphicon glyphicon-search form-control-feedback" aria-hidden="true"></span>
          </div>
  
          <input id="search_group" type="hidden" value="ICLR.cc/2019/Conference" />
          <input id="search_content" type="hidden" value="all" />
        <ul id="ui-id-1" tabindex="0" class="ui-menu ui-widget ui-widget-content ui-autocomplete ui-front" style="display: none;"></ul></form>
  
        <ul class="nav navbar-nav navbar-right">
        
            <li id="user-menu"><a href="/login">Login</a></li>
        </ul>
      </div>
  
    </div>
  </nav>

  <div id="or-banner" class="banner" style="">
  <div class="container">
    <div class="row">
      <div class="col-xs-12"><a href="/group?id=ICLR.cc/2019/Conference" title="Venue Homepage"><img class="icon" src="/static/images/arrow_left.svg" /> Go to <strong>ICLR 2019 Conference</strong> homepage</a></div>
    </div>
  </div>
</div>
<div id="flash-message-container" class="alert alert-danger" role="alert" style="display: none;">
  <div class="container">
    <div class="row">
      <div class="col-xs-12">
        <div class="alert-content">
          <button type="button" class="close" aria-label="Close"><span aria-hidden="true">×</span></button>
        </div>
      </div>
    </div>
  </div>
</div>

<div class="container">
  <div class="row">
    <div class="col-xs-12">
      <main id="content" class="clearfix openbanner-visible legacy-styles"><div id="note_H1ERcs09KQ" class="note panel"><div class="title_pdf_row clearfix"><h2 class="note_content_title"><a>Hierarchically Clustered Representation Learning</a> <a class="note_content_pdf" href="/pdf?id=H1ERcs09KQ" title="Download PDF" target="_blank"><img src="/static/images/pdf_icon_blue.svg" /></a> </h2></div><div class="meta_row"><span class="signatures">Anonymous</span></div><div class="clearfix"><div class="meta_row pull-left"><span class="date item">28 Sep 2018 (modified: 11 Oct 2018)</span><span class="item">ICLR 2019 Conference Blind Submission</span><span class="item">Readers: <span class="readers-icon glyphicon glyphicon-globe"></span> Everyone</span><span class="item"><a class="action-bibtex-modal" data-bibtex="@inproceedings{    &#10;anonymous2019hierarchically,    &#10;title={Hierarchically Clustered Representation Learning},    &#10;author={Anonymous},    &#10;booktitle={Submitted to International Conference on Learning Representations},    &#10;year={2019},    &#10;url={https://openreview.net/forum?id=H1ERcs09KQ},    &#10;note={under review}    &#10;}">Show Bibtex</a></span></div><div class="meta_row meta_actions"></div></div><div class="note_contents"><span class="note_content_field">Abstract: </span><span class="note_content_value">The joint optimization of representation learning and clustering in the embedding space has experienced a breakthrough in recent years. In spite of the advance, clustering with representation learning has been limited to flat-level categories, which oftentimes involves cohesive clustering with a focus on instance relations. To overcome the limitations of flat clustering, we introduce hierarchically clustered representation learning (HCRL), which simultaneously optimizes representation learning and hierarchical clustering in the embedding space. Specifically, we place a nonparametric Bayesian prior on embeddings to handle dynamic mixture hierarchies under the variational autoencoder framework, and to adopt the generative process of a hierarchical-versioned Gaussian mixture model. Compared with a few prior works focusing on unifying representation learning and hierarchical clustering, HCRL is the first model to consider a generation of deep embeddings from every component of the hierarchy, not just leaf components. This generation process enables more meaningful separations and mergers of clusters via branches in a hierarchy. In addition to obtaining hierarchically clustered embeddings, we can reconstruct data by the various abstraction levels, infer the intrinsic hierarchical structure, and learn the level-proportion features. We conducted evaluations with image and text domains, and our quantitative analyses showed competent likelihoods and the best accuracies compared with the baselines.</span></div><div class="note_contents"><span class="note_content_field">Keywords: </span><span class="note_content_value">Representation learning, Hierarchical clustering, Nonparametric Bayesian modeling</span></div><div class="note_contents"><span class="note_content_field">TL;DR: </span><span class="note_content_value">We introduce hierarchically clustered representation learning (HCRL), which simultaneously optimizes representation learning and hierarchical clustering in the embedding space.</span></div><div class="reply_row clearfix"><div class="item" id="reply_count">7 Replies</div></div></div><hr class="small" /><div id="note_children"><div class="note_with_children"><div id="note_HJx9wwdJaQ" class="note panel"><div class="title_pdf_row clearfix"><h2 class="note_content_title"><a>Interesting and probably useful, but presentation needs work and there are some technical issues</a>  <button class="btn btn-xs btn-default permalink-button" title="Link to this comment" data-permalink-url="https://openreview.net/forum?id=H1ERcs09KQ&amp;noteId=HJx9wwdJaQ"><span class="glyphicon glyphicon-link" aria-hidden="true"></span></button></h2></div><div class="meta_row"><span class="signatures">ICLR 2019 Conference Paper583 AnonReviewer3</span></div><div class="clearfix"><div class="meta_row pull-left"><span class="date item">07 Nov 2018</span><span class="item">ICLR 2019 Conference Paper583 Official Review</span><span class="item">Readers: <span class="readers-icon glyphicon glyphicon-globe"></span> Everyone</span></div><div class="meta_row meta_actions"></div></div><div class="note_contents"><span class="note_content_field">Review: </span><span class="note_content_value">This paper proposes using a variant of the nested CRP as a prior on the latent space of a variational autoencoder. The authors demonstrate that this approach is able to simultaneously learn a meaningful latent representation of high-dimensional data (text and images) and do hierarchical clustering in that space.

Pros:
* The high-level idea is compelling.
* The empirical results are compelling, and the evaluation is thorough.

Cons:
* The prose is pretty rough. The paper is full of sentences like "VAE-nCRP trade-off is the direct dependency modeling among clusters against the mean-field variational approach" that don't convey their intended meaning (at least to me).
* The random variable η seems completely superfluous. It only affects the likelihood through the level indicator l, but the marginal distribution p(l) = ∫_η p(η, l)dη \propto α is tractable, since only one level is drawn per observation. (This is not the case for the traditional nCRP as used in topic modeling, since there a different level is chosen for each word.)
* The novelty over the nCRP-VAE approach of Goyal et al. (2017) is pretty minor. The main difference seems to be that the model can select clusters at different levels, but I didn't quite get the intuition for why this should be desirable. In topic modeling, higher-level clusters tend to contain less-specialized words, and each document is a mix of specialized and general topics. But in this model, only one level is used to explain an entire image or document, and the idea that an entire image or document is much "more specialized" than another doesn't seem very intuitive to me.</span></div><div class="note_contents"><span class="note_content_field">Rating: </span><span class="note_content_value">5: Marginally below acceptance threshold</span></div><div class="note_contents"><span class="note_content_field">Confidence: </span><span class="note_content_value">4: The reviewer is confident but not absolutely certain that the evaluation is correct</span></div><div class="reply_row clearfix"></div></div><div class="children"><div class="note_with_children"><div id="note_BJg7xJMuT7" class="note panel"><div class="title_pdf_row clearfix"><h2 class="note_content_title"><a>Response to Reviewer 3 (1)</a>  <button class="btn btn-xs btn-default permalink-button" title="Link to this comment" data-permalink-url="https://openreview.net/forum?id=H1ERcs09KQ&amp;noteId=BJg7xJMuT7"><span class="glyphicon glyphicon-link" aria-hidden="true"></span></button></h2></div><div class="meta_row"><span class="signatures">ICLR 2019 Conference Paper583 Authors</span></div><div class="clearfix"><div class="meta_row pull-left"><span class="date item">13 Nov 2018</span><span class="item">ICLR 2019 Conference Paper583 Official Comment</span><span class="item">Readers: <span class="readers-icon glyphicon glyphicon-globe"></span> Everyone</span></div><div class="meta_row meta_actions"></div></div><div class="note_contents"><span class="note_content_field">Comment: </span><span class="note_content_value">[Q] This paper proposes using a variant of the nested CRP as a prior on the latent space of a variational autoencoder. The authors demonstrate that this approach is able to simultaneously learn a meaningful latent representation of high-dimensional data (text and images) and do hierarchical clustering in that space.
[A1] Dear Reviewer 3, thank you for the constructive review. As you summarized out, we proposed a joint optimization of representation learning and hierarchical clustering in the embedding space. For the hierarchical clustering, we placed a hierarchical-versioned Gaussian mixture model prior, which is mentioned by the reviewer as ‘a variant of the nested CRP prior.’

[Q] Pros: * The high-level idea is compelling. * The empirical results are compelling, and the evaluation is thorough.
[A1] Thank you for your positive feedbacks. To assert the need for hierarchically clustered representation learning, we performed various quantitative and qualitative experiments with datasets from multiple domains.

[Q] Cons: * The prose is pretty rough. The paper is full of sentences like "VAE-nCRP trade-off is the direct dependency modeling among clusters against the mean-field variational approach" that don't convey their intended meaning (at least to me). *
[A1] Thank you for the reviewer's thoughtful comment. Especially, in the case of the given example sentence, we agree with the sentence should be supplemented. We will add more explanation at the end of the sentence after the reviewer’s agreement, and the sentence to be modified is as follows:
[A2] "VAE-nCRP trade-off is the direct dependency modeling among clusters against the mean-field variational approach, which means that the joint distribution, q(\alpha_p, \alpha_par(p)) is no longer factorized like q(\alpha_p)q(\alpha_par(p))."

[Q] The random variable η seems completely superfluous. It only affects the likelihood through the level indicator l, but the marginal distribution p(l) = ∫_η p(η, l)dη \propto α is tractable since only one level is drawn per observation. (This is not the case for the traditional nCRP as used in topic modeling since there a different level is chosen for each word.) *
[A1] The issue “the marginal distribution p(l) = ∫_η p(η, l)dη \propto α is tractable, since only one level is drawn per observation.” that the reviewer mentioned is related to the conjugacy relationship between the categorical distribution (= one trial of multinomial distribution) and the Dirichlet distribution. The Dirichlet distribution is a conjugate prior for both the categorical distribution and the multinomial distribution regardless of the number of trials so that the \eta variable can be marginalized out.
[A2] In fact, we intended to model the level proportion as shown in the third part of our generative process on page 4. Often, for grouped-data, the level proportion (or topic proportion) is modeled as a group-specific variable. Under our non-grouped data setting, for example, two following approaches are possible: 1) as the reviewer mentioned, globally define a level proportion once, take multiple level samplings for each data, and 2) as our modeling, locally define the data-specific level proportion, followed by sampling the level (this is actually auxiliary variable for specifying the Gaussian distribution). The reason we chose the latter approach is for modeling more flexible prior. The Gaussian mixture distributions exist separately for each level, and we assume the generative process that the mixing coefficient for the level would be different for each data. Please consider that the data-instance we handled is a high-dimensional data of a document/an image rather than a word/a pixel. The hierarchically Gaussian mixture distributions are learned for different levels, and here assuming a common level proportion for all data forcefully limits the expressive power of the model. Also, for preventing the overfitting, we placed the common prior, Dirichlet(\alpha), on the data-specific level proportion, which can be considered as one of the regularization terms.</span></div><div class="reply_row clearfix"></div></div><div class="children"></div></div><div class="note_with_children"><div id="note_H1ejpCZuT7" class="note panel"><div class="title_pdf_row clearfix"><h2 class="note_content_title"><a>Response to Reviewer 3 (2)</a>  <button class="btn btn-xs btn-default permalink-button" title="Link to this comment" data-permalink-url="https://openreview.net/forum?id=H1ERcs09KQ&amp;noteId=H1ejpCZuT7"><span class="glyphicon glyphicon-link" aria-hidden="true"></span></button></h2></div><div class="meta_row"><span class="signatures">ICLR 2019 Conference Paper583 Authors</span></div><div class="clearfix"><div class="meta_row pull-left"><span class="date item">13 Nov 2018</span><span class="item">ICLR 2019 Conference Paper583 Official Comment</span><span class="item">Readers: <span class="readers-icon glyphicon glyphicon-globe"></span> Everyone</span></div><div class="meta_row meta_actions"></div></div><div class="note_contents"><span class="note_content_field">Comment: </span><span class="note_content_value">[Q] The novelty over the nCRP-VAE approach of Goyal et al. (2017) is pretty minor. The main difference seems to be that the model can select clusters at different levels, but I didn't quite get the intuition for why this should be desirable.
[A1] The reviewers addressed one of the main differentiated features between VAE-nCRP and our model. We already described the directly related to the issue at the end of the fifth paragraph of Introduction:
[A2] "Hierarchical mixture density estimation (Vasconcelos &amp; Lippman, 1999), where all internal and leaf components are directly modeled to generate data, is a flexible framework for hierarchical mixture modeling, such as hierarchical topic modeling (Mimno et al., 2007; Griffiths et al., 2004), with regard to the learning of the internal components."
[A3] In particular, (Mimno et al., 2007) [1] also argued that the modeling assumption of direct generation of data from internal components is a flexible framework, which strongly supports the generative process of our study. Besides, VAE-nCRP has a limitation in performing density estimation for embeddings only at the leaf-level, which contrasts with our model having the ability of hierarchical mixture density estimation.
[1] Mimno, David, Wei Li, and Andrew McCallum. "Mixtures of hierarchical topics with pachinko allocation." Proceedings of the 24th international conference on Machine learning. ACM, 2007.
[A4] We would like to stress that our theoretical contribution lies in the first trial of performing the hierarchical mixture density estimation in the embedding space with the unified model under the variational autoencoder framework. 

[Q] In topic modeling, higher-level clusters tend to contain less-specialized words, and each document is a mix of specialized and general topics. However, in this model, only one level is used to explain an entire image or document, 
[A1] This supports why we model the level proportion as a data-specific variable. We sample a level proportion for inferring the specialization level of the data and then, sample a level from the level proportion for specifying the Gaussian distribution among the Gaussian distributions lying on the sampled path.
[A2] Also, I would like to explain the reviewer’s comment as the formulae. The prior we suggested is this: \sum_{\zeta, l} nCRP(\zeta_n) * \eta_{nl} * Normal(-) (-&gt; please refer to the Figure 3(a).). Moreover, the point that the reviewer pointed out is on \eta_{nl}, i.e., ‘the reason for designing \eta as \eta_{nl}, why \eta is data-specific variable?’. There are similar works, which previously published [2-4]. They designed data-specific mixing coefficients of Gaussian mixture models, for improving more flexibility like ours.
[2] Ban, Zhihua, Jianguo Liu, and Li Cao. "Superpixel Segmentation Using Gaussian Mixture Model." IEEE Transactions on Image Processing 27.8 (2018): 4105-4117.
[3] Zhang, Hui, et al. "Automatic Visual Detection System of Railway Surface Defects With Curvature Filter and Improved Gaussian Mixture Model." IEEE Transactions on Instrumentation and Measurement 67.7 (2018): 1593-1608.
[4] Ji, Zexuan, et al. "A spatially constrained generative asymmetric Gaussian mixture model for image segmentation." Journal of Visual Communication and Image Representation 40 (2016): 611-626.
Under the newly proposed Gaussian mixture models from the above papers, the cluster assignment of data is sampled once from the data-specific mixing coefficient, where there is no theoretical problem as a fully Bayesian formalization.
[A3] We were very impressed with the mathematical detail of the reviewer’s comment and thanked you to the reviewer again. If the reviewer agrees with our argument, we will reflect the argument in our paper.

[Q] and the idea that an entire image or document is much "more specialized" than another doesn't seem very intuitive to me.
[A1] We would like to illustrate with an example. Please refer to Figure 1 (a) and (b) in Introduction. From the Figure 1(a), we learned embeddings of digits 7, 4, and 9, which are very close to each other and inferred the several Gaussian mixture components such as internal components of 33, and leaf components of 66, 89, 60, and 88, which generate the embeddings with the high probabilities. Even with the same digit 7, 7 with the upper left edge, which is a shape shared with the digits 4 and 9, is generated from the internal mixture component, whereas the more specialized 7 containing the unique shape of digit 7 tend to be generated from the leaf mixture components.

Best regards,
</span></div><div class="reply_row clearfix"></div></div><div class="children"></div></div></div></div><div class="note_with_children"><div id="note_HyedILfT2X" class="note panel"><div class="title_pdf_row clearfix"><h2 class="note_content_title"><a>nested CRP plus neural network</a>  <button class="btn btn-xs btn-default permalink-button" title="Link to this comment" data-permalink-url="https://openreview.net/forum?id=H1ERcs09KQ&amp;noteId=HyedILfT2X"><span class="glyphicon glyphicon-link" aria-hidden="true"></span></button></h2></div><div class="meta_row"><span class="signatures">ICLR 2019 Conference Paper583 AnonReviewer2</span></div><div class="clearfix"><div class="meta_row pull-left"><span class="date item">05 Nov 2018 (modified: 07 Nov 2018)</span><span class="item">ICLR 2019 Conference Paper583 Official Review</span><span class="item">Readers: <span class="readers-icon glyphicon glyphicon-globe"></span> Everyone</span></div><div class="meta_row meta_actions"></div></div><div class="note_contents"><span class="note_content_field">Review: </span><span class="note_content_value">The paper proposes using the nested CRP as a clustering model rather than a topic model. The clustering is on the latent vector input into a neural network for generating the observation. A variational approach is derived.

The proposed model seems like a straightforward extension of the nCRP with a deep model hanging off the end of it. A significant concern/confusion for me is that this doesn't seem to be a mixed membership model, and so I don't know how meaningful it is to generate a level distribution from a Dirichlet and then draw from that mixture one time. From the generative model it seems every data point has its own Dirichlet vector on levels. For topic models this makes sense since that vector is then drawn from multiple times (once per word) from a Discrete, so there's a distribution to actually learn. My understanding is that this isn't being done here.</span></div><div class="note_contents"><span class="note_content_field">Rating: </span><span class="note_content_value">5: Marginally below acceptance threshold</span></div><div class="note_contents"><span class="note_content_field">Confidence: </span><span class="note_content_value">4: The reviewer is confident but not absolutely certain that the evaluation is correct</span></div><div class="reply_row clearfix"></div></div><div class="children"><div class="note_with_children"><div id="note_H1xXoJM_a7" class="note panel"><div class="title_pdf_row clearfix"><h2 class="note_content_title"><a>Response to Reviewer 2</a>  <button class="btn btn-xs btn-default permalink-button" title="Link to this comment" data-permalink-url="https://openreview.net/forum?id=H1ERcs09KQ&amp;noteId=H1xXoJM_a7"><span class="glyphicon glyphicon-link" aria-hidden="true"></span></button></h2></div><div class="meta_row"><span class="signatures">ICLR 2019 Conference Paper583 Authors</span></div><div class="clearfix"><div class="meta_row pull-left"><span class="date item">13 Nov 2018</span><span class="item">ICLR 2019 Conference Paper583 Official Comment</span><span class="item">Readers: <span class="readers-icon glyphicon glyphicon-globe"></span> Everyone</span></div><div class="meta_row meta_actions"></div></div><div class="note_contents"><span class="note_content_field">Comment: </span><span class="note_content_value">[Q] The paper proposes using the nested CRP as a clustering model rather than a topic model. The clustering is on the latent vector input into a neural network for generating the observation. A variational approach is derived. The proposed model seems like a straightforward extension of the nCRP with a deep model hanging off the end of it.
[A1] Dear Reviewer 2, thank you for the thoughtful review. As the reviewer mentioned, we exploited the nested CRP prior to the path selection process. For performing a hierarchical density estimation task in embedding space, we additionally designed a hierarchical-versioned Gaussian mixture model prior with the nested CRP prior.

[Q] A significant concern/confusion for me is that this doesn't seem to be a mixed membership model, and so I don't know how meaningful it is to generate a level distribution from a Dirichlet and then draw from that mixture one time. From the generative model, it seems every data point has its own Dirichlet vector on levels. For topic models, this makes sense since that vector is then drawn from multiple times (once per word) from a Discrete, so there's a distribution to actually learn. My understanding is that this isn't being done here.
[A1] Thank you for the very constructive comments. In fact, we intended to model the level proportion as shown in the third part of our generative process on page 4. Often, for grouped-data, the level proportion (or topic proportion) is modeled as a group-specific variable. Under our non-grouped data setting, for example, two following approaches are possible: 1) as the reviewer mentioned, globally define a level proportion once, take multiple level samplings for each data, and 2) as our modeling, locally define the data-specific level proportion, followed by sampling the level (this is actually auxiliary variable for specifying the Gaussian distribution). The reason we chose the latter approach is for modeling more flexible prior. The Gaussian mixture distributions exist separately for each level, and we assume the generative process that the mixing coefficient for the level would be different for each data. Please consider that the data-instance we handled is a high-dimensional data of a document/an image rather than a word/a pixel. The hierarchically Gaussian mixture distributions are learned for different levels, and here assuming a common level proportion for all data forcefully limits the expressive power of the model. Also, for preventing the overfitting, we placed the common prior, Dirichlet(\alpha), on the data-specific level proportion, which can be considered as one of the regularization terms.
[A2] Also, I would like to explain the reviewer’s comment as the formulae. The prior we suggested is this: \sum_{\zeta, l} nCRP(\zeta_n) * \eta_{nl} * Normal(-) ( please refer to the Figure 3(a).). Moreover, the point that the reviewer pointed out is on \eta_{nl}, i.e., ‘the reason for designing \eta as \eta_{nl}, why \eta is data-specific variable?’. There are similar works, which previously published [1-3]. They designed data-specific mixing coefficients of Gaussian mixture models, for improving more flexibility like ours.
[1] Ban, Zhihua, Jianguo Liu, and Li Cao. "Superpixel Segmentation Using Gaussian Mixture Model." IEEE Transactions on Image Processing 27.8 (2018): 4105-4117.
[2] Zhang, Hui, et al. "Automatic Visual Detection System of Railway Surface Defects With Curvature Filter and Improved Gaussian Mixture Model." IEEE Transactions on Instrumentation and Measurement 67.7 (2018): 1593-1608.
[3] Ji, Zexuan, et al. "A spatially constrained generative asymmetric Gaussian mixture model for image segmentation." Journal of Visual Communication and Image Representation 40 (2016): 611-626.
Under the newly proposed Gaussian mixture models from the above papers, the cluster assignment of data is sampled once from the data-specific mixing coefficient, where there is no theoretical problem as a fully Bayesian formalization.
[A3] We were very impressed with the mathematical detail of the reviewer’s comment and thanked you to the reviewer again. If the reviewer agrees with our argument, we will reflect the argument in our paper.

Best regards,
</span></div><div class="reply_row clearfix"></div></div><div class="children"></div></div></div></div><div class="note_with_children"><div id="note_BkgQFESc37" class="note panel"><div class="title_pdf_row clearfix"><h2 class="note_content_title"><a>Relevant problem, sound solution and convincing experimental evaluation. But limited novelty. Incremental work.</a>  <button class="btn btn-xs btn-default permalink-button" title="Link to this comment" data-permalink-url="https://openreview.net/forum?id=H1ERcs09KQ&amp;noteId=BkgQFESc37"><span class="glyphicon glyphicon-link" aria-hidden="true"></span></button></h2></div><div class="meta_row"><span class="signatures">ICLR 2019 Conference Paper583 AnonReviewer1</span></div><div class="clearfix"><div class="meta_row pull-left"><span class="date item">03 Nov 2018 (modified: 07 Nov 2018)</span><span class="item">ICLR 2019 Conference Paper583 Official Review</span><span class="item">Readers: <span class="readers-icon glyphicon glyphicon-globe"></span> Everyone</span></div><div class="meta_row meta_actions"></div></div><div class="note_contents"><span class="note_content_field">Review: </span><span class="note_content_value">The paper presents a novel hierarchical clustering method over an embedding space. In the presented approach, both the embedding space and the hierarchical clustering are simultaneously learnt. The hierarchical clustering algorithm aims to recover complex clustering hierarchies which cannot be captured by previously proposed methods. 

The paper address a relevant problem, which is of great interest for extracting knowledge from data. In general, the quality of the paper is high. The presented approach is based on a sound formalization of hierarchical clustering and deep generative models. The paper is easy to follow in spite of the technical difficulty. The experimental evaluation is really extensive. It compares against many state-of-the-art methods. And the results are promising from both a quantitative and qualitative point view. 

The only issue with this paper is its degree of novelty, which is narrow. The proposed method adapt a previously presented hierarchical clustering method in the "standard space" (Griffiths et al., 2004) to an embedding space defined by a variational autoencoder model. The inference algorithm builds on standard techniques of deep generative models and, also, on previously proposed methods (Wand and Blei, 2003) for dealing with the complex hierarchical priors involved in this kind of models.</span></div><div class="note_contents"><span class="note_content_field">Rating: </span><span class="note_content_value">6: Marginally above acceptance threshold</span></div><div class="note_contents"><span class="note_content_field">Confidence: </span><span class="note_content_value">3: The reviewer is fairly confident that the evaluation is correct</span></div><div class="reply_row clearfix"></div></div><div class="children"><div class="note_with_children"><div id="note_HJgnkeMd67" class="note panel"><div class="title_pdf_row clearfix"><h2 class="note_content_title"><a>Response to Reviewer 1</a>  <button class="btn btn-xs btn-default permalink-button" title="Link to this comment" data-permalink-url="https://openreview.net/forum?id=H1ERcs09KQ&amp;noteId=HJgnkeMd67"><span class="glyphicon glyphicon-link" aria-hidden="true"></span></button></h2></div><div class="meta_row"><span class="signatures">ICLR 2019 Conference Paper583 Authors</span></div><div class="clearfix"><div class="meta_row pull-left"><span class="date item">13 Nov 2018</span><span class="item">ICLR 2019 Conference Paper583 Official Comment</span><span class="item">Readers: <span class="readers-icon glyphicon glyphicon-globe"></span> Everyone</span></div><div class="meta_row meta_actions"></div></div><div class="note_contents"><span class="note_content_field">Comment: </span><span class="note_content_value">[Q] The paper presents a novel hierarchical clustering method over an embedding space. In the presented approach, both the embedding space and the hierarchical clustering are simultaneously learned. The hierarchical clustering algorithm aims to recover complex clustering hierarchies which cannot be captured by previously proposed methods.
[A1] Dear Reviewer 1, thank you for the thoughtful review. The reviewer mentioned our key point correctly. Many works on flat-clustered representation learning except for VAE-nCRP, has been limited to capture flat-level data structure.

[Q] The paper address a relevant problem, which is of great interest for extracting knowledge from data.
[A1] There are a lot of high-dimensional data around us, and it obviously contains complex structures inside. What we would like to argue through this study is that we can analyze the complex structure of data in the embedding space learned by a deep neural network.

[Q] In general, the quality of the paper is high. The presented approach is based on a sound formalization of hierarchical clustering and deep generative models. The paper is easy to follow in spite of the technical difficulty. The experimental evaluation is really extensive. It compares against many state-of-the-art methods. And the results are promising from both a quantitative and qualitative point view.
[A1] Thank you for the comment. As we assume a rather complex prior to embedding for flexibility, the technical depth of formalization has deepened. We concerned that it would be confused for the readers including the reviewers, to understand. Therefore, we carefully presented the figures, especially in Figure 3(a) showing an example of the variable values. In the case of experiments, we have devised various quantitative and qualitative experiments to assert why we need this hierarchically clustered representation learning. We empirically observed the performance improvement of both density estimation and hierarchical clustering, which motivates the joint optimization. Additionally, we qualitatively showed embedding plot, image generation, and result hierarchy with various datasets.

[Q] The only issue with this paper is its degree of novelty, which is narrow. The proposed method adapt a previously presented hierarchical clustering method in the "standard space" (Griffiths et al., 2004) to an embedding space defined by a variational autoencoder model. The inference algorithm builds on standard techniques of deep generative models and, also, on previously proposed methods (Wand and Blei, 2003) for dealing with the complex hierarchical priors involved in this kind of models.
[A1] As the reviewer pointed out, we adopted the partial components with the previously proposed techniques or methodologies. The theoretical contribution of our study can be considered in conjunction with the unified model based on the fully Bayesian approach of the probabilistic graphical model and the neural network. Additionally, we tuned the several detailed heuristic algorithms for operations such as GROW, PRUNE, and MERGE. Also, if we take a naive pipelined approach of iterative training between the hierarchical Gaussian mixture model and representation learning, then this work would be an obviously incremental work.
[A2] VAE imposes a single Gaussian prior on embeddings, which leads to 1) the over-regularization, and 2) poor representations [1,2,5]. 
[1] Chen, Xi, et al. "Infogan: Interpretable representation learning by information maximizing generative adversarial nets." NIPS. 2016.
[2] Hoffman, Matthew D., and Matthew J. Johnson. "Elbo surgery: yet another way to carve up the variational evidence lower bound." Workshop in Advances in Approximate Bayesian Inference, NIPS. 2016.
Therefore, the recently published researches can be divided into two branches: 1) designing of an objective function by introducing the additional regularized terms, or 2) constructing of a more flexible prior. Our work attempts to the latter approach, which proposes a new prior called a hierarchical-versioned Gaussian mixture distribution prior to the first trial of hierarchical density estimation in the embedding space. Another work of the latter approach is: 
- Variational Deep Embedding (VaDE) [3]: VAE+GMM
- VAE-nCRP [4]: VAE+(nCRP+GMM)
- VAE with a VampPrior [5]: VAE+ a variational mixture of posteriors prior.
The contribution of these studies lies on 1) the formalization as a unified model based on the newly proposed prior, though not the original technique proposed by the authors, and 2) demonstrating the superiority of the prior.
[3] Jiang, Zhuxi, et al. "Variational deep embedding: an unsupervised and generative approach to clustering." IJCAI, 2017.
[4] Goyal, Prasoon, et al. "Nonparametric Variational Auto-Encoders for Hierarchical Representation Learning." ICCV. 2017.
[5] Tomczak, Jakub, and Max Welling. "VAE with a VampPrior." AISTATS. 2018.

Best regards,
</span></div><div class="reply_row clearfix"></div></div><div class="children"></div></div></div></div></div></main></div>
  </div>
</div>


    <!-- Footer -->
    <footer class="sitemap">
      <div class="container">
        <div class="row hidden-xs">
          <div class="col-sm-4">
            <ul class="list-unstyled">
              <li><a href="/" class="home">Home</a></li>
              <li><a href="/venues">All Venues</a></li>
            </ul>
          </div>
    
          <div class="col-sm-4">
            <ul class="list-unstyled">
              <li><a href="/about">About OpenReview</a></li>
              <li><a href="/contact">Contact</a></li>
              <li><a href="#" data-toggle="modal" data-target="#feedback-modal">Feedback</a></li>
            </ul>
          </div>
    
          <div class="col-sm-4">
            <ul class="list-unstyled">
              <li><a href="/terms">Terms of Service</a></li>
              <li><a href="/privacy">Privacy Policy</a></li>
            </ul>
          </div>
        </div>
    
        <div class="row visible-xs-block">
          <div class="col-xs-6">
            <ul class="list-unstyled">
              <li><a href="/" class="home">Home</a></li>
              <li><a href="/about">About OpenReview</a></li>
              <li><a href="#" data-toggle="modal" data-target="#feedback-modal">Feedback</a></li>
            </ul>
          </div>
    
          <div class="col-xs-6">
            <ul class="list-unstyled">
              <li><a href="/contact">Contact</a></li>
              <li><a href="/terms">Terms of Service</a></li>
              <li><a href="/privacy">Privacy Policy</a></li>
            </ul>
          </div>
        </div>
      </div>
    </footer>
    
    <footer class="sponsor">
      <div class="container">
        <div class="row">
          <div class="col-sm-10 col-sm-offset-1">
            <p class="text-center">
              OpenReview is created by the <a href="http://www.iesl.cs.umass.edu/" target="_blank">Information Extraction and Synthesis Laboratory</a>, College of Information and Computer Science, University of Massachusetts Amherst. We gratefully acknowledge the support of the OpenReview sponsors:  Google,  Facebook, NSF, the University of Massachusetts Amherst Center for Data Science, and Center for Intelligent Information Retrieval, as well as the Google Cloud Platform for donating the computing and networking services on which OpenReview.net runs.
            </p>
          </div>
        </div>
      </div>
    </footer>

  <div id="feedback-modal" class="modal fade" tabindex="-1" role="dialog">
    <div class="modal-dialog">
      <div class="modal-content">
        <div class="modal-header">
          <button type="button" class="close" data-dismiss="modal" aria-label="Close"><span aria-hidden="true">×</span></button>
          <h3 class="modal-title">Send Feedback</h3>
        </div>
        <div class="modal-body">
          <p>Enter your feedback below and we'll get back to you as soon as possible.</p>
          <form action="/feedback" method="POST">
            <div class="form-group">
              <input type="email" name="from" class="form-control" placeholder="Email" />
            </div>
            <div class="form-group">
              <input type="text" name="subject" class="form-control" placeholder="Subject" />
            </div>
            <div class="form-group">
              <textarea name="message" class="form-control feedback-input" rows="5" placeholder="Message" required=""></textarea>
            </div>
          <ul id="ui-id-2" tabindex="0" class="ui-menu ui-widget ui-widget-content" style="display: none;"></ul></form>
        </div>
        <div class="modal-footer">
          <button type="button" class="btn btn-default" data-dismiss="modal">Cancel</button>
          <button type="button" class="btn btn-primary">Send</button>
        </div>
      </div>
    </div>
  </div>

  <script type="text/javascript" async="" src="https://www.google-analytics.com/analytics.js"></script><script src="//cdnjs.cloudflare.com/ajax/libs/jquery/2.2.4/jquery.min.js"></script>
  <script>window.jQuery || document.write('&lt;script src="/static/js/vendor/jquery-2.2.4.min.js"&gt;&lt;\/script&gt;')</script>

  <script src="/static/dist/vendor.min.js"></script>
  <script src="/static/dist/templates.min.js"></script>
  <script src="/static/dist/openreview.min.js"></script>

    <script>window.legacyScripts = true;</script>


    <script async="" src="https://www.googletagmanager.com/gtag/js?id=UA-108703919-1"></script>
    <script>
      window.dataLayer = window.dataLayer || [];
      function gtag() { dataLayer.push(arguments); }
      gtag('js', new Date());
      gtag('config', 'UA-108703919-1');
    </script>


<div role="status" aria-live="assertive" aria-relevant="additions" class="ui-helper-hidden-accessible"></div></body></html>