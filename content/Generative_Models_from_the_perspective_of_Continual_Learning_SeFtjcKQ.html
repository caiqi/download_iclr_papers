<!DOCTYPE html><html xmlns="http://www.w3.org/1999/xhtml" class="no-js"><head>
  <meta charset="utf-8" />
  <meta http-equiv="X-UA-Compatible" content="IE=edge,chrome=1" />
  <meta name="viewport" content="width=device-width, initial-scale=1" />

  <title>Generative Models from the perspective of Continual Learning | OpenReview</title>
  <meta name="description" content="" />

      <meta name="citation_title" content="Generative Models from the perspective of Continual Learning" />
        <meta name="citation_author" content="Anonymous" />
      <meta name="citation_publication_date" content="2018/09/27" />
      <meta name="citation_online_date" content="2018/09/27" />
      <meta name="citation_pdf_url" content="https://openreview.net/pdf?id=S1eFtj0cKQ" />
      <meta name="twitter:card" content="summary" />
      <meta name="twitter:site" content="@openreviewnet" />
      <meta name="og:title" content="Generative Models from the perspective of Continual Learning" />
      <meta name="og:description" content="Which generative model is the most suitable for Continual Learning? This paper aims at evaluating and comparing generative models on disjoint sequential image generation tasks. We investigate how..." />
      <meta name="og:image" content="https://openreview.net/static/images/openreview_logo_512.png" />

  <link rel="stylesheet" href="https://fonts.googleapis.com/css?family=Noto+Sans:400,400i,700,700i" />

  <link rel="stylesheet" href="/static/css/bootstrap.min.css" />
  <link rel="stylesheet" href="/static/css/jquery-ui.min.css" />
  <link rel="stylesheet" href="/static/css/main.min.css" />
</head>

<body class="forum">
  <nav class="navbar navbar-inverse navbar-fixed-top" role="navigation">
    <div class="container">
  
      <div class="navbar-header">
        <button type="button" class="navbar-toggle collapsed" data-toggle="collapse" data-target="#navbar" aria-expanded="false" aria-controls="navbar">
          <span class="sr-only">Toggle navigation</span>
          <span class="icon-bar"></span>
          <span class="icon-bar"></span>
          <span class="icon-bar"></span>
        </button>
        <a class="navbar-brand home push-link" href="/"><strong>OpenReview</strong>.net</a>
      </div>
  
      <div id="navbar" class="navbar-collapse collapse">
        <form class="navbar-form navbar-left profile-search" role="search">
          <div class="form-group has-feedback">
            <input id="search_input" type="text" class="form-control ui-autocomplete-input" placeholder="Search ICLR 2019 Conference" autocomplete="off" />
            <span class="glyphicon glyphicon-search form-control-feedback" aria-hidden="true"></span>
          </div>
  
          <input id="search_group" type="hidden" value="ICLR.cc/2019/Conference" />
          <input id="search_content" type="hidden" value="all" />
        <ul id="ui-id-1" tabindex="0" class="ui-menu ui-widget ui-widget-content ui-autocomplete ui-front" style="display: none;"></ul></form>
  
        <ul class="nav navbar-nav navbar-right">
        
            <li id="user-menu"><a href="/login">Login</a></li>
        </ul>
      </div>
  
    </div>
  </nav>

  <div id="or-banner" class="banner" style="">
  <div class="container">
    <div class="row">
      <div class="col-xs-12"><a href="/group?id=ICLR.cc/2019/Conference" title="Venue Homepage"><img class="icon" src="/static/images/arrow_left.svg" /> Go to <strong>ICLR 2019 Conference</strong> homepage</a></div>
    </div>
  </div>
</div>
<div id="flash-message-container" class="alert alert-danger" role="alert" style="display: none;">
  <div class="container">
    <div class="row">
      <div class="col-xs-12">
        <div class="alert-content">
          <button type="button" class="close" aria-label="Close"><span aria-hidden="true">×</span></button>
        </div>
      </div>
    </div>
  </div>
</div>

<div class="container">
  <div class="row">
    <div class="col-xs-12">
      <main id="content" class="clearfix openbanner-visible legacy-styles"><div id="note_S1eFtj0cKQ" class="note panel"><div class="title_pdf_row clearfix"><h2 class="note_content_title"><a>Generative Models from the perspective of Continual Learning</a> <a class="note_content_pdf" href="/pdf?id=S1eFtj0cKQ" title="Download PDF" target="_blank"><img src="/static/images/pdf_icon_blue.svg" /></a> </h2></div><div class="meta_row"><span class="signatures">Anonymous</span></div><div class="clearfix"><div class="meta_row pull-left"><span class="date item">28 Sep 2018 (modified: 11 Oct 2018)</span><span class="item">ICLR 2019 Conference Blind Submission</span><span class="item">Readers: <span class="readers-icon glyphicon glyphicon-globe"></span> Everyone</span><span class="item"><a class="action-bibtex-modal" data-bibtex="@inproceedings{    &#10;anonymous2019generative,    &#10;title={Generative Models from the perspective of Continual Learning},    &#10;author={Anonymous},    &#10;booktitle={Submitted to International Conference on Learning Representations},    &#10;year={2019},    &#10;url={https://openreview.net/forum?id=S1eFtj0cKQ},    &#10;note={under review}    &#10;}">Show Bibtex</a></span></div><div class="meta_row meta_actions"></div></div><div class="note_contents"><span class="note_content_field">Abstract: </span><span class="note_content_value">Which generative model is the most suitable for Continual Learning? This paper aims at evaluating and comparing generative models on disjoint sequential image generation tasks. We investigate how several models learn and forget, considering various strategies: rehearsal, regularization, generative replay and fine-tuning. We used two quantitative metrics to estimate the generation quality and memory ability. We experiment with sequential tasks on three commonly used benchmarks for Continual Learning (MNIST, Fashion MNIST and CIFAR10). We found that among all models, the original GAN performs best and among Continual Learning strategies, generative replay outperforms all other methods. Even if we found satisfactory combinations on MNIST and Fashion MNIST, training generative models sequentially on CIFAR10 is particularly instable, and remains a challenge.</span></div><div class="note_contents"><span class="note_content_field">Keywords: </span><span class="note_content_value">Generative Models, Continual Learning</span></div><div class="note_contents"><span class="note_content_field">TL;DR: </span><span class="note_content_value">A comparative study of generative models on Continual Learning scenarios.</span></div><div class="reply_row clearfix"><div class="item" id="reply_count">3 Replies</div></div></div><hr class="small" /><div id="note_children"><div class="note_with_children"><div id="note_SkxPAq452m" class="note panel"><div class="title_pdf_row clearfix"><h2 class="note_content_title"><a>A comprehensive evaluation of existing methods lacking novelty and insight</a>  <button class="btn btn-xs btn-default permalink-button" title="Link to this comment" data-permalink-url="https://openreview.net/forum?id=S1eFtj0cKQ&amp;noteId=SkxPAq452m"><span class="glyphicon glyphicon-link" aria-hidden="true"></span></button></h2></div><div class="meta_row"><span class="signatures">ICLR 2019 Conference Paper463 AnonReviewer1</span></div><div class="clearfix"><div class="meta_row pull-left"><span class="date item">03 Nov 2018 (modified: 07 Nov 2018)</span><span class="item">ICLR 2019 Conference Paper463 Official Review</span><span class="item">Readers: <span class="readers-icon glyphicon glyphicon-globe"></span> Everyone</span></div><div class="meta_row meta_actions"></div></div><div class="note_contents"><span class="note_content_field">Review: </span><span class="note_content_value">This paper evaluates and compares various methods for learning GANs in a Continual Learning setting, i.e., only some of the classes are available during training. It evaluates different continual learning methods including rehearsal, EWC and generative replay applied to training several deep generative models, like GAN, CGAN, WGAN, WGAN-GP, VAE and CVAE on MNIST, Fashion MNIST and CIFAR. The authors conclude with these experimental results that generative replay is the most effective method for such a setting, and found it is difficult to generate CIFAR10 images that can be classified successfully by an image classifier.

I appreciate the authors for providing so much detailed experimental results to the community, but this paper lacks novelty in general. All the CL methods the authors evaluate come from other papers that are already using these methods for generative models: rehearsal has been used in VCL Nguyen et al. (2017), EWC comes directly from Seff et al. (2017), and generative replay has been used by Wu et al. (2018a). The authors also fail to provide any valuable insight with these experimental results, e.g., analyzing why generative replay fails to improve VAEs. 

I expect to see more exciting results coming from the authors, but the paper is not mature enough for acceptance this time.
</span></div><div class="note_contents"><span class="note_content_field">Rating: </span><span class="note_content_value">4: Ok but not good enough - rejection</span></div><div class="note_contents"><span class="note_content_field">Confidence: </span><span class="note_content_value">4: The reviewer is confident but not absolutely certain that the evaluation is correct</span></div><div class="reply_row clearfix"></div></div><div class="children"></div></div><div class="note_with_children"><div id="note_Hyx3Jj1qnQ" class="note panel"><div class="title_pdf_row clearfix"><h2 class="note_content_title"><a>Empirical analysis of CL is welcomed, but a few concerns with the experimental set-up.</a>  <button class="btn btn-xs btn-default permalink-button" title="Link to this comment" data-permalink-url="https://openreview.net/forum?id=S1eFtj0cKQ&amp;noteId=Hyx3Jj1qnQ"><span class="glyphicon glyphicon-link" aria-hidden="true"></span></button></h2></div><div class="meta_row"><span class="signatures">ICLR 2019 Conference Paper463 AnonReviewer3</span></div><div class="clearfix"><div class="meta_row pull-left"><span class="date item">02 Nov 2018 (modified: 07 Nov 2018)</span><span class="item">ICLR 2019 Conference Paper463 Official Review</span><span class="item">Readers: <span class="readers-icon glyphicon glyphicon-globe"></span> Everyone</span></div><div class="meta_row meta_actions"></div></div><div class="note_contents"><span class="note_content_field">Review: </span><span class="note_content_value">This paper performs an empirical comparison of models and CL methods in a generative setting. The main motivation of the paper is to make statements about which model/method combinations are best to use for generative tasks in the CL setting. In short, the paper provides an empirical analysis and evaluation of the combination of CL methods and generative models.

The datasets used for comparison are MNIST, Fashion MNIST, and CIFAR10. For each dataset, sequential (class by class) generative tasks are introduced, aligning with the CL setting. The models investigated are VAEs, GANs, and WGANs, along with their (class) conditional counter-parts. The CL methods investigated are (i) fine-tuning (a simple baseline), (ii) rehearsal methods, (iii) elastic weight consolidation (EWC), and (iv) generative replay (GR). The authors propose to use two evaluation metrics: Fréchet Inception Distance (FID) measures the quality of the generated images, and fitting capacity (FC) measures the usefulness of the images to train classifiers.

Pros:
- The authors are correct in pointing out that most of the work on CL has been restricted to the discriminative case, and that there is value in exploring generative tasks in the CL setting.
- Empirical and experimental evaluation of this sort are useful, and help the community better understand the relationship between model, CL method, and task. Such an evaluation and in-depth analysis is welcomed in CL, especially in the generative setting.
- The authors draw a number of useful conclusions e.g., regarding the usefulness and dangers of employing the different CL methods.

Cons:
- My main concern with this paper regards the evaluation metrics used. The authors propose quality metrics for the generative model, both of which (directly or indirectly) measure the quality of the generated images. In this setting, it is unsurprising that GANs outperform VAEs, as they are known to generate higher-quality images. This however, does not necessarily mean that they are better at the continual learning task (i.e., avoiding catastrophic forgetting). It seems to me that one source from which to draw would be [1], which conducted a very rigorous and useful empirical evaluation of generative models, and the methodology followed there (i.e., evaluating marginal log-likelihoods via annealed importance sampling) would be more convincing evidence for empirical comparison of models, as it would somewhat detach the quality of the generated images from the ability of the model to avoid catastrophic forgetting.

Using their proposed image-quality metrics, the authors make statements such as: "Our results do not give a clear distinction between conditional and unconditional models. However, adversarial methods perform significantly better than variational methods. GANs variants are able to produce better, sharper quality and variety of samples, as observed in Fig. 13 and 14 in Appendix G. Hence, adversarial methods seem more viable for CL." My impression is that this statement on the viability of VAEs vs GANs for CL, which is a major point of the paper, does not follow from the empirical results on the quality of the generated images. It seems quite predictable that the GAN-based models would produce higher quality images, regardless of catastrophic forgetting.

Additional (minor) comments:
- Sec. 2 could consist of a more thorough review of the literature, with a more in-depth comparison of the different CL methods proposed and evaluated in the paper.
- Sec. 2 contains a number of statements of the form: "restricted to VAEs only". For many of the cases it is not immediately clear why this is true, and in my opinion the authors should either drop those comments, or make them rigorous.
- VCL "use specific weights for each task, which only works for the setting where the number of tasks is known in advance". Unclear what exactly this means or why this is true.
- "while the teacher retains knowledge" - how does it "retain knowledge", how is this then transferred to the student, and why is this restricted to VAEs?

Experimental protocol:
- Core-sets for the rehearsal as proposed by [1] could be an interesting extension. It is unclear how the samples were selected for rehearsal, and core-sets represent a principled way to do so, that would also be interesting to compare in this setting to a random baseline.
- For VAEs, a potentially better metric of their ability (other than the log-likelihood as suggested by [2]) would be fitting capacity (or other metric) over learned latent space rather than the reconstructed image-space.

Overall, my impression is that while an empirical analysis of CL methods in the generative setting is a useful concept, the submission in its current form requires some improvement. In particular, I am worried that the choice of evaluation metrics may lead to incorrect (or partially correct) conclusions, which could of course have a negative impact on the research into CL. It also seems that the paper could use some further polishing in both writing and presentation. As such, I encourage the authors to continue the work on this empirical analysis, and perhaps submit in again to future conferences.

[1] - Nguyen et al. Variational Continual Learning, ICLR 2018
[2] - Wu et al. On the Quantitative Analysis of Decoder-Based Generative Models, ICLR 2017</span></div><div class="note_contents"><span class="note_content_field">Rating: </span><span class="note_content_value">5: Marginally below acceptance threshold</span></div><div class="note_contents"><span class="note_content_field">Confidence: </span><span class="note_content_value">3: The reviewer is fairly confident that the evaluation is correct</span></div><div class="reply_row clearfix"></div></div><div class="children"></div></div><div class="note_with_children"><div id="note_SJg-oxTunm" class="note panel"><div class="title_pdf_row clearfix"><h2 class="note_content_title"><a>Potentially nice empirical study, but needs more work on experimental analysis and discussion</a>  <button class="btn btn-xs btn-default permalink-button" title="Link to this comment" data-permalink-url="https://openreview.net/forum?id=S1eFtj0cKQ&amp;noteId=SJg-oxTunm"><span class="glyphicon glyphicon-link" aria-hidden="true"></span></button></h2></div><div class="meta_row"><span class="signatures">ICLR 2019 Conference Paper463 AnonReviewer2</span></div><div class="clearfix"><div class="meta_row pull-left"><span class="date item">02 Nov 2018 (modified: 07 Nov 2018)</span><span class="item">ICLR 2019 Conference Paper463 Official Review</span><span class="item">Readers: <span class="readers-icon glyphicon glyphicon-globe"></span> Everyone</span></div><div class="meta_row meta_actions"></div></div><div class="note_contents"><span class="note_content_field">Review: </span><span class="note_content_value">This paper presents an empirical evaluation of continual learning approaches for generative modelling. Noting that much of previous work focuses on supervised tasks, the paper evaluates various combinations of continual learning strategies (EWC, rehearsal/replay-based, or generative replay) and generative models (GANs or likelihood-based).
The experiments evaluate all combinations on MNIST and Fashion MNIST, and the resulting best-performing combination on CIFAR.
The paper is well-written and structured, and although there are no new proposed algorithms or measures, I think this has the potential to be a useful empirical study on the relatively unstudied topic of continual learning with generative models.

However, my main concern is in the detail of analysis and discussion: for an empirical study, it would be much more beneficial to empirically investigate *why* certain combinations are more effective than others. For example:
- Is the reason GANs are better than likelihood models with generative replay purely because of sample quality? Or is it sufficient for the generator to learn some key characteristics for a class that lead to sufficient discriminability?
- Why is rehearsal better for likelihood models? (And how does this relate to the hypothesis of overfitting to a few real examples?)

The CIFAR-10 results also require more work - it is unclear why the existing approaches could not be made to work, and whether this is a fundamental deficiency in the existing approaches or other factors (hyperparameters, architecture choices, lack of time, etc). Presuming the sample quality is as good as in the WGAN-GP work (given the original implementation is used for experiments), why is this insufficient for generative replay? More detailed analysis / discussion, or another combinatorial study, would help for CIFAR too.

Some comments:
- The poor performance of EWC across the board is concerning. Was this implemented by computing the Fisher of the ELBO with respect to parameters? Was the empirical or true Fisher used? Why does the performance appear so poor compared to Seff et al (2017)? This suggests that either more thought is required on how to best protect parameters of generative models, or the baseline has not been properly implemented/tuned.
- Given this is an entirely empirical study, I would strongly encourage the authors to release code sooner than the acceptance deadline - this can be achieved using an anonymous repository.
- Figure 2 and 3 plots are a little difficult to parse without axis labels.</span></div><div class="note_contents"><span class="note_content_field">Rating: </span><span class="note_content_value">4: Ok but not good enough - rejection</span></div><div class="note_contents"><span class="note_content_field">Confidence: </span><span class="note_content_value">4: The reviewer is confident but not absolutely certain that the evaluation is correct</span></div><div class="reply_row clearfix"></div></div><div class="children"></div></div></div></main></div>
  </div>
</div>


    <!-- Footer -->
    <footer class="sitemap">
      <div class="container">
        <div class="row hidden-xs">
          <div class="col-sm-4">
            <ul class="list-unstyled">
              <li><a href="/" class="home">Home</a></li>
              <li><a href="/venues">All Venues</a></li>
            </ul>
          </div>
    
          <div class="col-sm-4">
            <ul class="list-unstyled">
              <li><a href="/about">About OpenReview</a></li>
              <li><a href="/contact">Contact</a></li>
              <li><a href="#" data-toggle="modal" data-target="#feedback-modal">Feedback</a></li>
            </ul>
          </div>
    
          <div class="col-sm-4">
            <ul class="list-unstyled">
              <li><a href="/terms">Terms of Service</a></li>
              <li><a href="/privacy">Privacy Policy</a></li>
            </ul>
          </div>
        </div>
    
        <div class="row visible-xs-block">
          <div class="col-xs-6">
            <ul class="list-unstyled">
              <li><a href="/" class="home">Home</a></li>
              <li><a href="/about">About OpenReview</a></li>
              <li><a href="#" data-toggle="modal" data-target="#feedback-modal">Feedback</a></li>
            </ul>
          </div>
    
          <div class="col-xs-6">
            <ul class="list-unstyled">
              <li><a href="/contact">Contact</a></li>
              <li><a href="/terms">Terms of Service</a></li>
              <li><a href="/privacy">Privacy Policy</a></li>
            </ul>
          </div>
        </div>
      </div>
    </footer>
    
    <footer class="sponsor">
      <div class="container">
        <div class="row">
          <div class="col-sm-10 col-sm-offset-1">
            <p class="text-center">
              OpenReview is created by the <a href="http://www.iesl.cs.umass.edu/" target="_blank">Information Extraction and Synthesis Laboratory</a>, College of Information and Computer Science, University of Massachusetts Amherst. We gratefully acknowledge the support of the OpenReview sponsors:  Google,  Facebook, NSF, the University of Massachusetts Amherst Center for Data Science, and Center for Intelligent Information Retrieval, as well as the Google Cloud Platform for donating the computing and networking services on which OpenReview.net runs.
            </p>
          </div>
        </div>
      </div>
    </footer>

  <div id="feedback-modal" class="modal fade" tabindex="-1" role="dialog">
    <div class="modal-dialog">
      <div class="modal-content">
        <div class="modal-header">
          <button type="button" class="close" data-dismiss="modal" aria-label="Close"><span aria-hidden="true">×</span></button>
          <h3 class="modal-title">Send Feedback</h3>
        </div>
        <div class="modal-body">
          <p>Enter your feedback below and we'll get back to you as soon as possible.</p>
          <form action="/feedback" method="POST">
            <div class="form-group">
              <input type="email" name="from" class="form-control" placeholder="Email" />
            </div>
            <div class="form-group">
              <input type="text" name="subject" class="form-control" placeholder="Subject" />
            </div>
            <div class="form-group">
              <textarea name="message" class="form-control feedback-input" rows="5" placeholder="Message" required=""></textarea>
            </div>
          <ul id="ui-id-2" tabindex="0" class="ui-menu ui-widget ui-widget-content" style="display: none;"></ul></form>
        </div>
        <div class="modal-footer">
          <button type="button" class="btn btn-default" data-dismiss="modal">Cancel</button>
          <button type="button" class="btn btn-primary">Send</button>
        </div>
      </div>
    </div>
  </div>

  <script type="text/javascript" async="" src="https://www.google-analytics.com/analytics.js"></script><script src="//cdnjs.cloudflare.com/ajax/libs/jquery/2.2.4/jquery.min.js"></script>
  <script>window.jQuery || document.write('&lt;script src="/static/js/vendor/jquery-2.2.4.min.js"&gt;&lt;\/script&gt;')</script>

  <script src="/static/dist/vendor.min.js"></script>
  <script src="/static/dist/templates.min.js"></script>
  <script src="/static/dist/openreview.min.js"></script>

    <script>window.legacyScripts = true;</script>


    <script async="" src="https://www.googletagmanager.com/gtag/js?id=UA-108703919-1"></script>
    <script>
      window.dataLayer = window.dataLayer || [];
      function gtag() { dataLayer.push(arguments); }
      gtag('js', new Date());
      gtag('config', 'UA-108703919-1');
    </script>


<div role="status" aria-live="assertive" aria-relevant="additions" class="ui-helper-hidden-accessible"></div></body></html>