<!DOCTYPE html><html xmlns="http://www.w3.org/1999/xhtml" class="no-js"><head>
  <meta charset="utf-8" />
  <meta http-equiv="X-UA-Compatible" content="IE=edge,chrome=1" />
  <meta name="viewport" content="width=device-width, initial-scale=1" />

  <title>SOLAR: Deep Structured Representations for Model-Based Reinforcement Learning | OpenReview</title>
  <meta name="description" content="" />

      <meta name="citation_title" content="SOLAR: Deep Structured Representations for Model-Based Reinforcement Learning" />
        <meta name="citation_author" content="Anonymous" />
      <meta name="citation_publication_date" content="2018/09/27" />
      <meta name="citation_online_date" content="2018/09/27" />
      <meta name="citation_pdf_url" content="https://openreview.net/pdf?id=Bke96sC5tm" />
      <meta name="twitter:card" content="summary" />
      <meta name="twitter:site" content="@openreviewnet" />
      <meta name="og:title" content="SOLAR: Deep Structured Representations for Model-Based..." />
      <meta name="og:description" content="Model-based reinforcement learning (RL) methods can be broadly categorized as global model methods, which depend on learning models that provide sensible predictions in a wide range of states, or..." />
      <meta name="og:image" content="https://openreview.net/static/images/openreview_logo_512.png" />

  <link rel="stylesheet" href="https://fonts.googleapis.com/css?family=Noto+Sans:400,400i,700,700i" />

  <link rel="stylesheet" href="/static/css/bootstrap.min.css" />
  <link rel="stylesheet" href="/static/css/jquery-ui.min.css" />
  <link rel="stylesheet" href="/static/css/main.min.css" />
</head>

<body class="forum">
  <nav class="navbar navbar-inverse navbar-fixed-top" role="navigation">
    <div class="container">
  
      <div class="navbar-header">
        <button type="button" class="navbar-toggle collapsed" data-toggle="collapse" data-target="#navbar" aria-expanded="false" aria-controls="navbar">
          <span class="sr-only">Toggle navigation</span>
          <span class="icon-bar"></span>
          <span class="icon-bar"></span>
          <span class="icon-bar"></span>
        </button>
        <a class="navbar-brand home push-link" href="/"><strong>OpenReview</strong>.net</a>
      </div>
  
      <div id="navbar" class="navbar-collapse collapse">
        <form class="navbar-form navbar-left profile-search" role="search">
          <div class="form-group has-feedback">
            <input id="search_input" type="text" class="form-control ui-autocomplete-input" placeholder="Search ICLR 2019 Conference" autocomplete="off" />
            <span class="glyphicon glyphicon-search form-control-feedback" aria-hidden="true"></span>
          </div>
  
          <input id="search_group" type="hidden" value="ICLR.cc/2019/Conference" />
          <input id="search_content" type="hidden" value="all" />
        <ul id="ui-id-1" tabindex="0" class="ui-menu ui-widget ui-widget-content ui-autocomplete ui-front" style="display: none;"></ul></form>
  
        <ul class="nav navbar-nav navbar-right">
        
            <li id="user-menu"><a href="/login">Login</a></li>
        </ul>
      </div>
  
    </div>
  </nav>

  <div id="or-banner" class="banner" style="">
  <div class="container">
    <div class="row">
      <div class="col-xs-12"><a href="/group?id=ICLR.cc/2019/Conference" title="Venue Homepage"><img class="icon" src="/static/images/arrow_left.svg" /> Go to <strong>ICLR 2019 Conference</strong> homepage</a></div>
    </div>
  </div>
</div>
<div id="flash-message-container" class="alert alert-danger" role="alert" style="display: none;">
  <div class="container">
    <div class="row">
      <div class="col-xs-12">
        <div class="alert-content">
          <button type="button" class="close" aria-label="Close"><span aria-hidden="true">×</span></button>
        </div>
      </div>
    </div>
  </div>
</div>

<div class="container">
  <div class="row">
    <div class="col-xs-12">
      <main id="content" class="clearfix openbanner-visible legacy-styles"><div id="note_Bke96sC5tm" class="note panel"><div class="title_pdf_row clearfix"><h2 class="note_content_title"><a>SOLAR: Deep Structured Representations for Model-Based Reinforcement Learning</a> <a class="note_content_pdf" href="/pdf?id=Bke96sC5tm" title="Download PDF" target="_blank"><img src="/static/images/pdf_icon_blue.svg" /></a> </h2></div><div class="meta_row"><span class="signatures">Anonymous</span></div><div class="clearfix"><div class="meta_row pull-left"><span class="date item">28 Sep 2018 (modified: 14 Nov 2018)</span><span class="item">ICLR 2019 Conference Blind Submission</span><span class="item">Readers: <span class="readers-icon glyphicon glyphicon-globe"></span> Everyone</span><span class="item"><a class="action-bibtex-modal" data-bibtex="@inproceedings{    &#10;anonymous2019solar:,    &#10;title={SOLAR: Deep Structured Representations for Model-Based Reinforcement Learning},    &#10;author={Anonymous},    &#10;booktitle={Submitted to International Conference on Learning Representations},    &#10;year={2019},    &#10;url={https://openreview.net/forum?id=Bke96sC5tm},    &#10;note={under review}    &#10;}">Show Bibtex</a></span><a class="note_content_pdf item" href="/revisions?id=Bke96sC5tm" target="_blank">Show Revisions</a></div><div class="meta_row meta_actions"></div></div><div class="note_contents"><span class="note_content_field">Abstract: </span><span class="note_content_value">Model-based reinforcement learning (RL) methods can be broadly categorized as global model methods, which depend on learning models that provide sensible predictions in a wide range of states, or local model methods, which iteratively refit simple models that are used for policy improvement. While predicting future states that will result from the current actions is difficult, local model methods only attempt to understand system dynamics in the neighborhood of the current policy, making it possible to produce local improvements without ever learning to predict accurately far into the future. The main idea in this paper is that we can learn representations that make it easy to retrospectively infer simple dynamics given the data from the current policy, thus enabling local models to be used for policy learning in complex systems. We evaluate our approach against other model-based and model-free RL methods on a suite of robotics tasks, including manipulation tasks on a real Sawyer robotic arm directly from camera images.</span></div><div class="note_contents"><span class="note_content_field">Keywords: </span><span class="note_content_value">model-based reinforcement learning, structured representation learning, robotics</span></div><div class="reply_row clearfix"><div class="item" id="reply_count">5 Replies</div></div></div><hr class="small" /><div id="note_children"><div class="note_with_children"><div id="note_B1g3stit6Q" class="note panel"><div class="title_pdf_row clearfix"><h2 class="note_content_title"><a>Interesting model based RL approach that needs additional evaluation and clearer algorithm description</a>  <button class="btn btn-xs btn-default permalink-button" title="Link to this comment" data-permalink-url="https://openreview.net/forum?id=Bke96sC5tm&amp;noteId=B1g3stit6Q"><span class="glyphicon glyphicon-link" aria-hidden="true"></span></button></h2></div><div class="meta_row"><span class="signatures">ICLR 2019 Conference Paper831 AnonReviewer1</span></div><div class="clearfix"><div class="meta_row pull-left"><span class="date item">14 Nov 2018</span><span class="item">ICLR 2019 Conference Paper831 Official Review</span><span class="item">Readers: <span class="readers-icon glyphicon glyphicon-globe"></span> Everyone</span></div><div class="meta_row meta_actions"></div></div><div class="note_contents"><span class="note_content_field">Review: </span><span class="note_content_value">Summary: 

The paper proposes SOLAR a model based RL algorithm that learns a low dimensional embedding such that the dynamics within the latent space are linear. Within this latent space the linear dynamics  are learned using a Bayesian regression. In addition, a quadratic cost function is approximated. The learned dynamics and the cost function are used to update the policy, while simultaneously bounding the change in policy by a KL-bound.  In contrast to other model-based RL algorithms the learned dynamics are not used for planning or imaginary roll-outs and are only used to improve the policy.

Review:
The introduction and experiment section is clearly written but the algorithm description lacks clarity and details, which hinder the understanding of the complete algorithm. One understands the motivation and the main approach but lacks a detailed understanding. For my personal taste the detailed description of learning the embedding is missing. I personally would prefer the statement of the cost-functions and the optimisation problem within the paper and not the appendix. The same holds true for the policy improvement. Therefore, I do not fully understand the approach without extensively studying the appendix or the references. Especially the contribution remains unclear. I am not aware how much the previous work had to be extended.

The experimental evaluation focuses on learning control signal to achieve certain trajectories, where the observations are high-dimensional images rather than low-dimensional representations. I personally think that these tasks are unnecessarily made more complex to incorporate high-dimensional images. Especially, the Sawyer experiment throws away all joint information even though the reward function is solely defined in joint/end-effector position. However, I am aware that this is general practice in the RL community. From the learning curves it seems that the approach is working and achieving good sample complexity compared to model free approaches. However, the improvement over the naive VAE approach remains unclear. I would like to see more comparisons to other model-based approaches. In addition, I am missing qualitative comparisons as the learning curves can be misleading. Especially, the videos on the homepage are really short and do not provide a good overview about the actual performance. Furthermore, you are not providing videos for all models in comparison. The 1s video of a single episode on the reacher task make me wonder what happens in the other episodes. Could you please add longer videos for all comparisons. Furthermore, it would be interesting how the trajectories evolve over time. Could you plot these trajectories? 

Furthermore, it would be really interesting to try your approach on breakout. And test if your approach is learning the actual game dynamics and does not overfit to the block configuration. 

Further minor comments:
- "This shifts our problem setting to that of a partially observed MDP, as we do not observe the latent state"
You are mentioning that you are solving a POMDP. Could you elaborate how you exploit the POMDP formulation and relate your work to POMDP algorithms. In addition, how do you define partial observability? 

- You claim "our method is also successful at handling the complex, contact-rich dynamics of block stacking, which poses a significant challenge compared to the other contactfree tasks." I am quite doubt-full about the claim. Is the dynamics model really modelling contacts and is your policy really reacting to these contacts? Or is your policy just tying to follow a trajectory? From your current evaluation and the videos, I personally wouldn't conclude this. Could you elaborate how you come to this conclusion and provide additional evaluations to solidify your argument? 

- You are not describing the action space for the Sawyer experiment. Are you using torques, velocities or positions? Can you guarantee that the control sequence is smooth? If not how do you ensure that the policy does not harm the robot? 
 
- Could you please incorporate the exact  reward functions for each experiment within the appendix.

- Figure 4. Thanks a lot for including the additional model free baselines and adding all learning curves. However, the learning curves raise multiple questions:

(1) The Global Model Ablation, i.e. the MPC in latent space, works well in the the navigation and car experiment however fails 
to achieve a meaning-full policy within the reacher task. Even though the initial performance is significantly better than 
the other policies. Do you have an explanation for this failure?

(2) The LDS SVAE and VAE Solar version on the reacher task experiences jumps in performance even though the change between policies is bounded by a KL-Bound and the cost function is smooth. How do you explain these jumps? Furthermore, why are these jumps only occurring within the reacher tasks and not the other experiments. 

(3) You are still missing the PPO baselines for the reacher and car experiment. Could you further explain the qualitative difference between the model-free and model-based policies. The difference in learning curves can be misleading. 

(4) What is the unit of "Average Distance to Final Goal"? Is this measured in pixel or a different unit? 

- Figure 5: You are plotting the distance to the goal as performance measure for the Sawyer experiment. The final policy has an approximate error of 2.5 cm. From just the learning curve I cannot conclude that the robot actually learns the task successfully. Is the block really stacked or can it also be wedged? Could you please provide image overlays of the last 10 episodes such that one can evaluate the qualitative performance? 

 

</span></div><div class="note_contents"><span class="note_content_field">Rating: </span><span class="note_content_value">5: Marginally below acceptance threshold</span></div><div class="note_contents"><span class="note_content_field">Confidence: </span><span class="note_content_value">4: The reviewer is confident but not absolutely certain that the evaluation is correct</span></div><div class="reply_row clearfix"></div></div><div class="children"></div></div><div class="note_with_children"><div id="note_SklP9FFR3Q" class="note panel"><div class="title_pdf_row clearfix"><h2 class="note_content_title"><a>Interesting idea, but insufficient comparison to baselines</a>  <button class="btn btn-xs btn-default permalink-button" title="Link to this comment" data-permalink-url="https://openreview.net/forum?id=Bke96sC5tm&amp;noteId=SklP9FFR3Q"><span class="glyphicon glyphicon-link" aria-hidden="true"></span></button></h2></div><div class="meta_row"><span class="signatures">ICLR 2019 Conference Paper831 AnonReviewer2</span></div><div class="clearfix"><div class="meta_row pull-left"><span class="date item">06 Nov 2018 (modified: 07 Nov 2018)</span><span class="item">ICLR 2019 Conference Paper831 Official Review</span><span class="item">Readers: <span class="readers-icon glyphicon glyphicon-globe"></span> Everyone</span></div><div class="meta_row meta_actions"></div></div><div class="note_contents"><span class="note_content_field">Review: </span><span class="note_content_value">This paper proposes a model-based reinforcement learning approach, called SOLAR, 
which consists of mapping complex, high-dimensional observations to low-dimensional 
representations where transition dynamics between consecutive states are approximately linear. 
In this low-dimensional space, local models can easily be fit in closed form and then used to optimize a policy, using a similar method to Guided Policy Search (GPS). The method is evaluated in 4 different settings (3 simulated, 1 on a real robot). 

*Quality: the method seems to work well in the experiments. However, there are issues with the experimental evaluation (detailed below) which make it unclear whether the method is better than standard baselines.

*Clarity: the paper is well-written and clear overall.  

*Originality: the paper proposes an extension of GPS, which to my knowledge is novel. 

*Significance: the idea of learning representations where transitions are linear seems well-founded and potentially useful. However the merits of this method are not yet clear from the experiments. 


Specific Comments:

- Please include an illustration of the 2D navigation task in Figure 3a
- I'm confused by the poor performance of E2C in the 2D navigation task. 
The previous works of [Watter et. al, 2015] and [Banijalami et. al, 2017] report close to 100% accuracy using similar methods. Is the task formulated differently here? 
- I would think a global action-conditional forward model (represented as convnet+deconvnet, and trained unrolled on its own predictions to reduce model errors) would perform quite well on the 2D navigation task, and possibly on the reacher task. Even though these are represented as images, they are very simple images with little distracting information, no changes in illumination/perspective, etc. It seems the model essentially just needs to learn a pixel translation for each action for the navigation task, and some rotations for the reacher. It already seems to work quite well for the non-holonomic car, which requires learning similar transformations. This baseline should be included for all the tasks. 
- Although it does seem that the method performs well on the stacking tasks for the real robot, there are no baselines included. However, there are many works which have explored representation learning and control for robotics using neural networks. A couple examples (+see references within):

"Learning to poke by poking: Experiential learning of intuitive physics" Pulkit Agrawal, Ashvin V Nair, Pieter Abbeel, Jitendra Malik, Sergey Levine. NIPS 2016
"Deep Visual Foresight for Planning Robot Motion" Chelsea Finn, Sergey Levine ICRA 2017

At the very least, the method should be compared to pixel-based global models and representations learned with some kind of autoencoder or forward model for the robot task. 

The paper proposes what seems to be a good idea, but it is not yet demonstrated by the current experiments. </span></div><div class="note_contents"><span class="note_content_field">Rating: </span><span class="note_content_value">5: Marginally below acceptance threshold</span></div><div class="note_contents"><span class="note_content_field">Confidence: </span><span class="note_content_value">3: The reviewer is fairly confident that the evaluation is correct</span></div><div class="reply_row clearfix"></div></div><div class="children"><div class="note_with_children"><div id="note_r1eTCmWFTm" class="note panel"><div class="title_pdf_row clearfix"><h2 class="note_content_title"><a>Thank you for your detailed feedback and insightful comments.</a>  <button class="btn btn-xs btn-default permalink-button" title="Link to this comment" data-permalink-url="https://openreview.net/forum?id=Bke96sC5tm&amp;noteId=r1eTCmWFTm"><span class="glyphicon glyphicon-link" aria-hidden="true"></span></button></h2></div><div class="meta_row"><span class="signatures">ICLR 2019 Conference Paper831 Authors</span></div><div class="clearfix"><div class="meta_row pull-left"><span class="date item">14 Nov 2018</span><span class="item">ICLR 2019 Conference Paper831 Official Comment</span><span class="item">Readers: <span class="readers-icon glyphicon glyphicon-globe"></span> Everyone</span></div><div class="meta_row meta_actions"></div></div><div class="note_contents"><span class="note_content_field">Comment: </span><span class="note_content_value">We have addressed the issues you raised by adding the requested comparisons and evaluations. We also clarify the performance of the E2C comparison in this comment. We believe that these changes and our response address all issues, but we would appreciate any other feedback you might offer.

As requested, we now have a comparison to a version of deep visual foresight, which is a pixel space global model, on the real robot block stacking task in Figure 5 (brown line). We will include a full and fair comparison to deep visual foresight on all tasks in the final, however, we note that the main difficulty here is data efficiency -- both deep visual foresight and Agrawal/Nair 2016 utilize weeks of robot data, whereas we demonstrate successful block stacking in about 30 minutes of interaction time. We believe that our method’s data efficiency in solving image-based tasks on a real robot is a significant result.

We have also now included the other requested baselines on the block stacking task in Figure 5, specifically using an autoencoder representation (VAE ablation, pink line) and using a forward model (global model ablation, blue line). The ablations in this setting are competitive with our method, though our method still achieves a better final policy that is able to more consistently stack the block successfully. We also now compare against these baselines in Figure 4 for all simulated tasks as requested, and though they work well for 2D navigation, they are less successful on the reacher. Finally, as suggested, we include a diagram of the 2D navigation task in Appendix E, Figure 6.

As you noted, E2C performs rather poorly on our version of the 2D navigation task. Our task is harder than that of Watter 2015 and Banijamali 2017, since the policy must reach every target position (which is appended to the state) rather than just a single position in the bottom right. In Appendix F.1, we show that E2C does perform better on the single-target task. Also note that we have used code directly from Banijamali and we have had discussions with these authors who noted similar challenges in reproducing the results of E2C. These works have not made their code publicly available, and to our knowledge no one has been able to reproduce their reported results. However, we are confident that our implementation is correct and that the E2C results accurately reflect the performance of that method.

We would appreciate it if you could take another look at our changes and additional results, and let us know if you would like to either revise your rating of the paper, or request additional changes that would alleviate your concerns.</span></div><div class="reply_row clearfix"></div></div><div class="children"></div></div></div></div><div class="note_with_children"><div id="note_S1gFTfz537" class="note panel"><div class="title_pdf_row clearfix"><h2 class="note_content_title"><a>Nice work but methodology is not clear enough in some parts. It can also benefit from a broader experimental evaluation.</a>  <button class="btn btn-xs btn-default permalink-button" title="Link to this comment" data-permalink-url="https://openreview.net/forum?id=Bke96sC5tm&amp;noteId=S1gFTfz537"><span class="glyphicon glyphicon-link" aria-hidden="true"></span></button></h2></div><div class="meta_row"><span class="signatures">ICLR 2019 Conference Paper831 AnonReviewer3</span></div><div class="clearfix"><div class="meta_row pull-left"><span class="date item">03 Nov 2018 (modified: 07 Nov 2018)</span><span class="item">ICLR 2019 Conference Paper831 Official Review</span><span class="item">Readers: <span class="readers-icon glyphicon glyphicon-globe"></span> Everyone</span></div><div class="meta_row meta_actions"></div></div><div class="note_contents"><span class="note_content_field">Review: </span><span class="note_content_value">In this work the authors propose an end to end approach for model based reinforcement learning from images, where the main building blocks are locally-linear dynamical systems and variational auto-encoders (VAE). Specifically, it is assumed that the input features (i.e., the images) are generated from a low dimensional latent representation mapped through parametric random functions; the latter are modeled via neural networks. A recognition model based on convolutional neural networks operates on the reverse way and is responsible for projecting the input features to the latent space, in order to proceed with the reinforcement learning task. The variational framework is employed in order to jointly learn the VAE and the linear dynamics on the latent state. As a final step, once the model is fitted a linear quadratic system (LQS) is solved in order to learn the cost function and the optimal policy. 

* The paper is well motivated and tries to solve an interesting problem, that of data-efficient reinforcement learning. The experiments are well picked and demonstrate the advantages of the proposed approach towards solving the task, however, the method is only evaluated on few environments and compared against only a couple of other methods. I would expect a broader evaluation and/or comparison against more methods. Since the model is able to reach TRPO’s performance in much less steps it would be nice to see how it performs against PPO from [Schulman et al. 2017] (at least on the simulated environments). Also, would it make sense to compare against [Levine et al. 2016] that has been evaluated on similar tasks?

[Schulman et al 2017] “Proximal Policy Optimization Algorithms”.
[Levine et al. 2016] “End-to-End Training of Deep Visuomotor Policies”.

* Methodologically, the paper is sound. The model part (as the authors point out) is based on [Johnson et al. 2016] and is well explained. On the other hand, the policy part, and in particular the policy update in Section 4.2 has some issues regarding readability. There is a strong interplay between Section 4.2, Section 2.1 and Appendix D and the authors did not manage to nicely explain what exactly is happening during the update phase. In the beginning the reader has the impression that we are finding the optimal policy via the closed-form LQS. Later on we switch to constrained optimisation for the cost by accounting for the KL divergence between the policy on two episodes. Finally, in the appendix we are back to the original quadratic cost. The authors need to clarify all the above. Also, they need to explicitly mention why they opt for stochastic optimisation (is it because of minibatching?)

* To continue with the policy, in Section 4.2 the authors argue that although the optimal policy can be found in closed form this is not desirable because the policy will overfit the model and will not generalise well in the real environment. I disagree with this statement. If this happens it effectively means that the learned model or the assumption/learning of the linear dynamics is not right. The authors seem to also agree with this since they clearly state in the the experimental section that “... our method does not heavily rely on an accurate model...”. To my understanding, this means that we need to refine the modelling strategy and not learn a sub-optimal policy. I am really interested in the authors opinion on that.

* The above argument is also directly related to the recognition model and learning of the policy in the latent state (I completely agree with that). The recognition network, which in this case is a convolutional neural network, is used as an inference mechanism to project the observations to the latent space. We learn the (variational) parameters of the recognition model by optimising the likelihood’s lower bound. This means that we are “allowed” to overfit the variational parameters as long as the bound gets tighter. This can possibly result in degraded performance during the policy update. Furthermore, the variational distribution of the latent state, i.e., q(z_t | s_t) is assumed to be mean field across time (independent z’s), while clearly this is not the case in the posterior. You somehow mitigate that by augmenting the observed state (feeding consecutive frames to the network), but still this is not ideal. Finally, is there a reason why we only use the mean of the recognition model to fit the cost on the projected latent states? Why are we throwing away the uncertainty? Especially since you do not use an exact solver and follow a stochastic gradient.

* In the end of Section 2.1, the authors argue regarding the fact that the prior work assumes access to a compact low-dimensional representation which does not allow them to perform well on images. Reference is needed.

* In the related work the authors mention modelling bias as a downside of prior work. Can you please elaborate on that? Where does the bias come from and, more importantly, how does your approach overcome this issue?

* In the experiment and specifically in Figure 4 am I right in assuming that the distance to target is measured in actual pixels? Furthermore, why the relevant plot for the reacher task is depicting rewards instead of the distance to target. To me this suggests that the task is not solved. In general what I find very upsetting in the field are plots that only depict accumulated reward for a specific task. There are many situations where the agent learns a weird behaviour that happens to give good rewards (e.g., spinning around the cart-pole), and unfortunately such behaviours are not spotted on the reward plots.

Overall, the paper is nicely presented and definitely an interesting work. However, given the fact that methodologically we have not learned anything new from this paper and in combination with the not satisfying experimental evaluation I warrant for rejection.</span></div><div class="note_contents"><span class="note_content_field">Rating: </span><span class="note_content_value">4: Ok but not good enough - rejection</span></div><div class="note_contents"><span class="note_content_field">Confidence: </span><span class="note_content_value">4: The reviewer is confident but not absolutely certain that the evaluation is correct</span></div><div class="reply_row clearfix"></div></div><div class="children"><div class="note_with_children"><div id="note_HygFzNbFam" class="note panel"><div class="title_pdf_row clearfix"><h2 class="note_content_title"><a>Thank you for your detailed review.</a>  <button class="btn btn-xs btn-default permalink-button" title="Link to this comment" data-permalink-url="https://openreview.net/forum?id=Bke96sC5tm&amp;noteId=HygFzNbFam"><span class="glyphicon glyphicon-link" aria-hidden="true"></span></button></h2></div><div class="meta_row"><span class="signatures">ICLR 2019 Conference Paper831 Authors</span></div><div class="clearfix"><div class="meta_row pull-left"><span class="date item">14 Nov 2018</span><span class="item">ICLR 2019 Conference Paper831 Official Comment</span><span class="item">Readers: <span class="readers-icon glyphicon glyphicon-globe"></span> Everyone</span></div><div class="meta_row meta_actions"></div></div><div class="note_contents"><span class="note_content_field">Comment: </span><span class="note_content_value">To address your concerns, we have added the requested comparisons and clarifications to the paper. We also clarify several points about our methodology in this comment. We believe that these changes and our response address all issues, but we would appreciate any other feedback you might offer.

We have included in Figure 9a a comparison to PPO (purple line) on the 2D navigation task as requested, and we will include the other simulated tasks as well in the final. This method performs well and converges to a better policy than our method and TRPO, though the improvement is marginal and our method is still several orders of magnitude more data efficient. Note that we do not assume access to the underlying state as in GPS (Levine 2016), and we have compared our method to LQR-FLM directly on images which was unable to learn the tasks. If LQR-FLM is unsuccessful, then GPS will be unsuccessful as the neural network policy is supervised by the LQR-FLM policies. We have now emphasized this point in Section 6.2.

We have clarified various parts of the paper as suggested. Regarding the specific point on closed-form vs constrained LQR, we note that both of these procedures utilize a quadratic cost, thus there is not a significant implementation difference in terms of the constrained vs unconstrained procedures, and we have clarified this in Section 4.2. We have added citations for prior work that assumes access to a simple representation, in particular, Levine 2014, Deisenroth 2014, and Nagabandi 2018. We have also cited Deisenroth 2014 to elaborate on modeling bias, which is discussed in detail in that paper.

In response to your concerns about using a constrained policy update, we further clarify and justify our policy update method in Section 4.2. We would appreciate if you can take a look at this section and see if it helps explain our modeling and policy learning choices. Finally, to address your concern about the presentation of the results, we justify why we choose to plot the reward rather than the final distance for the reacher task in Section 6.2.

In response to “methodologically we have not learned anything new”, we believe that the main methodological contribution of this paper is to wed structured representation learning and local model-based RL into a more principled method compared to prior work, such that approximate inference within the model exactly enables the local model method we derive. These individual components are based on prior work, however they are extended to meet the needs of our overall method, and to our knowledge this combination is a novel contribution. This is combined with a demonstration that our method can solve real world robotics tasks such as block stacking from only image observations in about half an hour of interaction time, which again to our knowledge has not been done to the level of performance we demonstrate.

We would appreciate it if you could take another look at our changes and additional results, and let us know if you would like to either revise your rating of the paper, or request additional changes that would alleviate your concerns.</span></div><div class="reply_row clearfix"></div></div><div class="children"></div></div></div></div></div></main></div>
  </div>
</div>


    <!-- Footer -->
    <footer class="sitemap">
      <div class="container">
        <div class="row hidden-xs">
          <div class="col-sm-4">
            <ul class="list-unstyled">
              <li><a href="/" class="home">Home</a></li>
              <li><a href="/venues">All Venues</a></li>
            </ul>
          </div>
    
          <div class="col-sm-4">
            <ul class="list-unstyled">
              <li><a href="/about">About OpenReview</a></li>
              <li><a href="/contact">Contact</a></li>
              <li><a href="#" data-toggle="modal" data-target="#feedback-modal">Feedback</a></li>
            </ul>
          </div>
    
          <div class="col-sm-4">
            <ul class="list-unstyled">
              <li><a href="/terms">Terms of Service</a></li>
              <li><a href="/privacy">Privacy Policy</a></li>
            </ul>
          </div>
        </div>
    
        <div class="row visible-xs-block">
          <div class="col-xs-6">
            <ul class="list-unstyled">
              <li><a href="/" class="home">Home</a></li>
              <li><a href="/about">About OpenReview</a></li>
              <li><a href="#" data-toggle="modal" data-target="#feedback-modal">Feedback</a></li>
            </ul>
          </div>
    
          <div class="col-xs-6">
            <ul class="list-unstyled">
              <li><a href="/contact">Contact</a></li>
              <li><a href="/terms">Terms of Service</a></li>
              <li><a href="/privacy">Privacy Policy</a></li>
            </ul>
          </div>
        </div>
      </div>
    </footer>
    
    <footer class="sponsor">
      <div class="container">
        <div class="row">
          <div class="col-sm-10 col-sm-offset-1">
            <p class="text-center">
              OpenReview is created by the <a href="http://www.iesl.cs.umass.edu/" target="_blank">Information Extraction and Synthesis Laboratory</a>, College of Information and Computer Science, University of Massachusetts Amherst. We gratefully acknowledge the support of the OpenReview sponsors:  Google,  Facebook, NSF, the University of Massachusetts Amherst Center for Data Science, and Center for Intelligent Information Retrieval, as well as the Google Cloud Platform for donating the computing and networking services on which OpenReview.net runs.
            </p>
          </div>
        </div>
      </div>
    </footer>

  <div id="feedback-modal" class="modal fade" tabindex="-1" role="dialog">
    <div class="modal-dialog">
      <div class="modal-content">
        <div class="modal-header">
          <button type="button" class="close" data-dismiss="modal" aria-label="Close"><span aria-hidden="true">×</span></button>
          <h3 class="modal-title">Send Feedback</h3>
        </div>
        <div class="modal-body">
          <p>Enter your feedback below and we'll get back to you as soon as possible.</p>
          <form action="/feedback" method="POST">
            <div class="form-group">
              <input type="email" name="from" class="form-control" placeholder="Email" />
            </div>
            <div class="form-group">
              <input type="text" name="subject" class="form-control" placeholder="Subject" />
            </div>
            <div class="form-group">
              <textarea name="message" class="form-control feedback-input" rows="5" placeholder="Message" required=""></textarea>
            </div>
          <ul id="ui-id-2" tabindex="0" class="ui-menu ui-widget ui-widget-content" style="display: none;"></ul></form>
        </div>
        <div class="modal-footer">
          <button type="button" class="btn btn-default" data-dismiss="modal">Cancel</button>
          <button type="button" class="btn btn-primary">Send</button>
        </div>
      </div>
    </div>
  </div>

  <script type="text/javascript" async="" src="https://www.google-analytics.com/analytics.js"></script><script src="//cdnjs.cloudflare.com/ajax/libs/jquery/2.2.4/jquery.min.js"></script>
  <script>window.jQuery || document.write('&lt;script src="/static/js/vendor/jquery-2.2.4.min.js"&gt;&lt;\/script&gt;')</script>

  <script src="/static/dist/vendor.min.js"></script>
  <script src="/static/dist/templates.min.js"></script>
  <script src="/static/dist/openreview.min.js"></script>

    <script>window.legacyScripts = true;</script>


    <script async="" src="https://www.googletagmanager.com/gtag/js?id=UA-108703919-1"></script>
    <script>
      window.dataLayer = window.dataLayer || [];
      function gtag() { dataLayer.push(arguments); }
      gtag('js', new Date());
      gtag('config', 'UA-108703919-1');
    </script>


<div role="status" aria-live="assertive" aria-relevant="additions" class="ui-helper-hidden-accessible"></div></body></html>