<!DOCTYPE html><html xmlns="http://www.w3.org/1999/xhtml" class="no-js"><head>
  <meta charset="utf-8" />
  <meta http-equiv="X-UA-Compatible" content="IE=edge,chrome=1" />
  <meta name="viewport" content="width=device-width, initial-scale=1" />

  <title>Are Generative Classifiers More Robust to Adversarial Attacks? | OpenReview</title>
  <meta name="description" content="" />

      <meta name="citation_title" content="Are Generative Classifiers More Robust to Adversarial Attacks?" />
        <meta name="citation_author" content="Anonymous" />
      <meta name="citation_publication_date" content="2018/09/27" />
      <meta name="citation_online_date" content="2018/09/27" />
      <meta name="citation_pdf_url" content="https://openreview.net/pdf?id=HygUOoC5KX" />
      <meta name="twitter:card" content="summary" />
      <meta name="twitter:site" content="@openreviewnet" />
      <meta name="og:title" content="Are Generative Classifiers More Robust to Adversarial Attacks?" />
      <meta name="og:description" content="There is a rising interest in studying the robustness of deep neural network classifiers against adversaries, with both advanced attack and defence techniques being actively developed. However..." />
      <meta name="og:image" content="https://openreview.net/static/images/openreview_logo_512.png" />

  <link rel="stylesheet" href="https://fonts.googleapis.com/css?family=Noto+Sans:400,400i,700,700i" />

  <link rel="stylesheet" href="/static/css/bootstrap.min.css" />
  <link rel="stylesheet" href="/static/css/jquery-ui.min.css" />
  <link rel="stylesheet" href="/static/css/main.min.css" />
</head>

<body class="forum">
  <nav class="navbar navbar-inverse navbar-fixed-top" role="navigation">
    <div class="container">
  
      <div class="navbar-header">
        <button type="button" class="navbar-toggle collapsed" data-toggle="collapse" data-target="#navbar" aria-expanded="false" aria-controls="navbar">
          <span class="sr-only">Toggle navigation</span>
          <span class="icon-bar"></span>
          <span class="icon-bar"></span>
          <span class="icon-bar"></span>
        </button>
        <a class="navbar-brand home push-link" href="/"><strong>OpenReview</strong>.net</a>
      </div>
  
      <div id="navbar" class="navbar-collapse collapse">
        <form class="navbar-form navbar-left profile-search" role="search">
          <div class="form-group has-feedback">
            <input id="search_input" type="text" class="form-control ui-autocomplete-input" placeholder="Search ICLR 2019 Conference" autocomplete="off" />
            <span class="glyphicon glyphicon-search form-control-feedback" aria-hidden="true"></span>
          </div>
  
          <input id="search_group" type="hidden" value="ICLR.cc/2019/Conference" />
          <input id="search_content" type="hidden" value="all" />
        <ul id="ui-id-1" tabindex="0" class="ui-menu ui-widget ui-widget-content ui-autocomplete ui-front" style="display: none;"></ul></form>
  
        <ul class="nav navbar-nav navbar-right">
        
            <li id="user-menu"><a href="/login">Login</a></li>
        </ul>
      </div>
  
    </div>
  </nav>

  <div id="or-banner" class="banner" style="">
  <div class="container">
    <div class="row">
      <div class="col-xs-12"><a href="/group?id=ICLR.cc/2019/Conference" title="Venue Homepage"><img class="icon" src="/static/images/arrow_left.svg" /> Go to <strong>ICLR 2019 Conference</strong> homepage</a></div>
    </div>
  </div>
</div>
<div id="flash-message-container" class="alert alert-danger" role="alert" style="display: none;">
  <div class="container">
    <div class="row">
      <div class="col-xs-12">
        <div class="alert-content">
          <button type="button" class="close" aria-label="Close"><span aria-hidden="true">×</span></button>
        </div>
      </div>
    </div>
  </div>
</div>

<div class="container">
  <div class="row">
    <div class="col-xs-12">
      <main id="content" class="clearfix openbanner-visible legacy-styles"><div id="note_HygUOoC5KX" class="note panel"><div class="title_pdf_row clearfix"><h2 class="note_content_title"><a>Are Generative Classifiers More Robust to Adversarial Attacks?</a> <a class="note_content_pdf" href="/pdf?id=HygUOoC5KX" title="Download PDF" target="_blank"><img src="/static/images/pdf_icon_blue.svg" /></a> </h2></div><div class="meta_row"><span class="signatures">Anonymous</span></div><div class="clearfix"><div class="meta_row pull-left"><span class="date item">28 Sep 2018 (modified: 20 Nov 2018)</span><span class="item">ICLR 2019 Conference Blind Submission</span><span class="item">Readers: <span class="readers-icon glyphicon glyphicon-globe"></span> Everyone</span><span class="item"><a class="action-bibtex-modal" data-bibtex="@inproceedings{    &#10;anonymous2019are,    &#10;title={Are Generative Classifiers More Robust to Adversarial Attacks?},    &#10;author={Anonymous},    &#10;booktitle={Submitted to International Conference on Learning Representations},    &#10;year={2019},    &#10;url={https://openreview.net/forum?id=HygUOoC5KX},    &#10;note={under review}    &#10;}">Show Bibtex</a></span><a class="note_content_pdf item" href="/revisions?id=HygUOoC5KX" target="_blank">Show Revisions</a></div><div class="meta_row meta_actions"></div></div><div class="note_contents"><span class="note_content_field">Abstract: </span><span class="note_content_value">There is a rising interest in studying the robustness of deep neural network classifiers against adversaries, with both advanced attack and defence techniques being actively developed. However, most recent work focuses on discriminative classifiers, which only model the conditional distribution of the labels given the inputs. In this paper, we propose and investigate the deep Bayes classifier, which improves classical naive Bayes with conditional deep generative models. We further develop detection methods for adversarial examples, which reject inputs with low likelihood under the generative model. Experimental results suggest that deep Bayes classifiers are more robust than deep discriminative classifiers, and that the proposed detection methods are effective against many recently proposed attacks.</span></div><div class="note_contents"><span class="note_content_field">Keywords: </span><span class="note_content_value">generative models, adversarial attack, defence, detection, Bayes' rule</span></div><div class="note_contents"><span class="note_content_field">TL;DR: </span><span class="note_content_value">We proposed a generative classifier based on deep generative models, and show improved robustness and detection results against adversarial attacks. </span></div><div class="reply_row clearfix"><div class="item" id="reply_count">10 Replies</div></div></div><hr class="small" /><div id="note_children"><div class="note_with_children"><div id="note_HklDlvjx07" class="note panel"><div class="title_pdf_row clearfix"><h2 class="note_content_title"><a>Revision available, please consider updating your review</a>  <button class="btn btn-xs btn-default permalink-button" title="Link to this comment" data-permalink-url="https://openreview.net/forum?id=HygUOoC5KX&amp;noteId=HklDlvjx07"><span class="glyphicon glyphicon-link" aria-hidden="true"></span></button></h2></div><div class="meta_row"><span class="signatures">ICLR 2019 Conference Paper356 Authors</span></div><div class="clearfix"><div class="meta_row pull-left"><span class="date item">20 Nov 2018</span><span class="item">ICLR 2019 Conference Paper356 Official Comment</span><span class="item">Readers: <span class="readers-icon glyphicon glyphicon-globe"></span> Everyone</span></div><div class="meta_row meta_actions"></div></div><div class="note_contents"><span class="note_content_field">Comment: </span><span class="note_content_value">Thank you for your reviews, we have revised our submission according to some of your suggests.

Summary of major edits:
1. We reported the SPSA attack (a black-box score-based attack) results on the full generative classifiers. Results show that gradient masking is unlikely to explain the robustness of the generative classifiers.
2. We reported minimum L_inf perturbations on the L_inf white-box attacks (mean/median reported). Again generative classifiers require larger perturbations to be fooled.
3. We discussed the differences between our work and (Schott et. al. 2018) which is **independent and concurrent to us**. Our approach is much much more scalable, we tested on both MNIST and CIFAR-10, and we showed that the graphical model of the LVM does matter for robustness.

Again we believe our contribution is highly novel and significant. Please consider the revised version, and it would be much appreciated if you can update your reviews accordingly. 

Best,
Paper356 Authors</span></div><div class="reply_row clearfix"></div></div><div class="children"></div></div><div class="note_with_children"><div id="note_H1eqrk0K6X" class="note panel"><div class="title_pdf_row clearfix"><h2 class="note_content_title"><a>Good work with thorough experimental study</a>  <button class="btn btn-xs btn-default permalink-button" title="Link to this comment" data-permalink-url="https://openreview.net/forum?id=HygUOoC5KX&amp;noteId=H1eqrk0K6X"><span class="glyphicon glyphicon-link" aria-hidden="true"></span></button></h2></div><div class="meta_row"><span class="signatures">ICLR 2019 Conference Paper356 AnonReviewer1</span></div><div class="clearfix"><div class="meta_row pull-left"><span class="date item">15 Nov 2018</span><span class="item">ICLR 2019 Conference Paper356 Official Review</span><span class="item">Readers: <span class="readers-icon glyphicon glyphicon-globe"></span> Everyone</span></div><div class="meta_row meta_actions"></div></div><div class="note_contents"><span class="note_content_field">Review: </span><span class="note_content_value">This work investigates an interesting direction of improving robustness of classifiers against adversarial attacks by using generative models. The authors propose the *deep Bayes classifier*, which is a deep LVM based extension of naive Bayes. Furthermore, the authors extensively explore 7 possible factorisations of the classifier. Thorough experiments are conducted to assess the capability of defending or detecting adversarial examples.  Besides, the authors incorporate discriminative features to generative classifiers and demonstrate clear robustness gain.

### Highlights
* This work proposes an attractive direction -- the use of generative model in defending or detecting adversarial attacks. I suggest this idea should be follower by more further studies.
* The presented models are quite straight-forward but exhibit good robustness against attacks listed in the experiments.
* Various structural possibilities of the graphical model are examined which is preferable and helps assess the effectiveness of generative classifiers.

### Minors
* Although the major point here is robustness against adversarial attacks, as mentioned by the authors, the performance on clear cases (i.e. no attacks) is unsatisfactory. Also, experiments on CIFAR are too much simplified (only 2 very unlike classes) and therefore not very convincing. 
* For the combination of generative classifier and discriminative features, I’m curious about the results on the clear CIFAR-10 multi-class problem. It should be a very positive plus if results are satisfactory.
* The writing is sometimes hard to follow. For examples, many ad-hoc abbreviations are used across the paper causing difficulties of understanding the core idea and results.

### Conclusion 
In general, this paper brings our attention to a previously less investigated but seemingly promising research direction, i.e. robustness of generative model against adversarial attacks. The idea is insightful and proposed models are straight-forward. While only on small-scale  problems (with the presence of attacks), extensive experimental results in this paper can assist further study on this field. Thus, I recommend this paper to be accepted.   
</span></div><div class="note_contents"><span class="note_content_field">Rating: </span><span class="note_content_value">6: Marginally above acceptance threshold</span></div><div class="note_contents"><span class="note_content_field">Confidence: </span><span class="note_content_value">3: The reviewer is fairly confident that the evaluation is correct</span></div><div class="reply_row clearfix"></div></div><div class="children"><div class="note_with_children"><div id="note_S1xMSRGjaX" class="note panel"><div class="title_pdf_row clearfix"><h2 class="note_content_title"><a>We did test the full CIFAR-10 multi-class case</a>  <button class="btn btn-xs btn-default permalink-button" title="Link to this comment" data-permalink-url="https://openreview.net/forum?id=HygUOoC5KX&amp;noteId=S1xMSRGjaX"><span class="glyphicon glyphicon-link" aria-hidden="true"></span></button></h2></div><div class="meta_row"><span class="signatures">ICLR 2019 Conference Paper356 Authors</span></div><div class="clearfix"><div class="meta_row pull-left"><span class="date item">16 Nov 2018</span><span class="item">ICLR 2019 Conference Paper356 Official Comment</span><span class="item">Readers: <span class="readers-icon glyphicon glyphicon-globe"></span> Everyone</span></div><div class="meta_row meta_actions"></div></div><div class="note_contents"><span class="note_content_field">Comment: </span><span class="note_content_value">Thank you for your review, we are glad that you liked our paper in general. We thank for your suggestions on better writing and will include them in revision. 

We clarify on our CIFAR experiments in below.

1. The test on CIFAR-binary is an important contribution:
1a) Carlini &amp; Wagner (2017b) have shown that many defense techniques that works on MNIST didn’t work very well on natural images. Therefore it is important to have a natural image classification task, to see whether the robustness results on MNIST also extends to natural images.
1b) We did try full generative classifiers on CIFAR-10 multi-class classification. Unfortunately it is still a research challenge to make full generative classifiers work beyond MNIST. In fact we have tried even more powerful architectures like PixelCNN++ to parameterise each of p(x|y), and the clean accuracy in this case is 72.4%. Therefore we don’t think it’s fair to compare this PixelCNN++ based classifier against e.g. VGG-16 (clean accuracy &gt;93%), since the gap on clean accuracy is huge.
1c) Still due to the importance of testing natural image classification tasks, we derived from CIFAR-10 a binary classification task that the deep Bayes classifiers work reasonably well (&gt;90% accuracy), and tested the robustness of them on this task. The general conclusions here are consistent with MNIST experiments, providing evidences that the robustness results of generative classifiers do extend to natural images.

2. The results of the fusion model are indeed on**full CIFAR-10 multi-class classification**. 
2a) Since full generative classifiers don’t work very well on full CIFAR-10, we decided to take discriminative features from VGG-16 and train generative classifiers on the features. We have the clean accuracy results reported in table D.2 for multi-class classification. In this case all classifiers have &gt;88% accuracy on clean data, so we can have a reasonably fair comparison here.
2b) The robustness tests (Figure 10) still favours the fusion model, and the bottleneck discriminative classifiers (DBX-) didn’t improve robustness against PGD &amp; MIM. To the best of our knowledge this is the first robustness test on this fusion model. Our results show that combining generative modelling and discriminative features is an exciting future research direction.
2c) Importantly, we show that using lower-layer features for the fusion model (i.e.~the model is less “discriminative”) returns even better results. This is a clear evidence favouring generative classifiers, and we expect in future developments, a full generative classifier that can achieve &gt;90% clean accuracy on CIFAR-10 will be even more robust.

Thank you for your review again, let us know if you have more questions.</span></div><div class="reply_row clearfix"></div></div><div class="children"></div></div></div></div><div class="note_with_children"><div id="note_B1ePMbbcnQ" class="note panel"><div class="title_pdf_row clearfix"><h2 class="note_content_title"><a>Interesting direction but some important prior work is missing and evaluation does not yet convincingly support claims</a>  <button class="btn btn-xs btn-default permalink-button" title="Link to this comment" data-permalink-url="https://openreview.net/forum?id=HygUOoC5KX&amp;noteId=B1ePMbbcnQ"><span class="glyphicon glyphicon-link" aria-hidden="true"></span></button></h2></div><div class="meta_row"><span class="signatures">ICLR 2019 Conference Paper356 AnonReviewer3</span></div><div class="clearfix"><div class="meta_row pull-left"><span class="date item">03 Nov 2018 (modified: 07 Nov 2018)</span><span class="item">ICLR 2019 Conference Paper356 Official Review</span><span class="item">Readers: <span class="readers-icon glyphicon glyphicon-globe"></span> Everyone</span></div><div class="meta_row meta_actions"></div></div><div class="note_contents"><span class="note_content_field">Review: </span><span class="note_content_value">In this work the authors propose and analyse generative models as defences against adversarial examples. In addition, three detection methods are introduced and an extension to deep features is suggested.

My main concerns are as follows (details below):
* Important prior work is not mentioned.
* Evaluation with direct attacks is only based on (very few) gradient-based techniques, many results are not reliable.
* There are signs of gradient masking (the common problem of robustness evaluation, in particular of only gradient-based techniques are used).
* The way detection rates are taken into account in the perfect knowledge scenario is confusing.

### Style
I like the idea of testing many different factorisation structures. However, that comes with the drawback that one needs to constantly check back what the abbreviations mean. Together with the three detection methods, the manuscript is quite confusing at times and should definitely be streamlined. One suggestion: remove the detection methods: I did not find any real conclusion about them but they are definitely side-tracking users away from the main results.

### Prior work
There is at least one closely related prior work not mentioned here: the analysis by synthesis model [1]. This model uses a variational auto encoder to learn class conditional distributions and shows high robustness on MNIST. Please make clear what your contribution is over this paper (other than testing several other factorisations).

### Evaluation problems
The robustness of models should be evaluated on different direct attacks ranging from gradient-based to score-based (e.g. NES [2]) to decision-based attacks [3]. Please take a look at [1] to see how a very extensive evaluation might look like. The results can be astonishingly different for different attacks, and so basing conclusion on only one or two attacks is dangerous (in particular if you only use gradient-based ones). One can also see that in your results, just check the variations you get between MIM and PGD. Also, rather then discussing (and showing in detail) results for individual attacks, the minimum adversarial distance for a given sample that can be found by any attack is much more comparable between models (which can also streamline the manuscript).

One can see signs of gradient masking in your results. For example, in Figure 3 the MIM attacks levels out at 20% for the DBX model. That can happen for iterative attacks if the gradient is masked. Similarly, in Figure 5 DBX-ZK (zero knowledge) is better in both accuracy and detection rate than DBX-PKK (which takes the KL-detection method into account and should thus either be better in accuracy or detection rate).

More generally, the perfect knowledge case, in which the attacker knows about the detector, should only count samples as adversarials which evade the detector and change the model decision. Thus, the detection rate should be zero. Otherwise I have no idea what trade-off between accuracy and detection rate you are actually targeting and how to compare the results.

Also, some intermediate results are conflicting with each other. E.g. in 4.1 you state “the usage of bottleneck is beneficial for better robustness”, but for L2 this is not true.

Also, I am not sure how conclusive the grey-box and black-box scenarios really are: since the substitute is basically a DFX or DFZ, it’s unsurprising that adversarials transfer best to those two models.

### Minor
 * In 4.1 you say “as they fail to find near manifold adversarials”, but I don’t see how there can be L-infty adversarials on MNIST that are on-manifold (remember, MNIST pixel values are basically binary). Plus, in the zero-knowledge scenario there is nothing that enforces staying on this manifold.
 * Result presentation (Figure 3/5 &amp; Table 1) is very different for different attack scenarios, which makes them hard to compare. Please unify.
 * Is the L2 distance you report in Table 1 the mean (or median) distance to adversarial examples. If so, GBZ (for which you state that C&amp;W “failed on attacking” has actually a smaller mean adversarial distance than some other models (for which C&amp;W is actually quite successful).
 * Grey-box scenario doesn’t make a lot of sense: since the substitute is basically a DFX or DFZ, it’s unsurprising that adversarials transfer best to those two models. A similar confounder makes the black-box results difficult to interpret.
* Also, taking into account that the paper is two pages longer and thus calls for higher standards

Taken together, I find the general direction of the paper very interesting and I’d definitely encourage the authors to go further. At the current stage, however, I feel that (1) contributions are not sufficiently delineated to prior work, (2) the evaluation is not convincingly supporting the claims and that  (3) the manuscript needs to be streamlined (both in terms of text and figures).

[1] Schott et al. (2018) “Towards the first adversarially robust neural network model on MNIST” (<a href="https://arxiv.org/abs/1805.09190)" target="_blank" rel="nofollow">https://arxiv.org/abs/1805.09190)</a>
[2] Ilyas et al. (2018) “Black-box Adversarial Attacks with Limited Queries and Information” ( [https://arxiv.org/abs/1804.08598)](https://arxiv.org/abs/1804.08598)) 
[3] Brendel et al. (2018) “Decision-Based Adversarial Attacks: Reliable Attacks Against Black-Box Machine Learning Models” (https://arxiv.org/abs/1712.04248)</span></div><div class="note_contents"><span class="note_content_field">Rating: </span><span class="note_content_value">4: Ok but not good enough - rejection</span></div><div class="note_contents"><span class="note_content_field">Confidence: </span><span class="note_content_value">5: The reviewer is absolutely certain that the evaluation is correct and very familiar with the relevant literature</span></div><div class="reply_row clearfix"></div></div><div class="children"><div class="note_with_children"><div id="note_Syg6yc0La7" class="note panel"><div class="title_pdf_row clearfix"><h2 class="note_content_title"><a>Apart from independent work statement, on your other questions:</a>  <button class="btn btn-xs btn-default permalink-button" title="Link to this comment" data-permalink-url="https://openreview.net/forum?id=HygUOoC5KX&amp;noteId=Syg6yc0La7"><span class="glyphicon glyphicon-link" aria-hidden="true"></span></button></h2></div><div class="meta_row"><span class="signatures">ICLR 2019 Conference Paper356 Authors</span></div><div class="clearfix"><div class="meta_row pull-left"><span class="date item">12 Nov 2018 (modified: 20 Nov 2018)</span><span class="item">ICLR 2019 Conference Paper356 Official Comment</span><span class="item">Readers: <span class="readers-icon glyphicon glyphicon-globe"></span> Everyone</span></div><div class="meta_row meta_actions"></div></div><div class="note_contents"><span class="note_content_field">Comment: </span><span class="note_content_value">We address your other concerns in below.

Q1: “perfect knowledge case: definition of adversarial example”
We show quantitative results for both cases: 
1. Crafted adv inputs that make the classifier predict wrong class, but are detected by the detection method; 
2. Crafted adv inputs that both make the classifier predict wrong class, and evade the detection.”.
The second case might align with your definition of adversarial example in this perfect-knowledge setting, and the value to look for this case is the white space between the top of the shaded bar and 100. However by studying the first case we also show the breakdown between classification tricking &amp; detection avoiding and only classification tricking as an indication of one point on the Pareto front.

Q2: “grey-box/black-box experiment not conclusive”
What we were trying to do here was allay fears that positive results for the generative models could be due to gradient masking. So we evaluated two types of attack (see the revised manuscript):
1. Distillation-based attack. We took a CNN, not vulnerable to the gradient masking problem, and tried to get it to mimic the function learnt by the generative model as closely as possible by training it on its predictions rather than the ground truth predictions. Then we transfer the adversarial examples on the student CNN to the victim classifier.
2. Score-based attack. We applied the SPSA attack, which used 2000 samples to numerically estimate the gradient from the victim classifier's outputs. 
If gradient masking is the main reason for the white-box robustness, then the above two attacks should achieve higher success rate (i.e. lower victim accuracies) than the white-box case. This is not the case as shown in the experiments, therefore, gradient masking is unlikely to be responsible for generative classifier's robustness properties.

Q3: “GBZ (for which you state that C&amp;W “failed on attacking” has actually a smaller mean adversarial distance”
Again our goal is to compare between generative and discriminative classifiers, and we showed in Table 1 that generative classifiers (GFZ, GFY, GBZ, GBY) has higher mean perturbation distances compared to other discriminative classifiers. Also in Figure 4(b) we showed crafted adversarial examples on GFZ (see Figure D.1(h) for GBZ), and many of them are sitting at the perceptual boundary between the original and the adversarial classes. Paper [1] also showed similar results in their Figure 3.

Q4: “usage of bottleneck is beneficial” contradict L_2 results
We wanted to make this claim for the L_inf case, indeed our quantitative analysis in the appendix was done on L_inf attacks. 

Q5: “how there can be L-infty adversarials on MNIST that are on-manifold”
If an adversarial input has visible noise/artifacts then it’s clearly off the manifold of clean data. But even when the perturbation is difficult for humans to see, generative classifiers are more robust than discriminative ones, see the visualisations in Figure 1 (epsilon=0.1, 0.2) and Figure 8 (epsilon=0.01, 0.02, 0.05). </span></div><div class="reply_row clearfix"></div></div><div class="children"><div class="note_with_children"><div id="note_r1x6ax3lAX" class="note panel"><div class="title_pdf_row clearfix"><h2 class="note_content_title"><a>Please comment on specific indications of gradient masking</a>  <button class="btn btn-xs btn-default permalink-button" title="Link to this comment" data-permalink-url="https://openreview.net/forum?id=HygUOoC5KX&amp;noteId=r1x6ax3lAX"><span class="glyphicon glyphicon-link" aria-hidden="true"></span></button></h2></div><div class="meta_row"><span class="signatures">ICLR 2019 Conference Paper356 AnonReviewer3</span></div><div class="clearfix"><div class="meta_row pull-left"><span class="date item">20 Nov 2018</span><span class="item">ICLR 2019 Conference Paper356 Official Comment</span><span class="item">Readers: <span class="readers-icon glyphicon glyphicon-globe"></span> Everyone</span></div><div class="meta_row meta_actions"></div></div><div class="note_contents"><span class="note_content_field">Comment: </span><span class="note_content_value">In my initial review I wrote

"One can see signs of gradient masking in your results. For example, in Figure 3 the MIM attacks levels out at 20% for the DBX model. That can happen for iterative attacks if the gradient is masked. Similarly, in Figure 5 DBX-ZK (zero knowledge) is better in both accuracy and detection rate than DBX-PKK (which takes the KL-detection method into account and should thus either be better in accuracy or detection rate)."

Could you please comment on that?</span></div><div class="reply_row clearfix"></div></div><div class="children"><div class="note_with_children"><div id="note_SJejnID-0Q" class="note panel"><div class="title_pdf_row clearfix"><h2 class="note_content_title"><a>On gradient masking issues for DBX, a **discriminative** classifier with bottleneck</a>  <button class="btn btn-xs btn-default permalink-button" title="Link to this comment" data-permalink-url="https://openreview.net/forum?id=HygUOoC5KX&amp;noteId=SJejnID-0Q"><span class="glyphicon glyphicon-link" aria-hidden="true"></span></button></h2></div><div class="meta_row"><span class="signatures">ICLR 2019 Conference Paper356 Authors</span></div><div class="clearfix"><div class="meta_row pull-left"><span class="date item">20 Nov 2018</span><span class="item">ICLR 2019 Conference Paper356 Official Comment</span><span class="item">Readers: <span class="readers-icon glyphicon glyphicon-globe"></span> Everyone</span></div><div class="meta_row meta_actions"></div></div><div class="note_contents"><span class="note_content_field">Comment: </span><span class="note_content_value">Thank you for your reply. 

First we emphasize that our goal is to validate that gradient masking is unlikely to explain the robustness of **generative classifiers**. Indeed our experiments provide strong evidence supporting this, and in the revised version we edited the discussions to emphasize this.

DBX is a **discriminative** classifier. So the following discussion only applies to DBX and does not relate to any observations we had on generative classifiers.

On MNIST, our observations for FGSM &amp; MIM attacks on DBX are similar to (Alemi et al. 2017). In Alemi et al. (2017), the deep VIB model has the same architecture as DBX in our case, and they showed robustness results on MNIST against FGSM and CW-L2. 

We quickly conducted the white-box MIM attack again but with epsilon = 0.9. This attack achieved 99.5% success rate on MNIST test data, and the success rate for adv inputs crafted on **corrected classified images** is 100%. Furthermore, the white-box PGD can achieve 99.8% success rate on DBX for epsilon = 0.5 (see Figure 3 and table E.2). 

Additionally, our results for CIFAR-binary and CIFAR-10 fusion model show that DBX is less robust than other generative classifiers. E.g. on CIFAR-binary, white-box MIM with epsilon=0.2 achieved 99.9% success rate against DBX (see Table E.7).

Lastly, our quantitative evaluation on the bottleneck effect (Figure C.4) show that DBX with dim(z)=128 failed to evade white-box MIM attacks. The main text results are for DBX with dim(z)=64, which has stronger bottleneck effect.

Combining all the above observations, we would conclude that gradient masking is unlikely to exist for DBX. The robustness of DBX against FGSM &amp; MIM with reasonably big epsilon only applies for MNIST, and it requires a carefully selected bottleneck size (or in deep VIB, the beta coefficient that encourages the bottleneck effect).

From the above conclusion, the robustness &amp; detection results for DBX in the perfect-knowledge case has very little to do with gradient masking. 

We will add a paragraph explaining these in the appendix. Let us know if you have more questions.</span></div><div class="reply_row clearfix"></div></div><div class="children"></div></div></div></div></div></div><div class="note_with_children"><div id="note_SylS6FCITm" class="note panel"><div class="title_pdf_row clearfix"><h2 class="note_content_title"><a>Our work is concurrent to [1], and our work has significant novel contributions</a>  <button class="btn btn-xs btn-default permalink-button" title="Link to this comment" data-permalink-url="https://openreview.net/forum?id=HygUOoC5KX&amp;noteId=SylS6FCITm"><span class="glyphicon glyphicon-link" aria-hidden="true"></span></button></h2></div><div class="meta_row"><span class="signatures">ICLR 2019 Conference Paper356 Authors</span></div><div class="clearfix"><div class="meta_row pull-left"><span class="date item">12 Nov 2018 (modified: 20 Nov 2018)</span><span class="item">ICLR 2019 Conference Paper356 Official Comment</span><span class="item">Readers: <span class="readers-icon glyphicon glyphicon-globe"></span> Everyone</span></div><div class="meta_row meta_actions"></div></div><div class="note_contents"><span class="note_content_field">Comment: </span><span class="note_content_value">Thank you for pointing out the work in [1] to us. Our work is independent to [1], and the majority of the experiments (MNIST &amp; CIFAR binary) was done at the same time when v1 of [1] was released on ArXiv (May 2018). We have submitted evidences to the area chair. 

We were not aware of [1] until you kindly pointed it to us. We also found that [1] is currently under review at ICLR 2019 as well, so we feel that this somewhat excuses our lack of knowledge of a paper that can be at best described as concurrent work! 

Since our work is concurrent to [1], we believe our evaluation of generative classifiers on MNIST is a novel and significant contribution. **In the revised version we have a paragraph discussing the difference between our work and the concurrent work [1].** We would like to explain the main differences in the following:

1. We consider a greater number of discriminative and generative classifiers, showing how the factorization of the generative model is important.

2. We do not only evaluate on MNIST. We look at a CIFAR binary task. This is very important as Carlini and Wagner (2017b) showed that robustness properties shown on MNIST often do not hold on more complicated dataset, e.g. natural images. Our results on CIFAR-binary are consistent with MNIST results, showing that generative classifiers indeed have robustness properties distinct from discriminative ones.

3. We provide ideas and experiments on how to scale robust generative classifiers to more representative color datasets such as CIFAR-10, by building generative classifiers on pre-trained discriminative features. To the best of our knowledge, this is the first evaluation of the robustness on this fusion model, and we believe our results show that the combined approach offers an exciting research direction. 

4. Our approach gives data log-likelihood meaning to the logits for free, and these logit values can be used for detecting adversarial examples. This is different from previous approaches that require training an extra detector or generative model. Also since the classifier and the detector share the same generative model, detected adversarial inputs are indeed far away from the classifier’s manifold, thus we can use both accuracies and detection rates to verify the “off-manifold” conjecture.

5. Throughout training and adversarial evaluation we amortize the cost of inference and use K=10 Monte Carlo samples for the latent variable z. By contrast [1]’s method is much much slower: to classify a single image, their method requires K=8000 (!) initial z samples and then 50 gradient steps for z refinement. As these generative classifiers already require more computation over regular CNNs, this computation can often ill be afforded. Also the logit values in [1]’s classifier are ELBOs of log p(x|y) with a very simple Gaussian q distribution, which can be very different from the actual log p(x|y). Instead the logit values of our generative classifier are importance sampling estimates of log p(x|y) which are more accurate.
 
We thank you for your suggestion on the extra attacks to run. We agree that covering more attacks is always nice! However, we argue that we have already covered a very strong arsenal of attacks. This is evidenced when comparing to [1], as you can see that we evaluate the strongest attacks they find. Furthermore, we have also evaluated the C&amp;W attack, which they have not. We believe this is important as it has been demonstrated to be a very powerful attack (Carlini &amp; Wagner, 2017b), which is also corroborated by our work.

**In the revised version we also show that SPSA as a score based attack fails to fool generative classifiers, which also indicates that the robustness results are unlikely to be caused by gradient masking.** </span></div><div class="reply_row clearfix"></div></div><div class="children"></div></div></div></div><div class="note_with_children"><div id="note_ryl3wzTYn7" class="note panel"><div class="title_pdf_row clearfix"><h2 class="note_content_title"><a>Solid and insightful experimental study</a>  <button class="btn btn-xs btn-default permalink-button" title="Link to this comment" data-permalink-url="https://openreview.net/forum?id=HygUOoC5KX&amp;noteId=ryl3wzTYn7"><span class="glyphicon glyphicon-link" aria-hidden="true"></span></button></h2></div><div class="meta_row"><span class="signatures">ICLR 2019 Conference Paper356 AnonReviewer2</span></div><div class="clearfix"><div class="meta_row pull-left"><span class="date item">02 Nov 2018 (modified: 07 Nov 2018)</span><span class="item">ICLR 2019 Conference Paper356 Official Review</span><span class="item">Readers: <span class="readers-icon glyphicon glyphicon-globe"></span> Everyone</span></div><div class="meta_row meta_actions"></div></div><div class="note_contents"><span class="note_content_field">Review: </span><span class="note_content_value">This paper aims to test the robustness of generative classifiers [1] w.r.t. adversarial examples, considering their use as a potentially more robust alternative to adversarial training of discriminative classifiers. To achieve this, *Deep Bayes*, a generalization of the Naive Bayes classifier using a latent variable model and trained in a fashion similar to variational autoencoders [2] is introduced, and 7 different latent variable models are compared, covering a spectrum of generative or discriminative classification models, with or without bottlenecks. Their DFX and DBX architectures in particular closely match traditional discriminative classifiers, without and with a latent bottleneck.

These 7 models are compared against a large range of adversarial attacks, depending on the kind of noise added (l_2 or l_inf) and how much the adversary can access (the full gradients of the model, its output on training data, or only the model as a black-box). The performance of the models is assessed depending on two criteria: how the performance of the classifier resists to adversarial noise, and how quickly the model can detect adversarial samples. Three methods for detecting adversarial samples are compared: the first (only applicable to generative classifiers) discards samples with a low likelihood, according to the off-manifold assumption [3], the second discards samples for which the classifier has low confidence in its classification (p(y|x) is under some threshold), and the third compares the output probability vector of the classifier on a sample to the mean classification vector of this class over the train data, and discards the sample if the two vectors are too dissimilar (meaning the classifier is over-confident or under-confident).

The main contribution of this paper is the extensive experiments that have been done to compare the models against the various adversarial attacks. While experiments were only done on small datasets like MNIST and CIFAR (generative classifiers don't scale as easily on large image datasets), they nonetheless give very interesting insights and the authors provided encouraging results on applying generative classifiers on features learned by discriminative classifiers. Theirs result shows that generative architecture are in general more robust to the current state-of-the-art adversarial attacks, and detect adversarial examples more easily. The authors also recognize that these results may be biased by the fact that current adversarial attacks have been specifically optimized towards discriminative classifier.

This is a solid paper in my opinion. The experimental setup and motivations are clearly detailed, and the paper was easy to follow. Extensive results and description of the experimental protocol are provided in the appendices, giving me confidence that the results should be reproducible. The results of this paper give interesting insights regarding how to approach robustness to adversarial examples in classification tasks, and provide realistic ways to try and apply generative classifiers in real-worlds tasks, using pre-learned features from discriminative networks.


[1] <a href="http://papers.nips.cc/paper/2020-on-discriminative-vs-generative-classifiers-a-comparison-of-logistic-regression-and-naive-bayes.pdf" target="_blank" rel="nofollow">http://papers.nips.cc/paper/2020-on-discriminative-vs-generative-classifiers-a-comparison-of-logistic-regression-and-naive-bayes.pdf</a>
[2] https://arxiv.org/abs/1312.6114
[3] https://arxiv.org/abs/1801.02774</span></div><div class="note_contents"><span class="note_content_field">Rating: </span><span class="note_content_value">8: Top 50% of accepted papers, clear accept</span></div><div class="note_contents"><span class="note_content_field">Confidence: </span><span class="note_content_value">3: The reviewer is fairly confident that the evaluation is correct</span></div><div class="reply_row clearfix"></div></div><div class="children"><div class="note_with_children"><div id="note_Hkl63gkw67" class="note panel"><div class="title_pdf_row clearfix"><h2 class="note_content_title"><a>Thank you for your positive comments!</a>  <button class="btn btn-xs btn-default permalink-button" title="Link to this comment" data-permalink-url="https://openreview.net/forum?id=HygUOoC5KX&amp;noteId=Hkl63gkw67"><span class="glyphicon glyphicon-link" aria-hidden="true"></span></button></h2></div><div class="meta_row"><span class="signatures">ICLR 2019 Conference Paper356 Authors</span></div><div class="clearfix"><div class="meta_row pull-left"><span class="date item">12 Nov 2018</span><span class="item">ICLR 2019 Conference Paper356 Official Comment</span><span class="item">Readers: <span class="readers-icon glyphicon glyphicon-globe"></span> Everyone</span></div><div class="meta_row meta_actions"></div></div><div class="note_contents"><span class="note_content_field">Comment: </span><span class="note_content_value">Glad to hear that you like our manuscript!

The motivation of this work is to test whether recently developed attacks can break generative classifiers, and our results show that generative classifiers are more robust than discriminative classifiers against recent attacks. Sure research in adversarial attacks and defenses is similar to a "cat and mouse game", and we anticipate in the future an attack tailored to generative classifiers can break our models. But we have done the best to test mainstream attacks available now, and indeed we showed that generative classifiers have properties different from discriminative ones. We expect future work on developing attacks &amp; defenses on generative models can make generative models more powerful and robust!

An important insight of our approach is that the generative classifiers and the proposed detection methods are based on the same generative model. This means detected adversarial inputs are indeed far away from the classifier's manifold, and the classifier's manifold is also an approximation to the data manifold. By contrast, previous approaches require training an extra copy of generative model/auto-encoder. Thus it's very likely that the detector and the classifier do not have aligned manifold representations, thus the classifier cannot enjoy many benefits from the generative model.

We also wanted to encourage future research on combining generative and discriminative approaches. At least from our robustness test on CIFAR-10, this fusion approach indeed is worth further investigations, and it offers an exciting venue for future research.</span></div><div class="reply_row clearfix"></div></div><div class="children"></div></div></div></div></div></main></div>
  </div>
</div>


    <!-- Footer -->
    <footer class="sitemap">
      <div class="container">
        <div class="row hidden-xs">
          <div class="col-sm-4">
            <ul class="list-unstyled">
              <li><a href="/" class="home">Home</a></li>
              <li><a href="/venues">All Venues</a></li>
            </ul>
          </div>
    
          <div class="col-sm-4">
            <ul class="list-unstyled">
              <li><a href="/about">About OpenReview</a></li>
              <li><a href="/contact">Contact</a></li>
              <li><a href="#" data-toggle="modal" data-target="#feedback-modal">Feedback</a></li>
            </ul>
          </div>
    
          <div class="col-sm-4">
            <ul class="list-unstyled">
              <li><a href="/terms">Terms of Service</a></li>
              <li><a href="/privacy">Privacy Policy</a></li>
            </ul>
          </div>
        </div>
    
        <div class="row visible-xs-block">
          <div class="col-xs-6">
            <ul class="list-unstyled">
              <li><a href="/" class="home">Home</a></li>
              <li><a href="/about">About OpenReview</a></li>
              <li><a href="#" data-toggle="modal" data-target="#feedback-modal">Feedback</a></li>
            </ul>
          </div>
    
          <div class="col-xs-6">
            <ul class="list-unstyled">
              <li><a href="/contact">Contact</a></li>
              <li><a href="/terms">Terms of Service</a></li>
              <li><a href="/privacy">Privacy Policy</a></li>
            </ul>
          </div>
        </div>
      </div>
    </footer>
    
    <footer class="sponsor">
      <div class="container">
        <div class="row">
          <div class="col-sm-10 col-sm-offset-1">
            <p class="text-center">
              OpenReview is created by the <a href="http://www.iesl.cs.umass.edu/" target="_blank">Information Extraction and Synthesis Laboratory</a>, College of Information and Computer Science, University of Massachusetts Amherst. We gratefully acknowledge the support of the OpenReview sponsors:  Google,  Facebook, NSF, the University of Massachusetts Amherst Center for Data Science, and Center for Intelligent Information Retrieval, as well as the Google Cloud Platform for donating the computing and networking services on which OpenReview.net runs.
            </p>
          </div>
        </div>
      </div>
    </footer>

  <div id="feedback-modal" class="modal fade" tabindex="-1" role="dialog">
    <div class="modal-dialog">
      <div class="modal-content">
        <div class="modal-header">
          <button type="button" class="close" data-dismiss="modal" aria-label="Close"><span aria-hidden="true">×</span></button>
          <h3 class="modal-title">Send Feedback</h3>
        </div>
        <div class="modal-body">
          <p>Enter your feedback below and we'll get back to you as soon as possible.</p>
          <form action="/feedback" method="POST">
            <div class="form-group">
              <input type="email" name="from" class="form-control" placeholder="Email" />
            </div>
            <div class="form-group">
              <input type="text" name="subject" class="form-control" placeholder="Subject" />
            </div>
            <div class="form-group">
              <textarea name="message" class="form-control feedback-input" rows="5" placeholder="Message" required=""></textarea>
            </div>
          <ul id="ui-id-2" tabindex="0" class="ui-menu ui-widget ui-widget-content" style="display: none;"></ul></form>
        </div>
        <div class="modal-footer">
          <button type="button" class="btn btn-default" data-dismiss="modal">Cancel</button>
          <button type="button" class="btn btn-primary">Send</button>
        </div>
      </div>
    </div>
  </div>

  <script type="text/javascript" async="" src="https://www.google-analytics.com/analytics.js"></script><script src="//cdnjs.cloudflare.com/ajax/libs/jquery/2.2.4/jquery.min.js"></script>
  <script>window.jQuery || document.write('&lt;script src="/static/js/vendor/jquery-2.2.4.min.js"&gt;&lt;\/script&gt;')</script>

  <script src="/static/dist/vendor.min.js"></script>
  <script src="/static/dist/templates.min.js"></script>
  <script src="/static/dist/openreview.min.js"></script>

    <script>window.legacyScripts = true;</script>


    <script async="" src="https://www.googletagmanager.com/gtag/js?id=UA-108703919-1"></script>
    <script>
      window.dataLayer = window.dataLayer || [];
      function gtag() { dataLayer.push(arguments); }
      gtag('js', new Date());
      gtag('config', 'UA-108703919-1');
    </script>


<div role="status" aria-live="assertive" aria-relevant="additions" class="ui-helper-hidden-accessible"></div></body></html>