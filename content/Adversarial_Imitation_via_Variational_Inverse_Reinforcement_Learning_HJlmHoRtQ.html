<!DOCTYPE html><html xmlns="http://www.w3.org/1999/xhtml" class="no-js"><head>
  <meta charset="utf-8" />
  <meta http-equiv="X-UA-Compatible" content="IE=edge,chrome=1" />
  <meta name="viewport" content="width=device-width, initial-scale=1" />

  <title>Adversarial Imitation via Variational Inverse Reinforcement Learning | OpenReview</title>
  <meta name="description" content="" />

      <meta name="citation_title" content="Adversarial Imitation via Variational Inverse Reinforcement Learning" />
        <meta name="citation_author" content="Anonymous" />
      <meta name="citation_publication_date" content="2018/09/27" />
      <meta name="citation_online_date" content="2018/09/27" />
      <meta name="citation_pdf_url" content="https://openreview.net/pdf?id=HJlmHoR5tQ" />
      <meta name="twitter:card" content="summary" />
      <meta name="twitter:site" content="@openreviewnet" />
      <meta name="og:title" content="Adversarial Imitation via Variational Inverse Reinforcement Learning" />
      <meta name="og:description" content="We consider a problem of learning the reward and policy from expert examples under unknown dynamics in high-dimensional scenarios. Our proposed method builds on the framework of generative..." />
      <meta name="og:image" content="https://openreview.net/static/images/openreview_logo_512.png" />

  <link rel="stylesheet" href="https://fonts.googleapis.com/css?family=Noto+Sans:400,400i,700,700i" />

  <link rel="stylesheet" href="/static/css/bootstrap.min.css" />
  <link rel="stylesheet" href="/static/css/jquery-ui.min.css" />
  <link rel="stylesheet" href="/static/css/main.min.css" />
</head>

<body class="forum">
  <nav class="navbar navbar-inverse navbar-fixed-top" role="navigation">
    <div class="container">
  
      <div class="navbar-header">
        <button type="button" class="navbar-toggle collapsed" data-toggle="collapse" data-target="#navbar" aria-expanded="false" aria-controls="navbar">
          <span class="sr-only">Toggle navigation</span>
          <span class="icon-bar"></span>
          <span class="icon-bar"></span>
          <span class="icon-bar"></span>
        </button>
        <a class="navbar-brand home push-link" href="/"><strong>OpenReview</strong>.net</a>
      </div>
  
      <div id="navbar" class="navbar-collapse collapse">
        <form class="navbar-form navbar-left profile-search" role="search">
          <div class="form-group has-feedback">
            <input id="search_input" type="text" class="form-control ui-autocomplete-input" placeholder="Search ICLR 2019 Conference" autocomplete="off" />
            <span class="glyphicon glyphicon-search form-control-feedback" aria-hidden="true"></span>
          </div>
  
          <input id="search_group" type="hidden" value="ICLR.cc/2019/Conference" />
          <input id="search_content" type="hidden" value="all" />
        <ul id="ui-id-1" tabindex="0" class="ui-menu ui-widget ui-widget-content ui-autocomplete ui-front" style="display: none;"></ul></form>
  
        <ul class="nav navbar-nav navbar-right">
        
            <li id="user-menu"><a href="/login">Login</a></li>
        </ul>
      </div>
  
    </div>
  </nav>

  <div id="or-banner" class="banner" style="">
  <div class="container">
    <div class="row">
      <div class="col-xs-12"><a href="/group?id=ICLR.cc/2019/Conference" title="Venue Homepage"><img class="icon" src="/static/images/arrow_left.svg" /> Go to <strong>ICLR 2019 Conference</strong> homepage</a></div>
    </div>
  </div>
</div>
<div id="flash-message-container" class="alert alert-danger" role="alert" style="display: none;">
  <div class="container">
    <div class="row">
      <div class="col-xs-12">
        <div class="alert-content">
          <button type="button" class="close" aria-label="Close"><span aria-hidden="true">×</span></button>
        </div>
      </div>
    </div>
  </div>
</div>

<div class="container">
  <div class="row">
    <div class="col-xs-12">
      <main id="content" class="clearfix openbanner-visible legacy-styles"><div id="note_HJlmHoR5tQ" class="note panel"><div class="title_pdf_row clearfix"><h2 class="note_content_title"><a>Adversarial Imitation via Variational Inverse Reinforcement Learning</a> <a class="note_content_pdf" href="/pdf?id=HJlmHoR5tQ" title="Download PDF" target="_blank"><img src="/static/images/pdf_icon_blue.svg" /></a> </h2></div><div class="meta_row"><span class="signatures">Anonymous</span></div><div class="clearfix"><div class="meta_row pull-left"><span class="date item">28 Sep 2018 (modified: 17 Nov 2018)</span><span class="item">ICLR 2019 Conference Blind Submission</span><span class="item">Readers: <span class="readers-icon glyphicon glyphicon-globe"></span> Everyone</span><span class="item"><a class="action-bibtex-modal" data-bibtex="@inproceedings{    &#10;anonymous2019adversarial,    &#10;title={Adversarial Imitation via Variational Inverse Reinforcement Learning},    &#10;author={Anonymous},    &#10;booktitle={Submitted to International Conference on Learning Representations},    &#10;year={2019},    &#10;url={https://openreview.net/forum?id=HJlmHoR5tQ},    &#10;note={under review}    &#10;}">Show Bibtex</a></span><a class="note_content_pdf item" href="/revisions?id=HJlmHoR5tQ" target="_blank">Show Revisions</a></div><div class="meta_row meta_actions"></div></div><div class="note_contents"><span class="note_content_field">Abstract: </span><span class="note_content_value">We consider a problem of learning the reward and policy from expert examples under unknown dynamics in high-dimensional scenarios. Our proposed method builds on the framework of generative adversarial networks and introduces the empowerment-regularized maximum-entropy inverse reinforcement learning to learn near-optimal rewards and policies. Empowerment-based regularization prevents the policy from overfitting expert demonstration, thus leads to a generalized behavior which results in learning near-optimal rewards. Our method simultaneously learns empowerment through variational information maximization along with the reward and policy under the adversarial learning formulation. We evaluate our approach on various high-dimensional complex control tasks. We also test our learned rewards in challenging transfer learning problems where training and testing environments are made to be different from each other in terms of dynamics or structure. Our experimentation shows that our proposed method not only learns near-optimal rewards and policies that are matching expert behavior but also performs significantly better than state-of-the-art inverse reinforcement learning algorithms.</span></div><div class="note_contents"><span class="note_content_field">TL;DR: </span><span class="note_content_value">Our proposed method builds on GANs and exploits potential-based reward shaping to learn near-optimal rewards and policies from expert demonstrations.</span></div><div class="note_contents"><span class="note_content_field">Keywords: </span><span class="note_content_value">Inverse Reinforcement Learning, Imitation learning, Variational lnference, Learning from demonstrations</span></div><div class="reply_row clearfix"><div class="item" id="reply_count">9 Replies</div></div></div><hr class="small" /><div id="note_children"><div class="note_with_children"><div id="note_rJlvBZqP6X" class="note panel"><div class="title_pdf_row clearfix"><h2 class="note_content_title"><a>General response to the reviewers </a>  <button class="btn btn-xs btn-default permalink-button" title="Link to this comment" data-permalink-url="https://openreview.net/forum?id=HJlmHoR5tQ&amp;noteId=rJlvBZqP6X"><span class="glyphicon glyphicon-link" aria-hidden="true"></span></button></h2></div><div class="meta_row"><span class="signatures">ICLR 2019 Conference Paper72 Authors</span></div><div class="clearfix"><div class="meta_row pull-left"><span class="date item">13 Nov 2018 (modified: 15 Nov 2018)</span><span class="item">ICLR 2019 Conference Paper72 Official Comment</span><span class="item">Readers: <span class="readers-icon glyphicon glyphicon-globe"></span> Everyone</span></div><div class="meta_row meta_actions"></div></div><div class="note_contents"><span class="note_content_field">Comment: </span><span class="note_content_value">We like to thank the anonymous reviewers for their helpful and constructive comments. We provide the individual response to each reviewer's comments. Here we report the list of main changes which we have added to the new revision.

1- We motivate our method through Empowerment-Regularized Maximum Entropy IRL.
2- A discussion on the policy update rule which maximizes both the learned reward function and Empowerment (Appendix B). To leave the derivation simple, we have modified the equation (6) to absolute error instead of the mean-square error, and all experimental results are updated accordingly.
3- Further clarifications on why state-action formulation of reward function is vital to both reward and policy learning (Section 5, Paragraph 3).
4- Further explanations on transfer learning tasks that we use standard RL formulation using only learned rewards, no empowerment to train the agents.
5-Addressed all typological errors mentioned by the reviewers.
</span></div><div class="reply_row clearfix"></div></div><div class="children"></div></div><div class="note_with_children"><div id="note_SJdOg6R3Q" class="note panel"><div class="title_pdf_row clearfix"><h2 class="note_content_title"><a>Good empirical results, but overall lacking in clarity with potentially problematic issues</a>  <button class="btn btn-xs btn-default permalink-button" title="Link to this comment" data-permalink-url="https://openreview.net/forum?id=HJlmHoR5tQ&amp;noteId=SJdOg6R3Q"><span class="glyphicon glyphicon-link" aria-hidden="true"></span></button></h2></div><div class="meta_row"><span class="signatures">ICLR 2019 Conference Paper72 AnonReviewer1</span></div><div class="clearfix"><div class="meta_row pull-left"><span class="date item">06 Nov 2018 (modified: 07 Nov 2018)</span><span class="item">ICLR 2019 Conference Paper72 Official Review</span><span class="item">Readers: <span class="readers-icon glyphicon glyphicon-globe"></span> Everyone</span></div><div class="meta_row meta_actions"></div></div><div class="note_contents"><span class="note_content_field">Review: </span><span class="note_content_value">Summary/Contribution:
This paper builds on the AIRL framework (Fu et al., 2017) by combining the empowerment maximization objective for optimizing both the policy and reward function. Algorithmically, the main difference is that this introduces the need to optimize a inverse model (q), an empowerment function (Phi) and alters the AIRL updates to the reward function and policy. This paper presents experiments on the original set of AIRL tasks, and shows improved performance on some tasks.

Pros:
    - The approach outperform AIRL by a convincing margin on the crippled ant problem, while obtaining comparable/favorable performance on other benchmarks.

Cons:
    - The justification for using the empowerment maximization framework to learn the shaping parameters is unclear. The formulation introduces a potentially confounding factor by biasing the policy optimization which clouds the experimental picture. 

Justification for rating:
This paper presents good empirical results, but without a clear identification of the source of improvement. I lean on the side of rejecting unless the authors can better eliminate any potential bias in their formulation (see question below). The justification for combining the empowerment maximization objective is also unclear while being integral to the novelty of the proposed method. 

Questions I could not resolve from my reading:
    - The "imitation learning benchmark" numbers in Table 2 are different from the original AIRL paper. Do the authors have an explanation as to why? Is this only due to a difference in the expert performance?
    - Can the authors confirm that in the transfer experiments, the policy is optimized with only the transfered reward and no empowerment bonus? Otherwise, can the authors comment on whether the performance benefits could be explained by the additional bonus.
    - In equation (12), \Phi is optimized as an (approximate) mutual information, not a value function, so it is not clear why this term approximates the advantage (I suspect this is untrue in EAIRL as V* is recovered at optimality in the AIRL/GAN-GCL formulation). Can the authors comment?
    - Why is w* unnormalized? Unless I am misunderstanding something, in the definition immediately above it, there is a normalization term Z(s). 

Other comments:
    - "AIRL(s, a) fails to learn rewards whereas EAIRL recovers the near optimal rewards function" -&gt; This characterization is strange since on some tasks AIRL(s,a) outperforms or is within one standard deviation of EAIRL (e.g. on Half Cheetah, Ant, Swimmer, Pendulum).
    -  "Our experimentation highlights the importance of modeling discriminator/reward functions.. as a function of both state and action". AIRL(s) is better on both the pointmass and crippled-ant task than AIRL(s,a). Can the authors clarify?
    - "Our method leverages .. and therefore learns both reward and policy simultaneously". Can the authors clarify in what sense the reward and policy is being learned simultaneously in EAIRL where it is not in AIRL?
    - In all the tables, the authors' approach is bolded as oppose to the best numbers. I would instead prefer that the authors bold the best numbers to avoid confusion.

- Typos:
    - "the imitation learning methods were proposed"
    - "quantify an extent to which" 
    - "GAIL uses Generative Adversarial Networks formulation"
    - "grantee"
    - "no prior work has reported the practical approach"
    - "but, to"
    - "(see (Fu et al., 2017))"
</span></div><div class="note_contents"><span class="note_content_field">Rating: </span><span class="note_content_value">5: Marginally below acceptance threshold</span></div><div class="note_contents"><span class="note_content_field">Confidence: </span><span class="note_content_value">4: The reviewer is confident but not absolutely certain that the evaluation is correct</span></div><div class="reply_row clearfix"></div></div><div class="children"><div class="note_with_children"><div id="note_BJlixw6ep7" class="note panel"><div class="title_pdf_row clearfix"><h2 class="note_content_title"><a>Response to Reviewer1</a>  <button class="btn btn-xs btn-default permalink-button" title="Link to this comment" data-permalink-url="https://openreview.net/forum?id=HJlmHoR5tQ&amp;noteId=BJlixw6ep7"><span class="glyphicon glyphicon-link" aria-hidden="true"></span></button></h2></div><div class="meta_row"><span class="signatures">ICLR 2019 Conference Paper72 Authors</span></div><div class="clearfix"><div class="meta_row pull-left"><span class="date item">08 Nov 2018</span><span class="item">ICLR 2019 Conference Paper72 Official Comment</span><span class="item">Readers: <span class="readers-icon glyphicon glyphicon-globe"></span> Everyone</span></div><div class="meta_row meta_actions"></div></div><div class="note_contents"><span class="note_content_field">Comment: </span><span class="note_content_value">We would like to thank our reviewer for such comprehensive reviews.  The response summaries are as follow.

1: The "imitation learning benchmark" numbers in Table 2 are different from the original AIRL paper. Do the authors have an explanation as to why? Is this only due to a difference in the expert performance?

Response: Yes, the different values are because of the difference in expert performances. For instance, if you notice half-cheetah in our results Table 2, and in AIRL(s,a) (Fu et al., 2017), the results are similar as experts performed comparably.

2: Can the authors confirm that in the transfer experiments, the policy is optimized with only the transferred reward and no empowerment bonus? Otherwise, can the authors comment on whether the performance benefits could be explained by the additional bonus.

Response: Yes, the policy is optimized using the transferred reward only (no empowerment bonus) using standard reinforcement learning approach.

3: In equation (12), \Phi is optimized as an (approximate) mutual information, not a value function, so it is not clear why this term approximates the advantage (I suspect this is untrue in EAIRL as V* is recovered at optimality in the AIRL/GAN-GCL formulation). Can the authors comment?

Response: Yes, you are right, equation 12 doesn’t hold for the proposed method. 

4: Why is w* unnormalized? Unless I am misunderstanding something, in the definition immediately above it, there is a normalization term Z(s).

Response: Although w* is defined to be normalized by Z(s), however, there is no direct mechanism for sampling actions or computing Z(s). Therefore, w* is implicitly unnormalized, for more details, please refer to 4.2.2 of ( Mohamed &amp; Rezende 2015).

5: "AIRL(s, a) fails to learn rewards whereas EAIRL recovers the near optimal rewards function" -&gt; This characterization is strange since on some tasks AIRL(s,a) outperforms or is within one standard deviation of EAIRL (e.g. on Half Cheetah, Ant, Swimmer, Pendulum).

Response: The paper attempts to solve two separate problems, i.e., 1) policy learning and 2) reward learning. For instance, GAIL only solves the policy learning problem and does not recover a reward function. Likewise, AIRL (s, a) can learn a policy (see Table 2) but fails to recover reward function (see Table 1) as it performs poorly on the transfer learning tasks.  

6: Our experimentation highlights the importance of modeling discriminator/reward functions.. as a function of both state and action". AIRL(s) is better on both the pointmass and crippled-ant task than AIRL(s,a). Can the authors clarify?

Response: Please refer to section 5 for details. We highlight the importance of modeling rewards as a function of states and actions in both reward and policy learning problems.
Policy learning:
The results show that AIRL with state-only rewards, AIRL(s), fails to learn a policy whereas EAIRL, GAIL, and AIRL that include state-action reward/discriminator formulation successfully recover the policies (see Table 2). Hence, our empirical results show that it is crucial to model reward/discriminator as a function of state-action as otherwise, adversarial imitation learning fails to retrieve policy from expert data. 
Reward learning:
The results in Table 1 shows that AIRL with state-only rewards (AIRL(s)) does not recover the action dependent terms of the ground-truth reward function that penalizes high torques. Therefore, the agent shows aggressive behavior and flips over after few steps (see the accompanying video). The formulation of rewards as a function of both states and actions is crucial for action regularization in any locomotion or ambulation tasks that discourage actions with large magnitudes. This need for action regularization is well known in optimal control literature and limits the use cases of a state-only reward function in most practical, real-life applications.

7: "Our method leverages .. and therefore learns both reward and policy simultaneously". Can the authors clarify in what sense the reward and policy is being learned simultaneously in EAIRL where it is not in AIRL?

Response: AIRL with state-action reward formulation (AIRL (s, a)) learns a policy but fails to recover a ground-truth reward function (see Table 1). To determine the reward function, AIRL restricts state-only reward formulation which might be suitable for learning the reward but fails to learn the expert-like behavior policy. Hence, AIRL requires state-only formulation for reward learning and state-action formulation for policy learning whereas our method requires only state-action formulation to learn both rewards and policies from expert demonstrations. 

8: In all the tables, the authors' approach is bolded as opposed to the best numbers. I would instead prefer that the authors bold the best numbers to avoid confusion.
Response: Modifications made. 

9: Typos
Response: All the typo errors are removed. 
</span></div><div class="reply_row clearfix"></div></div><div class="children"></div></div></div></div><div class="note_with_children"><div id="note_rJgOYVPcnX" class="note panel"><div class="title_pdf_row clearfix"><h2 class="note_content_title"><a>AR3: Adversarial Imitation via Variational Inverse Reinforcement Learning</a>  <button class="btn btn-xs btn-default permalink-button" title="Link to this comment" data-permalink-url="https://openreview.net/forum?id=HJlmHoR5tQ&amp;noteId=rJgOYVPcnX"><span class="glyphicon glyphicon-link" aria-hidden="true"></span></button></h2></div><div class="meta_row"><span class="signatures">ICLR 2019 Conference Paper72 AnonReviewer3</span></div><div class="clearfix"><div class="meta_row pull-left"><span class="date item">03 Nov 2018 (modified: 07 Nov 2018)</span><span class="item">ICLR 2019 Conference Paper72 Official Review</span><span class="item">Readers: <span class="readers-icon glyphicon glyphicon-globe"></span> Everyone</span></div><div class="meta_row meta_actions"></div></div><div class="note_contents"><span class="note_content_field">Review: </span><span class="note_content_value">The authors propose empowerment-based adversarial inverse reinforcement learning (EAIRL), an extension of AIRL which uses empowerment (which quantifies the extent that an agent can influence its state, see eq. 3) as a reward-shaping potential to recover more faithful learned reward functions. 

Evaluation:     4/5     Experiments are more preliminary but establish the benefit of the approach.
Clarity:        4/5     Well written. Just a few typos (see below minor comments)
Significance:   4/5     Effective, well motivated approach. Excellent transfer learning results.
Originality:    3.5/5   As the empowerment subroutine is existing work, as is AIRL, combining previous work, but effectively.

Rating:         7/10
Confidence:     3/5     Reviewed this paper in a little less detail than I would prefer, due to time constraints. I will review in more detail and update this and add any additional questions/comments below the minor comments below.

Pros:
- Extension of AIRL which utilizes empowerment to advance the SOE in reward learning
- Well written, related previous work well explained.
Cons:
- Experiments more preliminary
- Combines existing approaches, somewhat incremental

Minor comments: 
- grantee (typo), barely utilized -&gt; not fully realized?, 
</span></div><div class="note_contents"><span class="note_content_field">Rating: </span><span class="note_content_value">7: Good paper, accept</span></div><div class="note_contents"><span class="note_content_field">Confidence: </span><span class="note_content_value">3: The reviewer is fairly confident that the evaluation is correct</span></div><div class="reply_row clearfix"></div></div><div class="children"><div class="note_with_children"><div id="note_rke1hVag6Q" class="note panel"><div class="title_pdf_row clearfix"><h2 class="note_content_title"><a>Response to reviewer3 </a>  <button class="btn btn-xs btn-default permalink-button" title="Link to this comment" data-permalink-url="https://openreview.net/forum?id=HJlmHoR5tQ&amp;noteId=rke1hVag6Q"><span class="glyphicon glyphicon-link" aria-hidden="true"></span></button></h2></div><div class="meta_row"><span class="signatures">ICLR 2019 Conference Paper72 Authors</span></div><div class="clearfix"><div class="meta_row pull-left"><span class="date item">08 Nov 2018</span><span class="item">ICLR 2019 Conference Paper72 Official Comment</span><span class="item">Readers: <span class="readers-icon glyphicon glyphicon-globe"></span> Everyone</span></div><div class="meta_row meta_actions"></div></div><div class="note_contents"><span class="note_content_field">Comment: </span><span class="note_content_value">We would like to thank our reviewer for positive feedback.  We would like to satisfy the reviewer concerns about the paper as follow.
Issue 1: Experiments more preliminary
Response: 
The transfer learning tasks are challenging. In the case of crippled-ant (see Appendix B.1), the standard ant can move sideways whereas the crippled-ant must rotate to move forward.  Similarly, in-case of point-mass (see Appendix B.2), the agent must take the opposite route compared to training environment to reach the target. These environments test our method for generalizability and ability to learn transferable/portable reward functions.
---------
Issue 2: Combines existing approaches, somewhat incremental
Response:
We agree that our method combines the existing approaches. However, the combination is not straightforward, and we combine two approaches in a novel way. In (Mohamed &amp; Rezende 2015), the method uses variational information maximization to learn the empowerment. Once empowerment is determined, it is used as an intrinsic motivation to train a reinforcement learning agent, and the results are presented in simple 2D environments. On the other hand, AIRL learns disentangled reward by restricting state-only reward function, which is a major drawback of their method. Our method uses variational information maximization to learn reward-shaping potential function as empowerment in parallel to learning the reward and policy from expert data, unlike (Mohamed &amp; Rezende 2015) where Empowerment is learned offline. As a result, our method successfully learns portable, near-optimal rewards without being restricted to learning state-only reward functions.  Furthermore, AIRL (Fu et al., 2017),) requires state-only formulation for reward learning and state-action formulation for policy learning whereas our method requires only state-action formulation to learn both rewards and policies from expert demonstrations.
</span></div><div class="reply_row clearfix"></div></div><div class="children"></div></div></div></div><div class="note_with_children"><div id="note_BklSaQRunm" class="note panel"><div class="title_pdf_row clearfix"><h2 class="note_content_title"><a>Good work, but the important aspects are not discussed</a>  <button class="btn btn-xs btn-default permalink-button" title="Link to this comment" data-permalink-url="https://openreview.net/forum?id=HJlmHoR5tQ&amp;noteId=BklSaQRunm"><span class="glyphicon glyphicon-link" aria-hidden="true"></span></button></h2></div><div class="meta_row"><span class="signatures">ICLR 2019 Conference Paper72 AnonReviewer2</span></div><div class="clearfix"><div class="meta_row pull-left"><span class="date item">02 Nov 2018 (modified: 07 Nov 2018)</span><span class="item">ICLR 2019 Conference Paper72 Official Review</span><span class="item">Readers: <span class="readers-icon glyphicon glyphicon-globe"></span> Everyone</span></div><div class="meta_row meta_actions"></div></div><div class="note_contents"><span class="note_content_field">Review: </span><span class="note_content_value">The paper proposes a method for inverse reinforcement learning based on AIRL. It's main contribution is that the shaping function is not learned while training the discriminator, but separately as an approximation of the empowerment (maximum mutual information). This shaping term aims to learn disentangled rewards without being restricted to learning state-only reward functions, which is a major restriction of AIRL.

The main weakness of the paper is, that it does not justify or motivate the main deviations compared to AIRL. The new objective for updating the policy is especially problematic because it does no longer correspond to the RL objective but includes an additional term that biases the policy towards actions that increase its empowerment. Although both terms of the update can be derived independently from an IRL and Empowerment perspective respectively, optimizing the sum was not derived from a common problem formulation. By combining these objectives, the learned reward function may lead to policies that fail to match the expert demonstration without such bias. This does not imply that the approach is not sound per se, however, simply presenting such update without any discussion is insufficient--especially given that it constitutes the main novelty of the approach. I think the paper would be much stronger if the update was derived from an empowerment-regularized IRL formulation. And even then, the implications of such bias/regularization would need to be properly discussed and evaluated, in particular with respect to the trade-off lambda, which--again--is hardly mentioned in the submission. I'm also not sure if the story of the paper works out; when we simply want to use empowerment as shaping term, why not use two separate policies for computing the empowerment and reward function respectively. Is the bias in the policy update maybe more important than the shaping term in the discriminator update for learning disentangled rewards?

Keeping these issues aside, I actually like the paper. It tackles the main drawback of AIRL and the idea seems quite nice. Having a reward function that does not actively induce actions that can be explained by empowerment, may not always be appropriate, but often enough it may be a sensible approach to get more generalizable reward functions. The paper is also well written with few typos. The parts that are discussed are clear and the experimental results seem fine as well (although more experiments on the reward transfer would be nice).

Minor notes:
I think there is a sign error in the policy update
Typo in the theorem, grantee should be guarantee

Question:
Please confirm that the reward transfer was learned with a standard RL formulation. Does the learned policy change, when we use the empowerment objective as well?</span></div><div class="note_contents"><span class="note_content_field">Rating: </span><span class="note_content_value">5: Marginally below acceptance threshold</span></div><div class="note_contents"><span class="note_content_field">Confidence: </span><span class="note_content_value">4: The reviewer is confident but not absolutely certain that the evaluation is correct</span></div><div class="reply_row clearfix"></div></div><div class="children"><div class="note_with_children"><div id="note_BJldKTtvpX" class="note panel"><div class="title_pdf_row clearfix"><h2 class="note_content_title"><a>Response to Reviewer2</a>  <button class="btn btn-xs btn-default permalink-button" title="Link to this comment" data-permalink-url="https://openreview.net/forum?id=HJlmHoR5tQ&amp;noteId=BJldKTtvpX"><span class="glyphicon glyphicon-link" aria-hidden="true"></span></button></h2></div><div class="meta_row"><span class="signatures">ICLR 2019 Conference Paper72 Authors</span></div><div class="clearfix"><div class="meta_row pull-left"><span class="date item">13 Nov 2018</span><span class="item">ICLR 2019 Conference Paper72 Official Comment</span><span class="item">Readers: <span class="readers-icon glyphicon glyphicon-globe"></span> Everyone</span></div><div class="meta_row meta_actions"></div></div><div class="note_contents"><span class="note_content_field">Comment: </span><span class="note_content_value">We would like to thank our reviewer for such comprehensive feedback. We have revised the manuscript to address reviewer comments. The response summaries are as follow:

Issue 1: Please confirm that the reward transfer was learned with a standard RL formulation.
Response:
Yes, we use standard RL formulation in reward transfer tasks, i.e., the policy is optimized with only the transferred reward and no empowerment bonus.
Issue 2: Does the learned policy change, when we use the empowerment objective as well? 
Response:
For the stated values of entropy (λ_h) and information-gain regularizers (λ_I), the policy maximizes the shaped reward and entropy. Shaping rewards induce a policy behavior that leads to learning a generalized reward function. Furthermore, our experiment shows that the policy converges to an expert-like (demonstrated) behavior despite that it maximizes both reward and empowerment. 

We include a derivation in the paper to highlight the impact of trade-off lambda on the policy bias towards maximizing the empowerment or imitating the expert behavior. To leave the derivation simple, we have modified the equation (6) to absolute error instead of the mean-square error, and all experimental results are updated accordingly. We have verified that the modification doesn’t impact the results since the purpose of equation (6) is to measure the discrepancy between forward and inverse models. In our paper, we show that the discriminative reward r ̂ simplifies to the following:

r ̂=log⁡[D(s,a,s' )] - log[⁡1-D(s,a,s' )]=f(s,a,s')-λ_h log⁡[π(a│s)]
⁡
The policy is trained to maximize r_π (s,a,s' )=r ̂(s,a,s')-λ_I L_I, that leads to following expression:

r_π (s,a,s' )=f(s,a,s' )+(λ_I-λ_h)log⁡π(a│s)-λ_I log⁡q(a│s,s' )+λ_I Φ(s)

Note that the inverse model q(⋅) is trained using the trajectories generated by the policy π(⋅) (see Algorithm 1) and both models learn distribution over actions. Therefore, maximizing the entropy of q(⋅) is equivalent to maximizing the entropy of π(⋅). Thus, the entropy terms can be combined together as:

r_π (s,a,s' )=f(s,a,s' )+λH(⋅)+λ_I Φ(s)

whereas λ is a function of λ_I and λ_h, and H(⋅) is the entropy. Since, f(s,a,s' )=r(s,a,s' )+γΦ(s' )-Φ(s). The overall policy update rule becomes:

r_π (s,a,s' )=r(s,a,s' )+γΦ(s' )-(1-λ_I)Φ(s)+λH(⋅)

Hence, when λ_h&lt;λ_I&lt;1, the policy objective will be to maximize the shaped reward as well as the entropy. For the stated values of λ_I and λ_h, the policy training is slightly biased toward maximizing the empowerment. The bias of our policy training towards maximizing the empowerment leads to a generalized policy behavior which results in robust reward learning. 
</span></div><div class="reply_row clearfix"></div></div><div class="children"><div class="note_with_children"><div id="note_r1xjTdf_TX" class="note panel"><div class="title_pdf_row clearfix"><h2 class="note_content_title"><a>Still too much discrepancy between the text and the algorithm</a>  <button class="btn btn-xs btn-default permalink-button" title="Link to this comment" data-permalink-url="https://openreview.net/forum?id=HJlmHoR5tQ&amp;noteId=r1xjTdf_TX"><span class="glyphicon glyphicon-link" aria-hidden="true"></span></button></h2></div><div class="meta_row"><span class="signatures">ICLR 2019 Conference Paper72 AnonReviewer2</span></div><div class="clearfix"><div class="meta_row pull-left"><span class="date item">13 Nov 2018</span><span class="item">ICLR 2019 Conference Paper72 Official Comment</span><span class="item">Readers: <span class="readers-icon glyphicon glyphicon-globe"></span> Everyone</span></div><div class="meta_row meta_actions"></div></div><div class="note_contents"><span class="note_content_field">Comment: </span><span class="note_content_value">Thanks for the new derivation. I think it sheds some more light on the policy bias, although I think that setting the inverse model equal to the current policy is going too far and it does not really make sense to talk about "maximizing the entropy of q(.)" given that q is a variational distribution that is fixed during the policy improvement. 
However, treating the last equation of Appendix B as a rough approximation of the actual objective that is maximized by the policy updates and further assuming that lambda_I=0.99 is close enough to 1, we can see that the policy roughly optimizes "reward + next empowerment". I wonder whether, we could show similar generalization benefits by directly optimizing this objective, e.g. the discriminator could be computed as exp(r+\gamma*\Phi(s'))/(exp(r+\gamma*\Phi(s'))+\pi) and a standard TRPO/PPO update could be used. According to the derivations in Appendix B, this should roughly correspond to the same algorithm. Let's say we can get similar results (potentially replacing \gamma by a hyper-parameter and using higher entropy regularization), such algorithm could be derived in a principled way--from and empowerment-regularized MaxEnt-IRL formulation.
In the submitted version, there is a huge discrepancy between the text (generalization is achieved by using empowerment as potential for reward (un)shaping) and the actual algorithm (generalization is achieved by 1% of reward shaping and 99% of policy biasing). These are two completely different approaches; the former does not affect the learned policy (at least in theory) whereas the latter approach relates to regularization and has the potential to lead to much better generalization by preventing overfitting the demonstrations. As these are different approaches, deriving the algorithm from a reward shaping (better: "advantage unshaping") perspective can not be fully sound--which ultimately manifests in the form of a modified policy update rule which is not properly derived. From a reward shaping perspective, the policy for computing the empowerment should not be related at all to the policy that maximizes the reward.
I think there is not much missing to turn the submission into a nice paper (if my suggested variant would work out of the box, it might even be possible to revise the submission), however, in the current state the submission is in my opinion not sufficiently sound and almost dangerous, because it gives a wrong impression about the way generalization is achieved.</span></div><div class="reply_row clearfix"></div></div><div class="children"><div class="note_with_children"><div id="note_HJeiRvOqTm" class="note panel"><div class="title_pdf_row clearfix"><h2 class="note_content_title"><a>Response to Reviewer2: We motivate our work based on Empowerment-regularized MaxEnt-IRL</a>  <button class="btn btn-xs btn-default permalink-button" title="Link to this comment" data-permalink-url="https://openreview.net/forum?id=HJlmHoR5tQ&amp;noteId=HJeiRvOqTm"><span class="glyphicon glyphicon-link" aria-hidden="true"></span></button></h2></div><div class="meta_row"><span class="signatures">ICLR 2019 Conference Paper72 Authors</span></div><div class="clearfix"><div class="meta_row pull-left"><span class="date item">15 Nov 2018</span><span class="item">ICLR 2019 Conference Paper72 Official Comment</span><span class="item">Readers: <span class="readers-icon glyphicon glyphicon-globe"></span> Everyone</span></div><div class="meta_row meta_actions"></div></div><div class="note_contents"><span class="note_content_field">Comment: </span><span class="note_content_value">We thank our anonymous reviewer for providing comprehensive and constructive feedback which helped us significantly improve the quality of our paper. We agree with the reviewer, and all modifications have been made to pivot our work around Empowerment-regularized MaxEnt-IRL.
 
In the paper (Appendix B), we include the derivation of Empowerment-regularized MaxEnt-IRL. It is highlighted that under empowerment regularization, the policy/importance-sampler is trained to minimize its divergence from the true distribution over expert demonstrations and to maximize the empowerment. The resulting policy update rule (see Eqn. 14 in the paper) becomes:

max_π⁡ E_π [∑_(t=0)^T r(s,a)+Φ(s' )-log⁡π(a|s)]

In Appendix B.1, we show that our policy training objective r_π is equivalent to above equation, i.e.,

r_π (s,a,s' )=log⁡[D(s,a,s' )]-  log[(⁡1-D(s,a,s' )) ]- λ_I L_I      (1)
r_π (s,a,s' )=r(s,a,s' )+γΦ(s' )+λH(⋅)      (2)

whereas γ and λ are hyperparameters and H(⋅) contains the entropy terms.

Reviewer’s comment 1: The policy roughly optimizes "reward + next empowerment." I wonder whether we could show similar generalization benefits by directly optimizing this objective.

Response: We have verified by rerunning the experiments using the above-mentioned simplified policy objective, and it turns out that we obtain the same generalization as obtained by optimizing (1).  Hence, just as our reviewer expected, the empowerment-based regularization prevents the policy from overfitting expert demonstration, thus leads to a generalized behavior which results in learning near-optimal rewards.

Reviewer’s comment 2: In the submitted version, there is a huge discrepancy between the text and the actual algorithm.

Response: The discrepancy has been removed. The revised paper now motivates the algorithm based on the notion of Empowerment-regularized MaxEnt-IRL. 
</span></div><div class="reply_row clearfix"></div></div><div class="children"></div></div></div></div></div></div></div></div></div></main></div>
  </div>
</div>


    <!-- Footer -->
    <footer class="sitemap">
      <div class="container">
        <div class="row hidden-xs">
          <div class="col-sm-4">
            <ul class="list-unstyled">
              <li><a href="/" class="home">Home</a></li>
              <li><a href="/venues">All Venues</a></li>
            </ul>
          </div>
    
          <div class="col-sm-4">
            <ul class="list-unstyled">
              <li><a href="/about">About OpenReview</a></li>
              <li><a href="/contact">Contact</a></li>
              <li><a href="#" data-toggle="modal" data-target="#feedback-modal">Feedback</a></li>
            </ul>
          </div>
    
          <div class="col-sm-4">
            <ul class="list-unstyled">
              <li><a href="/terms">Terms of Service</a></li>
              <li><a href="/privacy">Privacy Policy</a></li>
            </ul>
          </div>
        </div>
    
        <div class="row visible-xs-block">
          <div class="col-xs-6">
            <ul class="list-unstyled">
              <li><a href="/" class="home">Home</a></li>
              <li><a href="/about">About OpenReview</a></li>
              <li><a href="#" data-toggle="modal" data-target="#feedback-modal">Feedback</a></li>
            </ul>
          </div>
    
          <div class="col-xs-6">
            <ul class="list-unstyled">
              <li><a href="/contact">Contact</a></li>
              <li><a href="/terms">Terms of Service</a></li>
              <li><a href="/privacy">Privacy Policy</a></li>
            </ul>
          </div>
        </div>
      </div>
    </footer>
    
    <footer class="sponsor">
      <div class="container">
        <div class="row">
          <div class="col-sm-10 col-sm-offset-1">
            <p class="text-center">
              OpenReview is created by the <a href="http://www.iesl.cs.umass.edu/" target="_blank">Information Extraction and Synthesis Laboratory</a>, College of Information and Computer Science, University of Massachusetts Amherst. We gratefully acknowledge the support of the OpenReview sponsors:  Google,  Facebook, NSF, the University of Massachusetts Amherst Center for Data Science, and Center for Intelligent Information Retrieval, as well as the Google Cloud Platform for donating the computing and networking services on which OpenReview.net runs.
            </p>
          </div>
        </div>
      </div>
    </footer>

  <div id="feedback-modal" class="modal fade" tabindex="-1" role="dialog">
    <div class="modal-dialog">
      <div class="modal-content">
        <div class="modal-header">
          <button type="button" class="close" data-dismiss="modal" aria-label="Close"><span aria-hidden="true">×</span></button>
          <h3 class="modal-title">Send Feedback</h3>
        </div>
        <div class="modal-body">
          <p>Enter your feedback below and we'll get back to you as soon as possible.</p>
          <form action="/feedback" method="POST">
            <div class="form-group">
              <input type="email" name="from" class="form-control" placeholder="Email" />
            </div>
            <div class="form-group">
              <input type="text" name="subject" class="form-control" placeholder="Subject" />
            </div>
            <div class="form-group">
              <textarea name="message" class="form-control feedback-input" rows="5" placeholder="Message" required=""></textarea>
            </div>
          <ul id="ui-id-2" tabindex="0" class="ui-menu ui-widget ui-widget-content" style="display: none;"></ul></form>
        </div>
        <div class="modal-footer">
          <button type="button" class="btn btn-default" data-dismiss="modal">Cancel</button>
          <button type="button" class="btn btn-primary">Send</button>
        </div>
      </div>
    </div>
  </div>

  <script type="text/javascript" async="" src="https://www.google-analytics.com/analytics.js"></script><script src="//cdnjs.cloudflare.com/ajax/libs/jquery/2.2.4/jquery.min.js"></script>
  <script>window.jQuery || document.write('&lt;script src="/static/js/vendor/jquery-2.2.4.min.js"&gt;&lt;\/script&gt;')</script>

  <script src="/static/dist/vendor.min.js"></script>
  <script src="/static/dist/templates.min.js"></script>
  <script src="/static/dist/openreview.min.js"></script>

    <script>window.legacyScripts = true;</script>


    <script async="" src="https://www.googletagmanager.com/gtag/js?id=UA-108703919-1"></script>
    <script>
      window.dataLayer = window.dataLayer || [];
      function gtag() { dataLayer.push(arguments); }
      gtag('js', new Date());
      gtag('config', 'UA-108703919-1');
    </script>


<div role="status" aria-live="assertive" aria-relevant="additions" class="ui-helper-hidden-accessible"></div></body></html>