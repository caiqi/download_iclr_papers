<!DOCTYPE html><html xmlns="http://www.w3.org/1999/xhtml" class="no-js"><head>
  <meta charset="utf-8" />
  <meta http-equiv="X-UA-Compatible" content="IE=edge,chrome=1" />
  <meta name="viewport" content="width=device-width, initial-scale=1" />

  <title>Label Smoothing and Logit Squeezing: A Replacement for Adversarial Training? | OpenReview</title>
  <meta name="description" content="" />

      <meta name="citation_title" content="Label Smoothing and Logit Squeezing: A Replacement for Adversarial Training?" />
        <meta name="citation_author" content="Anonymous" />
      <meta name="citation_publication_date" content="2018/09/27" />
      <meta name="citation_online_date" content="2018/09/27" />
      <meta name="citation_pdf_url" content="https://openreview.net/pdf?id=BJlr0j0ctX" />
      <meta name="twitter:card" content="summary" />
      <meta name="twitter:site" content="@openreviewnet" />
      <meta name="og:title" content="Label Smoothing and Logit Squeezing: A Replacement for Adversarial..." />
      <meta name="og:description" content="Adversarial training is one of the strongest defenses against adversarial attacks, but it requires adversarial examples to be generated for every mini-batch during optimization.  The expense of..." />
      <meta name="og:image" content="https://openreview.net/static/images/openreview_logo_512.png" />

  <link rel="stylesheet" href="https://fonts.googleapis.com/css?family=Noto+Sans:400,400i,700,700i" />

  <link rel="stylesheet" href="/static/css/bootstrap.min.css" />
  <link rel="stylesheet" href="/static/css/jquery-ui.min.css" />
  <link rel="stylesheet" href="/static/css/main.min.css" />
</head>

<body class="forum">
  <nav class="navbar navbar-inverse navbar-fixed-top" role="navigation">
    <div class="container">
  
      <div class="navbar-header">
        <button type="button" class="navbar-toggle collapsed" data-toggle="collapse" data-target="#navbar" aria-expanded="false" aria-controls="navbar">
          <span class="sr-only">Toggle navigation</span>
          <span class="icon-bar"></span>
          <span class="icon-bar"></span>
          <span class="icon-bar"></span>
        </button>
        <a class="navbar-brand home push-link" href="/"><strong>OpenReview</strong>.net</a>
      </div>
  
      <div id="navbar" class="navbar-collapse collapse">
        <form class="navbar-form navbar-left profile-search" role="search">
          <div class="form-group has-feedback">
            <input id="search_input" type="text" class="form-control ui-autocomplete-input" placeholder="Search ICLR 2019 Conference" autocomplete="off" />
            <span class="glyphicon glyphicon-search form-control-feedback" aria-hidden="true"></span>
          </div>
  
          <input id="search_group" type="hidden" value="ICLR.cc/2019/Conference" />
          <input id="search_content" type="hidden" value="all" />
        <ul id="ui-id-1" tabindex="0" class="ui-menu ui-widget ui-widget-content ui-autocomplete ui-front" style="display: none;"></ul></form>
  
        <ul class="nav navbar-nav navbar-right">
        
            <li id="user-menu"><a href="/login">Login</a></li>
        </ul>
      </div>
  
    </div>
  </nav>

  <div id="or-banner" class="banner" style="">
  <div class="container">
    <div class="row">
      <div class="col-xs-12"><a href="/group?id=ICLR.cc/2019/Conference" title="Venue Homepage"><img class="icon" src="/static/images/arrow_left.svg" /> Go to <strong>ICLR 2019 Conference</strong> homepage</a></div>
    </div>
  </div>
</div>
<div id="flash-message-container" class="alert alert-danger" role="alert" style="display: none;">
  <div class="container">
    <div class="row">
      <div class="col-xs-12">
        <div class="alert-content">
          <button type="button" class="close" aria-label="Close"><span aria-hidden="true">×</span></button>
        </div>
      </div>
    </div>
  </div>
</div>

<div class="container">
  <div class="row">
    <div class="col-xs-12">
      <main id="content" class="clearfix openbanner-visible legacy-styles"><div id="note_BJlr0j0ctX" class="note panel"><div class="title_pdf_row clearfix"><h2 class="note_content_title"><a>Label Smoothing and Logit Squeezing: A Replacement for Adversarial Training?</a> <a class="note_content_pdf" href="/pdf?id=BJlr0j0ctX" title="Download PDF" target="_blank"><img src="/static/images/pdf_icon_blue.svg" /></a> </h2></div><div class="meta_row"><span class="signatures">Anonymous</span></div><div class="clearfix"><div class="meta_row pull-left"><span class="date item">28 Sep 2018 (modified: 11 Oct 2018)</span><span class="item">ICLR 2019 Conference Blind Submission</span><span class="item">Readers: <span class="readers-icon glyphicon glyphicon-globe"></span> Everyone</span><span class="item"><a class="action-bibtex-modal" data-bibtex="@inproceedings{    &#10;anonymous2019label,    &#10;title={Label Smoothing and Logit Squeezing: A Replacement for Adversarial Training?},    &#10;author={Anonymous},    &#10;booktitle={Submitted to International Conference on Learning Representations},    &#10;year={2019},    &#10;url={https://openreview.net/forum?id=BJlr0j0ctX},    &#10;note={under review}    &#10;}">Show Bibtex</a></span></div><div class="meta_row meta_actions"></div></div><div class="note_contents"><span class="note_content_field">Abstract: </span><span class="note_content_value">Adversarial training is one of the strongest defenses against adversarial attacks, but it requires adversarial examples to be generated for every mini-batch during optimization.  The expense of producing these examples during training often precludes adversarial training from use on large and high-resolution image datasets.  In this study, we explore the mechanisms by which adversarial training improves classifier robustness, and show that these mechanisms can be effectively mimicked using simple regularization methods, including label smoothing and logit squeezing. Remarkably, using these simple regularization methods in combination with Gaussian noise injection, we are able to achieve strong adversarial robustness -- often exceeding that of adversarial training -- using no adversarial examples.</span></div><div class="note_contents"><span class="note_content_field">Keywords: </span><span class="note_content_value">adversarial machine learning, machine learning security</span></div><div class="note_contents"><span class="note_content_field">TL;DR: </span><span class="note_content_value">Achieving strong adversarial robustness and exceeding adversarial training without training on adversarial examples</span></div><div class="reply_row clearfix"><div class="item" id="reply_count">18 Replies</div></div></div><hr class="small" /><div id="note_children"><div class="note_with_children"><div id="note_rJeoRBA3hX" class="note panel"><div class="title_pdf_row clearfix"><h2 class="note_content_title"><a>Logit squeezing is (indeed) not robust</a>  <button class="btn btn-xs btn-default permalink-button" title="Link to this comment" data-permalink-url="https://openreview.net/forum?id=BJlr0j0ctX&amp;noteId=rJeoRBA3hX"><span class="glyphicon glyphicon-link" aria-hidden="true"></span></button></h2></div><div class="meta_row"><span class="signatures"><a href="/profile?id=~Marius_Mosbach1" class="profile-link">Marius Mosbach</a></span></div><div class="clearfix"><div class="meta_row pull-left"><span class="date item">05 Nov 2018</span><span class="item">ICLR 2019 Conference Paper892 Public Comment</span><span class="item">Readers: <span class="readers-icon glyphicon glyphicon-globe"></span> Everyone</span></div><div class="meta_row meta_actions"></div></div><div class="note_contents"><span class="note_content_field">Comment: </span><span class="note_content_value">I would like to point you to our paper <a href="https://arxiv.org/abs/1810.12042" target="_blank" rel="nofollow">https://arxiv.org/abs/1810.12042</a> where we show that logit squeezing (combined with gaussian noise), as proposed by Harini et al., does not provide actual robustness. We could successfully break it on MNIST, CIFAR10 and Tiny ImageNet. Further, we find that the robustness of logit squeezing mainly comes from the fact that it makes gradient based optimization in the input space significantly more difficult by introducing many local maxima near the clean inputs. This can be seen as gradient masking. Crucial for our evaluation was the fact that we performed many random restarts when performing PGD (up to 10000) and additionally performed a proper grid search over the step size used during optimization. So just increasing the number of iterations (as mentioned in the comments below) might not be sufficient to break a defense that significantly alters the input space loss surface. 

Therefore, it would be interesting to see the robustness of your models against a PGD attack with large number of iterations, large step size, and many random restarts. Based on our experiments, this should reduce the adversarial accuracy of these models down to (almost) 0%.
</span></div><div class="reply_row clearfix"></div></div><div class="children"></div></div><div class="note_with_children"><div id="note_HkepJPIth7" class="note panel"><div class="title_pdf_row clearfix"><h2 class="note_content_title"><a>Nice motivation and impressive empirical result. More experiments needed.</a>  <button class="btn btn-xs btn-default permalink-button" title="Link to this comment" data-permalink-url="https://openreview.net/forum?id=BJlr0j0ctX&amp;noteId=HkepJPIth7"><span class="glyphicon glyphicon-link" aria-hidden="true"></span></button></h2></div><div class="meta_row"><span class="signatures">ICLR 2019 Conference Paper892 AnonReviewer2</span></div><div class="clearfix"><div class="meta_row pull-left"><span class="date item">02 Nov 2018 (modified: 07 Nov 2018)</span><span class="item">ICLR 2019 Conference Paper892 Official Review</span><span class="item">Readers: <span class="readers-icon glyphicon glyphicon-globe"></span> Everyone</span></div><div class="meta_row meta_actions"></div></div><div class="note_contents"><span class="note_content_field">Review: </span><span class="note_content_value">This paper studies training adversarially robust models without generating adversarial examples online during training. This way, the training time can be significantly reduced. Firstly, the authors draw inspiration from linear (approximation of) classifier and work out the epsilon_L formula. Based on this, the authors proposes three ways of making the model more robust. And specifically, the authors focus on label smoothing and logit squeezing, because both of them have an effect of reducing the logit gap and shrinking the logit. Coupled with Gaussian Noise trick, both methods work pretty well. On MNIST, it come, to some extent, close to PGD7 trained models. On CIAFR10, using aggressive smoothing or squeezing hyper-parameters, it surpasses PGD7 trained models.

1. On Table 3, I would like to see PGD-100 steps and PGD-200 steps for xent loss and CW loss.

2. From Table 2,3,4, it seems like the accuracy varies a lot when the hyper-parameter changes. Is there a good rule of thumb on how to set up a good hyper-parameters in case of a new dataset? 

3. Have you checked if the accuracy goes down as the perturbation budget epsilon goes up, say 2, 4, 6, 8, 10, 12, using PGD20 or stronger attacks?

4. I would like to see results on CIFAR100, which is a harder dataset, containing 100 classes and 500 images per class. I think CIFAR10 alone is not sufficient for empirical justification nowadays (maybe it is enough one year ago). I don't know how the accuracy becomes under aggressive \alpha and \beta settings for 100-way classification. Also, evaluate them using PGD20, PGD100, PGD200 in order to make rigorous claims.

5. It seems that Gaussian trick is the key to good performance. Have you tried other form of random noise? Uniform or Laplace?
</span></div><div class="note_contents"><span class="note_content_field">Rating: </span><span class="note_content_value">6: Marginally above acceptance threshold</span></div><div class="note_contents"><span class="note_content_field">Confidence: </span><span class="note_content_value">5: The reviewer is absolutely certain that the evaluation is correct and very familiar with the relevant literature</span></div><div class="reply_row clearfix"></div></div><div class="children"></div></div><div class="note_with_children"><div id="note_HJgc9eAB3Q" class="note panel"><div class="title_pdf_row clearfix"><h2 class="note_content_title"><a>Interesting results on CIFAR-10, but how conclusive are they?</a>  <button class="btn btn-xs btn-default permalink-button" title="Link to this comment" data-permalink-url="https://openreview.net/forum?id=BJlr0j0ctX&amp;noteId=HJgc9eAB3Q"><span class="glyphicon glyphicon-link" aria-hidden="true"></span></button></h2></div><div class="meta_row"><span class="signatures">ICLR 2019 Conference Paper892 AnonReviewer3</span></div><div class="clearfix"><div class="meta_row pull-left"><span class="date item">30 Oct 2018 (modified: 07 Nov 2018)</span><span class="item">ICLR 2019 Conference Paper892 Official Review</span><span class="item">Readers: <span class="readers-icon glyphicon glyphicon-globe"></span> Everyone</span></div><div class="meta_row meta_actions"></div></div><div class="note_contents"><span class="note_content_field">Review: </span><span class="note_content_value">The paper addresses a highly relevant problem in Adversarial Machine Learning: devising alternatives to adversarial training that scale to high-dimensional datasets.

The proposed approach is a combination of known techniques: label smoothing, logit squeezing and Gaussian data augmentation. While those techniques had previously been studied in isolation, the authors argue that the combination thereof "saves the day", i.e. yielding high accuracy on clean data samples while exhibiting robustness against adversarial inputs.

The authors evaluate their approach on the MNIST and CIFAR-10 datasets. They compare against the adversarially trained and publicly available models by Madry et al (ICLR 2018). The results are mixed: on MNIST, Madry's model clearly outperforms the proposed approach in terms of adversarial robustness. On CIFAR-10, for a certain combination of logit squeezing and Gaussian data augmentation, the authors report results suggesting that their approach outperforms Madry's model both in terms of accuracy on clean samples and robustness against adversarial samples (white- and black-box). Specifically, the (alpha=0, beta=0, sigma=30, k=160k)-model in Table 3 achieves 90.5% accuracy on clean test samples (compared to Madry's 87.3%), 49.7% under white-box attacks (compared to 45.8%), and 67.0% under black-box attacks (compared to 64.2%).

Overall, a lot of the material in this manuscript didn't strike me as particularly deep or original. For instance, I am not convinced by the analysis "What does adversarial training do?" (Section 1) which is based on a linearity assumption, 
the importance of which isn't properly discussed.

The proposed alternative to adversarial training seems a bit unprincipled: the building blocks had been there before, but it appears the combination with specific parameter choices does the trick. It is a bit disconcerting that the approach doesn't perform that well for MNIST. A key selling point of the proposed approach is its computational efficiency; from that regard it would have been nice to see results on large-scale datasets like ImageNet. 

Having said all this, if the findings on CIFAR-10 indeed hold true, then I think this is an important result that is worth publishing at ICLR. It would indicate important directions for future research besides the mainstream work on adversarial training.

One concern that I have though is whether the CIFAR-10 models trained by the authors can withstand a wider range of attacks. To put it into perspective: Madry et al - in their CIFAR-10 challenge (<a href="https://github.com/MadryLab/cifar10_challenge)" target="_blank" rel="nofollow">https://github.com/MadryLab/cifar10_challenge)</a> - report 44.71% accuracy under white-box attacks as the worst-case result *under any epsilon-bounded attack* that researchers have tried since this model was published about a year ago. The 49.7% accuracy for the (alpha=0, beta=0, sigma=30, k=160k)-model in Table 3 is the worst-case result under only 4 different attacks. One has to question: what would the worst-case result for this model be in one year from now if the community had white-box access to it?

Is there any way the authors could make their models publicly available during the review period?

One specific question: Madry's 46.8% accuracy under the CW attack was obtained for 30-step PGD. Table 3 states that only 20-step PGD was performed. Can the authors shed more light on what exact setup they used for the CW attack and what - if not reported in Table 3 - was the result under a 30-step attack?

A minor comment:
- Figure 2 (a) vs Footnote 1: was 20- or 40-step PGD being used?</span></div><div class="note_contents"><span class="note_content_field">Rating: </span><span class="note_content_value">4: Ok but not good enough - rejection</span></div><div class="note_contents"><span class="note_content_field">Confidence: </span><span class="note_content_value">3: The reviewer is fairly confident that the evaluation is correct</span></div><div class="reply_row clearfix"></div></div><div class="children"></div></div><div class="note_with_children"><div id="note_Bke8Kbgb27" class="note panel"><div class="title_pdf_row clearfix"><h2 class="note_content_title"><a>Will not stand up to scrutiny</a>  <button class="btn btn-xs btn-default permalink-button" title="Link to this comment" data-permalink-url="https://openreview.net/forum?id=BJlr0j0ctX&amp;noteId=Bke8Kbgb27"><span class="glyphicon glyphicon-link" aria-hidden="true"></span></button></h2></div><div class="meta_row"><span class="signatures">ICLR 2019 Conference Paper892 AnonReviewer1</span></div><div class="clearfix"><div class="meta_row pull-left"><span class="date item">27 Oct 2018 (modified: 07 Nov 2018)</span><span class="item">ICLR 2019 Conference Paper892 Official Review</span><span class="item">Readers: <span class="readers-icon glyphicon glyphicon-globe"></span> Everyone</span></div><div class="meta_row meta_actions"></div></div><div class="note_contents"><span class="note_content_field">Review: </span><span class="note_content_value">This paper proposes using label smoothing and logit squeezing together with Gaussian noise injection at training time as a replacement for adversarial training. The paper builds on the flawed premise that label smoothing and logit squeezing can lead to more robustness - it simply masks the gradients so that the optimization landscape is harder. Also I am unconvinced that simply adding Gaussian noise together with these tricks can lead to any robustness against a worst case adversary - Gaussian noise augmentation is known to not lead to increased robustness (see e.g. [1]) while logit squeezing is also known to not lead to increased robustness (see e.g. [2]). So I cannot fathom why the two combined suddenly start working?

Also a glance at their experimental evaluation immediately uncovers the flaw that they only used 20 step PGD attack to evaluate the robustness of their model on CIFAR-10. Label smoothing and logit squeezing result in gradient masking and a more difficult terrain to optimize over, so increasing the number of attack steps and doing many random restarts will reveal that their claimed model is not as robust as the authors think.

[1] <a href="https://arxiv.org/pdf/1711.08478.pdf" target="_blank" rel="nofollow">https://arxiv.org/pdf/1711.08478.pdf</a>
[2] https://arxiv.org/pdf/1807.10272.pdf</span></div><div class="note_contents"><span class="note_content_field">Rating: </span><span class="note_content_value">2: Strong rejection</span></div><div class="note_contents"><span class="note_content_field">Confidence: </span><span class="note_content_value">5: The reviewer is absolutely certain that the evaluation is correct and very familiar with the relevant literature</span></div><div class="reply_row clearfix"></div></div><div class="children"></div></div><div class="note_with_children"><div id="note_SyxP8tOIcX" class="note panel"><div class="title_pdf_row clearfix"><h2 class="note_content_title"><a>Unbounded PGD Attack Sanity Check</a>  <button class="btn btn-xs btn-default permalink-button" title="Link to this comment" data-permalink-url="https://openreview.net/forum?id=BJlr0j0ctX&amp;noteId=SyxP8tOIcX"><span class="glyphicon glyphicon-link" aria-hidden="true"></span></button></h2></div><div class="meta_row"><span class="signatures"><a href="/profile?id=~Alex_Lamb1" class="profile-link">Alex Lamb</a></span></div><div class="clearfix"><div class="meta_row pull-left"><span class="date item">07 Oct 2018</span><span class="item">ICLR 2019 Conference Paper892 Public Comment</span><span class="item">Readers: <span class="readers-icon glyphicon glyphicon-globe"></span> Everyone</span></div><div class="meta_row meta_actions"></div></div><div class="note_contents"><span class="note_content_field">Comment: </span><span class="note_content_value">Hello, 

I apologize if i missed it, but can you run the unbounded PGD sanity check from (Athalye 2018).  Basically you just run PGD with an unlimited epsilon budget for a large number of steps, and confirm that accuracy goes to 0%.  If it doesn't go to 0%, it definitively suggests that there are significant issues with the gradient.  </span></div><div class="reply_row clearfix"></div></div><div class="children"><div class="note_with_children"><div id="note_S1xnbzc997" class="note panel"><div class="title_pdf_row clearfix"><h2 class="note_content_title"><a>Unbounded epsilon results</a>  <button class="btn btn-xs btn-default permalink-button" title="Link to this comment" data-permalink-url="https://openreview.net/forum?id=BJlr0j0ctX&amp;noteId=S1xnbzc997"><span class="glyphicon glyphicon-link" aria-hidden="true"></span></button></h2></div><div class="meta_row"><span class="signatures">ICLR 2019 Conference Paper892 Authors</span></div><div class="clearfix"><div class="meta_row pull-left"><span class="date item">10 Oct 2018</span><span class="item">ICLR 2019 Conference Paper892 Official Comment</span><span class="item">Readers: <span class="readers-icon glyphicon glyphicon-globe"></span> Everyone</span></div><div class="meta_row meta_actions"></div></div><div class="note_contents"><span class="note_content_field">Comment: </span><span class="note_content_value">Hi Alex,
Thank you for the interest in our paper and insightful suggestion. We ran “unlimited epsilon” attacks on the model trained with logit_squeeze=10, sigma=10, for 160k iterations (one of the parameter settings used in our paper). The results are consistent with what is expected and the accuracy on unbounded adversarial examples does drop to 0 as the size of the adversarial perturbation (epsilon) gets very large:

Iterations	step-size	epsilon		accuracy
     30		       2		    50		  14.48%
     40		       2		    60		  9.02%
     50		       3		   100		  2.07%
     50		       4		   150		  0.22%
     100		       4		   150		  0.13%
     200		       2		   255		  0.00%
</span></div><div class="reply_row clearfix"></div></div><div class="children"></div></div></div></div><div class="note_with_children"><div id="note_BJgJT1K-9X" class="note panel"><div class="title_pdf_row clearfix"><h2 class="note_content_title"><a>Logit squeezing is not robust</a>  <button class="btn btn-xs btn-default permalink-button" title="Link to this comment" data-permalink-url="https://openreview.net/forum?id=BJlr0j0ctX&amp;noteId=BJgJT1K-9X"><span class="glyphicon glyphicon-link" aria-hidden="true"></span></button></h2></div><div class="meta_row"><span class="signatures">(anonymous)</span></div><div class="clearfix"><div class="meta_row pull-left"><span class="date item">03 Oct 2018</span><span class="item">ICLR 2019 Conference Paper892 Public Comment</span><span class="item">Readers: <span class="readers-icon glyphicon glyphicon-globe"></span> Everyone</span></div><div class="meta_row meta_actions"></div></div><div class="note_contents"><span class="note_content_field">Comment: </span><span class="note_content_value">I wonder if the authors are aware of the work <a href="https://arxiv.org/abs/1807.10272" target="_blank" rel="nofollow">https://arxiv.org/abs/1807.10272</a> which shows that logit squeezing does not lead to any increased robustness of the model?</span></div><div class="reply_row clearfix"></div></div><div class="children"><div class="note_with_children"><div id="note_ByxJvgLH5Q" class="note panel"><div class="title_pdf_row clearfix"><h2 class="note_content_title"><a>We study logit squeezing, not logit pairing</a>  <button class="btn btn-xs btn-default permalink-button" title="Link to this comment" data-permalink-url="https://openreview.net/forum?id=BJlr0j0ctX&amp;noteId=ByxJvgLH5Q"><span class="glyphicon glyphicon-link" aria-hidden="true"></span></button></h2></div><div class="meta_row"><span class="signatures">ICLR 2019 Conference Paper892 Authors</span></div><div class="clearfix"><div class="meta_row pull-left"><span class="date item">06 Oct 2018</span><span class="item">ICLR 2019 Conference Paper892 Official Comment</span><span class="item">Readers: <span class="readers-icon glyphicon glyphicon-globe"></span> Everyone</span></div><div class="meta_row meta_actions"></div></div><div class="note_contents"><span class="note_content_field">Comment: </span><span class="note_content_value">The arxiv paper mentioned above is on logit pairing, not label smoothing or logit squeezing, which are the regularizers studied here.  Logit pairing is an interesting regularizer in its own right, but it is more computationally expensive than the regularizers studied here, and is not the topic of our paper. 
</span></div><div class="reply_row clearfix"></div></div><div class="children"></div></div><div class="note_with_children"><div id="note_r1exXTRZcm" class="note panel"><div class="title_pdf_row clearfix"><h2 class="note_content_title"><a>The study of this arxiv paper is on ImageNet, while this paper is on MNIST &amp; CIFAR</a>  <button class="btn btn-xs btn-default permalink-button" title="Link to this comment" data-permalink-url="https://openreview.net/forum?id=BJlr0j0ctX&amp;noteId=r1exXTRZcm"><span class="glyphicon glyphicon-link" aria-hidden="true"></span></button></h2></div><div class="meta_row"><span class="signatures">(anonymous)</span></div><div class="clearfix"><div class="meta_row pull-left"><span class="date item">03 Oct 2018</span><span class="item">ICLR 2019 Conference Paper892 Public Comment</span><span class="item">Readers: <span class="readers-icon glyphicon glyphicon-globe"></span> Everyone</span></div><div class="meta_row meta_actions"></div></div><div class="note_contents"><span class="note_content_field">Comment: </span><span class="note_content_value">This arxiv paper states that adversarial logits pairing is not robust on ImageNet, but it does not put any conclusions on MNIST &amp; CIFAR. 

Also, in the footnote 4 of this arxiv paper --- "the authors state that unreleased models were used to generate the results
in Kannan et al. [9]. The authors are currently investigating these models; for the sake of comparison, we give the claim
from Kannan et al. [9] here."  So the conclusion in the arxiv paper MAY not valid if the official model is released.

In summary, I do not think conclusions from this arxiv paper have any relations to this ICLR submission.</span></div><div class="reply_row clearfix"></div></div><div class="children"><div class="note_with_children"><div id="note_SJg2f1Zz9X" class="note panel"><div class="title_pdf_row clearfix"><h2 class="note_content_title"><a>I think it's definitely worth reporting on numbers with varying PGD iterations</a>  <button class="btn btn-xs btn-default permalink-button" title="Link to this comment" data-permalink-url="https://openreview.net/forum?id=BJlr0j0ctX&amp;noteId=SJg2f1Zz9X"><span class="glyphicon glyphicon-link" aria-hidden="true"></span></button></h2></div><div class="meta_row"><span class="signatures">(anonymous)</span></div><div class="clearfix"><div class="meta_row pull-left"><span class="date item">03 Oct 2018 (modified: 04 Oct 2018)</span><span class="item">ICLR 2019 Conference Paper892 Public Comment</span><span class="item">Readers: <span class="readers-icon glyphicon glyphicon-globe"></span> Everyone</span></div><div class="meta_row meta_actions"></div></div><div class="note_contents"><span class="note_content_field">Comment: </span><span class="note_content_value">Maybe not directly, but the arxiv paper does highlight the phenomena that the landscape becomes more jagged and this is why gradient based attacks fail.  In that direction, two choices are to evaluate against gradient-free attacks and to vary the PGD attack steps till you see a convergence... without that, I find it hard that the conclusions drawn here at reliable.  Similar to what happens in the arxiv paper, it quite possible (and I think perhaps likely) that the accuracy drops drastically when you have a 1000 step PGD running. 

Just to clarify: I am not the anonymous OP :) </span></div><div class="reply_row clearfix"></div></div><div class="children"><div class="note_with_children"><div id="note_S1eu4Ntf5X" class="note panel"><div class="title_pdf_row clearfix"><h2 class="note_content_title"><a>Varying PGD iterations is needed</a>  <button class="btn btn-xs btn-default permalink-button" title="Link to this comment" data-permalink-url="https://openreview.net/forum?id=BJlr0j0ctX&amp;noteId=S1eu4Ntf5X"><span class="glyphicon glyphicon-link" aria-hidden="true"></span></button></h2></div><div class="meta_row"><span class="signatures">(anonymous)</span></div><div class="clearfix"><div class="meta_row pull-left"><span class="date item">04 Oct 2018</span><span class="item">ICLR 2019 Conference Paper892 Public Comment</span><span class="item">Readers: <span class="readers-icon glyphicon glyphicon-globe"></span> Everyone</span></div><div class="meta_row meta_actions"></div></div><div class="note_contents"><span class="note_content_field">Comment: </span><span class="note_content_value">Yes, I agree with you that varying PGD iteration numbers is needed. The authors should add this experiment to further verify the robustness.

But I just want to point out the following things:

(1) Adversarial logits pairing is different from label smoothing or logit squeezing. They are completely different things. Thus the arxiv paper you post here has no value for this paper

(2) Large scale dataset adversarial training (e.g. ImageNet) is quite different from small scale dataset adversarial training (e.g. CIFAR). Even if this arxiv paper is right, the conclusion of robustness cannot be directly transfer to MNIST/CIFAR. Moreover, from my personal experiments, ALP can work on CIFAR/MNIST. You can try to reproduce the results if you do not believe. However, I do not have any experiments about logit squeezing on CIFAR/MNIST</span></div><div class="reply_row clearfix"></div></div><div class="children"><div class="note_with_children"><div id="note_SyxxfXIH9X" class="note panel"><div class="title_pdf_row clearfix"><h2 class="note_content_title"><a>As requested: more iterations</a>  <button class="btn btn-xs btn-default permalink-button" title="Link to this comment" data-permalink-url="https://openreview.net/forum?id=BJlr0j0ctX&amp;noteId=SyxxfXIH9X"><span class="glyphicon glyphicon-link" aria-hidden="true"></span></button></h2></div><div class="meta_row"><span class="signatures">ICLR 2019 Conference Paper892 Authors</span></div><div class="clearfix"><div class="meta_row pull-left"><span class="date item">06 Oct 2018</span><span class="item">ICLR 2019 Conference Paper892 Official Comment</span><span class="item">Readers: <span class="readers-icon glyphicon glyphicon-globe"></span> Everyone</span></div><div class="meta_row meta_actions"></div></div><div class="note_contents"><span class="note_content_field">Comment: </span><span class="note_content_value">We chose a 20 iterations attack because this seems to be a common choice across many papers (including several submitted to ICLR 2019), and we wanted to replicate this standard experimental setting. 

That being said, it’s legitimate to wonder how this parameter effects results, and so we have run white-box experiments with increased numbers of PGD iterations.  We use the CW loss, which is the worst case for label smoothing and logit squeezing in our experiments.  Models were trained for 160K iterations (we see a trend of accuracy going up a bit with more iterations, but we cut off training here to keep things quick).  We used parameters logit_squeeze=10, and Gaussian noise with sigma=30.  These are the same parameters used in the paper for 20 steps; there may be something to be gained by optimizing these for 1000 step, but we have not had time to explore this.  
 
PGD Iterations		Accuracy
20			     	         49.73%
40				         45.33%
100				         42.36%
400				         41.58%
1000				 40.94%

It seems that running with more iterations does cause some loss in accuracy, although this effect seems to plateau.  We also ran some quick experiments with larger and smaller stepsizes, but found that this parameter had little effect on final performance.

Also, please note that white box performance isn’t everything.  Additional benefits of label smoothing and logit squeezing include:  
- Very high robustness in the black box setting and against non-iterative attacks
- High test accuracy on clean data
- Very fast training time compared to adversarial training</span></div><div class="reply_row clearfix"></div></div><div class="children"></div></div><div class="note_with_children"><div id="note_HyxCLKFGcm" class="note panel"><div class="title_pdf_row clearfix"><h2 class="note_content_title"><a>Label smoothing and Logit squeezing = recipe for Gradient masking</a>  <button class="btn btn-xs btn-default permalink-button" title="Link to this comment" data-permalink-url="https://openreview.net/forum?id=BJlr0j0ctX&amp;noteId=HyxCLKFGcm"><span class="glyphicon glyphicon-link" aria-hidden="true"></span></button></h2></div><div class="meta_row"><span class="signatures">(anonymous)</span></div><div class="clearfix"><div class="meta_row pull-left"><span class="date item">04 Oct 2018</span><span class="item">ICLR 2019 Conference Paper892 Public Comment</span><span class="item">Readers: <span class="readers-icon glyphicon glyphicon-globe"></span> Everyone</span></div><div class="meta_row meta_actions"></div></div><div class="note_contents"><span class="note_content_field">Comment: </span><span class="note_content_value">It could be that logit pairing and logit squeezing are different in their formulation. But I do not buy that this leads to any increased robustness compared to PGD. Both label smoothing and logit squeezing break the optimizer by masking gradients, so one just needs to run PGD for more iterations (as was shown in the arxiv paper). Gradient masking does not lead to more robust models.</span></div><div class="reply_row clearfix"></div></div><div class="children"><div class="note_with_children"><div id="note_SyxLs6sf5m" class="note panel"><div class="title_pdf_row clearfix"><h2 class="note_content_title"><a>Is there any evidence for your argument?</a>  <button class="btn btn-xs btn-default permalink-button" title="Link to this comment" data-permalink-url="https://openreview.net/forum?id=BJlr0j0ctX&amp;noteId=SyxLs6sf5m"><span class="glyphicon glyphicon-link" aria-hidden="true"></span></button></h2></div><div class="meta_row"><span class="signatures">(anonymous)</span></div><div class="clearfix"><div class="meta_row pull-left"><span class="date item">04 Oct 2018</span><span class="item">ICLR 2019 Conference Paper892 Public Comment</span><span class="item">Readers: <span class="readers-icon glyphicon glyphicon-globe"></span> Everyone</span></div><div class="meta_row meta_actions"></div></div><div class="note_contents"><span class="note_content_field">Comment: </span><span class="note_content_value">Do you have any evidence for your argument? Have you done ALP on MNIST/CIFAR and then obtain such conclusions, or you have any reference which clearly state that ALP cannot increase robustness on MNIST/CIFAR? </span></div><div class="reply_row clearfix"></div></div><div class="children"><div class="note_with_children"><div id="note_S1lz1ehzqm" class="note panel"><div class="title_pdf_row clearfix"><h2 class="note_content_title"><a>Yes definitely</a>  <button class="btn btn-xs btn-default permalink-button" title="Link to this comment" data-permalink-url="https://openreview.net/forum?id=BJlr0j0ctX&amp;noteId=S1lz1ehzqm"><span class="glyphicon glyphicon-link" aria-hidden="true"></span></button></h2></div><div class="meta_row"><span class="signatures">(anonymous)</span></div><div class="clearfix"><div class="meta_row pull-left"><span class="date item">04 Oct 2018</span><span class="item">ICLR 2019 Conference Paper892 Public Comment</span><span class="item">Readers: <span class="readers-icon glyphicon glyphicon-globe"></span> Everyone</span></div><div class="meta_row meta_actions"></div></div><div class="note_contents"><span class="note_content_field">Comment: </span><span class="note_content_value">I have done experiments on CIFAR-10, where for comparable number of steps to Madry et al ALP appears more robust, but that is because it makes the optimization landscape jagged. Increasing the number of steps, makes it no more robust than PGD training (assuming you were PGD training with ALP).</span></div><div class="reply_row clearfix"></div></div><div class="children"><div class="note_with_children"><div id="note_Hkln-c_an7" class="note panel"><div class="title_pdf_row clearfix"><h2 class="note_content_title"><a>Our results contradict your statement</a>  <button class="btn btn-xs btn-default permalink-button" title="Link to this comment" data-permalink-url="https://openreview.net/forum?id=BJlr0j0ctX&amp;noteId=Hkln-c_an7"><span class="glyphicon glyphicon-link" aria-hidden="true"></span></button></h2></div><div class="meta_row"><span class="signatures"><a href="/profile?id=~Maksym_Andriushchenko1" class="profile-link">Maksym Andriushchenko</a></span></div><div class="clearfix"><div class="meta_row pull-left"><span class="date item">05 Nov 2018</span><span class="item">ICLR 2019 Conference Paper892 Public Comment</span><span class="item">Readers: <span class="readers-icon glyphicon glyphicon-globe"></span> Everyone</span></div><div class="meta_row meta_actions"></div></div><div class="note_contents"><span class="note_content_field">Comment: </span><span class="note_content_value">Our results on CIFAR-10 (<a href="https://arxiv.org/abs/1810.12042," target="_blank" rel="nofollow">https://arxiv.org/abs/1810.12042,</a> table 2) contradict your statement.
We observe that ALP leads to more robust models compared to plain adversarial training. We measure it by applying PGD attack with many iterations and many random restarts. The gap is around 3%. But note that we report results for eps=16.0. We did not report results for eps=8.0, but the conclusion is roughly the same.

Results on other datasets (MNIST, Tiny ImageNet) also suggest that there may be a slight benefit of using ALP, but certainly it does not improve robustness by a factor of 20 as claimed in the original ALP paper.

So could you please share your settings under which you observed no benefit of using ALP over plain adv. training?</span></div><div class="reply_row clearfix"></div></div><div class="children"></div></div></div></div></div></div></div></div></div></div></div></div></div></div></div></div><div class="note_with_children"><div id="note_BJgAwHDW57" class="note panel"><div class="title_pdf_row clearfix"><h2 class="note_content_title"><a>Possible contradiction with another submission?</a>  <button class="btn btn-xs btn-default permalink-button" title="Link to this comment" data-permalink-url="https://openreview.net/forum?id=BJlr0j0ctX&amp;noteId=BJgAwHDW57"><span class="glyphicon glyphicon-link" aria-hidden="true"></span></button></h2></div><div class="meta_row"><span class="signatures">(anonymous)</span></div><div class="clearfix"><div class="meta_row pull-left"><span class="date item">03 Oct 2018</span><span class="item">ICLR 2019 Conference Paper892 Public Comment</span><span class="item">Readers: <span class="readers-icon glyphicon glyphicon-globe"></span> Everyone</span></div><div class="meta_row meta_actions"></div></div><div class="note_contents"><span class="note_content_field">Comment: </span><span class="note_content_value">This paper appears to contradict another submission: <a href="https://openreview.net/pdf?id=Bylj6oC5K7" target="_blank" rel="nofollow">https://openreview.net/pdf?id=Bylj6oC5K7</a>

This paper states "Our experimental results suggest that simple regularizers can hurt adversarial robustness", where the simple regularizers include among other things label smoothing. The other paper states "Very surprisingly, using only label smoothing can result in a model that is nearly as robust as models trained with PGD-based adversarial training".

Is there some subtle difference between these two papers' experiments that explains this difference?</span></div><div class="reply_row clearfix"></div></div><div class="children"><div class="note_with_children"><div id="note_HkggU6FM5X" class="note panel"><div class="title_pdf_row clearfix"><h2 class="note_content_title"><a>Perhaps they use Mixup instead of Gaussian?</a>  <button class="btn btn-xs btn-default permalink-button" title="Link to this comment" data-permalink-url="https://openreview.net/forum?id=BJlr0j0ctX&amp;noteId=HkggU6FM5X"><span class="glyphicon glyphicon-link" aria-hidden="true"></span></button></h2></div><div class="meta_row"><span class="signatures">ICLR 2019 Conference Paper892 Authors</span></div><div class="clearfix"><div class="meta_row pull-left"><span class="date item">04 Oct 2018 (modified: 06 Oct 2018)</span><span class="item">ICLR 2019 Conference Paper892 Official Comment</span><span class="item">Readers: <span class="readers-icon glyphicon glyphicon-globe"></span> Everyone</span></div><div class="meta_row meta_actions"></div></div><div class="note_contents"><span class="note_content_field">Comment: </span><span class="note_content_value">Thank you for your interest and feedback. In our paper, we mention that label smoothing and logit squeezing without Gaussian augmentation decrease robustness, as seen in our MNIST experiments (appendix) and as mentioned in Zantedeschi et al., 2017.  However, adding random gaussian noise augmentation during label smoothing (and logit squeezing) increases robustness. In fact, by using large smoothing and noise parameters, we are able to match or sometimes even beat adversarial training.  This motivated us to explore the effects of random gaussian augmentation (which we believe is a contributions of our paper)

While we were not able to find any random gaussian augmentations reported in the other paper you mention, we did notice that they use “mixup” when training a network using their method (LRM). From the accuracies they report on non-adversarial clean examples (Table1 in their paper), it seems likely that they use mixup when training with the other methods (including Label smoothing) as well. This would explain the accuracies they report;  the “Regular training” in Table1 of their paper achieves only 87.4% accuracy on “Natural” examples, while the Naturally trained model from Madry has 95.2% accuracy on clean test examples (Madry et al., 2018). Mixup may have the same “off-manifold” generalization property that gaussian augmentation has. But as it can be seen, it hurts the accuracy on clean examples. Our test accuracy is above 90% (using the same ResNet that Madry uses). While the test accuracy they report using LRM is 68.5%.

To be quantitative:  we just ran an experiment with label_smoothing=0.8 without any gaussian augmentation on CIFAR-10, and attacked it with a 20-step PGD attack on the CW loss.  The attack degrades the model accuracy to only  5%.</span></div><div class="reply_row clearfix"></div></div><div class="children"></div></div></div></div></div></main></div>
  </div>
</div>


    <!-- Footer -->
    <footer class="sitemap">
      <div class="container">
        <div class="row hidden-xs">
          <div class="col-sm-4">
            <ul class="list-unstyled">
              <li><a href="/" class="home">Home</a></li>
              <li><a href="/venues">All Venues</a></li>
            </ul>
          </div>
    
          <div class="col-sm-4">
            <ul class="list-unstyled">
              <li><a href="/about">About OpenReview</a></li>
              <li><a href="/contact">Contact</a></li>
              <li><a href="#" data-toggle="modal" data-target="#feedback-modal">Feedback</a></li>
            </ul>
          </div>
    
          <div class="col-sm-4">
            <ul class="list-unstyled">
              <li><a href="/terms">Terms of Service</a></li>
              <li><a href="/privacy">Privacy Policy</a></li>
            </ul>
          </div>
        </div>
    
        <div class="row visible-xs-block">
          <div class="col-xs-6">
            <ul class="list-unstyled">
              <li><a href="/" class="home">Home</a></li>
              <li><a href="/about">About OpenReview</a></li>
              <li><a href="#" data-toggle="modal" data-target="#feedback-modal">Feedback</a></li>
            </ul>
          </div>
    
          <div class="col-xs-6">
            <ul class="list-unstyled">
              <li><a href="/contact">Contact</a></li>
              <li><a href="/terms">Terms of Service</a></li>
              <li><a href="/privacy">Privacy Policy</a></li>
            </ul>
          </div>
        </div>
      </div>
    </footer>
    
    <footer class="sponsor">
      <div class="container">
        <div class="row">
          <div class="col-sm-10 col-sm-offset-1">
            <p class="text-center">
              OpenReview is created by the <a href="http://www.iesl.cs.umass.edu/" target="_blank">Information Extraction and Synthesis Laboratory</a>, College of Information and Computer Science, University of Massachusetts Amherst. We gratefully acknowledge the support of the OpenReview sponsors:  Google,  Facebook, NSF, the University of Massachusetts Amherst Center for Data Science, and Center for Intelligent Information Retrieval, as well as the Google Cloud Platform for donating the computing and networking services on which OpenReview.net runs.
            </p>
          </div>
        </div>
      </div>
    </footer>

  <div id="feedback-modal" class="modal fade" tabindex="-1" role="dialog">
    <div class="modal-dialog">
      <div class="modal-content">
        <div class="modal-header">
          <button type="button" class="close" data-dismiss="modal" aria-label="Close"><span aria-hidden="true">×</span></button>
          <h3 class="modal-title">Send Feedback</h3>
        </div>
        <div class="modal-body">
          <p>Enter your feedback below and we'll get back to you as soon as possible.</p>
          <form action="/feedback" method="POST">
            <div class="form-group">
              <input type="email" name="from" class="form-control" placeholder="Email" />
            </div>
            <div class="form-group">
              <input type="text" name="subject" class="form-control" placeholder="Subject" />
            </div>
            <div class="form-group">
              <textarea name="message" class="form-control feedback-input" rows="5" placeholder="Message" required=""></textarea>
            </div>
          <ul id="ui-id-2" tabindex="0" class="ui-menu ui-widget ui-widget-content" style="display: none;"></ul></form>
        </div>
        <div class="modal-footer">
          <button type="button" class="btn btn-default" data-dismiss="modal">Cancel</button>
          <button type="button" class="btn btn-primary">Send</button>
        </div>
      </div>
    </div>
  </div>

  <script type="text/javascript" async="" src="https://www.google-analytics.com/analytics.js"></script><script src="//cdnjs.cloudflare.com/ajax/libs/jquery/2.2.4/jquery.min.js"></script>
  <script>window.jQuery || document.write('&lt;script src="/static/js/vendor/jquery-2.2.4.min.js"&gt;&lt;\/script&gt;')</script>

  <script src="/static/dist/vendor.min.js"></script>
  <script src="/static/dist/templates.min.js"></script>
  <script src="/static/dist/openreview.min.js"></script>

    <script>window.legacyScripts = true;</script>


    <script async="" src="https://www.googletagmanager.com/gtag/js?id=UA-108703919-1"></script>
    <script>
      window.dataLayer = window.dataLayer || [];
      function gtag() { dataLayer.push(arguments); }
      gtag('js', new Date());
      gtag('config', 'UA-108703919-1');
    </script>


<div role="status" aria-live="assertive" aria-relevant="additions" class="ui-helper-hidden-accessible"></div></body></html>