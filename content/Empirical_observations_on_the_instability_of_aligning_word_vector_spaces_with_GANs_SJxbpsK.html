<!DOCTYPE html><html xmlns="http://www.w3.org/1999/xhtml" class="no-js"><head>
  <meta charset="utf-8" />
  <meta http-equiv="X-UA-Compatible" content="IE=edge,chrome=1" />
  <meta name="viewport" content="width=device-width, initial-scale=1" />

  <title>Empirical observations on the instability of aligning word vector spaces with GANs | OpenReview</title>
  <meta name="description" content="" />

      <meta name="citation_title" content="Empirical observations on the instability of aligning word vector spaces with GANs" />
        <meta name="citation_author" content="Mareike Hartmann" />
        <meta name="citation_author" content="Yova Kementchedjhieva" />
        <meta name="citation_author" content="Anders Søgaard" />
      <meta name="citation_publication_date" content="2018/09/27" />
      <meta name="citation_online_date" content="2018/09/27" />
      <meta name="citation_pdf_url" content="https://openreview.net/pdf?id=SJxbps09K7" />
      <meta name="twitter:card" content="summary" />
      <meta name="twitter:site" content="@openreviewnet" />
      <meta name="og:title" content="Empirical observations on the instability of aligning word vector..." />
      <meta name="og:description" content="Unsupervised bilingual dictionary induction (UBDI) is useful for unsupervised machine translation and for cross-lingual transfer of models into low-resource languages. One approach to UBDI is to..." />
      <meta name="og:image" content="https://openreview.net/static/images/openreview_logo_512.png" />

  <link rel="stylesheet" href="https://fonts.googleapis.com/css?family=Noto+Sans:400,400i,700,700i" />

  <link rel="stylesheet" href="/static/css/bootstrap.min.css" />
  <link rel="stylesheet" href="/static/css/jquery-ui.min.css" />
  <link rel="stylesheet" href="/static/css/main.min.css" />
</head>

<body class="forum">
  <nav class="navbar navbar-inverse navbar-fixed-top" role="navigation">
    <div class="container">
  
      <div class="navbar-header">
        <button type="button" class="navbar-toggle collapsed" data-toggle="collapse" data-target="#navbar" aria-expanded="false" aria-controls="navbar">
          <span class="sr-only">Toggle navigation</span>
          <span class="icon-bar"></span>
          <span class="icon-bar"></span>
          <span class="icon-bar"></span>
        </button>
        <a class="navbar-brand home push-link" href="/"><strong>OpenReview</strong>.net</a>
      </div>
  
      <div id="navbar" class="navbar-collapse collapse">
        <form class="navbar-form navbar-left profile-search" role="search">
          <div class="form-group has-feedback">
            <input id="search_input" type="text" class="form-control ui-autocomplete-input" placeholder="Search ICLR 2019 Conference" autocomplete="off" />
            <span class="glyphicon glyphicon-search form-control-feedback" aria-hidden="true"></span>
          </div>
  
          <input id="search_group" type="hidden" value="ICLR.cc/2019/Conference" />
          <input id="search_content" type="hidden" value="all" />
        <ul id="ui-id-1" tabindex="0" class="ui-menu ui-widget ui-widget-content ui-autocomplete ui-front" style="display: none;"></ul></form>
  
        <ul class="nav navbar-nav navbar-right">
        
            <li id="user-menu"><a href="/login">Login</a></li>
        </ul>
      </div>
  
    </div>
  </nav>

  <div id="or-banner" class="banner" style="">
  <div class="container">
    <div class="row">
      <div class="col-xs-12"><a href="/group?id=ICLR.cc/2019/Conference" title="Venue Homepage"><img class="icon" src="/static/images/arrow_left.svg" /> Go to <strong>ICLR 2019 Conference</strong> homepage</a></div>
    </div>
  </div>
</div>
<div id="flash-message-container" class="alert alert-danger" role="alert" style="display: none;">
  <div class="container">
    <div class="row">
      <div class="col-xs-12">
        <div class="alert-content">
          <button type="button" class="close" aria-label="Close"><span aria-hidden="true">×</span></button>
        </div>
      </div>
    </div>
  </div>
</div>

<div class="container">
  <div class="row">
    <div class="col-xs-12">
      <main id="content" class="clearfix openbanner-visible legacy-styles"><div id="note_SJxbps09K7" class="note panel"><div class="title_pdf_row clearfix"><h2 class="note_content_title"><a>Empirical observations on the instability of aligning word vector spaces with GANs</a> <a class="note_content_pdf" href="/pdf?id=SJxbps09K7" title="Download PDF" target="_blank"><img src="/static/images/pdf_icon_blue.svg" /></a> </h2></div><div class="meta_row"><span class="signatures"><a href="/profile?email=hartmann%40di.ku.dk" class="profile-link" data-toggle="tooltip" data-placement="bottom" title="" data-original-title="hartmann@di.ku.dk">Mareike Hartmann</a>, <a href="/profile?email=yova%40di.ku.dk" class="profile-link" data-toggle="tooltip" data-placement="bottom" title="" data-original-title="yova@di.ku.dk">Yova Kementchedjhieva</a>, <a href="/profile?email=soegaard%40di.ku.dk" class="profile-link" data-toggle="tooltip" data-placement="bottom" title="" data-original-title="soegaard@di.ku.dk">Anders Søgaard</a></span></div><div class="clearfix"><div class="meta_row pull-left"><span class="date item">28 Sep 2018 (modified: 17 Nov 2018)</span><span class="item">ICLR 2019 Conference Withdrawn Submission</span><span class="item">Readers: <span class="readers-icon glyphicon glyphicon-globe"></span> Everyone</span></div><div class="meta_row meta_actions"></div></div><div class="note_contents"><span class="note_content_field">Abstract: </span><span class="note_content_value">Unsupervised bilingual dictionary induction (UBDI) is useful for unsupervised machine translation and for cross-lingual transfer of models into low-resource languages. One approach to UBDI is to align word vector spaces in different languages using Generative adversarial networks (GANs) with linear generators, achieving state-of-the-art performance for several language pairs. For some pairs, however, GAN-based induction is unstable or completely fails to align the vector spaces. We focus on cases where linear transformations provably exist, but the performance of GAN-based UBDI depends heavily on the model initialization. We show that the instability depends on the shape and density of the vector sets, but not on noise; it is the result of local optima, but neither over-parameterization nor changing the batch size or the learning rate consistently reduces instability. Nevertheless, we can stabilize GAN-based UBDI through best-of-N model selection, based on an unsupervised stopping criterion. </span></div><div class="note_contents"><span class="note_content_field">Keywords: </span><span class="note_content_value">natural language processing, bilingual dictionary induction, unsupervised learning, generative adversarial networks</span></div><div class="note_contents"><span class="note_content_field">TL;DR: </span><span class="note_content_value">An empirical investigation of GAN-based alignment of word vector spaces, focusing on cases, where linear transformations provably exist, but training is unstable.</span></div><div class="reply_row clearfix"><div class="item" id="reply_count">4 Replies</div></div></div><hr class="small" /><div id="note_children"><div class="note_with_children"><div id="note_B1lGiwWmp7" class="note panel"><div class="title_pdf_row clearfix"><h2 class="note_content_title"><a>Submission Withdrawn by the Authors</a>  <button class="btn btn-xs btn-default permalink-button" title="Link to this comment" data-permalink-url="https://openreview.net/forum?id=SJxbps09K7&amp;noteId=B1lGiwWmp7"><span class="glyphicon glyphicon-link" aria-hidden="true"></span></button></h2></div><div class="meta_row"><span class="signatures">ICLR 2019 Conference Paper777 Authors</span></div><div class="clearfix"><div class="meta_row pull-left"><span class="date item">09 Nov 2018</span><span class="item">ICLR 2019 Conference Paper777 Withdraw Submission</span><span class="item">Readers: <span class="readers-icon glyphicon glyphicon-globe"></span> Everyone</span></div><div class="meta_row meta_actions"></div></div><div class="note_contents"><span class="note_content_field">Withdrawal confirmation: </span><span class="note_content_value">I have read and agree with the withdrawal statement on behalf of myself and my co-authors.</span></div><div class="reply_row clearfix"></div></div><div class="children"></div></div><div class="note_with_children"><div id="note_HJx26ceY2X" class="note panel"><div class="title_pdf_row clearfix"><h2 class="note_content_title"><a>Interesting analysis, but not much new</a>  <button class="btn btn-xs btn-default permalink-button" title="Link to this comment" data-permalink-url="https://openreview.net/forum?id=SJxbps09K7&amp;noteId=HJx26ceY2X"><span class="glyphicon glyphicon-link" aria-hidden="true"></span></button></h2></div><div class="meta_row"><span class="signatures">ICLR 2019 Conference Paper777 AnonReviewer2</span></div><div class="clearfix"><div class="meta_row pull-left"><span class="date item">02 Nov 2018 (modified: 07 Nov 2018)</span><span class="item">ICLR 2019 Conference Paper777 Official Review</span><span class="item">Readers: <span class="readers-icon glyphicon glyphicon-globe"></span> Everyone</span></div><div class="meta_row meta_actions"></div></div><div class="note_contents"><span class="note_content_field">Review: </span><span class="note_content_value">This paper presents a series of empirical observations when aligning word embeddings in different languages with the unsupervised method of Conneau et al. (2018). While I am the first to defend that research should not be only about fighting for SOTA and I would love to see more analysis papers like this, I think that this one in particular does not have enough novelty/substance to get accepted at such a competitive venue. The paper is well written and I enjoyed reading it, but I was left with the feeling that I did not learn much from it.

Given that the paper is well structured into 9 different observations, I will next comment them one by one:

- Observation 1: Impossible cases are not solely the results of linguistic differences, but also of corpus characteristics. This was already shown in Sogaard et al. (2018) and, in my opinion, their paper does a better work at systematically studying this effect in their ablation study.

- Observation 2: Impossible cases can also be the result of the inductive biases of the underlying word embedding algorithms. Same as above: this was already shown (and better supported in my opinion) in Sogaard et al. (2018).

- Observation 3: GAN-based UBDI becomes more unstable and performance deteriorates with unit length normalization. This is a somewhat minor detail, but I find it interesting, as it challenges a common practice in supervised bilingual dictionary induction.

- Observation 4: GAN-based UBDI becomes more unstable and performance deteriorates with PCA pruning. I also find this one interesting, although it is essentially a negative result.

- Observation 5: GAN-based UBDI is largely unaffected by noise injection. I do not see why this could be relevant, as one should never find this type of synthetic noise in realistic settings.

- Observation 6: In the hard cases, GAN-based UBDI gets stuck in local optima. I might be missing something here, but this seems pretty obvious. Why would the method fail to converge to the correct solution otherwise (assuming that the unsupervised objective function is appropriate)?

- Observation 7: Over-parametrization does not consistently help in the hard cases. This is another negative result: not bad to know, but does not have much value on its own.

- Observation 8: Changing the batch size or the learning rate to hurt the discriminator also does not help. Just another negative result. The authors themselves state that "it seems the MUSE default hyperparameters are close to optimal". Again, not bad to know, but this only confirms that Conneau et al. (2018) did a good job.

- Observation 9: In the hard cases, model selection with cosine similarity can stabilize GAN-based UBDI. Your model selection criterion is the one proposed by Conneau et al. (2018) themselves, so I do not see where the novelty is here.

Other contributions:

- As cited in the paper, the procrustes fit was proposed by Kementchedjhieva et al. (2018) for this exact same problem, so there is no original contribution here either (other than applying it to a different dataset).

- The paper classifies different language pairs as "easy", "hard" and "impossible". I think that the distinction between "easy" and "hard" is well supported, as it is putting Bengali and Cebuano into a third (even harder) group. However, I do not see enough evidence to name this third group "impossible". Such a strong statement should be rigorously supported, and I do not find this to be the case. At most, you could speculate that some language pairs might be impossible, but we cannot be 100% sure that they are with the provided evidence.</span></div><div class="note_contents"><span class="note_content_field">Rating: </span><span class="note_content_value">4: Ok but not good enough - rejection</span></div><div class="note_contents"><span class="note_content_field">Confidence: </span><span class="note_content_value">4: The reviewer is confident but not absolutely certain that the evaluation is correct</span></div><div class="reply_row clearfix"></div></div><div class="children"></div></div><div class="note_with_children"><div id="note_HyghBqh83Q" class="note panel"><div class="title_pdf_row clearfix"><h2 class="note_content_title"><a>This paper provides interesting insight into the usefulness of GANs to perform alignment of embedding spaces across languages. The paper is well motivated and introduces the problem well. While existing experiments are sufficient, there is scope to add more experiments.</a>  <button class="btn btn-xs btn-default permalink-button" title="Link to this comment" data-permalink-url="https://openreview.net/forum?id=SJxbps09K7&amp;noteId=HyghBqh83Q"><span class="glyphicon glyphicon-link" aria-hidden="true"></span></button></h2></div><div class="meta_row"><span class="signatures">ICLR 2019 Conference Paper777 AnonReviewer3</span></div><div class="clearfix"><div class="meta_row pull-left"><span class="date item">31 Oct 2018 (modified: 07 Nov 2018)</span><span class="item">ICLR 2019 Conference Paper777 Official Review</span><span class="item">Readers: <span class="readers-icon glyphicon glyphicon-globe"></span> Everyone</span></div><div class="meta_row meta_actions"></div></div><div class="note_contents"><span class="note_content_field">Review: </span><span class="note_content_value">This paper is clear and well written, except for the occasional typos, eg. missing a bracket in the very first equation.
The problem of learning transformations to align vector spaces across languages is extremely interesting and relevant. While the paper does a thorough job in explaining the metrics used and in detailing the procedure followed, I would have liked to see the following the details in the paper,
1)Number of unique word tokens in Bengali, Cebuano etc. While clicking on the reference link to MUSE provides these details, including the same in the paper, would make the writing wholesome.
2)While a thorough theoretical analysis is provided to explain the quality of the transformation/rotation matrix, there are no empirical results provided on any cross-lingual classification tasks. While not too important to the paper, such an analysis would provide the reader with an idea of how much can the proposed techniques be used in end-to-end train system for cross lingual tasks.
3)The authors comment on the quality of embeddings for languages such as Bengali, when studying the problem of alignment. Do the authors have any opinion if use of meta-embeddings i.e a weighted combination of say GloVe, word2vec and FastText can over come some of this limitations?Since each of these methods exploits a different feature of the training corpus, and there is rich literature showing the effectiveness of meta-embeddings, would this be something worth pursuing?</span></div><div class="note_contents"><span class="note_content_field">Rating: </span><span class="note_content_value">6: Marginally above acceptance threshold</span></div><div class="note_contents"><span class="note_content_field">Confidence: </span><span class="note_content_value">3: The reviewer is fairly confident that the evaluation is correct</span></div><div class="reply_row clearfix"></div></div><div class="children"></div></div><div class="note_with_children"><div id="note_Bkl9clbLnm" class="note panel"><div class="title_pdf_row clearfix"><h2 class="note_content_title"><a>Another empirical analysis related to instability of fully unsupervised cross-lingual word embedding models</a>  <button class="btn btn-xs btn-default permalink-button" title="Link to this comment" data-permalink-url="https://openreview.net/forum?id=SJxbps09K7&amp;noteId=Bkl9clbLnm"><span class="glyphicon glyphicon-link" aria-hidden="true"></span></button></h2></div><div class="meta_row"><span class="signatures">ICLR 2019 Conference Paper777 AnonReviewer1</span></div><div class="clearfix"><div class="meta_row pull-left"><span class="date item">31 Oct 2018 (modified: 07 Nov 2018)</span><span class="item">ICLR 2019 Conference Paper777 Official Review</span><span class="item">Readers: <span class="readers-icon glyphicon glyphicon-globe"></span> Everyone</span></div><div class="meta_row meta_actions"></div></div><div class="note_contents"><span class="note_content_field">Review: </span><span class="note_content_value">The recent methods on learning cross-lingual word embeddings without any bilingual supervision (i.e., in a fully unsupervised way) have generated a lot of traction in NLP and beyond. Follow-up papers from Artetxe et al. (ICLR 2018), and Lample et al. (ICLR 2018, EMNLP 2018) have shown that such cross-lingual word embeddings can seed unsupervised (neural and statistical) machine translation, and other applications are also possible (e.g., unsupervised cross-lingual retrieval). 

This paper in particular focuses on further empirical analyses related to previously observed instability of unsupervised mapping-based approaches for learning unsupervised cross-lingual word embeddings, but remains focused on the task of unsupervised bilingual dictionary induction (UBDI). The paper is largely a continuation of the preliminary paper which focuses on the same problem and research question: the paper of Sogaard et al. (ACL 2018). In fact, this paper reads a lot like a more elaborated (or incremental) version of the preliminary paper of Sogaard et al. (ACL 2018), with the paper structure and some descriptions borrowed from the previous work (e.g., the focus on the MUSE algorithm, the description of k-isospectrality), with two main enhancements compared to the prior work: 1) The introduction of Procrustes fit as a means to diagnose and anticipate (im)possibility of unsupervised embedding learning. Arguably, the proposed diagnostic tool is more efficient than the alternatives; 2) A very simple method for unsupervised model selection (Section 4) based on the mean cosine similarity predicted by the CSLS method.

Although some of the listed empirical observations are very straightforward if the reader knows anything about the entire procedure, it is good to see them empirically supported and presented in a very organised fashion. However, the two main methodological contributions of the paper seem quite thin to me. First, the method for unsupervised model selection is based on the unsupervised stopping criterion of Conneau et al. (2018) and it is a very simple (even simplistic) idea which happens to work well in practice already. Second, I am not fully bought by the Procrustes fit test and its comparison to two other tests has to be further examined, especially its relation to two similarity measures from Sogaard et al. For instance, k-subgraph isospectrality in the work of Sogaard et al also displayed a very strong correlation with UBDI performance (&gt;0.9), while the authors in this paper claim that the correlation coefficient with their data in this paper is -27% for the same method. Could the authors explain the discrepancy?

The authors claim that the Procrustes fit is much less computationally demanding as a test than the two alternative tests. However, I would like to see this claim empirically validated by running some actual tests and measuring test times. How are the two alternative tests impractical if they were used previously by Sogaard et al.?

Some observations have been already confirmed in prior work. For instance, Observation 2 is already analysed in the work of Sogaard et al. Observation 3 and Observation 5 are also straightforward given the technicalities and hyper-parameter setup of the MUSE procedure. 

I wonder if the authors are aware of the more recent work on UBDI from Artetxe et al. (ACL 2018). A crucial analysis for this paper should focus on Artetxe's more recent work rather than the MUSE algorithm. I acknowledge the fact that this field in particular is moving rapidly, but the paper would be much more impactful by focusing on the more recent and more appropriate starting ground rather than recycling the ideas already covered in prior work (e.g., in Sogaard et al.'s paper). 

The paper operates only in the fully unsupervised setting, which is, in my opinion, an artificial (even "artistic" setting). As Artetxe et al. (ACL 2017) showed and Sogaard et al. verified: relying on some sort of supervision (identical words, cognates, even numerals for languages with different scripts) already mitigates the problems of instability to a large extent. Why would we want to tie our analysis only to the fully unsupervised setting and go about it at length when it seems obvious that some sort of (cheap) supervision can already help the mapping-based algorithms to a large extent? I would like to see more experiments in such weakly-supervised settings. I would also like to see if the model selection criterion is applicable to this more real-life setting.

Although I am happy to see a clear division into possible and impossible UBDI setups, the paper also does not provide any solutions (or rules of thumb) on how to proceed if we end up having an impossible setup. What are the alternatives to the fully unsupervised BDI?

I am also missing explanations to some empirical observations: e.g., while the discrepancy between this work and the original work of Hoshen and Wolf has been reported, there is no insight on what causes the discrepancy and how this could be theoretically justified. In simple words, it is just observed, without getting a proper explanation supporting it.

The paper is very well written and easy to follow, but my impression is that, as an empirically driven paper, it could contribute from further experimentation as well as from linking key empirical findings to theoretical justifications. Also, the two main contributions of the work are not so substantial and overall the paper mostly proves an already established fact: that fully unsupervised models for cross-lingual word embedding learning are very unstable and their success relies on a large spectrum of design choices and hyper-parameters. I would also like to see the analysis expanded beyond UBDI to other (more downstream) tasks where such low-resource representation learning regimes might be useful.</span></div><div class="note_contents"><span class="note_content_field">Rating: </span><span class="note_content_value">5: Marginally below acceptance threshold</span></div><div class="note_contents"><span class="note_content_field">Confidence: </span><span class="note_content_value">4: The reviewer is confident but not absolutely certain that the evaluation is correct</span></div><div class="reply_row clearfix"></div></div><div class="children"></div></div></div></main></div>
  </div>
</div>


    <!-- Footer -->
    <footer class="sitemap">
      <div class="container">
        <div class="row hidden-xs">
          <div class="col-sm-4">
            <ul class="list-unstyled">
              <li><a href="/" class="home">Home</a></li>
              <li><a href="/venues">All Venues</a></li>
            </ul>
          </div>
    
          <div class="col-sm-4">
            <ul class="list-unstyled">
              <li><a href="/about">About OpenReview</a></li>
              <li><a href="/contact">Contact</a></li>
              <li><a href="#" data-toggle="modal" data-target="#feedback-modal">Feedback</a></li>
            </ul>
          </div>
    
          <div class="col-sm-4">
            <ul class="list-unstyled">
              <li><a href="/terms">Terms of Service</a></li>
              <li><a href="/privacy">Privacy Policy</a></li>
            </ul>
          </div>
        </div>
    
        <div class="row visible-xs-block">
          <div class="col-xs-6">
            <ul class="list-unstyled">
              <li><a href="/" class="home">Home</a></li>
              <li><a href="/about">About OpenReview</a></li>
              <li><a href="#" data-toggle="modal" data-target="#feedback-modal">Feedback</a></li>
            </ul>
          </div>
    
          <div class="col-xs-6">
            <ul class="list-unstyled">
              <li><a href="/contact">Contact</a></li>
              <li><a href="/terms">Terms of Service</a></li>
              <li><a href="/privacy">Privacy Policy</a></li>
            </ul>
          </div>
        </div>
      </div>
    </footer>
    
    <footer class="sponsor">
      <div class="container">
        <div class="row">
          <div class="col-sm-10 col-sm-offset-1">
            <p class="text-center">
              OpenReview is created by the <a href="http://www.iesl.cs.umass.edu/" target="_blank">Information Extraction and Synthesis Laboratory</a>, College of Information and Computer Science, University of Massachusetts Amherst. We gratefully acknowledge the support of the OpenReview sponsors:  Google,  Facebook, NSF, the University of Massachusetts Amherst Center for Data Science, and Center for Intelligent Information Retrieval, as well as the Google Cloud Platform for donating the computing and networking services on which OpenReview.net runs.
            </p>
          </div>
        </div>
      </div>
    </footer>

  <div id="feedback-modal" class="modal fade" tabindex="-1" role="dialog">
    <div class="modal-dialog">
      <div class="modal-content">
        <div class="modal-header">
          <button type="button" class="close" data-dismiss="modal" aria-label="Close"><span aria-hidden="true">×</span></button>
          <h3 class="modal-title">Send Feedback</h3>
        </div>
        <div class="modal-body">
          <p>Enter your feedback below and we'll get back to you as soon as possible.</p>
          <form action="/feedback" method="POST">
            <div class="form-group">
              <input type="email" name="from" class="form-control" placeholder="Email" />
            </div>
            <div class="form-group">
              <input type="text" name="subject" class="form-control" placeholder="Subject" />
            </div>
            <div class="form-group">
              <textarea name="message" class="form-control feedback-input" rows="5" placeholder="Message" required=""></textarea>
            </div>
          <ul id="ui-id-2" tabindex="0" class="ui-menu ui-widget ui-widget-content" style="display: none;"></ul></form>
        </div>
        <div class="modal-footer">
          <button type="button" class="btn btn-default" data-dismiss="modal">Cancel</button>
          <button type="button" class="btn btn-primary">Send</button>
        </div>
      </div>
    </div>
  </div>

  <script type="text/javascript" async="" src="https://www.google-analytics.com/analytics.js"></script><script src="//cdnjs.cloudflare.com/ajax/libs/jquery/2.2.4/jquery.min.js"></script>
  <script>window.jQuery || document.write('&lt;script src="/static/js/vendor/jquery-2.2.4.min.js"&gt;&lt;\/script&gt;')</script>

  <script src="/static/dist/vendor.min.js"></script>
  <script src="/static/dist/templates.min.js"></script>
  <script src="/static/dist/openreview.min.js"></script>

    <script>window.legacyScripts = true;</script>


    <script async="" src="https://www.googletagmanager.com/gtag/js?id=UA-108703919-1"></script>
    <script>
      window.dataLayer = window.dataLayer || [];
      function gtag() { dataLayer.push(arguments); }
      gtag('js', new Date());
      gtag('config', 'UA-108703919-1');
    </script>


<div role="status" aria-live="assertive" aria-relevant="additions" class="ui-helper-hidden-accessible"></div></body></html>