<!DOCTYPE html><html xmlns="http://www.w3.org/1999/xhtml" class="no-js"><head>
  <meta charset="utf-8" />
  <meta http-equiv="X-UA-Compatible" content="IE=edge,chrome=1" />
  <meta name="viewport" content="width=device-width, initial-scale=1" />

  <title>Quantization for Rapid Deployment of Deep Neural Networks | OpenReview</title>
  <meta name="description" content="" />

      <meta name="citation_title" content="Quantization for Rapid Deployment of Deep Neural Networks" />
        <meta name="citation_author" content="Anonymous" />
      <meta name="citation_publication_date" content="2018/09/27" />
      <meta name="citation_online_date" content="2018/09/27" />
      <meta name="citation_pdf_url" content="https://openreview.net/pdf?id=HkzZBi0cFQ" />
      <meta name="twitter:card" content="summary" />
      <meta name="twitter:site" content="@openreviewnet" />
      <meta name="og:title" content="Quantization for Rapid Deployment of Deep Neural Networks" />
      <meta name="og:description" content="This paper aims at rapid deployment of the state-of-the-art deep neural networks (DNNs) to energy efficient accelerators without time-consuming fine tuning or the availability of the full datasets...." />
      <meta name="og:image" content="https://openreview.net/static/images/openreview_logo_512.png" />

  <link rel="stylesheet" href="https://fonts.googleapis.com/css?family=Noto+Sans:400,400i,700,700i" />

  <link rel="stylesheet" href="/static/css/bootstrap.min.css" />
  <link rel="stylesheet" href="/static/css/jquery-ui.min.css" />
  <link rel="stylesheet" href="/static/css/main.min.css" />
</head>

<body class="forum">
  <nav class="navbar navbar-inverse navbar-fixed-top" role="navigation">
    <div class="container">
  
      <div class="navbar-header">
        <button type="button" class="navbar-toggle collapsed" data-toggle="collapse" data-target="#navbar" aria-expanded="false" aria-controls="navbar">
          <span class="sr-only">Toggle navigation</span>
          <span class="icon-bar"></span>
          <span class="icon-bar"></span>
          <span class="icon-bar"></span>
        </button>
        <a class="navbar-brand home push-link" href="/"><strong>OpenReview</strong>.net</a>
      </div>
  
      <div id="navbar" class="navbar-collapse collapse">
        <form class="navbar-form navbar-left profile-search" role="search">
          <div class="form-group has-feedback">
            <input id="search_input" type="text" class="form-control ui-autocomplete-input" placeholder="Search ICLR 2019 Conference" autocomplete="off" />
            <span class="glyphicon glyphicon-search form-control-feedback" aria-hidden="true"></span>
          </div>
  
          <input id="search_group" type="hidden" value="ICLR.cc/2019/Conference" />
          <input id="search_content" type="hidden" value="all" />
        <ul id="ui-id-1" tabindex="0" class="ui-menu ui-widget ui-widget-content ui-autocomplete ui-front" style="display: none;"></ul></form>
  
        <ul class="nav navbar-nav navbar-right">
        
            <li id="user-menu"><a href="/login">Login</a></li>
        </ul>
      </div>
  
    </div>
  </nav>

  <div id="or-banner" class="banner" style="">
  <div class="container">
    <div class="row">
      <div class="col-xs-12"><a href="/group?id=ICLR.cc/2019/Conference" title="Venue Homepage"><img class="icon" src="/static/images/arrow_left.svg" /> Go to <strong>ICLR 2019 Conference</strong> homepage</a></div>
    </div>
  </div>
</div>
<div id="flash-message-container" class="alert alert-danger" role="alert" style="display: none;">
  <div class="container">
    <div class="row">
      <div class="col-xs-12">
        <div class="alert-content">
          <button type="button" class="close" aria-label="Close"><span aria-hidden="true">×</span></button>
        </div>
      </div>
    </div>
  </div>
</div>

<div class="container">
  <div class="row">
    <div class="col-xs-12">
      <main id="content" class="clearfix openbanner-visible legacy-styles"><div id="note_HkzZBi0cFQ" class="note panel"><div class="title_pdf_row clearfix"><h2 class="note_content_title"><a>Quantization for Rapid Deployment of Deep Neural Networks</a> <a class="note_content_pdf" href="/pdf?id=HkzZBi0cFQ" title="Download PDF" target="_blank"><img src="/static/images/pdf_icon_blue.svg" /></a> </h2></div><div class="meta_row"><span class="signatures">Anonymous</span></div><div class="clearfix"><div class="meta_row pull-left"><span class="date item">28 Sep 2018 (modified: 20 Nov 2018)</span><span class="item">ICLR 2019 Conference Blind Submission</span><span class="item">Readers: <span class="readers-icon glyphicon glyphicon-globe"></span> Everyone</span><span class="item"><a class="action-bibtex-modal" data-bibtex="@inproceedings{    &#10;anonymous2019quantization,    &#10;title={Quantization for Rapid Deployment of Deep Neural Networks},    &#10;author={Anonymous},    &#10;booktitle={Submitted to International Conference on Learning Representations},    &#10;year={2019},    &#10;url={https://openreview.net/forum?id=HkzZBi0cFQ},    &#10;note={under review}    &#10;}">Show Bibtex</a></span><a class="note_content_pdf item" href="/revisions?id=HkzZBi0cFQ" target="_blank">Show Revisions</a></div><div class="meta_row meta_actions"></div></div><div class="note_contents"><span class="note_content_field">Abstract: </span><span class="note_content_value">This paper aims at rapid deployment of the state-of-the-art deep neural networks (DNNs) to energy efficient accelerators without time-consuming fine tuning or the availability of the full datasets.  Converting DNNs in full precision to limited precision is essential in taking advantage of the accelerators with reduced memory footprint and computation power. However, such a task is not trivial since it often requires the full training and validation datasets for profiling the network statistics and fine tuning the networks to recover the accuracy lost after quantization. To address these issues, we propose a simple method recognizing channel-level distribution to reduce the quantization-induced accuracy loss and minimize the required image samples for profiling. We evaluated our method on eleven networks trained on the ImageNet classification benchmark and a network trained on the Pascal VOC object detection benchmark. The results prove that the networks can be quantized into 8-bit integer precision without fine tuning.</span></div><div class="reply_row clearfix"><div class="item" id="reply_count">6 Replies</div></div></div><hr class="small" /><div id="note_children"><div class="note_with_children"><div id="note_Hklvh2J_6m" class="note panel"><div class="title_pdf_row clearfix"><h2 class="note_content_title"><a>promising method for 8-bit quantization without sensitivity to outliers, limited in novelty and presentation clarity</a>  <button class="btn btn-xs btn-default permalink-button" title="Link to this comment" data-permalink-url="https://openreview.net/forum?id=HkzZBi0cFQ&amp;noteId=Hklvh2J_6m"><span class="glyphicon glyphicon-link" aria-hidden="true"></span></button></h2></div><div class="meta_row"><span class="signatures">ICLR 2019 Conference Paper62 AnonReviewer4</span></div><div class="clearfix"><div class="meta_row pull-left"><span class="date item">13 Nov 2018</span><span class="item">ICLR 2019 Conference Paper62 Official Review</span><span class="item">Readers: <span class="readers-icon glyphicon glyphicon-globe"></span> Everyone</span></div><div class="meta_row meta_actions"></div></div><div class="note_contents"><span class="note_content_field">Review: </span><span class="note_content_value">The paper proposes channel-wise 8-bit quantization rather than layer-wise. It further takes advantage of work using moment analysis instead of just MAX values to avoid susceptibility to outliers. The main take-away seems to be that channel-wise set ups limit the need for outlier removal and the care with which you select your data subset when performing quantization.

Pros:
- using channel-wise quantization (with MAX values or moment-analysis) yields improvement over layer-wise MAX approaches
- limits the amount of care that is needed to be taken when applying quantization (e.g. size of data subset used)
- shows differences in degradation when blindly applying quantization methods to different networks; with less (but still some) variation in degradation when applying channel-wise quantization

Cons:
- unclear how much is gained over layer-wise and MAX value methods with careful tuning/removal of outliers; would be good to see if careful tuning closes the gap or if channel-wise methods are the clear winner
- unclear if the layer-wise set up with moment-analysis could help to avoid the need for outlier removal altogether and (potentially) offer similar improvements to the channel-wise set up; a few more experiments are important to determine specifically if improvement is with respect to channel-wise or moment-analysis since only layer-wise MAX results are presented
- clarity, presentation, and organization can be improved to help with flow, avoid confusion, and improve readability

Overall:
The paper offers nice empirical results regarding the relative ease with which one can quantize networks when considering channel-wise quantization (and moment-analysis), but the overall novelty is limited. With the limited novelty, the primary benefits appear to be the ease of quantization for rapid deployment and channel-wise setups. Comparisons with stronger baseline numbers when using layer-wise methods would give a more complete picture. In addition, having these stronger tuned baseline numbers on even more networks would be great to show that the channel-wise method has clear impact across the board, even with respect to well-tuned layer-wise baselines. These results could give better support for the importance of the novelty.</span></div><div class="note_contents"><span class="note_content_field">Rating: </span><span class="note_content_value">5: Marginally below acceptance threshold</span></div><div class="note_contents"><span class="note_content_field">Confidence: </span><span class="note_content_value">3: The reviewer is fairly confident that the evaluation is correct</span></div><div class="reply_row clearfix"></div></div><div class="children"><div class="note_with_children"><div id="note_HygNgn6eRQ" class="note panel"><div class="title_pdf_row clearfix"><h2 class="note_content_title"><a>Response to Reviewer</a>  <button class="btn btn-xs btn-default permalink-button" title="Link to this comment" data-permalink-url="https://openreview.net/forum?id=HkzZBi0cFQ&amp;noteId=HygNgn6eRQ"><span class="glyphicon glyphicon-link" aria-hidden="true"></span></button></h2></div><div class="meta_row"><span class="signatures">ICLR 2019 Conference Paper62 Authors</span></div><div class="clearfix"><div class="meta_row pull-left"><span class="date item">20 Nov 2018</span><span class="item">ICLR 2019 Conference Paper62 Official Comment</span><span class="item">Readers: <span class="readers-icon glyphicon glyphicon-globe"></span> Everyone</span></div><div class="meta_row meta_actions"></div></div><div class="note_contents"><span class="note_content_field">Comment: </span><span class="note_content_value">Thank you very much for your helpful comments. We have addressed them as follows:

1. Results of layer-wise quantization with moment-analysis method were added
: The layer-wise quantization with moment-analysis method could remove the effect of outliers. To show this clearly, we added the results of applying moment-analysis method to layer-wise quantization in Table 1 and 2 in the revised manuscript. However, even if outliers were to be removed, channel-wise method consistently outperforms layer-wise one. Channel-wise method in comparison to the layer-wise method both with moment-analysis (or MAX) clearly demonstrates that channel-wise has the advantage.

2. Novelty of the paper
: The main contribution of the paper is manipulation of the weights prior to inference for channel-wise quantization as shown in Figure 1 and Algorithm 1. Naïve channel-wise implementation requires shifters to be in place for each channel which would cause huge hardware cost (please see Figure 1 (b) in the revised manuscript). Thus, it is not a practical solution at all in HW perspective. The proposed method performs channel-wise quantization WITHOUT the need for any additional HW and we believe there’s novelty in that it enables channel-wise quantization for real-world HW. We modified Figure 1 and improved manuscript to reflect this point clearly. 
</span></div><div class="reply_row clearfix"></div></div><div class="children"></div></div></div></div><div class="note_with_children"><div id="note_HkltNd_637" class="note panel"><div class="title_pdf_row clearfix"><h2 class="note_content_title"><a>Novelty is limited</a>  <button class="btn btn-xs btn-default permalink-button" title="Link to this comment" data-permalink-url="https://openreview.net/forum?id=HkzZBi0cFQ&amp;noteId=HkltNd_637"><span class="glyphicon glyphicon-link" aria-hidden="true"></span></button></h2></div><div class="meta_row"><span class="signatures">ICLR 2019 Conference Paper62 AnonReviewer2</span></div><div class="clearfix"><div class="meta_row pull-left"><span class="date item">05 Nov 2018 (modified: 07 Nov 2018)</span><span class="item">ICLR 2019 Conference Paper62 Official Review</span><span class="item">Readers: <span class="readers-icon glyphicon glyphicon-globe"></span> Everyone</span></div><div class="meta_row meta_actions"></div></div><div class="note_contents"><span class="note_content_field">Review: </span><span class="note_content_value">This paper proposes an new 8-bit quantization strategy for rapid deployment. 

8-bit quantization has attracted many attentions recently. And it is already well used in GPU servers (cudnn), phones, ARM chips and various ASIC neural network chips. In these situations, almost no performance drop is observed for classification and detection tasks.

So, the novelty of this paper is limited.</span></div><div class="note_contents"><span class="note_content_field">Rating: </span><span class="note_content_value">5: Marginally below acceptance threshold</span></div><div class="note_contents"><span class="note_content_field">Confidence: </span><span class="note_content_value">4: The reviewer is confident but not absolutely certain that the evaluation is correct</span></div><div class="reply_row clearfix"></div></div><div class="children"><div class="note_with_children"><div id="note_B1eFz3plRX" class="note panel"><div class="title_pdf_row clearfix"><h2 class="note_content_title"><a>Response to Reviewer</a>  <button class="btn btn-xs btn-default permalink-button" title="Link to this comment" data-permalink-url="https://openreview.net/forum?id=HkzZBi0cFQ&amp;noteId=B1eFz3plRX"><span class="glyphicon glyphicon-link" aria-hidden="true"></span></button></h2></div><div class="meta_row"><span class="signatures">ICLR 2019 Conference Paper62 Authors</span></div><div class="clearfix"><div class="meta_row pull-left"><span class="date item">20 Nov 2018</span><span class="item">ICLR 2019 Conference Paper62 Official Comment</span><span class="item">Readers: <span class="readers-icon glyphicon glyphicon-globe"></span> Everyone</span></div><div class="meta_row meta_actions"></div></div><div class="note_contents"><span class="note_content_field">Comment: </span><span class="note_content_value">Thank you very much for your comments. Competing methods in other papers require retraining or needs to cope with high accuracy loss when quantized in a layer-wise fashion. The proposed method is the first of its kind to resolve these issues by incorporating channel-wise quantization and moment-analysis method which DOES NOT require retraining or the training dataset. Naïve channel-wise quantization requires adding huge number of HW shifters and providing values for them which make it unrealistic for implementation (please see Figure 1 (b) in the revised manuscript). The biggest contribution of our paper is the HW-friendly channel-wise quantization by manipulating the kernels prior to inference. For your reference, Figure 1 has been modified to make the distinction clearer.</span></div><div class="reply_row clearfix"></div></div><div class="children"></div></div></div></div><div class="note_with_children"><div id="note_ryebGUDO2X" class="note panel"><div class="title_pdf_row clearfix"><h2 class="note_content_title"><a>Interesting approach to channel-wise CNN quantization with adaptive bit allocation, with evaluation on eleven modern CNNs, comparison with simple layer-wise baseline</a>  <button class="btn btn-xs btn-default permalink-button" title="Link to this comment" data-permalink-url="https://openreview.net/forum?id=HkzZBi0cFQ&amp;noteId=ryebGUDO2X"><span class="glyphicon glyphicon-link" aria-hidden="true"></span></button></h2></div><div class="meta_row"><span class="signatures">ICLR 2019 Conference Paper62 AnonReviewer1</span></div><div class="clearfix"><div class="meta_row pull-left"><span class="date item">01 Nov 2018 (modified: 07 Nov 2018)</span><span class="item">ICLR 2019 Conference Paper62 Official Review</span><span class="item">Readers: <span class="readers-icon glyphicon glyphicon-globe"></span> Everyone</span></div><div class="meta_row meta_actions"></div></div><div class="note_contents"><span class="note_content_field">Review: </span><span class="note_content_value">This paper proposes a technique for channel-wise quantization of CNNs
to 8-bit, fixed point precision. The authors propose several
techniques for analyzing the statistical properties of output channel
activations in order to select the best fractional bit length for each
channel. Experimental results on eleven different CNN architectures
demonstrate that the approaches proposed result in significantly less
accuracy loss when compared to a layer-wise baseline.

The paper has the following strengths:

 1. The experimental results on eleven different architectures (of
    varying depth and breadth) are convincing, and are consistently
    better than layer-wise MAX for choosing fractional bit length.

The paper has the following weak points:

 1. There is not much coherence between the description of the
    approach in section 2.1, Figure 1, Algorithm 1, and
    Figure 2. Notation is used in Algorithm 1 which is never defined.
 2. Related to the previous point, the proposed technique has a lot of
    moving parts and I don't feel that it would be easy to reproduce
    the results of the paper. There are some vague statements, like
    "We resolve this complication by pre-coordinating the fractional
    lengths of the weights", which require significantly more
    precision. This issue -- one of the main issues with channel-wise
    versus layer-wise quantization -- is never returned to in the
    definition of the method.
 3. The experimental comparison with layer-wise quantization is
    somewhat lacking. Is layer-wise MAX the state-of-the-art in CNN
    quantization? The results comparing channel-wise and layer-wise
    MAX are already convincing, but are the moment-analysis approaches
    not equally applicable to layer-wise quantization?
    State-of-the-art results that are less sensitive to outliers
    should be included in Table 1. A comparison with layer-wise
    approaches would be nice to have also in Figure 4 to show
    sensitivity to profiling set size.

The experimental results in the paper are impressive, and the analysis
motivating the approach is convincing. However, there are presentation
and clarity issues in the technical development, and the comparative
analysis is lacking broader comparisons with the state-of-the-art (to
be fair, the authors recognize that layer-wise MAX as a baseline is
particularly susceptible to outliers). These two aspects combined,
however, lead me to the opinion that this work is just not quite ready
for publication at ICLR.
</span></div><div class="note_contents"><span class="note_content_field">Rating: </span><span class="note_content_value">5: Marginally below acceptance threshold</span></div><div class="note_contents"><span class="note_content_field">Confidence: </span><span class="note_content_value">4: The reviewer is confident but not absolutely certain that the evaluation is correct</span></div><div class="reply_row clearfix"></div></div><div class="children"><div class="note_with_children"><div id="note_r1xlSn6l0X" class="note panel"><div class="title_pdf_row clearfix"><h2 class="note_content_title"><a>Response to Reviewer</a>  <button class="btn btn-xs btn-default permalink-button" title="Link to this comment" data-permalink-url="https://openreview.net/forum?id=HkzZBi0cFQ&amp;noteId=r1xlSn6l0X"><span class="glyphicon glyphicon-link" aria-hidden="true"></span></button></h2></div><div class="meta_row"><span class="signatures">ICLR 2019 Conference Paper62 Authors</span></div><div class="clearfix"><div class="meta_row pull-left"><span class="date item">20 Nov 2018</span><span class="item">ICLR 2019 Conference Paper62 Official Comment</span><span class="item">Readers: <span class="readers-icon glyphicon glyphicon-globe"></span> Everyone</span></div><div class="meta_row meta_actions"></div></div><div class="note_contents"><span class="note_content_field">Comment: </span><span class="note_content_value">Thank you very much for your helpful comments and suggestions. 
1. Notations have been added to Algorithm 1 and modified Figure 1 along with its description, as suggested.
2. The channel-wise quantization algorithm is calculated statically as show in Algorithm 1 and exemplified in Figure 1. Algorithm 1 demonstrates all aspects of “pre-coordinating the fractional lengths of the weights.” Therefore, should be able to be reproduced by anyone and didn’t think it would be necessary to write it out in words. 
3. The layer-wise moment-analysis results have been added to the paper in Table 1 and 2. Channel-wise out-performs layer-wise in most cases without the need for additional HW. The overhead introduced in channel-wise quantization is just a simple pre-processing step of determining the fraction lengths by executing Algorithm 1.
We would like to emphasize that our paper proposes a practical solution for channel-wise quantization. Naïve channel-wise quantization requires adding huge number of HW shifters and providing values for them which make it unrealistic for implementation (please see Figure 1 in the revised manuscript). The proposed method does not require such cost by manipulating the kernels prior to inference. For your reference, Figure 1 has been modified to make the distinction clearer.
</span></div><div class="reply_row clearfix"></div></div><div class="children"></div></div></div></div></div></main></div>
  </div>
</div>


    <!-- Footer -->
    <footer class="sitemap">
      <div class="container">
        <div class="row hidden-xs">
          <div class="col-sm-4">
            <ul class="list-unstyled">
              <li><a href="/" class="home">Home</a></li>
              <li><a href="/venues">All Venues</a></li>
            </ul>
          </div>
    
          <div class="col-sm-4">
            <ul class="list-unstyled">
              <li><a href="/about">About OpenReview</a></li>
              <li><a href="/contact">Contact</a></li>
              <li><a href="#" data-toggle="modal" data-target="#feedback-modal">Feedback</a></li>
            </ul>
          </div>
    
          <div class="col-sm-4">
            <ul class="list-unstyled">
              <li><a href="/terms">Terms of Service</a></li>
              <li><a href="/privacy">Privacy Policy</a></li>
            </ul>
          </div>
        </div>
    
        <div class="row visible-xs-block">
          <div class="col-xs-6">
            <ul class="list-unstyled">
              <li><a href="/" class="home">Home</a></li>
              <li><a href="/about">About OpenReview</a></li>
              <li><a href="#" data-toggle="modal" data-target="#feedback-modal">Feedback</a></li>
            </ul>
          </div>
    
          <div class="col-xs-6">
            <ul class="list-unstyled">
              <li><a href="/contact">Contact</a></li>
              <li><a href="/terms">Terms of Service</a></li>
              <li><a href="/privacy">Privacy Policy</a></li>
            </ul>
          </div>
        </div>
      </div>
    </footer>
    
    <footer class="sponsor">
      <div class="container">
        <div class="row">
          <div class="col-sm-10 col-sm-offset-1">
            <p class="text-center">
              OpenReview is created by the <a href="http://www.iesl.cs.umass.edu/" target="_blank">Information Extraction and Synthesis Laboratory</a>, College of Information and Computer Science, University of Massachusetts Amherst. We gratefully acknowledge the support of the OpenReview sponsors:  Google,  Facebook, NSF, the University of Massachusetts Amherst Center for Data Science, and Center for Intelligent Information Retrieval, as well as the Google Cloud Platform for donating the computing and networking services on which OpenReview.net runs.
            </p>
          </div>
        </div>
      </div>
    </footer>

  <div id="feedback-modal" class="modal fade" tabindex="-1" role="dialog">
    <div class="modal-dialog">
      <div class="modal-content">
        <div class="modal-header">
          <button type="button" class="close" data-dismiss="modal" aria-label="Close"><span aria-hidden="true">×</span></button>
          <h3 class="modal-title">Send Feedback</h3>
        </div>
        <div class="modal-body">
          <p>Enter your feedback below and we'll get back to you as soon as possible.</p>
          <form action="/feedback" method="POST">
            <div class="form-group">
              <input type="email" name="from" class="form-control" placeholder="Email" />
            </div>
            <div class="form-group">
              <input type="text" name="subject" class="form-control" placeholder="Subject" />
            </div>
            <div class="form-group">
              <textarea name="message" class="form-control feedback-input" rows="5" placeholder="Message" required=""></textarea>
            </div>
          <ul id="ui-id-2" tabindex="0" class="ui-menu ui-widget ui-widget-content" style="display: none;"></ul></form>
        </div>
        <div class="modal-footer">
          <button type="button" class="btn btn-default" data-dismiss="modal">Cancel</button>
          <button type="button" class="btn btn-primary">Send</button>
        </div>
      </div>
    </div>
  </div>

  <script type="text/javascript" async="" src="https://www.google-analytics.com/analytics.js"></script><script src="//cdnjs.cloudflare.com/ajax/libs/jquery/2.2.4/jquery.min.js"></script>
  <script>window.jQuery || document.write('&lt;script src="/static/js/vendor/jquery-2.2.4.min.js"&gt;&lt;\/script&gt;')</script>

  <script src="/static/dist/vendor.min.js"></script>
  <script src="/static/dist/templates.min.js"></script>
  <script src="/static/dist/openreview.min.js"></script>

    <script>window.legacyScripts = true;</script>


    <script async="" src="https://www.googletagmanager.com/gtag/js?id=UA-108703919-1"></script>
    <script>
      window.dataLayer = window.dataLayer || [];
      function gtag() { dataLayer.push(arguments); }
      gtag('js', new Date());
      gtag('config', 'UA-108703919-1');
    </script>


<div role="status" aria-live="assertive" aria-relevant="additions" class="ui-helper-hidden-accessible"></div></body></html>