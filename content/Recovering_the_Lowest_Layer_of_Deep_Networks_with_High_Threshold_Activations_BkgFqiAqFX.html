<!DOCTYPE html><html xmlns="http://www.w3.org/1999/xhtml" class="no-js"><head>
  <meta charset="utf-8" />
  <meta http-equiv="X-UA-Compatible" content="IE=edge,chrome=1" />
  <meta name="viewport" content="width=device-width, initial-scale=1" />

  <title>Recovering the Lowest Layer of Deep Networks with High Threshold Activations | OpenReview</title>
  <meta name="description" content="" />

      <meta name="citation_title" content="Recovering the Lowest Layer of Deep Networks with High Threshold Activations" />
        <meta name="citation_author" content="Anonymous" />
      <meta name="citation_publication_date" content="2018/09/27" />
      <meta name="citation_online_date" content="2018/09/27" />
      <meta name="citation_pdf_url" content="https://openreview.net/pdf?id=BkgFqiAqFX" />
      <meta name="twitter:card" content="summary" />
      <meta name="twitter:site" content="@openreviewnet" />
      <meta name="og:title" content="Recovering the Lowest Layer of Deep Networks with High Threshold..." />
      <meta name="og:description" content="Giving provable guarantees for learning neural networks is a core challenge of machine learning theory. Most prior work gives parameter recovery guarantees for one hidden layer networks, however..." />
      <meta name="og:image" content="https://openreview.net/static/images/openreview_logo_512.png" />

  <link rel="stylesheet" href="https://fonts.googleapis.com/css?family=Noto+Sans:400,400i,700,700i" />

  <link rel="stylesheet" href="/static/css/bootstrap.min.css" />
  <link rel="stylesheet" href="/static/css/jquery-ui.min.css" />
  <link rel="stylesheet" href="/static/css/main.min.css" />
</head>

<body class="forum">
  <nav class="navbar navbar-inverse navbar-fixed-top" role="navigation">
    <div class="container">
  
      <div class="navbar-header">
        <button type="button" class="navbar-toggle collapsed" data-toggle="collapse" data-target="#navbar" aria-expanded="false" aria-controls="navbar">
          <span class="sr-only">Toggle navigation</span>
          <span class="icon-bar"></span>
          <span class="icon-bar"></span>
          <span class="icon-bar"></span>
        </button>
        <a class="navbar-brand home push-link" href="/"><strong>OpenReview</strong>.net</a>
      </div>
  
      <div id="navbar" class="navbar-collapse collapse">
        <form class="navbar-form navbar-left profile-search" role="search">
          <div class="form-group has-feedback">
            <input id="search_input" type="text" class="form-control ui-autocomplete-input" placeholder="Search ICLR 2019 Conference" autocomplete="off" />
            <span class="glyphicon glyphicon-search form-control-feedback" aria-hidden="true"></span>
          </div>
  
          <input id="search_group" type="hidden" value="ICLR.cc/2019/Conference" />
          <input id="search_content" type="hidden" value="all" />
        <ul id="ui-id-1" tabindex="0" class="ui-menu ui-widget ui-widget-content ui-autocomplete ui-front" style="display: none;"></ul></form>
  
        <ul class="nav navbar-nav navbar-right">
        
            <li id="user-menu"><a href="/login">Login</a></li>
        </ul>
      </div>
  
    </div>
  </nav>

  <div id="or-banner" class="banner" style="">
  <div class="container">
    <div class="row">
      <div class="col-xs-12"><a href="/group?id=ICLR.cc/2019/Conference" title="Venue Homepage"><img class="icon" src="/static/images/arrow_left.svg" /> Go to <strong>ICLR 2019 Conference</strong> homepage</a></div>
    </div>
  </div>
</div>
<div id="flash-message-container" class="alert alert-danger" role="alert" style="display: none;">
  <div class="container">
    <div class="row">
      <div class="col-xs-12">
        <div class="alert-content">
          <button type="button" class="close" aria-label="Close"><span aria-hidden="true">×</span></button>
        </div>
      </div>
    </div>
  </div>
</div>

<div class="container">
  <div class="row">
    <div class="col-xs-12">
      <main id="content" class="clearfix openbanner-visible legacy-styles"><div id="note_BkgFqiAqFX" class="note panel"><div class="title_pdf_row clearfix"><h2 class="note_content_title"><a>Recovering the Lowest Layer of Deep Networks with High Threshold Activations</a> <a class="note_content_pdf" href="/pdf?id=BkgFqiAqFX" title="Download PDF" target="_blank"><img src="/static/images/pdf_icon_blue.svg" /></a> </h2></div><div class="meta_row"><span class="signatures">Anonymous</span></div><div class="clearfix"><div class="meta_row pull-left"><span class="date item">28 Sep 2018 (modified: 20 Nov 2018)</span><span class="item">ICLR 2019 Conference Blind Submission</span><span class="item">Readers: <span class="readers-icon glyphicon glyphicon-globe"></span> Everyone</span><span class="item"><a class="action-bibtex-modal" data-bibtex="@inproceedings{    &#10;anonymous2019recovering,    &#10;title={Recovering the Lowest Layer of Deep Networks with High Threshold Activations},    &#10;author={Anonymous},    &#10;booktitle={Submitted to International Conference on Learning Representations},    &#10;year={2019},    &#10;url={https://openreview.net/forum?id=BkgFqiAqFX},    &#10;note={under review}    &#10;}">Show Bibtex</a></span><a class="note_content_pdf item" href="/revisions?id=BkgFqiAqFX" target="_blank">Show Revisions</a></div><div class="meta_row meta_actions"></div></div><div class="note_contents"><span class="note_content_field">Abstract: </span><span class="note_content_value">Giving provable guarantees for learning neural networks is a core challenge of machine learning theory. Most prior work gives parameter recovery guarantees for one hidden layer networks, however, the networks used in practice have multiple non-linear layers. In this work, we show how we can strengthen such results to deeper networks -- we address the problem of uncovering the lowest layer in a deep neural network under the assumption that the lowest layer uses a high threshold before applying the activation, the upper network can be modeled as a well-behaved polynomial and the input distribution is gaussian.</span></div><div class="note_contents"><span class="note_content_field">Keywords: </span><span class="note_content_value">Deep Learning, Parameter Recovery, Non-convex optimization, high threshold activation</span></div><div class="note_contents"><span class="note_content_field">TL;DR: </span><span class="note_content_value">We provably recover the lowest layer in a deep neural network assuming that the lowest layer uses a "high threshold" activation and the above network is a "well-behaved" polynomial.</span></div><div class="reply_row clearfix"><div class="item" id="reply_count">6 Replies</div></div></div><hr class="small" /><div id="note_children"><div class="note_with_children"><div id="note_BJed2wl9Tm" class="note panel"><div class="title_pdf_row clearfix"><h2 class="note_content_title"><a>Review</a>  <button class="btn btn-xs btn-default permalink-button" title="Link to this comment" data-permalink-url="https://openreview.net/forum?id=BkgFqiAqFX&amp;noteId=BJed2wl9Tm"><span class="glyphicon glyphicon-link" aria-hidden="true"></span></button></h2></div><div class="meta_row"><span class="signatures">ICLR 2019 Conference Paper550 AnonReviewer4</span></div><div class="clearfix"><div class="meta_row pull-left"><span class="date item">15 Nov 2018</span><span class="item">ICLR 2019 Conference Paper550 Official Review</span><span class="item">Readers: <span class="readers-icon glyphicon glyphicon-globe"></span> Everyone</span></div><div class="meta_row meta_actions"></div></div><div class="note_contents"><span class="note_content_field">Review: </span><span class="note_content_value">This paper gives provable recovery guarantees for a class of neural networks which have high-threshold activation in the first layer, followed by a "well-behaved" polynomial, under Gaussian input. The algorithm is based on the approach by Ge et al. (2017), as well as an iterative refinement method.

While this could be an interesting result, I have several concerns regarding the assumptions, correctness, and writing.

1) It is required that the threshold is at least sqrt{log d} (Thm. 1), where d is the number of hidden neurons in the first layer. It seems that this essentially zeros out almost all the neurons, since the maximum among d Gaussian random variables is roughly sqrt{log d}. The authors should explain what exactly this model is doing, i.e., what kind of functions it can compute, in order to justify why this is an interesting model.

Furthermore, the authors claim that the studied model is a "deep" neural network, but I disagree. As I understand, the difference between this model and two-layer networks is that the second layer here is a polynomial instead of a linear function. This doesn't make it a deep network since the (polynomial) part above the first layer is not modeled in a layer-wise fashion, not to mention that under the setting considered in the paper the polynomial behaves similar to a linear function.

2) It is stated at the end of Section 2 that the angle can be reduced by a factor of 1-1/d ***with constant probability***. How does this ensure you can succeed after O(d log(1/nu)) iterations? As far as I see you need the success probability in one iteration to be at least something like 1-1/Omega(d) so that you can apply a union bound.

3) Even if the issues of motivation and correctness are clarified, I find it very difficult to understand the overall intuition and main technical contributions in this paper. The writing needs to be significantly improved to reach the level of a top conference.</span></div><div class="note_contents"><span class="note_content_field">Rating: </span><span class="note_content_value">4: Ok but not good enough - rejection</span></div><div class="note_contents"><span class="note_content_field">Confidence: </span><span class="note_content_value">4: The reviewer is confident but not absolutely certain that the evaluation is correct</span></div><div class="reply_row clearfix"></div></div><div class="children"><div class="note_with_children"><div id="note_HJeVEP-WCm" class="note panel"><div class="title_pdf_row clearfix"><h2 class="note_content_title"><a>response</a>  <button class="btn btn-xs btn-default permalink-button" title="Link to this comment" data-permalink-url="https://openreview.net/forum?id=BkgFqiAqFX&amp;noteId=HJeVEP-WCm"><span class="glyphicon glyphicon-link" aria-hidden="true"></span></button></h2></div><div class="meta_row"><span class="signatures">ICLR 2019 Conference Paper550 Authors</span></div><div class="clearfix"><div class="meta_row pull-left"><span class="date item">20 Nov 2018</span><span class="item">ICLR 2019 Conference Paper550 Official Comment</span><span class="item">Readers: <span class="readers-icon glyphicon glyphicon-globe"></span> Everyone</span></div><div class="meta_row meta_actions"></div></div><div class="note_contents"><span class="note_content_field">Comment: </span><span class="note_content_value">We would like to thank the reviewer for their feedback. We would like to answer for each point:
1) a) We agree that the threshold seems high however we note that this might be necessary for the following reason. Our work can be viewed in the SIGN activation case as learning a union of half spaces. Conditioning on the label being positive, the distribution of the points can be viewed as being drawn from a mixture of halfspaces. The distribution of points on the positive side of one hyperplane can be viewed as a truncated Gaussian distribution with mean as the normal to the halfspace. With this viewpoint, recovering the weight vectors corresponding to the union of halfspaces is intuitively similar to learning the means of a mixture of Gaussians.  The threshold value of Omega(\sqrt \log d) matches the bound between centers required to learn a mixture of Gaussians. The paper <a href="https://arxiv.org/abs/1710.11592" target="_blank" rel="nofollow">https://arxiv.org/abs/1710.11592</a> (FOCS 2017) shows a lower bound of Omega(\sqrt \log d) to be able to recover a collection of d Gaussians in polynomial sample complexity. In fact, for a mixture of Gaussians the FOCS 2017 paper only gives a polynomial sample complexity upper bound with exponential running time whereas for our problem we recover the half spaces in polynomial time. Also note that, as a by product, we are getting the first polynomial algorithm to learn intersection of halfspaces if they are \Omega(\sqrt \log d) far from the origin.
b) As for the criticism that the polynomial is close to a linear function, please see bullet 3 in response to AnonReviewer3.
c) As for why our framework of using a polynomial P applies to a deep network: Note that if the activation function is continuous such as sigmoid or log(1+e^x) (which is a continuous approximation of RELU) it will have a Taylor series, and further, stacking such activations layer by layer will also have a Taylor series. All we are assuming is that there is a Taylor series that models the function represented by the upper layers. We are just calling that Taylor representation as P.

2) With a constant probability we can improve after each iteration, however we can also check if we have improved or not by using our estimating technique, so we only change our estimate if we improve. We refer the reviewer to step 3 &amp; 4 of Algorithm 1. Thus we have to try only a constant times to get a good move and make progress, hence the bound.

We hope the reviewer will take the above discussion into consideration.</span></div><div class="reply_row clearfix"></div></div><div class="children"></div></div></div></div><div class="note_with_children"><div id="note_S1gTl83L6X" class="note panel"><div class="title_pdf_row clearfix"><h2 class="note_content_title"><a>Review</a>  <button class="btn btn-xs btn-default permalink-button" title="Link to this comment" data-permalink-url="https://openreview.net/forum?id=BkgFqiAqFX&amp;noteId=S1gTl83L6X"><span class="glyphicon glyphicon-link" aria-hidden="true"></span></button></h2></div><div class="meta_row"><span class="signatures">ICLR 2019 Conference Paper550 AnonReviewer5</span></div><div class="clearfix"><div class="meta_row pull-left"><span class="date item">12 Nov 2018</span><span class="item">ICLR 2019 Conference Paper550 Official Review</span><span class="item">Readers: <span class="readers-icon glyphicon glyphicon-globe"></span> Everyone</span></div><div class="meta_row meta_actions"></div></div><div class="note_contents"><span class="note_content_field">Review: </span><span class="note_content_value">This paper considers the problem of recovering the lowest layer of a deep neural network whose architecture is ReLU or sign function followed by a polynomial. This paper relies on three assumptions: 1) the lowest layer has a high threshold (\Omleg(\sqrt{d})), 2) the polynomial has 1/poly(d) lower bouned and O(1) upper bounded linear terms and is monotone 3) the input is Gaussian. Under these assumptions, this paper shows it is possible to learn the lowest layer in precision \eps in poly(1/eps, d) time.

The proposed algorithm has two steps. The first step is based on the landscape design approach proposed by Ge et al. (2017) and the second step is based on checking the correlation. 

Provably learning a neural network is a major problem in theoretical machine learning. The assumptions made in this paper are fine for me and I think this paper indeed has some new interesting observation. My major concern is the writing. There are several components of the algorithm. However, it is hard to digest the intuition behind each component and how the assumptions are used. I suggest authors providing a high-level and non-technical description of the whole algorithm at the beginning. If authors can significantly improve the writing, I am happy to re-evaluate my comments and increase my rating.</span></div><div class="note_contents"><span class="note_content_field">Rating: </span><span class="note_content_value">5: Marginally below acceptance threshold</span></div><div class="note_contents"><span class="note_content_field">Confidence: </span><span class="note_content_value">3: The reviewer is fairly confident that the evaluation is correct</span></div><div class="reply_row clearfix"></div></div><div class="children"><div class="note_with_children"><div id="note_HJxIkwbbAQ" class="note panel"><div class="title_pdf_row clearfix"><h2 class="note_content_title"><a>response</a>  <button class="btn btn-xs btn-default permalink-button" title="Link to this comment" data-permalink-url="https://openreview.net/forum?id=BkgFqiAqFX&amp;noteId=HJxIkwbbAQ"><span class="glyphicon glyphicon-link" aria-hidden="true"></span></button></h2></div><div class="meta_row"><span class="signatures">ICLR 2019 Conference Paper550 Authors</span></div><div class="clearfix"><div class="meta_row pull-left"><span class="date item">20 Nov 2018</span><span class="item">ICLR 2019 Conference Paper550 Official Comment</span><span class="item">Readers: <span class="readers-icon glyphicon glyphicon-globe"></span> Everyone</span></div><div class="meta_row meta_actions"></div></div><div class="note_contents"><span class="note_content_field">Comment: </span><span class="note_content_value">We would like to thank the reviewer for the constructive feedback. Following the reviewer's advice, we have added a high-level overview of the algorithm before describing the technical content (beginning of section 2).

We hope the reviewer will take this into consideration.</span></div><div class="reply_row clearfix"></div></div><div class="children"></div></div></div></div><div class="note_with_children"><div id="note_SJe9BFUY2m" class="note panel"><div class="title_pdf_row clearfix"><h2 class="note_content_title"><a>review</a>  <button class="btn btn-xs btn-default permalink-button" title="Link to this comment" data-permalink-url="https://openreview.net/forum?id=BkgFqiAqFX&amp;noteId=SJe9BFUY2m"><span class="glyphicon glyphicon-link" aria-hidden="true"></span></button></h2></div><div class="meta_row"><span class="signatures">ICLR 2019 Conference Paper550 AnonReviewer3</span></div><div class="clearfix"><div class="meta_row pull-left"><span class="date item">02 Nov 2018 (modified: 07 Nov 2018)</span><span class="item">ICLR 2019 Conference Paper550 Official Review</span><span class="item">Readers: <span class="readers-icon glyphicon glyphicon-globe"></span> Everyone</span></div><div class="meta_row meta_actions"></div></div><div class="note_contents"><span class="note_content_field">Review: </span><span class="note_content_value">This paper gives a new algorithm for learning parameters of neural network under several assumptions: 1. the threshold for the first layer is very high; 2. the future layers of the neural network can be approximated by a polynomial. 3. The input distribution is Gaussian.

It is unclear why any of these assumptions are true. For 1, the thresholds in neural networks are certainly not as high as required in the algorithm (for the threshold in the paper after the first layer the neurons will be super sparse/often even just equal to 0, this is not really observed in real neural networks). For 2, there are no general results showing neural networks can be effectively approximated by low degree polynomials, and, if the future networks can be approximated, what prevents you from just assuming the entire neural network is a low degree polynomial? People have tried fitting polynomials and that does not perform nearly as well as neural networks.

The proof of the paper makes the problem even more clear because the paper shows that with this high threshold in the first layer, the future layers just behave linearly. This is again very far from true in any real neural networks.

Overall I'm OK with making some strong assumptions in order to prove some results for neural networks - after all it is a very difficult problem. However, this paper makes too many unrealistic assumptions. It's OK to make one of these assumptions, maybe 2, but 3 is too much for me.</span></div><div class="note_contents"><span class="note_content_field">Rating: </span><span class="note_content_value">4: Ok but not good enough - rejection</span></div><div class="note_contents"><span class="note_content_field">Confidence: </span><span class="note_content_value">4: The reviewer is confident but not absolutely certain that the evaluation is correct</span></div><div class="reply_row clearfix"></div></div><div class="children"><div class="note_with_children"><div id="note_BygHGPWW0Q" class="note panel"><div class="title_pdf_row clearfix"><h2 class="note_content_title"><a>response</a>  <button class="btn btn-xs btn-default permalink-button" title="Link to this comment" data-permalink-url="https://openreview.net/forum?id=BkgFqiAqFX&amp;noteId=BygHGPWW0Q"><span class="glyphicon glyphicon-link" aria-hidden="true"></span></button></h2></div><div class="meta_row"><span class="signatures">ICLR 2019 Conference Paper550 Authors</span></div><div class="clearfix"><div class="meta_row pull-left"><span class="date item">20 Nov 2018</span><span class="item">ICLR 2019 Conference Paper550 Official Comment</span><span class="item">Readers: <span class="readers-icon glyphicon glyphicon-globe"></span> Everyone</span></div><div class="meta_row meta_actions"></div></div><div class="note_contents"><span class="note_content_field">Comment: </span><span class="note_content_value">We would like to thank the reviewer for their comments. We would like to clarify the assumptions:
1) We do not make a low degree assumption. For ease of presentation we assume that the polynomial has monomials with each variable having degree 1. Overall the degree can still be d. In the Appendix B.8, we extend the same to higher degree with a stronger assumption on the coefficients of the polynomial.

2) As for high thresholds at the first layer, we agree that high thresholds may not be present at the first layer -- but they tend to be present in the higher layers. Even though we assume this at the first layer we point out in section E how this idea may apply at a higher layer. Thus our overall point is that, if we have an algorithm for learning a k-hidden layer network and there are high thresholds at the kth layer, then we may still be able to recover it approximately with more layers over it. 

3) We note that the function given by the upper layers is close to linear layers only in expectation, not necessarily point-wise. This closeness may not be very small if d is viewed as a constant. Also note that even though it is close to linear, we are able to learn the first layer to any precision for monotone P.
Also note that even though the function may look like a simple two layer network with linearity on top most of the time, on a small fraction of inputs it is doing something more sophisticated than a 2 layer network -- its full power is visible only an a small fraction of the inputs in the distribution and it is this full power that is manifested less often that we are interested in learning. Our goal is in fact to learn the entire network layer by layer. What we are showing here is that given a complex multi-layer network with high threshold, it automatically makes it look close to a 2 layer network (most of the time) and so we can learn the first layer. If one could proceed this way to second, third layer and so on, we would in effect learn the entire complex network. Thus we are saying that a high threshold network may make it easier to learn the network layer by layer. Now it is true that so far we have only managed to learn the first layer -- but we believe this method may be extendable to other layers too.

We hope the reviewer will take the above discussion into consideration.</span></div><div class="reply_row clearfix"></div></div><div class="children"></div></div></div></div></div></main></div>
  </div>
</div>


    <!-- Footer -->
    <footer class="sitemap">
      <div class="container">
        <div class="row hidden-xs">
          <div class="col-sm-4">
            <ul class="list-unstyled">
              <li><a href="/" class="home">Home</a></li>
              <li><a href="/venues">All Venues</a></li>
            </ul>
          </div>
    
          <div class="col-sm-4">
            <ul class="list-unstyled">
              <li><a href="/about">About OpenReview</a></li>
              <li><a href="/contact">Contact</a></li>
              <li><a href="#" data-toggle="modal" data-target="#feedback-modal">Feedback</a></li>
            </ul>
          </div>
    
          <div class="col-sm-4">
            <ul class="list-unstyled">
              <li><a href="/terms">Terms of Service</a></li>
              <li><a href="/privacy">Privacy Policy</a></li>
            </ul>
          </div>
        </div>
    
        <div class="row visible-xs-block">
          <div class="col-xs-6">
            <ul class="list-unstyled">
              <li><a href="/" class="home">Home</a></li>
              <li><a href="/about">About OpenReview</a></li>
              <li><a href="#" data-toggle="modal" data-target="#feedback-modal">Feedback</a></li>
            </ul>
          </div>
    
          <div class="col-xs-6">
            <ul class="list-unstyled">
              <li><a href="/contact">Contact</a></li>
              <li><a href="/terms">Terms of Service</a></li>
              <li><a href="/privacy">Privacy Policy</a></li>
            </ul>
          </div>
        </div>
      </div>
    </footer>
    
    <footer class="sponsor">
      <div class="container">
        <div class="row">
          <div class="col-sm-10 col-sm-offset-1">
            <p class="text-center">
              OpenReview is created by the <a href="http://www.iesl.cs.umass.edu/" target="_blank">Information Extraction and Synthesis Laboratory</a>, College of Information and Computer Science, University of Massachusetts Amherst. We gratefully acknowledge the support of the OpenReview sponsors:  Google,  Facebook, NSF, the University of Massachusetts Amherst Center for Data Science, and Center for Intelligent Information Retrieval, as well as the Google Cloud Platform for donating the computing and networking services on which OpenReview.net runs.
            </p>
          </div>
        </div>
      </div>
    </footer>

  <div id="feedback-modal" class="modal fade" tabindex="-1" role="dialog">
    <div class="modal-dialog">
      <div class="modal-content">
        <div class="modal-header">
          <button type="button" class="close" data-dismiss="modal" aria-label="Close"><span aria-hidden="true">×</span></button>
          <h3 class="modal-title">Send Feedback</h3>
        </div>
        <div class="modal-body">
          <p>Enter your feedback below and we'll get back to you as soon as possible.</p>
          <form action="/feedback" method="POST">
            <div class="form-group">
              <input type="email" name="from" class="form-control" placeholder="Email" />
            </div>
            <div class="form-group">
              <input type="text" name="subject" class="form-control" placeholder="Subject" />
            </div>
            <div class="form-group">
              <textarea name="message" class="form-control feedback-input" rows="5" placeholder="Message" required=""></textarea>
            </div>
          <ul id="ui-id-2" tabindex="0" class="ui-menu ui-widget ui-widget-content" style="display: none;"></ul></form>
        </div>
        <div class="modal-footer">
          <button type="button" class="btn btn-default" data-dismiss="modal">Cancel</button>
          <button type="button" class="btn btn-primary">Send</button>
        </div>
      </div>
    </div>
  </div>

  <script type="text/javascript" async="" src="https://www.google-analytics.com/analytics.js"></script><script src="//cdnjs.cloudflare.com/ajax/libs/jquery/2.2.4/jquery.min.js"></script>
  <script>window.jQuery || document.write('&lt;script src="/static/js/vendor/jquery-2.2.4.min.js"&gt;&lt;\/script&gt;')</script>

  <script src="/static/dist/vendor.min.js"></script>
  <script src="/static/dist/templates.min.js"></script>
  <script src="/static/dist/openreview.min.js"></script>

    <script>window.legacyScripts = true;</script>


    <script async="" src="https://www.googletagmanager.com/gtag/js?id=UA-108703919-1"></script>
    <script>
      window.dataLayer = window.dataLayer || [];
      function gtag() { dataLayer.push(arguments); }
      gtag('js', new Date());
      gtag('config', 'UA-108703919-1');
    </script>


<div role="status" aria-live="assertive" aria-relevant="additions" class="ui-helper-hidden-accessible"></div></body></html>