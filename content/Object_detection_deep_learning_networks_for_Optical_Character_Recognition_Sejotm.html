<!DOCTYPE html><html xmlns="http://www.w3.org/1999/xhtml" class="no-js"><head>
  <meta charset="utf-8" />
  <meta http-equiv="X-UA-Compatible" content="IE=edge,chrome=1" />
  <meta name="viewport" content="width=device-width, initial-scale=1" />

  <title>Object detection deep learning networks for Optical Character Recognition | OpenReview</title>
  <meta name="description" content="" />

      <meta name="citation_title" content="Object detection deep learning networks for Optical Character Recognition" />
        <meta name="citation_author" content="Anonymous" />
      <meta name="citation_publication_date" content="2018/09/27" />
      <meta name="citation_online_date" content="2018/09/27" />
      <meta name="citation_pdf_url" content="https://openreview.net/pdf?id=S1ej8o05tm" />
      <meta name="twitter:card" content="summary" />
      <meta name="twitter:site" content="@openreviewnet" />
      <meta name="og:title" content="Object detection deep learning networks for Optical Character..." />
      <meta name="og:description" content="In this article, we show how we applied a simple approach coming from deep learning networks for object detection to the task of optical character recognition in order to build image features..." />
      <meta name="og:image" content="https://openreview.net/static/images/openreview_logo_512.png" />

  <link rel="stylesheet" href="https://fonts.googleapis.com/css?family=Noto+Sans:400,400i,700,700i" />

  <link rel="stylesheet" href="/static/css/bootstrap.min.css" />
  <link rel="stylesheet" href="/static/css/jquery-ui.min.css" />
  <link rel="stylesheet" href="/static/css/main.min.css" />
</head>

<body class="forum">
  <nav class="navbar navbar-inverse navbar-fixed-top" role="navigation">
    <div class="container">
  
      <div class="navbar-header">
        <button type="button" class="navbar-toggle collapsed" data-toggle="collapse" data-target="#navbar" aria-expanded="false" aria-controls="navbar">
          <span class="sr-only">Toggle navigation</span>
          <span class="icon-bar"></span>
          <span class="icon-bar"></span>
          <span class="icon-bar"></span>
        </button>
        <a class="navbar-brand home push-link" href="/"><strong>OpenReview</strong>.net</a>
      </div>
  
      <div id="navbar" class="navbar-collapse collapse">
        <form class="navbar-form navbar-left profile-search" role="search">
          <div class="form-group has-feedback">
            <input id="search_input" type="text" class="form-control ui-autocomplete-input" placeholder="Search ICLR 2019 Conference" autocomplete="off" />
            <span class="glyphicon glyphicon-search form-control-feedback" aria-hidden="true"></span>
          </div>
  
          <input id="search_group" type="hidden" value="ICLR.cc/2019/Conference" />
          <input id="search_content" type="hidden" value="all" />
        <ul id="ui-id-1" tabindex="0" class="ui-menu ui-widget ui-widget-content ui-autocomplete ui-front" style="display: none;"></ul></form>
  
        <ul class="nav navbar-nav navbar-right">
        
            <li id="user-menu"><a href="/login">Login</a></li>
        </ul>
      </div>
  
    </div>
  </nav>

  <div id="or-banner" class="banner" style="">
  <div class="container">
    <div class="row">
      <div class="col-xs-12"><a href="/group?id=ICLR.cc/2019/Conference" title="Venue Homepage"><img class="icon" src="/static/images/arrow_left.svg" /> Go to <strong>ICLR 2019 Conference</strong> homepage</a></div>
    </div>
  </div>
</div>
<div id="flash-message-container" class="alert alert-danger" role="alert" style="display: none;">
  <div class="container">
    <div class="row">
      <div class="col-xs-12">
        <div class="alert-content">
          <button type="button" class="close" aria-label="Close"><span aria-hidden="true">×</span></button>
        </div>
      </div>
    </div>
  </div>
</div>

<div class="container">
  <div class="row">
    <div class="col-xs-12">
      <main id="content" class="clearfix openbanner-visible legacy-styles"><div id="note_S1ej8o05tm" class="note panel"><div class="title_pdf_row clearfix"><h2 class="note_content_title"><a>Object detection deep learning networks for Optical Character Recognition</a> <a class="note_content_pdf" href="/pdf?id=S1ej8o05tm" title="Download PDF" target="_blank"><img src="/static/images/pdf_icon_blue.svg" /></a> </h2></div><div class="meta_row"><span class="signatures">Anonymous</span></div><div class="clearfix"><div class="meta_row pull-left"><span class="date item">28 Sep 2018 (modified: 11 Oct 2018)</span><span class="item">ICLR 2019 Conference Blind Submission</span><span class="item">Readers: <span class="readers-icon glyphicon glyphicon-globe"></span> Everyone</span><span class="item"><a class="action-bibtex-modal" data-bibtex="@inproceedings{    &#10;anonymous2019object,    &#10;title={Object detection deep learning networks for Optical Character Recognition},    &#10;author={Anonymous},    &#10;booktitle={Submitted to International Conference on Learning Representations},    &#10;year={2019},    &#10;url={https://openreview.net/forum?id=S1ej8o05tm},    &#10;note={under review}    &#10;}">Show Bibtex</a></span></div><div class="meta_row meta_actions"></div></div><div class="note_contents"><span class="note_content_field">Abstract: </span><span class="note_content_value">In this article, we show how we applied a simple approach coming from deep learning networks for object detection to the task of optical character recognition in order to build image features taylored for documents. In contrast to scene text reading in natural images using networks pretrained on ImageNet, our document reading is performed with small networks inspired by MNIST digit recognition challenge, at a small computational budget and a small stride. The object detection modern frameworks allow a direct end-to-end training, with no other algorithm than the deep learning and the non-max-suppression algorithm to filter the duplicate predictions. The trained weights can be used for higher level models, such as, for example, document classification, or document segmentation.
</span></div><div class="note_contents"><span class="note_content_field">Keywords: </span><span class="note_content_value">OCR, object detection, RCNN, Yolo</span></div><div class="note_contents"><span class="note_content_field">TL;DR: </span><span class="note_content_value">Yolo / RCNN neural network for object detection adapted to the task of OCR</span></div><div class="reply_row clearfix"><div class="item" id="reply_count">4 Replies</div></div></div><hr class="small" /><div id="note_children"><div class="note_with_children"><div id="note_rJgd4DB4TQ" class="note panel"><div class="title_pdf_row clearfix"><h2 class="note_content_title"><a>A technical report about the application of well-known and standard deep learning techniques to character detection/recognition in document images</a>  <button class="btn btn-xs btn-default permalink-button" title="Link to this comment" data-permalink-url="https://openreview.net/forum?id=S1ej8o05tm&amp;noteId=rJgd4DB4TQ"><span class="glyphicon glyphicon-link" aria-hidden="true"></span></button></h2></div><div class="meta_row"><span class="signatures">ICLR 2019 Conference Paper204 AnonReviewer4</span></div><div class="clearfix"><div class="meta_row pull-left"><span class="date item">10 Nov 2018</span><span class="item">ICLR 2019 Conference Paper204 Official Review</span><span class="item">Readers: <span class="readers-icon glyphicon glyphicon-globe"></span> Everyone</span></div><div class="meta_row meta_actions"></div></div><div class="note_contents"><span class="note_content_field">Review: </span><span class="note_content_value">This paper lacks any novelty/contribution as it just applies well-known and standard architectures for object detection (SSD) and image classification (LeNet) trained with standard algorithms and losses.

Moreover, I fail to see what is the purpose of the proposed pipeline and it is not clear at all how it may help improving existing OCR engines in any particular scenario (handwriting recognition, printed text, historical documents, etc.). No demonstration or comparison with state of the art is provided. 

The authors claim “This work is the first to apply modern object detection deep learning approaches to document data” but there are previously published works. For example:

Tuggener, Lukas, et al. "DeepScores--A Dataset for Segmentation, Detection and Classification of Tiny Objects." ICPR 2018.
Pacha, Alexander, et al. "Handwritten music object detection: Open issues and baseline results." DAS 2018.

Actually, in my opinion Music Object Detection in musical scores would be a much better test-bed/application for the proposed pipeline than any of the datasets used in this paper.  The datasets used in the experimental section seem to be created ad-hoc for the proposed pipeline and do not come from any real world application. 

Finally, the presentation of the paper is marginal. Data plots have very bad resolution, there are no captions in any table or figure and they are not correctly referenced within the text. There seem to be also missing content in the last sections which makes them impossible to read/understand.</span></div><div class="note_contents"><span class="note_content_field">Rating: </span><span class="note_content_value">1: Trivial or wrong</span></div><div class="note_contents"><span class="note_content_field">Confidence: </span><span class="note_content_value">5: The reviewer is absolutely certain that the evaluation is correct and very familiar with the relevant literature</span></div><div class="reply_row clearfix"></div></div><div class="children"></div></div><div class="note_with_children"><div id="note_Hkg0A3UJ67" class="note panel"><div class="title_pdf_row clearfix"><h2 class="note_content_title"><a>Object Detection for OCR</a>  <button class="btn btn-xs btn-default permalink-button" title="Link to this comment" data-permalink-url="https://openreview.net/forum?id=S1ej8o05tm&amp;noteId=Hkg0A3UJ67"><span class="glyphicon glyphicon-link" aria-hidden="true"></span></button></h2></div><div class="meta_row"><span class="signatures">ICLR 2019 Conference Paper204 AnonReviewer1</span></div><div class="clearfix"><div class="meta_row pull-left"><span class="date item">07 Nov 2018</span><span class="item">ICLR 2019 Conference Paper204 Official Review</span><span class="item">Readers: <span class="readers-icon glyphicon glyphicon-globe"></span> Everyone</span></div><div class="meta_row meta_actions"></div></div><div class="note_contents"><span class="note_content_field">Review: </span><span class="note_content_value">Unfortunately, the work does not introduce new contributions, with the point of the paper provided in the introduction:
In our experiments, we show that best performing approaches currently available for object detection
on natural images can be used with success at OCR tasks.

The work is applying established object detection algorithms to OCR. While the work provides a thorough experimental section exploring trade offs in network hyper-parameters, the application of object detection to the OCR domain does not provide enough novelty to warrant publication.
</span></div><div class="note_contents"><span class="note_content_field">Rating: </span><span class="note_content_value">2: Strong rejection</span></div><div class="note_contents"><span class="note_content_field">Confidence: </span><span class="note_content_value">5: The reviewer is absolutely certain that the evaluation is correct and very familiar with the relevant literature</span></div><div class="reply_row clearfix"></div></div><div class="children"></div></div><div class="note_with_children"><div id="note_rkg9q-Wc3m" class="note panel"><div class="title_pdf_row clearfix"><h2 class="note_content_title"><a>A pure tech report with no novel contribution and serious flaws in the formatting, technical exposition and experimental evaluation</a>  <button class="btn btn-xs btn-default permalink-button" title="Link to this comment" data-permalink-url="https://openreview.net/forum?id=S1ej8o05tm&amp;noteId=rkg9q-Wc3m"><span class="glyphicon glyphicon-link" aria-hidden="true"></span></button></h2></div><div class="meta_row"><span class="signatures">ICLR 2019 Conference Paper204 AnonReviewer2</span></div><div class="clearfix"><div class="meta_row pull-left"><span class="date item">03 Nov 2018 (modified: 07 Nov 2018)</span><span class="item">ICLR 2019 Conference Paper204 Official Review</span><span class="item">Readers: <span class="readers-icon glyphicon glyphicon-globe"></span> Everyone</span></div><div class="meta_row meta_actions"></div></div><div class="note_contents"><span class="note_content_field">Review: </span><span class="note_content_value">The authors experiment with building an SSD-like object detection network for text detection in documents, by replacing the usual VGG or ResNet base architecture with a light weight model inspired by the original digits classification CNN from [LeCun et al 1999].

This paper is a pure technical report with no novel contribution: all the authors do is replace the "body" network in the well-known SSD architecture with a simpler model (taken from existing literature) and evaluate it on two synthetic benchmarks of their creation.
The idea of employing object detection CNNs for OCR is not novel either, as pointed out in the related works section.

Beside the absence of novelty, the paper also suffers from several other serious flaws:

1) One of the main motivations provided by the authors for this work is that existing "classification [...] detection [...] or segmentation networks, cannot be applied directly, even with finetuning".
However, no experimental results are reported to justify this claim.
In fact, in the experimental section the proposed network is not compared against any existing baseline.

2) The text has serious clarity and formatting issues, in particular:
- most tables and figures have no caption, and the few that have one are not numbered
- the text exceed both the 8 pages limit and the extended 10 pages limit allowed in the case of big figures
- the experimental section is very confusing, in particular the way the authors refer to the various network variants using long code names makes it really hard for the reader to follow the ablation studies
- given the absence of proper captions and numbering, it is quite hard to understand which table refers to which experiment
- most of the graphs seem to be in the form of low-resolution bitmaps, which are quite hard to read even on screen
- many entries in the References section are either missing the venue, or point to an arXiv link even when a proper conference / journal reference would be available

3) Some important details about the network are missing, in particular the authors do not mention how labels are assigned to the network outputs, and only give a vague indication about the losses being used.
Similarly, there's no mention about the use of NMS, which is also an important component of the two architectures (SSD and YOLO) that inspire this work.
Assigning labels and performing NMS are actually some of the most crucial components in the training of object proposal / object detection networks, often requiring numerous meta-parameters to be properly configured and tuned, as testified by the meticulous descriptions given in previous works (e.g. YOLO and Fast / Faster / Mask r-CNN).

4) The experimental section is very poorly organized and formatted (as mentioned in (2) above), and completely lacks any comparison with other state of the art approaches.
A lot of space is devoted to presenting a detailed ablation study which, in my opinion, doesn't contribute much to the overall paper and actually reads more like a report on meta-parameter tuning.
Finally, starting from Section 5.3.1 the text seems to be copy-pasted without a second read from some differently formatted document, as entire phrases or possibly tables / figures seems to be missing.

In conclusion, in my opinion this paper does not meet the conference's minimum quality standards and should definitely be rejected.</span></div><div class="note_contents"><span class="note_content_field">Rating: </span><span class="note_content_value">1: Trivial or wrong</span></div><div class="note_contents"><span class="note_content_field">Confidence: </span><span class="note_content_value">5: The reviewer is absolutely certain that the evaluation is correct and very familiar with the relevant literature</span></div><div class="reply_row clearfix"></div></div><div class="children"></div></div><div class="note_with_children"><div id="note_BkxgKFy5hQ" class="note panel"><div class="title_pdf_row clearfix"><h2 class="note_content_title"><a>This paper applied an object detection network, like SSD, for optical character detection and recognition.</a>  <button class="btn btn-xs btn-default permalink-button" title="Link to this comment" data-permalink-url="https://openreview.net/forum?id=S1ej8o05tm&amp;noteId=BkxgKFy5hQ"><span class="glyphicon glyphicon-link" aria-hidden="true"></span></button></h2></div><div class="meta_row"><span class="signatures">ICLR 2019 Conference Paper204 AnonReviewer3</span></div><div class="clearfix"><div class="meta_row pull-left"><span class="date item">02 Nov 2018 (modified: 07 Nov 2018)</span><span class="item">ICLR 2019 Conference Paper204 Official Review</span><span class="item">Readers: <span class="readers-icon glyphicon glyphicon-globe"></span> Everyone</span></div><div class="meta_row meta_actions"></div></div><div class="note_contents"><span class="note_content_field">Review: </span><span class="note_content_value">weakness:
1. the paper is lack of novelty and the motivation is weak. I even can't find any contribution to OCR or object detection.

2. the paper is written badly so that I can't follow easily. In addition, the figures and tables are not always explained in the main body, which makes the experimental results confusing.

3. There are no titles in the figures and tables in this paper

4. the authors don't confirm the superiority of the proposed method to others.

minor comments
1. what's the meaning of Target mAP in the table?
2. It seems that Some figures are cropped from TensorBoard, with some extra shadows.</span></div><div class="note_contents"><span class="note_content_field">Rating: </span><span class="note_content_value">4: Ok but not good enough - rejection</span></div><div class="note_contents"><span class="note_content_field">Confidence: </span><span class="note_content_value">5: The reviewer is absolutely certain that the evaluation is correct and very familiar with the relevant literature</span></div><div class="reply_row clearfix"></div></div><div class="children"></div></div></div></main></div>
  </div>
</div>


    <!-- Footer -->
    <footer class="sitemap">
      <div class="container">
        <div class="row hidden-xs">
          <div class="col-sm-4">
            <ul class="list-unstyled">
              <li><a href="/" class="home">Home</a></li>
              <li><a href="/venues">All Venues</a></li>
            </ul>
          </div>
    
          <div class="col-sm-4">
            <ul class="list-unstyled">
              <li><a href="/about">About OpenReview</a></li>
              <li><a href="/contact">Contact</a></li>
              <li><a href="#" data-toggle="modal" data-target="#feedback-modal">Feedback</a></li>
            </ul>
          </div>
    
          <div class="col-sm-4">
            <ul class="list-unstyled">
              <li><a href="/terms">Terms of Service</a></li>
              <li><a href="/privacy">Privacy Policy</a></li>
            </ul>
          </div>
        </div>
    
        <div class="row visible-xs-block">
          <div class="col-xs-6">
            <ul class="list-unstyled">
              <li><a href="/" class="home">Home</a></li>
              <li><a href="/about">About OpenReview</a></li>
              <li><a href="#" data-toggle="modal" data-target="#feedback-modal">Feedback</a></li>
            </ul>
          </div>
    
          <div class="col-xs-6">
            <ul class="list-unstyled">
              <li><a href="/contact">Contact</a></li>
              <li><a href="/terms">Terms of Service</a></li>
              <li><a href="/privacy">Privacy Policy</a></li>
            </ul>
          </div>
        </div>
      </div>
    </footer>
    
    <footer class="sponsor">
      <div class="container">
        <div class="row">
          <div class="col-sm-10 col-sm-offset-1">
            <p class="text-center">
              OpenReview is created by the <a href="http://www.iesl.cs.umass.edu/" target="_blank">Information Extraction and Synthesis Laboratory</a>, College of Information and Computer Science, University of Massachusetts Amherst. We gratefully acknowledge the support of the OpenReview sponsors:  Google,  Facebook, NSF, the University of Massachusetts Amherst Center for Data Science, and Center for Intelligent Information Retrieval, as well as the Google Cloud Platform for donating the computing and networking services on which OpenReview.net runs.
            </p>
          </div>
        </div>
      </div>
    </footer>

  <div id="feedback-modal" class="modal fade" tabindex="-1" role="dialog">
    <div class="modal-dialog">
      <div class="modal-content">
        <div class="modal-header">
          <button type="button" class="close" data-dismiss="modal" aria-label="Close"><span aria-hidden="true">×</span></button>
          <h3 class="modal-title">Send Feedback</h3>
        </div>
        <div class="modal-body">
          <p>Enter your feedback below and we'll get back to you as soon as possible.</p>
          <form action="/feedback" method="POST">
            <div class="form-group">
              <input type="email" name="from" class="form-control" placeholder="Email" />
            </div>
            <div class="form-group">
              <input type="text" name="subject" class="form-control" placeholder="Subject" />
            </div>
            <div class="form-group">
              <textarea name="message" class="form-control feedback-input" rows="5" placeholder="Message" required=""></textarea>
            </div>
          <ul id="ui-id-2" tabindex="0" class="ui-menu ui-widget ui-widget-content" style="display: none;"></ul></form>
        </div>
        <div class="modal-footer">
          <button type="button" class="btn btn-default" data-dismiss="modal">Cancel</button>
          <button type="button" class="btn btn-primary">Send</button>
        </div>
      </div>
    </div>
  </div>

  <script type="text/javascript" async="" src="https://www.google-analytics.com/analytics.js"></script><script src="//cdnjs.cloudflare.com/ajax/libs/jquery/2.2.4/jquery.min.js"></script>
  <script>window.jQuery || document.write('&lt;script src="/static/js/vendor/jquery-2.2.4.min.js"&gt;&lt;\/script&gt;')</script>

  <script src="/static/dist/vendor.min.js"></script>
  <script src="/static/dist/templates.min.js"></script>
  <script src="/static/dist/openreview.min.js"></script>

    <script>window.legacyScripts = true;</script>


    <script async="" src="https://www.googletagmanager.com/gtag/js?id=UA-108703919-1"></script>
    <script>
      window.dataLayer = window.dataLayer || [];
      function gtag() { dataLayer.push(arguments); }
      gtag('js', new Date());
      gtag('config', 'UA-108703919-1');
    </script>


<div role="status" aria-live="assertive" aria-relevant="additions" class="ui-helper-hidden-accessible"></div></body></html>