<!DOCTYPE html><html xmlns="http://www.w3.org/1999/xhtml" class="no-js"><head>
  <meta charset="utf-8" />
  <meta http-equiv="X-UA-Compatible" content="IE=edge,chrome=1" />
  <meta name="viewport" content="width=device-width, initial-scale=1" />

  <title>Representation-Constrained Autoencoders and an Application to Wireless Positioning | OpenReview</title>
  <meta name="description" content="" />

      <meta name="citation_title" content="Representation-Constrained Autoencoders and an Application to Wireless Positioning" />
        <meta name="citation_author" content="Anonymous" />
      <meta name="citation_publication_date" content="2018/09/27" />
      <meta name="citation_online_date" content="2018/09/27" />
      <meta name="citation_pdf_url" content="https://openreview.net/pdf?id=ryemosC9tm" />
      <meta name="twitter:card" content="summary" />
      <meta name="twitter:site" content="@openreviewnet" />
      <meta name="og:title" content="Representation-Constrained Autoencoders and an Application to..." />
      <meta name="og:description" content="In a number of practical applications that rely on dimensionality reduction, the dataset or measurement process provides valuable side information that can be incorporated when learning..." />
      <meta name="og:image" content="https://openreview.net/static/images/openreview_logo_512.png" />

  <link rel="stylesheet" href="https://fonts.googleapis.com/css?family=Noto+Sans:400,400i,700,700i" />

  <link rel="stylesheet" href="/static/css/bootstrap.min.css" />
  <link rel="stylesheet" href="/static/css/jquery-ui.min.css" />
  <link rel="stylesheet" href="/static/css/main.min.css" />
</head>

<body class="forum">
  <nav class="navbar navbar-inverse navbar-fixed-top" role="navigation">
    <div class="container">
  
      <div class="navbar-header">
        <button type="button" class="navbar-toggle collapsed" data-toggle="collapse" data-target="#navbar" aria-expanded="false" aria-controls="navbar">
          <span class="sr-only">Toggle navigation</span>
          <span class="icon-bar"></span>
          <span class="icon-bar"></span>
          <span class="icon-bar"></span>
        </button>
        <a class="navbar-brand home push-link" href="/"><strong>OpenReview</strong>.net</a>
      </div>
  
      <div id="navbar" class="navbar-collapse collapse">
        <form class="navbar-form navbar-left profile-search" role="search">
          <div class="form-group has-feedback">
            <input id="search_input" type="text" class="form-control ui-autocomplete-input" placeholder="Search ICLR 2019 Conference" autocomplete="off" />
            <span class="glyphicon glyphicon-search form-control-feedback" aria-hidden="true"></span>
          </div>
  
          <input id="search_group" type="hidden" value="ICLR.cc/2019/Conference" />
          <input id="search_content" type="hidden" value="all" />
        <ul id="ui-id-1" tabindex="0" class="ui-menu ui-widget ui-widget-content ui-autocomplete ui-front" style="display: none;"></ul></form>
  
        <ul class="nav navbar-nav navbar-right">
        
            <li id="user-menu"><a href="/login">Login</a></li>
        </ul>
      </div>
  
    </div>
  </nav>

  <div id="or-banner" class="banner" style="">
  <div class="container">
    <div class="row">
      <div class="col-xs-12"><a href="/group?id=ICLR.cc/2019/Conference" title="Venue Homepage"><img class="icon" src="/static/images/arrow_left.svg" /> Go to <strong>ICLR 2019 Conference</strong> homepage</a></div>
    </div>
  </div>
</div>
<div id="flash-message-container" class="alert alert-danger" role="alert" style="display: none;">
  <div class="container">
    <div class="row">
      <div class="col-xs-12">
        <div class="alert-content">
          <button type="button" class="close" aria-label="Close"><span aria-hidden="true">×</span></button>
        </div>
      </div>
    </div>
  </div>
</div>

<div class="container">
  <div class="row">
    <div class="col-xs-12">
      <main id="content" class="clearfix openbanner-visible legacy-styles"><div id="note_ryemosC9tm" class="note panel"><div class="title_pdf_row clearfix"><h2 class="note_content_title"><a>Representation-Constrained Autoencoders and an Application to Wireless Positioning</a> <a class="note_content_pdf" href="/pdf?id=ryemosC9tm" title="Download PDF" target="_blank"><img src="/static/images/pdf_icon_blue.svg" /></a> </h2></div><div class="meta_row"><span class="signatures">Anonymous</span></div><div class="clearfix"><div class="meta_row pull-left"><span class="date item">28 Sep 2018 (modified: 11 Oct 2018)</span><span class="item">ICLR 2019 Conference Blind Submission</span><span class="item">Readers: <span class="readers-icon glyphicon glyphicon-globe"></span> Everyone</span><span class="item"><a class="action-bibtex-modal" data-bibtex="@inproceedings{    &#10;anonymous2019representation-constrained,    &#10;title={Representation-Constrained Autoencoders and an Application to Wireless Positioning},    &#10;author={Anonymous},    &#10;booktitle={Submitted to International Conference on Learning Representations},    &#10;year={2019},    &#10;url={https://openreview.net/forum?id=ryemosC9tm},    &#10;note={under review}    &#10;}">Show Bibtex</a></span></div><div class="meta_row meta_actions"></div></div><div class="note_contents"><span class="note_content_field">Abstract: </span><span class="note_content_value">In a number of practical applications that rely on dimensionality reduction, the dataset or measurement process provides valuable side information that can be incorporated when learning low-dimensional embeddings. We propose the inclusion of pairwise representation constraints into autoencoders (AEs) with the goal of promoting application-specific structure. We use synthetic results to show that only a small amount of AE representation constraints are required to substantially improve the local and global neighborhood preserving properties of the learned embeddings. To demonstrate the efficacy of our approach and to illustrate a practical application that naturally provides such representation constraints, we focus on wireless positioning using a recently proposed channel charting framework. We show that representation-constrained AEs recover the global geometry of the learned low-dimensional representations, which enables channel charting to perform approximate positioning without access to global navigation satellite systems or supervised learning methods that rely on extensive measurement campaigns. </span></div><div class="note_contents"><span class="note_content_field">Keywords: </span><span class="note_content_value">Autoencoder, dimensionality reduction, wireless positioning, channel charting, localization</span></div><div class="note_contents"><span class="note_content_field">TL;DR: </span><span class="note_content_value">We propose to impose representation constraints to autoencoders in order to localize wireless transmitters in space from their channel state information. </span></div><div class="reply_row clearfix"><div class="item" id="reply_count">6 Replies</div></div></div><hr class="small" /><div id="note_children"><div class="note_with_children"><div id="note_B1eOc1zw6Q" class="note panel"><div class="title_pdf_row clearfix"><h2 class="note_content_title"><a>Applies a distance constraint to the latent space of auto-encoders</a>  <button class="btn btn-xs btn-default permalink-button" title="Link to this comment" data-permalink-url="https://openreview.net/forum?id=ryemosC9tm&amp;noteId=B1eOc1zw6Q"><span class="glyphicon glyphicon-link" aria-hidden="true"></span></button></h2></div><div class="meta_row"><span class="signatures">ICLR 2019 Conference Paper606 AnonReviewer1</span></div><div class="clearfix"><div class="meta_row pull-left"><span class="date item">12 Nov 2018</span><span class="item">ICLR 2019 Conference Paper606 Official Review</span><span class="item">Readers: <span class="readers-icon glyphicon glyphicon-globe"></span> Everyone</span></div><div class="meta_row meta_actions"></div></div><div class="note_contents"><span class="note_content_field">Review: </span><span class="note_content_value">[I'm a fallback reviewer assigned after initial reviewer failed to submit]

Quality/Clarity:
The work is fine. The presentation is clear enough. The experiments are all on simulated data, with 2GHz scattering simulation derived from more sophisticated software suites than the 4 toy manifold problems initially considered.


Originality/Significance:
The work does not seem particularly novel. Perhaps the specific application of regularized autoencoders to the channel charting problem is novel. The regularizers end up looking a lot like a variety of margin losses. The idea of imposing some structure on the latent space of an autoencoder is not particularly new either. Consider, for example, conditional VAEs. Or this work from last year's ICLR <a href="https://openreview.net/forum?id=Sy8XvGb0-" target="_blank" rel="nofollow">https://openreview.net/forum?id=Sy8XvGb0-</a> This work is straightforward multi-task learning with dimensionality reduction with similarity loss tasks.

On the whole, I don't think there is enough novel work for the venue.</span></div><div class="note_contents"><span class="note_content_field">Rating: </span><span class="note_content_value">5: Marginally below acceptance threshold</span></div><div class="note_contents"><span class="note_content_field">Confidence: </span><span class="note_content_value">4: The reviewer is confident but not absolutely certain that the evaluation is correct</span></div><div class="reply_row clearfix"></div></div><div class="children"><div class="note_with_children"><div id="note_BJe4Oxm5pX" class="note panel"><div class="title_pdf_row clearfix"><h2 class="note_content_title"><a>Differences to conditional VAEs and related approaches</a>  <button class="btn btn-xs btn-default permalink-button" title="Link to this comment" data-permalink-url="https://openreview.net/forum?id=ryemosC9tm&amp;noteId=BJe4Oxm5pX"><span class="glyphicon glyphicon-link" aria-hidden="true"></span></button></h2></div><div class="meta_row"><span class="signatures">ICLR 2019 Conference Paper606 Authors</span></div><div class="clearfix"><div class="meta_row pull-left"><span class="date item">15 Nov 2018</span><span class="item">ICLR 2019 Conference Paper606 Official Comment</span><span class="item">Readers: <span class="readers-icon glyphicon glyphicon-globe"></span> Everyone</span></div><div class="meta_row meta_actions"></div></div><div class="note_contents"><span class="note_content_field">Comment: </span><span class="note_content_value">Thank you for your comment. 

We agree that the general idea of constrained representations in autoencoders is not novel per se. However, the type of constraints we are imposing are novel and the application to wireless positioning is new as well.

In the references you mentioned, constraints are added for the purpose of conditionally generating data. The constraints are used to (i) learn a latent variable that would offer a better output reconstruction, (ii) allow for a realistic (and diverse) data generation, or (iii) enable more control over the characteristics of the generated data (e.g., controlling attributes). For example, conditional GANs (CGAN) and conditional VAEs (CVAE) introduce conditioning using extra information (e.g., attribute labels) during training phase to influence the distribution in the latent space with the goal of learning better structure in the outputs. In our case, we are not trying to train a generative model but rather perform dimensionality reduction to learn a “meaningful” low dimensional embedding. Since we care about the geometry of the latent variable rather than the reconstructed output of the AE, we impose side-information constraints on pairs of data points in the low-dimensional representation (i.e, latent variable). We do not put a specific emphasis on the probabilistic inference between the latent variable and the output of the autoencoder, and thus on the distribution of the latent variable. Also, our constraints (pairwise distances) are not of probabilistic nature. This is because the primary goal is to find a low dimensional representation that preserve a (hidden) geometrical structure of the input data (e.g., finding relative positions from channel state information features in wireless positioning).

Our results are useful for applications where side information about the low dimensional representation arise naturally from the data or the application. For example, for positioning we wish to learn a 2D representation from high-dimensional channel state information features. Knowing (from the original channel charting paper), that autoencoders can find better embeddings (in terms of preserving the neighborhood) than other dimensionality reduction methods, we show that adding side information that arise from the user equipments’ finite velocity, or from some labeled positions (in a semi-supervised fashion), can help to significantly improve positioning.

We will include the suggested references on latent-variable constrained generative models as a reference to representation constrained models and highlight their differences to our work. 
</span></div><div class="reply_row clearfix"></div></div><div class="children"></div></div></div></div><div class="note_with_children"><div id="note_BJeOGpKe6X" class="note panel"><div class="title_pdf_row clearfix"><h2 class="note_content_title"><a>Useful approach, but insufficient experimental validation, and somewhat weak on novelty</a>  <button class="btn btn-xs btn-default permalink-button" title="Link to this comment" data-permalink-url="https://openreview.net/forum?id=ryemosC9tm&amp;noteId=BJeOGpKe6X"><span class="glyphicon glyphicon-link" aria-hidden="true"></span></button></h2></div><div class="meta_row"><span class="signatures">ICLR 2019 Conference Paper606 AnonReviewer2</span></div><div class="clearfix"><div class="meta_row pull-left"><span class="date item">08 Nov 2018</span><span class="item">ICLR 2019 Conference Paper606 Official Review</span><span class="item">Readers: <span class="readers-icon glyphicon glyphicon-globe"></span> Everyone</span></div><div class="meta_row meta_actions"></div></div><div class="note_contents"><span class="note_content_field">Review: </span><span class="note_content_value">Description:

This paper presents a variant of deep neural network autoencoders for low-dimensional embedding, where pairwise constraints are incorporated, and applies it to wireless positioning.

The four constraint types are about enforcing pairwise distance between low-dimensional points to be close to a desired value or below a maximal desired value, either as an "absolute" constraint where one point is fixed or a "relative" constraint where both points are optimized. The constraints are encoded as  nonconvex regularization terms. In addition to the constraints the method has a standard autoencoder cost function.

Authors point out that if a suitable importance weighting is done, one constraint type yields a parametric version of Sammon’s mapping.

The method is tested on four simple artificial manifolds and on a wireless positioning task.


Evaluation:

Combining autoencoders with suitable additional regularizers can be a meaningful approach. However, I find the evaluation of the proposed method very insufficient: there are no comparisons to any other dimensionality reduction methods. For example, Sammon's mapping is mentioned several times but is not compared to, and a parametric version of t-SNE is also mentioned but not compared to even though it is parametric like the authors' proposed method. I consider that to be a severe problem in a situation where numerous such methods have been proposed previously and would be applicable to the data used here.

In terms of novelty I find the method somewhat lacking: essentially it is close to simply a weighted combination of an AE cost function and a Sammon's mapping cost function when using the FRD constraints. The other types of constraints add some more novelty, however.



Detailed comments:


"Autoencoders have been shown to consistently outperform other dimensionality-reduction algorithms on real-world datasets (van der Maaten et al., 2009)": this is too old a reference, nine years old, and it does not contain numerous dimensionality reduction algorithms proposed more recently, such as any neighbor embedding based dimensionality reduction methods. Moreover, the test in van der Maate et al. 2009 was only on five data sets and in terms of a continuity measure only, too little evidence to claim consistent outperforming of other algorithms.

"van der Maaten (2009) proposes the use of AEs to learn a parametric mapping between high-dimensional datapoints and low-dimensional representations by enforcing structure obtained via Student-t stochastic neighborhood embedding (t-SNE)": this is not a correct description, van der Maaten (2009) optimizes the AE using t-SNE cost function (instead of running some separate t-SNE step to yield structural constraints as the description seems to say).

"the FRD regularizer resembles that of Sammon's mapping": actually in the general form it resembles the multidimensional scaling stress; it only becomes close to Sammon's mapping if you additionally weight each constraint by the inverse of the original distance as you suggest.

It is unclear to me where the absolute distance constraints (FAD or MAD) arise from in the synthetic experiments. You write "for FAD one of the two representations... is a constant known prior to AE learning": how can you know the desired low-dimensional output coordinate (or distance from such a coordinate) in the synthetic data case?

This reference is incorrect: "Laurens van der Maaten, Eric Postma, and Jaap Van den Herik. Dimensionality reduction: A comparative review. In Journal of Machine Learning Research, volume 10, pp. 66–71, 2009." This article has not been published in Journal of Machine Learning Research. It is only available as a technical report of Tilburg University.
</span></div><div class="note_contents"><span class="note_content_field">Rating: </span><span class="note_content_value">4: Ok but not good enough - rejection</span></div><div class="note_contents"><span class="note_content_field">Confidence: </span><span class="note_content_value">4: The reviewer is confident but not absolutely certain that the evaluation is correct</span></div><div class="reply_row clearfix"></div></div><div class="children"><div class="note_with_children"><div id="note_Syxkf7kMa7" class="note panel"><div class="title_pdf_row clearfix"><h2 class="note_content_title"><a>Thanks for the detailed set of comments</a>  <button class="btn btn-xs btn-default permalink-button" title="Link to this comment" data-permalink-url="https://openreview.net/forum?id=ryemosC9tm&amp;noteId=Syxkf7kMa7"><span class="glyphicon glyphicon-link" aria-hidden="true"></span></button></h2></div><div class="meta_row"><span class="signatures">ICLR 2019 Conference Paper606 Authors</span></div><div class="clearfix"><div class="meta_row pull-left"><span class="date item">09 Nov 2018</span><span class="item">ICLR 2019 Conference Paper606 Official Comment</span><span class="item">Readers: <span class="readers-icon glyphicon glyphicon-globe"></span> Everyone</span></div><div class="meta_row meta_actions"></div></div><div class="note_contents"><span class="note_content_field">Comment: </span><span class="note_content_value">In what follows, C: stands for the reviewer's comment and R: for our response.

C: "[...] I find the evaluation of the proposed method very insufficient: there are no comparisons to any other dimensionality reduction methods. [...]"

R: We emphasize that our paper is not on dimensionality reduction per se, but rather on the inclusion of  side constraints and their use in the application for wireless positioning. We do not propose new methods that might outperform other methods (such as t-SNE, Sammon’s mapping, isomap, neighbor embedding, etc.) at conventional dimensionality reduction tasks. Instead, we are interested in enforcing side information on the low-dimensional representation which may be available in certain applications. We have decided to focus on augmenting autoencoders with side constraints (rather than parametric t-SNE or others) for the following reasons: (i) they scale favorably to a large number of datapoints together with stochastic gradient descent, (ii) they provide a parametric mapping and the learned neural net can be used in real-time applications, (iii) AEs have already worked reasonably well for channel charting without side information (so a direct comparison with the state-of-the-art for channel charting was possible), and (iv) existing deep learning frameworks enable an efficient development of autoencoders in practice. We admit that other methods, such as the parametric version of t-SNE, may work as well or even better in our application, but our goal was not to perform a comparison of methods that *could* work---we were rather interested in developing one method that addresses the shortcomings of channel charting. The evaluation of potentially better methods is left for future work.

We admit, however, that for the synthetic experiments, we should have included a comparison to conventional Sammon mapping and t-SNE. We will do that in the final version.

C: "In terms of novelty I find the method somewhat lacking: [...]. The other types of constraints add some more novelty, however.”

R: The main focus of the paper was not on the Sammon’s mapping cost function but on including side constraints to autoencoders to enable positioning via channel charting. We see the fact that the FRD constraints enable a parametric version of Sammon’s mapping as a byproduct of our approach. Hence, we included the synthetic simulation results to showcase that our constraints are not only useful for wireless positioning.

C: "Autoencoders have been shown to consistently outperform other dimensionality reduction algorithms on real-world datasets (van der Maaten et al., 2009)": this is too old a reference [...]"

R: We used this reference as part to motivate the use of autoencoders and not other methods for our purposes. Furthermore, in the original channel charting paper, AEs were achieving better performance than other dimensionality reduction methods. Again, we are not proposing a new and better dimensionality reduction method for general tasks, but rather augmented an existing method (autoencoders) with our side constraints to improve applications that benefit from side information, such as channel charting. We will adjust the motivation in the final version of the paper..

C: "van der Maaten (2009) proposes the use of AEs to learn a parametric mapping between high-dimensional datapoints and low-dimensional representations by enforcing structure obtained via Student-t stochastic neighborhood embedding (t-SNE)": this is not a correct description, [...]”

R: By “enforcing structure obtained via Student-t stochastic neighborhood embedding” we meant that we learn the AE with the t-SNE cost function. We will rephrase our statement to more accurately reflect what is happening.

C: "the FRD regularizer resembles that of Sammon's mapping": actually in the general form it resembles the multidimensional scaling stress; [...]

R: Indeed, the plain FRD regularizers implements MDS. We will add this insight to the final version.

C: "It is unclear to me where the absolute distance constraints (FAD or MAD) arise from in the synthetic experiments. [...]"

R: Our synthetic experiments demonstrate that only a small amount of side information is sufficient to (often significantly) improve autoencoders. To showcase this behavior on synthetic data, we have used knowledge available from the true low-dimensional point set (used to generate the high-dimensional dataset). For example, for the fixed absolute distance (FAD) constraint we extract the distance between points in the low-dimensional representation and use a small fraction of  information while training the autoencoder. Clearly, this would not be possible in many dimensionality reduction applications. However, in our wireless positioning application, we can get the maximum distance between two points from the physics: a mobile terminal can only move at finite velocity which implies that subsequent CSI measurements cannot be too far apart in representation space.</span></div><div class="reply_row clearfix"></div></div><div class="children"></div></div></div></div><div class="note_with_children"><div id="note_HyghFgZh37" class="note panel"><div class="title_pdf_row clearfix"><h2 class="note_content_title"><a>Learning representation-constrained autoencoders</a>  <button class="btn btn-xs btn-default permalink-button" title="Link to this comment" data-permalink-url="https://openreview.net/forum?id=ryemosC9tm&amp;noteId=HyghFgZh37"><span class="glyphicon glyphicon-link" aria-hidden="true"></span></button></h2></div><div class="meta_row"><span class="signatures">ICLR 2019 Conference Paper606 AnonReviewer3</span></div><div class="clearfix"><div class="meta_row pull-left"><span class="date item">04 Nov 2018 (modified: 07 Nov 2018)</span><span class="item">ICLR 2019 Conference Paper606 Official Review</span><span class="item">Readers: <span class="readers-icon glyphicon glyphicon-globe"></span> Everyone</span></div><div class="meta_row meta_actions"></div></div><div class="note_contents"><span class="note_content_field">Review: </span><span class="note_content_value">The paper propose to learn autoencoders which incorporate pairwise constraints while learning the representation. Such constraints are motivated from the available side information in a given application. Inducing application-specific structure while training autoencoders allows to learn embeddings with better neighborhood preserving properties. In wireless positioning application, the paper proposes fixed absolute/relative distance and maximum absolute/relative distance constraints. Experiments on synthetic and real-word datasets show improved performance with the proposed approach.

Some comments/questions:

1. Table 1 shows different constraints along with the corresponding regularizers which are employed while training autoencoders. How is the regularization parameter set for (so many) regularizers?

2. Employing constraints (e.g. manifolds) while learning representation has recently attracted attention (see the references below). The proposed approach may benefit from learning the constraints directly on the manifolds (than via regularizers). Some of the constraints discussed in the paper can be modeled on manifolds.

Arjovsky et al (2016). Unitary evolution recurrent neural networks
Huang et al (2017). Orthogonal weight normalization: Solution to optimization over multiple dependent Stiefel manifolds in deep neural networks.
Huang et al (2018). Building deep networks on Grassmann manifolds
Ozay and Okatani (2018). Training CNNs with normalized kernels.</span></div><div class="note_contents"><span class="note_content_field">Rating: </span><span class="note_content_value">6: Marginally above acceptance threshold</span></div><div class="note_contents"><span class="note_content_field">Confidence: </span><span class="note_content_value">2: The reviewer is willing to defend the evaluation, but it is quite likely that the reviewer did not understand central parts of the paper</span></div><div class="reply_row clearfix"></div></div><div class="children"><div class="note_with_children"><div id="note_r1xT1HuC3X" class="note panel"><div class="title_pdf_row clearfix"><h2 class="note_content_title"><a>Thanks a lot for the comments!</a>  <button class="btn btn-xs btn-default permalink-button" title="Link to this comment" data-permalink-url="https://openreview.net/forum?id=ryemosC9tm&amp;noteId=r1xT1HuC3X"><span class="glyphicon glyphicon-link" aria-hidden="true"></span></button></h2></div><div class="meta_row"><span class="signatures">ICLR 2019 Conference Paper606 Authors</span></div><div class="clearfix"><div class="meta_row pull-left"><span class="date item">06 Nov 2018</span><span class="item">ICLR 2019 Conference Paper606 Official Comment</span><span class="item">Readers: <span class="readers-icon glyphicon glyphicon-globe"></span> Everyone</span></div><div class="meta_row meta_actions"></div></div><div class="note_contents"><span class="note_content_field">Comment: </span><span class="note_content_value">1. We are only using one type of regularizer at once (but a sum over many pairs of points). Hence, we only end up with one regularization parameter that must be tuned. To select the best parameter in practice, we use a simple grid search. In cases where one wants to use multiple different types of regularizers, selecting the best parameters requires a multidimensional grid search. We have not investigated more efficient (or even automated) ways to select the regularization parameter in practice. 

2. Thanks for pointing out these references. Whether these manifold constraints would help the performance of our algorithms is indeed an interesting question as we have not experimented with such methods in the context of our paper. In our application, we would like to impose such constraints not on the weights but rather on the embedded points. For example, enforcing orthogonal rows on a batch of embedded points may help to learn a more meaningful low-dimensional representation, similar to PCA. We will include the suggested references and outline potential applications of such manifold constraints on either the weights or the embedded points (which is more challenging, but may be more relevant in our scenario). </span></div><div class="reply_row clearfix"></div></div><div class="children"></div></div></div></div></div></main></div>
  </div>
</div>


    <!-- Footer -->
    <footer class="sitemap">
      <div class="container">
        <div class="row hidden-xs">
          <div class="col-sm-4">
            <ul class="list-unstyled">
              <li><a href="/" class="home">Home</a></li>
              <li><a href="/venues">All Venues</a></li>
            </ul>
          </div>
    
          <div class="col-sm-4">
            <ul class="list-unstyled">
              <li><a href="/about">About OpenReview</a></li>
              <li><a href="/contact">Contact</a></li>
              <li><a href="#" data-toggle="modal" data-target="#feedback-modal">Feedback</a></li>
            </ul>
          </div>
    
          <div class="col-sm-4">
            <ul class="list-unstyled">
              <li><a href="/terms">Terms of Service</a></li>
              <li><a href="/privacy">Privacy Policy</a></li>
            </ul>
          </div>
        </div>
    
        <div class="row visible-xs-block">
          <div class="col-xs-6">
            <ul class="list-unstyled">
              <li><a href="/" class="home">Home</a></li>
              <li><a href="/about">About OpenReview</a></li>
              <li><a href="#" data-toggle="modal" data-target="#feedback-modal">Feedback</a></li>
            </ul>
          </div>
    
          <div class="col-xs-6">
            <ul class="list-unstyled">
              <li><a href="/contact">Contact</a></li>
              <li><a href="/terms">Terms of Service</a></li>
              <li><a href="/privacy">Privacy Policy</a></li>
            </ul>
          </div>
        </div>
      </div>
    </footer>
    
    <footer class="sponsor">
      <div class="container">
        <div class="row">
          <div class="col-sm-10 col-sm-offset-1">
            <p class="text-center">
              OpenReview is created by the <a href="http://www.iesl.cs.umass.edu/" target="_blank">Information Extraction and Synthesis Laboratory</a>, College of Information and Computer Science, University of Massachusetts Amherst. We gratefully acknowledge the support of the OpenReview sponsors:  Google,  Facebook, NSF, the University of Massachusetts Amherst Center for Data Science, and Center for Intelligent Information Retrieval, as well as the Google Cloud Platform for donating the computing and networking services on which OpenReview.net runs.
            </p>
          </div>
        </div>
      </div>
    </footer>

  <div id="feedback-modal" class="modal fade" tabindex="-1" role="dialog">
    <div class="modal-dialog">
      <div class="modal-content">
        <div class="modal-header">
          <button type="button" class="close" data-dismiss="modal" aria-label="Close"><span aria-hidden="true">×</span></button>
          <h3 class="modal-title">Send Feedback</h3>
        </div>
        <div class="modal-body">
          <p>Enter your feedback below and we'll get back to you as soon as possible.</p>
          <form action="/feedback" method="POST">
            <div class="form-group">
              <input type="email" name="from" class="form-control" placeholder="Email" />
            </div>
            <div class="form-group">
              <input type="text" name="subject" class="form-control" placeholder="Subject" />
            </div>
            <div class="form-group">
              <textarea name="message" class="form-control feedback-input" rows="5" placeholder="Message" required=""></textarea>
            </div>
          <ul id="ui-id-2" tabindex="0" class="ui-menu ui-widget ui-widget-content" style="display: none;"></ul></form>
        </div>
        <div class="modal-footer">
          <button type="button" class="btn btn-default" data-dismiss="modal">Cancel</button>
          <button type="button" class="btn btn-primary">Send</button>
        </div>
      </div>
    </div>
  </div>

  <script type="text/javascript" async="" src="https://www.google-analytics.com/analytics.js"></script><script src="//cdnjs.cloudflare.com/ajax/libs/jquery/2.2.4/jquery.min.js"></script>
  <script>window.jQuery || document.write('&lt;script src="/static/js/vendor/jquery-2.2.4.min.js"&gt;&lt;\/script&gt;')</script>

  <script src="/static/dist/vendor.min.js"></script>
  <script src="/static/dist/templates.min.js"></script>
  <script src="/static/dist/openreview.min.js"></script>

    <script>window.legacyScripts = true;</script>


    <script async="" src="https://www.googletagmanager.com/gtag/js?id=UA-108703919-1"></script>
    <script>
      window.dataLayer = window.dataLayer || [];
      function gtag() { dataLayer.push(arguments); }
      gtag('js', new Date());
      gtag('config', 'UA-108703919-1');
    </script>


<div role="status" aria-live="assertive" aria-relevant="additions" class="ui-helper-hidden-accessible"></div></body></html>