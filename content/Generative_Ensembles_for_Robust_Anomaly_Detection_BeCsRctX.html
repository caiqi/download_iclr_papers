<!DOCTYPE html><html xmlns="http://www.w3.org/1999/xhtml" class="no-js"><head>
  <meta charset="utf-8" />
  <meta http-equiv="X-UA-Compatible" content="IE=edge,chrome=1" />
  <meta name="viewport" content="width=device-width, initial-scale=1" />

  <title>Generative Ensembles for Robust Anomaly Detection | OpenReview</title>
  <meta name="description" content="" />

      <meta name="citation_title" content="Generative Ensembles for Robust Anomaly Detection" />
        <meta name="citation_author" content="Anonymous" />
      <meta name="citation_publication_date" content="2018/09/27" />
      <meta name="citation_online_date" content="2018/09/27" />
      <meta name="citation_pdf_url" content="https://openreview.net/pdf?id=B1e8CsRctX" />
      <meta name="twitter:card" content="summary" />
      <meta name="twitter:site" content="@openreviewnet" />
      <meta name="og:title" content="Generative Ensembles for Robust Anomaly Detection" />
      <meta name="og:description" content="Deep generative models are capable of learning probability distributions over large, high-dimensional datasets such as images, video and natural language. Generative models trained on samples from..." />
      <meta name="og:image" content="https://openreview.net/static/images/openreview_logo_512.png" />

  <link rel="stylesheet" href="https://fonts.googleapis.com/css?family=Noto+Sans:400,400i,700,700i" />

  <link rel="stylesheet" href="/static/css/bootstrap.min.css" />
  <link rel="stylesheet" href="/static/css/jquery-ui.min.css" />
  <link rel="stylesheet" href="/static/css/main.min.css" />
</head>

<body class="forum">
  <nav class="navbar navbar-inverse navbar-fixed-top" role="navigation">
    <div class="container">
  
      <div class="navbar-header">
        <button type="button" class="navbar-toggle collapsed" data-toggle="collapse" data-target="#navbar" aria-expanded="false" aria-controls="navbar">
          <span class="sr-only">Toggle navigation</span>
          <span class="icon-bar"></span>
          <span class="icon-bar"></span>
          <span class="icon-bar"></span>
        </button>
        <a class="navbar-brand home push-link" href="/"><strong>OpenReview</strong>.net</a>
      </div>
  
      <div id="navbar" class="navbar-collapse collapse">
        <form class="navbar-form navbar-left profile-search" role="search">
          <div class="form-group has-feedback">
            <input id="search_input" type="text" class="form-control ui-autocomplete-input" placeholder="Search ICLR 2019 Conference" autocomplete="off" />
            <span class="glyphicon glyphicon-search form-control-feedback" aria-hidden="true"></span>
          </div>
  
          <input id="search_group" type="hidden" value="ICLR.cc/2019/Conference" />
          <input id="search_content" type="hidden" value="all" />
        <ul id="ui-id-1" tabindex="0" class="ui-menu ui-widget ui-widget-content ui-autocomplete ui-front" style="display: none;"></ul></form>
  
        <ul class="nav navbar-nav navbar-right">
        
            <li id="user-menu"><a href="/login">Login</a></li>
        </ul>
      </div>
  
    </div>
  </nav>

  <div id="or-banner" class="banner" style="">
  <div class="container">
    <div class="row">
      <div class="col-xs-12"><a href="/group?id=ICLR.cc/2019/Conference" title="Venue Homepage"><img class="icon" src="/static/images/arrow_left.svg" /> Go to <strong>ICLR 2019 Conference</strong> homepage</a></div>
    </div>
  </div>
</div>
<div id="flash-message-container" class="alert alert-danger" role="alert" style="display: none;">
  <div class="container">
    <div class="row">
      <div class="col-xs-12">
        <div class="alert-content">
          <button type="button" class="close" aria-label="Close"><span aria-hidden="true">×</span></button>
        </div>
      </div>
    </div>
  </div>
</div>

<div class="container">
  <div class="row">
    <div class="col-xs-12">
      <main id="content" class="clearfix openbanner-visible legacy-styles"><div id="note_B1e8CsRctX" class="note panel"><div class="title_pdf_row clearfix"><h2 class="note_content_title"><a>Generative Ensembles for Robust Anomaly Detection</a> <a class="note_content_pdf" href="/pdf?id=B1e8CsRctX" title="Download PDF" target="_blank"><img src="/static/images/pdf_icon_blue.svg" /></a> </h2></div><div class="meta_row"><span class="signatures">Anonymous</span></div><div class="clearfix"><div class="meta_row pull-left"><span class="date item">28 Sep 2018 (modified: 19 Nov 2018)</span><span class="item">ICLR 2019 Conference Blind Submission</span><span class="item">Readers: <span class="readers-icon glyphicon glyphicon-globe"></span> Everyone</span><span class="item"><a class="action-bibtex-modal" data-bibtex="@inproceedings{    &#10;anonymous2019generative,    &#10;title={Generative Ensembles for Robust Anomaly Detection},    &#10;author={Anonymous},    &#10;booktitle={Submitted to International Conference on Learning Representations},    &#10;year={2019},    &#10;url={https://openreview.net/forum?id=B1e8CsRctX},    &#10;note={under review}    &#10;}">Show Bibtex</a></span><a class="note_content_pdf item" href="/revisions?id=B1e8CsRctX" target="_blank">Show Revisions</a></div><div class="meta_row meta_actions"></div></div><div class="note_contents"><span class="note_content_field">Abstract: </span><span class="note_content_value">Deep generative models are capable of learning probability distributions over large, high-dimensional datasets such as images, video and natural language. Generative models trained on samples from p(x) ought to assign low likelihoods to out-of-distribution (OoD) samples from q(x), making them suitable for anomaly detection applications. We show that in practice, likelihood models are themselves susceptible to OoD errors, and even assign large likelihoods to images from other natural datasets. To mitigate these issues, we propose Generative Ensembles, a model-independent technique for OoD detection that combines density-based anomaly detection with uncertainty estimation. Our method outperforms ODIN and VIB baselines on image datasets, and achieves comparable performance to a classification model on the Kaggle Credit Fraud dataset.</span></div><div class="note_contents"><span class="note_content_field">Keywords: </span><span class="note_content_value">Anomaly Detection, Uncertainty, Out-of-Distribution, Generative Models</span></div><div class="note_contents"><span class="note_content_field">TL;DR: </span><span class="note_content_value">We use generative models to perform out-of-distribution detection, and improve their robustness with uncertainty estimation.</span></div><div class="reply_row clearfix"><div class="item" id="reply_count">7 Replies</div></div></div><hr class="small" /><div id="note_children"><div class="note_with_children"><div id="note_SyefX0ayAX" class="note panel"><div class="title_pdf_row clearfix"><h2 class="note_content_title"><a>Overall rebuttal comment from authors</a>  <button class="btn btn-xs btn-default permalink-button" title="Link to this comment" data-permalink-url="https://openreview.net/forum?id=B1e8CsRctX&amp;noteId=SyefX0ayAX"><span class="glyphicon glyphicon-link" aria-hidden="true"></span></button></h2></div><div class="meta_row"><span class="signatures">ICLR 2019 Conference Paper898 Authors</span></div><div class="clearfix"><div class="meta_row pull-left"><span class="date item">19 Nov 2018</span><span class="item">ICLR 2019 Conference Paper898 Official Comment</span><span class="item">Readers: <span class="readers-icon glyphicon glyphicon-globe"></span> Everyone</span></div><div class="meta_row meta_actions"></div></div><div class="note_contents"><span class="note_content_field">Comment: </span><span class="note_content_value">We thank the reviewers for helpful feedback and highlighting points of confusion in our paper.

In considering all 3 reviewers’ comments (R1 “reads more like a summary blog post”, R2 “the ideas as well as reasoning flow smoothly”, R3 “well-written and easy to follow while providing useful insight and connecting previous work to the subject of study”) we believe that all reviewers consider our presentation to be logically clear, but may be lacking in technical clarity (raised by Reviewer 1) or novelty (raised by Reviewer 2). There is especially some confusion regarding our notation and how it relates to GAN models for anomaly detection (e.g. “posterior distribution over alternate distributions”). 

To address technical clarity issues raised by R1, we’ve answered their questions in comments and made edits to our paper to make the problem setup and notation more clear.  We’ve responded directly to R2’s comment on why we believe our work is novel. 

Finally, we’ve updated the paper with improved VAE experiments on Fashion MNIST (confirming our hypothesis of posterior collapse).
</span></div><div class="reply_row clearfix"></div></div><div class="children"></div></div><div class="note_with_children"><div id="note_SJxjubiqhm" class="note panel"><div class="title_pdf_row clearfix"><h2 class="note_content_title"><a>Needs a lot of work on improving technical rigor and clarity</a>  <button class="btn btn-xs btn-default permalink-button" title="Link to this comment" data-permalink-url="https://openreview.net/forum?id=B1e8CsRctX&amp;noteId=SJxjubiqhm"><span class="glyphicon glyphicon-link" aria-hidden="true"></span></button></h2></div><div class="meta_row"><span class="signatures">ICLR 2019 Conference Paper898 AnonReviewer1</span></div><div class="clearfix"><div class="meta_row pull-left"><span class="date item">03 Nov 2018 (modified: 07 Nov 2018)</span><span class="item">ICLR 2019 Conference Paper898 Official Review</span><span class="item">Readers: <span class="readers-icon glyphicon glyphicon-globe"></span> Everyone</span></div><div class="meta_row meta_actions"></div></div><div class="note_contents"><span class="note_content_field">Review: </span><span class="note_content_value">Note to Area Chair: Another paper submitted to ICLR under the title “Do Deep Generative Models Know What They Don’t Know?” shares several similarities with the current submission.

This paper highlights a deficiency of current generative models in detecting out-of-distribution based samples based on likelihoods assigned by the model (in cases where the likelihoods are well-defined) or the discriminator distribution for GANs (where likelihoods are typically not defined). To remedy this deficiency, the paper proposes to use ensembles of generative models to obtain a robust WAIC criteria for anomaly detection.

My main concern is with the level of technical rigor of this work. Much of this has to do with the presentation, which reads to me more like a summary blog post rather than a technical paper.
- I couldn’t find a formal specification of the anomaly detection setup and how generative models are used for this task anywhere in the paper.
- Section 2 seems to be the major contribution of this work. But it was very hard to understand what exactly is going on. What is the notation for the generative distribution? Introduction uses p_theta. Page 2, Paragraph 1 uses q_theta (x). Eq. (1) uses p_theta and then the following paragraphs use q_theta.
- In Eq. (1), is theta a random variable?
- How are generative ensembles trained?  All the paper says is “independently trained”. Is the parameter initialization different? Is the dataset shuffling different? Is the dataset sampled with replacement (as in bootstrapping)?
- “By training an ensemble of GANs we can estimate the posterior distribution over model deciscion boundaries D_theta(x), or equivalently, the posterior distribution over alternate distributions q_theta. In other words, we can use uncertainty estimation on randomly sampled discriminators to de-correlate the OoD classification errors made by a single discriminator” Why is the discriminator parameterized by theta? What is an ensemble of GANs? Multiple generators or multiple discriminators or both? What are “randomly sampled discriminators”? What do the authors mean by "posterior distribution over alternate distributions"?

With regards to the technical assessment, I have the following questions for the authors:
- In Figure 1, how do the histograms look for the training distribution of CIFAR? If the histograms for train and test have an overlap much higher than the overlap between the train of CIFAR and test set of any other distribution, then ensembling seems unnecessary and anomaly detecting can simply be done via setting a maximum and a minimum threshold on the likelihood for a test point. In addition to the histograms, I'd be curious to see results with this baseline mechanism.
- Why should the WAIC criteria weigh the mean and variance equally?
- Did the authors actually try to fix the posterior collapse issue in Figure 3b using beta-VAEs as recommended? Given the simplicity of implementing beta-VAEs, this should be a rather easy experiment to include.

Minor typos:
- ODIN and VIB are not defined in the abstract
- Page 3: “deciscion”
- Page 2, para 2: “log_\theta p(x)”</span></div><div class="note_contents"><span class="note_content_field">Rating: </span><span class="note_content_value">4: Ok but not good enough - rejection</span></div><div class="note_contents"><span class="note_content_field">Confidence: </span><span class="note_content_value">3: The reviewer is fairly confident that the evaluation is correct</span></div><div class="reply_row clearfix"></div></div><div class="children"><div class="note_with_children"><div id="note_r1xAPnTy0X" class="note panel"><div class="title_pdf_row clearfix"><h2 class="note_content_title"><a>Addressed issues of technical clarity, performed follow-up experiments on posterior collapse</a>  <button class="btn btn-xs btn-default permalink-button" title="Link to this comment" data-permalink-url="https://openreview.net/forum?id=B1e8CsRctX&amp;noteId=r1xAPnTy0X"><span class="glyphicon glyphicon-link" aria-hidden="true"></span></button></h2></div><div class="meta_row"><span class="signatures">ICLR 2019 Conference Paper898 Authors</span></div><div class="clearfix"><div class="meta_row pull-left"><span class="date item">19 Nov 2018</span><span class="item">ICLR 2019 Conference Paper898 Official Comment</span><span class="item">Readers: <span class="readers-icon glyphicon glyphicon-globe"></span> Everyone</span></div><div class="meta_row meta_actions"></div></div><div class="note_contents"><span class="note_content_field">Comment: </span><span class="note_content_value">Thank you for the detailed review and critique.

We agree that “Do Deep ... They Don’t Know?” shares a concurrent discovery with us in identifying how generative models assign wrong likelihood to OoD inputs, and have updated our paper to cite their contribution. Our contributions differ in that their work performs analysis of why this phenomenon occurs, while we demonstrate that this can be fixed by using uncertainty estimation and WAIC, and then apply these fixed models to the OoD problem.

We agree that our paper could use more technical clarity, i.e. make this work easier to reproduce. The open-sourced code will be linked to the paper after double-blind review process, which we believe to be the highest standard of technical clarity when specifying our method and evaluation metrics. In the meantime, we’ve also done the following:

1. We’ve clarified Section 4 to re-iterate that our anomaly detection problem specification is identical to that of Liang et al. 2017 and Alemi et al. 2017, and our evaluation metric (AUROC) is the same.

2. Clarified the notation of our notation for p, q, p_theta, q_theta in the paper. We think that R1’s confusion on our GAN ensemble setup can be addressed by clarifying the reasoning behind our terminology, and explaining a bit further what it means to “randomly sample a discriminator from a posterior distribution over alternate distributions”

The choice of terminology is motivated by our GAN variant of generative ensembles. If p(x) is the true generative distribution, p_theta(x) is some generative model’s approximation of it. In Eq (1), theta is a (multivariate) random variable parameterizing an abstract generative model (e.g. weights in a neural network). We’ve clarified this in the intro. 

In the case of GANs, a subset of the variable theta parameterizes the generator and a subset of theta parameterizes the discriminator. Therefore, samples from the generator come from a generative distribution q_\theta(x). We notate a GAN generator’s distribution as q_\theta(x) and not p_\theta(x) (which we use for referring to normalizing flow and VAE likelihood models) is that in GANs, the discriminator is being optimized to learn a likelihood ratio p(x) / q_\theta(x). That is, separating true data samples from p(x) from OoD samples from q_\theta(x). 

Thus, q(x) and q_theta(x) always refer to OoD distributions. This also makes discussion more clear in the context of discriminative anomaly detection classifiers (which learn p(x)/q(x)) and GAN discriminators (which learn p(x)/q_theta(x)).

In Section 2.1, we mention “randomly sampled discriminators” and “posterior distribution over alternate distributions”. Models (theta) trained under SGD can be assumed to be drawn randomly from some posterior distribution over p(theta|x). In a GAN, random variable theta specifies the  alternate distribution q_\theta(x), or equivalently, the implicit discriminator likelihood ratio p(x) / q_\theta(x) (when the discriminator is trained with sigmoid cross entropy, which we do). Our GAN ensembles samples entire GANs (i.e. generator and discriminator) together, by training 5 GANs independently and then combining discriminator predictions for OoD classification. It would be problematic to sample only discriminators in the training process, since that does not change q_\theta(x) (and there is the question of how feedback to the generators should be accomplished in this manner). 

Technical assessment questions:

- Re: Histograms. This is a reasonable suggestion, and resembles the interpretation of likelihood predictions as a feature, rather than a scoring function. The scoring function you propose is a min/max function over the distribution of features. Another approach would be a statistical hyppothesis test using the training distribution’s likelihood predictions as the variable of interest. Unfortunately, the likelihoods of OoD distributions often overlap with the in-distribution test samples (MNIST and Fashion MNIST VAEs). In training a GLOW model, you will also find a gap between train and test likelihoods. So generative models are not good enough yet to reduce the generalization gap of likelihood models zero. 

- We refer the reviewer to "Understanding predictive information criteria for Bayesian models" (Gelman et al.) for a motivation of the WAIC objective. In short, the variance term is a correction for how much the fitting of k parameters will increase predictive accuracy, by chance alone. K is estimated by the variance.  

- Re: Posterior Collapse: Good suggestion! We went back to our VAE setup and ran a few follow-up experiments to prove this hypothesis. The short answer is that “yes, decreasing Beta reduced posterior collapse and made things better”. We’ve edited section 4.1 to document our findings. 

Minor typos: They have been fixed in the latest revision. Thank you so much for catching these!</span></div><div class="reply_row clearfix"></div></div><div class="children"></div></div></div></div><div class="note_with_children"><div id="note_ryl3dTZYhX" class="note panel"><div class="title_pdf_row clearfix"><h2 class="note_content_title"><a>Well below the ICLR level</a>  <button class="btn btn-xs btn-default permalink-button" title="Link to this comment" data-permalink-url="https://openreview.net/forum?id=B1e8CsRctX&amp;noteId=ryl3dTZYhX"><span class="glyphicon glyphicon-link" aria-hidden="true"></span></button></h2></div><div class="meta_row"><span class="signatures">ICLR 2019 Conference Paper898 AnonReviewer3</span></div><div class="clearfix"><div class="meta_row pull-left"><span class="date item">02 Nov 2018 (modified: 07 Nov 2018)</span><span class="item">ICLR 2019 Conference Paper898 Official Review</span><span class="item">Readers: <span class="readers-icon glyphicon glyphicon-globe"></span> Everyone</span></div><div class="meta_row meta_actions"></div></div><div class="note_contents"><span class="note_content_field">Review: </span><span class="note_content_value">- Novelty is minimal and is well below the level required by ICLR.

- The reasoning lists the problems of GANs and then the fact that GAN ensembles would target that, based on a toy example in Figure 2. 

- Why to choose GANs though in the first place? Given the buildup, and given the other well-known training issues about GANs, are they the right choice for the basic modeling units, i.e. the ensemble units, in such case? A GANs adversary bases its comparisons on individual data points, rather than on distribution comparisons or on groups of points like MMD, etc. I understand the reasoning behind the choice of generative models (GMs), but it is choosing GANs out of the set of GMs in this particular case that I am referring to. 

- The paper is quite well written. The ideas as well as the reasoning flow very smoothly. 

- Experiments are well prepared. 

Rather minor:
- page 1: "When training and test distributions differ, neural networks may provide ..." This is true but may be a clarification here regarding the fact that the neural networks involved with several modeling problems, e.g. the ones trained for domain adaptation or meta-learning tasks, target this shift or difference in domains, and typically provide a way to tackle this problem.</span></div><div class="note_contents"><span class="note_content_field">Rating: </span><span class="note_content_value">4: Ok but not good enough - rejection</span></div><div class="note_contents"><span class="note_content_field">Confidence: </span><span class="note_content_value">4: The reviewer is confident but not absolutely certain that the evaluation is correct</span></div><div class="reply_row clearfix"></div></div><div class="children"><div class="note_with_children"><div id="note_SyxJSTTy0Q" class="note panel"><div class="title_pdf_row clearfix"><h2 class="note_content_title"><a>Addressing concerns about novelty and use of GANs</a>  <button class="btn btn-xs btn-default permalink-button" title="Link to this comment" data-permalink-url="https://openreview.net/forum?id=B1e8CsRctX&amp;noteId=SyxJSTTy0Q"><span class="glyphicon glyphicon-link" aria-hidden="true"></span></button></h2></div><div class="meta_row"><span class="signatures">ICLR 2019 Conference Paper898 Authors</span></div><div class="clearfix"><div class="meta_row pull-left"><span class="date item">19 Nov 2018</span><span class="item">ICLR 2019 Conference Paper898 Official Comment</span><span class="item">Readers: <span class="readers-icon glyphicon glyphicon-globe"></span> Everyone</span></div><div class="meta_row meta_actions"></div></div><div class="note_contents"><span class="note_content_field">Comment: </span><span class="note_content_value">We thank Reviewer 2 for their praise and raising concerns about novelty. It is an important point worth discussing.

In addition to proposing a superior method for anomaly detection, part of the novel contribution in this work involved synthesizing concepts from multiple fields likelihood estimation techniques from deep generative models, adversarial defense, model uncertainty, challenging discriminative anomaly detection methods and their relationship to GAN discriminators. 

We tie these disparate concepts together into a unified perspective on the OoD problem. Therefore, we took great care into making sure the motivation of our work transitions smoothly, perhaps even to the point of stating the obvious to Reviewer 2. We emphasize that to our knowledge, our work is the first to extend our understanding of the OoD problem in context of prior work in generative modeling, Bayesian Deep Learning, and anomaly detection applications for modern generative models. These connections are not well known in the community and we hope that our paper will amend that. 

Additional novel aspects of this work: The observation that density estimators (as implemented by a deep generative model) are NOT robust to OoD inputs themselves is a novel observation, concurrent with another ICLR submission. To our knowledge, we are also the first work to leverage the modern advancements in deep generative models to perform anomaly detection on high-dimensional inputs such as images.  

To address R2’s comments “The reasoning lists the problems of GANs” and “Why to choose GANs though in the first place?”, we emphasize that we are not saying GANs shouldn’t be used for anomaly detection, only that their lack of exact likelihoods presents some challenges. We make an effort to make them work in our paper in our comparison to other generative model families.

&gt; - page 1: "When training and test distributions differ, neural networks may provide ..." 

There are varying degrees of “out-of-distribution-ness” at test time. One way to carve up the problem specification is to consider inputs that (1) are different than the training set but you want the model to perform well on anyway, e.g. a subtle change in physics parameters a robot encounters when deployed. (2) inputs the model has no business classifying, i.e. showing a picture of a building to a cat/dog classifier. 

The first situation is what you are describing, in which methods like sim2real, domain adaptation, meta-learning can address. As we stated in Section 3.1, our paper primarily deals with the second case, in which you don’t want the model to give bogus outputs for bogus inputs, which also may be adversarial. We appreciate the feedback that this might be confusing if the reader is assuming problem formulation (1); we welcome the other reviewers to chime in here if it would make things more clear to state this.
</span></div><div class="reply_row clearfix"></div></div><div class="children"></div></div></div></div><div class="note_with_children"><div id="note_BJxgXKZdhQ" class="note panel"><div class="title_pdf_row clearfix"><h2 class="note_content_title"><a>Interesting combination of the previous work with useful results.</a>  <button class="btn btn-xs btn-default permalink-button" title="Link to this comment" data-permalink-url="https://openreview.net/forum?id=B1e8CsRctX&amp;noteId=BJxgXKZdhQ"><span class="glyphicon glyphicon-link" aria-hidden="true"></span></button></h2></div><div class="meta_row"><span class="signatures">ICLR 2019 Conference Paper898 AnonReviewer2</span></div><div class="clearfix"><div class="meta_row pull-left"><span class="date item">01 Nov 2018 (modified: 07 Nov 2018)</span><span class="item">ICLR 2019 Conference Paper898 Official Review</span><span class="item">Readers: <span class="readers-icon glyphicon glyphicon-globe"></span> Everyone</span></div><div class="meta_row meta_actions"></div></div><div class="note_contents"><span class="note_content_field">Review: </span><span class="note_content_value">The authors present an OOD detection scheme with an ensemble of generative models. When the exact likelihood is available from the generative model, the authors approximate the WAIC score. For GAN models, the authors compute the variance over the discriminators for any given input. They show that this method outperforms ODIN and VIB on image datasets and also achieves comparable performance on Kaggle Credit Fraud dataset.

The paper is overall well-written and easy to follow. I only have a few comments about the work.

I think the authors should address the following points in the paper.
- What is the size of the ensemble for the experiments?
- How does the size of the ensemble influence the measured performance?
- It is Fast Gradient Sign Method (FGSM), not FSGM. See [1]. Citing [1] for FGSM would also be appropriate.

Quality. The submission is technically sound. The empirical results support the claims, and the authors discuss the failure cases. 
Clarity. The paper is well-written and easy to follow while providing useful insight and connecting previous work to the subject of study.
Originality. To the best my knowledge, the proposed approach is a novel combination of well-known techniques.
Significance. The presented idea improves over the state-of-the-art.


References
[1] I. Goodfellow, J. Shlens, and C. Szegedy, “Explaining and Harnessing Adversarial Examples,” in ICLR, 2015.

 </span></div><div class="note_contents"><span class="note_content_field">Rating: </span><span class="note_content_value">7: Good paper, accept</span></div><div class="note_contents"><span class="note_content_field">Confidence: </span><span class="note_content_value">3: The reviewer is fairly confident that the evaluation is correct</span></div><div class="reply_row clearfix"></div></div><div class="children"><div class="note_with_children"><div id="note_ByxCtppk0m" class="note panel"><div class="title_pdf_row clearfix"><h2 class="note_content_title"><a>Thanks!</a>  <button class="btn btn-xs btn-default permalink-button" title="Link to this comment" data-permalink-url="https://openreview.net/forum?id=B1e8CsRctX&amp;noteId=ByxCtppk0m"><span class="glyphicon glyphicon-link" aria-hidden="true"></span></button></h2></div><div class="meta_row"><span class="signatures">ICLR 2019 Conference Paper898 Authors</span></div><div class="clearfix"><div class="meta_row pull-left"><span class="date item">19 Nov 2018</span><span class="item">ICLR 2019 Conference Paper898 Official Comment</span><span class="item">Readers: <span class="readers-icon glyphicon glyphicon-globe"></span> Everyone</span></div><div class="meta_row meta_actions"></div></div><div class="note_contents"><span class="note_content_field">Comment: </span><span class="note_content_value">We thank Reviewer 3 for the review and highlighting missing details from our paper. We’ve added them into the paper.

&gt; - How does the size of the ensemble influence the measured performance?

For CIFAR10, we have found 5 ensembles to make a large difference over 3 ensembles (about .7 AUROC). There seem to be diminishing returns for models &gt; 5.

&gt; - It is Fast Gradient Sign Method (FGSM), not FSGM. See [1]. Citing [1] for FGSM would also be appropriate.

Fixed, and already cited. Thanks! 

</span></div><div class="reply_row clearfix"></div></div><div class="children"></div></div></div></div></div></main></div>
  </div>
</div>


    <!-- Footer -->
    <footer class="sitemap">
      <div class="container">
        <div class="row hidden-xs">
          <div class="col-sm-4">
            <ul class="list-unstyled">
              <li><a href="/" class="home">Home</a></li>
              <li><a href="/venues">All Venues</a></li>
            </ul>
          </div>
    
          <div class="col-sm-4">
            <ul class="list-unstyled">
              <li><a href="/about">About OpenReview</a></li>
              <li><a href="/contact">Contact</a></li>
              <li><a href="#" data-toggle="modal" data-target="#feedback-modal">Feedback</a></li>
            </ul>
          </div>
    
          <div class="col-sm-4">
            <ul class="list-unstyled">
              <li><a href="/terms">Terms of Service</a></li>
              <li><a href="/privacy">Privacy Policy</a></li>
            </ul>
          </div>
        </div>
    
        <div class="row visible-xs-block">
          <div class="col-xs-6">
            <ul class="list-unstyled">
              <li><a href="/" class="home">Home</a></li>
              <li><a href="/about">About OpenReview</a></li>
              <li><a href="#" data-toggle="modal" data-target="#feedback-modal">Feedback</a></li>
            </ul>
          </div>
    
          <div class="col-xs-6">
            <ul class="list-unstyled">
              <li><a href="/contact">Contact</a></li>
              <li><a href="/terms">Terms of Service</a></li>
              <li><a href="/privacy">Privacy Policy</a></li>
            </ul>
          </div>
        </div>
      </div>
    </footer>
    
    <footer class="sponsor">
      <div class="container">
        <div class="row">
          <div class="col-sm-10 col-sm-offset-1">
            <p class="text-center">
              OpenReview is created by the <a href="http://www.iesl.cs.umass.edu/" target="_blank">Information Extraction and Synthesis Laboratory</a>, College of Information and Computer Science, University of Massachusetts Amherst. We gratefully acknowledge the support of the OpenReview sponsors:  Google,  Facebook, NSF, the University of Massachusetts Amherst Center for Data Science, and Center for Intelligent Information Retrieval, as well as the Google Cloud Platform for donating the computing and networking services on which OpenReview.net runs.
            </p>
          </div>
        </div>
      </div>
    </footer>

  <div id="feedback-modal" class="modal fade" tabindex="-1" role="dialog">
    <div class="modal-dialog">
      <div class="modal-content">
        <div class="modal-header">
          <button type="button" class="close" data-dismiss="modal" aria-label="Close"><span aria-hidden="true">×</span></button>
          <h3 class="modal-title">Send Feedback</h3>
        </div>
        <div class="modal-body">
          <p>Enter your feedback below and we'll get back to you as soon as possible.</p>
          <form action="/feedback" method="POST">
            <div class="form-group">
              <input type="email" name="from" class="form-control" placeholder="Email" />
            </div>
            <div class="form-group">
              <input type="text" name="subject" class="form-control" placeholder="Subject" />
            </div>
            <div class="form-group">
              <textarea name="message" class="form-control feedback-input" rows="5" placeholder="Message" required=""></textarea>
            </div>
          <ul id="ui-id-2" tabindex="0" class="ui-menu ui-widget ui-widget-content" style="display: none;"></ul></form>
        </div>
        <div class="modal-footer">
          <button type="button" class="btn btn-default" data-dismiss="modal">Cancel</button>
          <button type="button" class="btn btn-primary">Send</button>
        </div>
      </div>
    </div>
  </div>

  <script type="text/javascript" async="" src="https://www.google-analytics.com/analytics.js"></script><script src="//cdnjs.cloudflare.com/ajax/libs/jquery/2.2.4/jquery.min.js"></script>
  <script>window.jQuery || document.write('&lt;script src="/static/js/vendor/jquery-2.2.4.min.js"&gt;&lt;\/script&gt;')</script>

  <script src="/static/dist/vendor.min.js"></script>
  <script src="/static/dist/templates.min.js"></script>
  <script src="/static/dist/openreview.min.js"></script>

    <script>window.legacyScripts = true;</script>


    <script async="" src="https://www.googletagmanager.com/gtag/js?id=UA-108703919-1"></script>
    <script>
      window.dataLayer = window.dataLayer || [];
      function gtag() { dataLayer.push(arguments); }
      gtag('js', new Date());
      gtag('config', 'UA-108703919-1');
    </script>


<div role="status" aria-live="assertive" aria-relevant="additions" class="ui-helper-hidden-accessible"></div></body></html>