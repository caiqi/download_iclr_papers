<!DOCTYPE html><html xmlns="http://www.w3.org/1999/xhtml" class="no-js"><head>
  <meta charset="utf-8" />
  <meta http-equiv="X-UA-Compatible" content="IE=edge,chrome=1" />
  <meta name="viewport" content="width=device-width, initial-scale=1" />

  <title>The Expressive Power of Gated Recurrent Units as a Continuous Dynamical System | OpenReview</title>
  <meta name="description" content="" />

      <meta name="citation_title" content="The Expressive Power of Gated Recurrent Units as a Continuous Dynamical System" />
        <meta name="citation_author" content="Anonymous" />
      <meta name="citation_publication_date" content="2018/09/27" />
      <meta name="citation_online_date" content="2018/09/27" />
      <meta name="citation_pdf_url" content="https://openreview.net/pdf?id=H1eiZnAqKm" />
      <meta name="twitter:card" content="summary" />
      <meta name="twitter:site" content="@openreviewnet" />
      <meta name="og:title" content="The Expressive Power of Gated Recurrent Units as a Continuous..." />
      <meta name="og:description" content="Gated recurrent units (GRUs) were inspired by the common gated recurrent unit, long short-term&#10;  memory (LSTM), as a means of capturing temporal structure with less complex memory unit..." />
      <meta name="og:image" content="https://openreview.net/static/images/openreview_logo_512.png" />

  <link rel="stylesheet" href="https://fonts.googleapis.com/css?family=Noto+Sans:400,400i,700,700i" />

  <link rel="stylesheet" href="/static/css/bootstrap.min.css" />
  <link rel="stylesheet" href="/static/css/jquery-ui.min.css" />
  <link rel="stylesheet" href="/static/css/main.min.css" />
</head>

<body class="forum">
  <nav class="navbar navbar-inverse navbar-fixed-top" role="navigation">
    <div class="container">
  
      <div class="navbar-header">
        <button type="button" class="navbar-toggle collapsed" data-toggle="collapse" data-target="#navbar" aria-expanded="false" aria-controls="navbar">
          <span class="sr-only">Toggle navigation</span>
          <span class="icon-bar"></span>
          <span class="icon-bar"></span>
          <span class="icon-bar"></span>
        </button>
        <a class="navbar-brand home push-link" href="/"><strong>OpenReview</strong>.net</a>
      </div>
  
      <div id="navbar" class="navbar-collapse collapse">
        <form class="navbar-form navbar-left profile-search" role="search">
          <div class="form-group has-feedback">
            <input id="search_input" type="text" class="form-control ui-autocomplete-input" placeholder="Search ICLR 2019 Conference" autocomplete="off" />
            <span class="glyphicon glyphicon-search form-control-feedback" aria-hidden="true"></span>
          </div>
  
          <input id="search_group" type="hidden" value="ICLR.cc/2019/Conference" />
          <input id="search_content" type="hidden" value="all" />
        <ul id="ui-id-1" tabindex="0" class="ui-menu ui-widget ui-widget-content ui-autocomplete ui-front" style="display: none;"></ul></form>
  
        <ul class="nav navbar-nav navbar-right">
        
            <li id="user-menu"><a href="/login">Login</a></li>
        </ul>
      </div>
  
    </div>
  </nav>

  <div id="or-banner" class="banner" style="">
  <div class="container">
    <div class="row">
      <div class="col-xs-12"><a href="/group?id=ICLR.cc/2019/Conference" title="Venue Homepage"><img class="icon" src="/static/images/arrow_left.svg" /> Go to <strong>ICLR 2019 Conference</strong> homepage</a></div>
    </div>
  </div>
</div>
<div id="flash-message-container" class="alert alert-danger" role="alert" style="display: none;">
  <div class="container">
    <div class="row">
      <div class="col-xs-12">
        <div class="alert-content">
          <button type="button" class="close" aria-label="Close"><span aria-hidden="true">×</span></button>
        </div>
      </div>
    </div>
  </div>
</div>

<div class="container">
  <div class="row">
    <div class="col-xs-12">
      <main id="content" class="clearfix openbanner-visible legacy-styles"><div id="note_H1eiZnAqKm" class="note panel"><div class="title_pdf_row clearfix"><h2 class="note_content_title"><a>The Expressive Power of Gated Recurrent Units as a Continuous Dynamical System</a> <a class="note_content_pdf" href="/pdf?id=H1eiZnAqKm" title="Download PDF" target="_blank"><img src="/static/images/pdf_icon_blue.svg" /></a> </h2></div><div class="meta_row"><span class="signatures">Anonymous</span></div><div class="clearfix"><div class="meta_row pull-left"><span class="date item">28 Sep 2018 (modified: 11 Oct 2018)</span><span class="item">ICLR 2019 Conference Blind Submission</span><span class="item">Readers: <span class="readers-icon glyphicon glyphicon-globe"></span> Everyone</span><span class="item"><a class="action-bibtex-modal" data-bibtex="@inproceedings{    &#10;anonymous2019the,    &#10;title={The Expressive Power of Gated Recurrent Units as a Continuous Dynamical System},    &#10;author={Anonymous},    &#10;booktitle={Submitted to International Conference on Learning Representations},    &#10;year={2019},    &#10;url={https://openreview.net/forum?id=H1eiZnAqKm},    &#10;note={under review}    &#10;}">Show Bibtex</a></span></div><div class="meta_row meta_actions"></div></div><div class="note_contents"><span class="note_content_field">Abstract: </span><span class="note_content_value">Gated recurrent units (GRUs) were inspired by the common gated recurrent unit, long short-term
memory (LSTM), as a means of capturing temporal structure with less complex memory unit architecture.
Despite their incredible success in tasks such as natural and artificial language processing,
speech, video, and polyphonic music, very little is understood about the specific dynamic features
representable in a GRU network. As a result, it is difficult to know a priori how successful a GRU-RNN
will perform on a given data set. In this paper, we develop a new theoretical framework to
analyze one and two dimensional GRUs as a continuous dynamical system, and classify the dynamic
features obtainable with such system. In addition, we show that a two dimensional GRU
cannot mimic the dynamics of a ring attractor, or more generally, any line attractor without near
zero constant curvature in phase space. These results were then experimentally verified by means of
time series prediction.</span></div><div class="note_contents"><span class="note_content_field">Keywords: </span><span class="note_content_value">Gated Recurrent Units, Recurrent Neural Network, Time Series Predictions, interpretable, Nonlinear Dynamics, Dynamical Systems</span></div><div class="note_contents"><span class="note_content_field">TL;DR: </span><span class="note_content_value">We classify the the dynamical features one and two GRU cells can and cannot capture in continuous time, and verify our findings experimentally with k-step time series prediction.</span></div><div class="reply_row clearfix"><div class="item" id="reply_count">3 Replies</div></div></div><hr class="small" /><div id="note_children"><div class="note_with_children"><div id="note_BJlZ9PDCnm" class="note panel"><div class="title_pdf_row clearfix"><h2 class="note_content_title"><a>a dynamical systems analysis of 1d and 2d gated recurrent units</a>  <button class="btn btn-xs btn-default permalink-button" title="Link to this comment" data-permalink-url="https://openreview.net/forum?id=H1eiZnAqKm&amp;noteId=BJlZ9PDCnm"><span class="glyphicon glyphicon-link" aria-hidden="true"></span></button></h2></div><div class="meta_row"><span class="signatures">ICLR 2019 Conference Paper1203 AnonReviewer1</span></div><div class="clearfix"><div class="meta_row pull-left"><span class="date item">06 Nov 2018 (modified: 07 Nov 2018)</span><span class="item">ICLR 2019 Conference Paper1203 Official Review</span><span class="item">Readers: <span class="readers-icon glyphicon glyphicon-globe"></span> Everyone</span></div><div class="meta_row meta_actions"></div></div><div class="note_contents"><span class="note_content_field">Review: </span><span class="note_content_value">This paper analyzes GRUs from a dynamical systems perspective, i.e. phase diagrams, fixed points, and bifurcations. The abstract and intro are well written and motivate the need for more theoretical framework to understand RNNs, especially how well they are able to represent and express temporal features in the training data. The dynamical systems analysis is well presented and visualized nicely. 

Most of the paper concentrates on the 1d (one single GRU) and 2d (two GRU's) case.  They show that 2d GRUs can be trained to adopt a variety of fixed points, can approximate a line attractors (an important feature for short-term memory), but cannot mimic a ring attractor.

My concerns are:

- The derivation of the continuous time dynamical system (Appendix A) is confusing to me. Unless I'm not following the derivation correctly, should there be another \Delta t in the denominator of the right-hand side of (23), from (22)? It's confusing to me that the continuous-time version in (26) has essentially the same form as the discrete-time version in (22).

- The applicability of this analysis to RNNs of even modest size is unclear. Generically, there's no reason to believe the intuitions from 2d should necessarily generalize to higher dimensions, and rigorous analysis of higher dimensional systems of this kind can be fairly challenging, even if one starts from a continuation analysis.

- Small typo: top of Page 4, figure should refer to 3A, not 2A.

</span></div><div class="note_contents"><span class="note_content_field">Rating: </span><span class="note_content_value">5: Marginally below acceptance threshold</span></div><div class="note_contents"><span class="note_content_field">Confidence: </span><span class="note_content_value">4: The reviewer is confident but not absolutely certain that the evaluation is correct</span></div><div class="reply_row clearfix"></div></div><div class="children"></div></div><div class="note_with_children"><div id="note_HkxD98Zv2Q" class="note panel"><div class="title_pdf_row clearfix"><h2 class="note_content_title"><a>There are several issues with this interesting analysis </a>  <button class="btn btn-xs btn-default permalink-button" title="Link to this comment" data-permalink-url="https://openreview.net/forum?id=H1eiZnAqKm&amp;noteId=HkxD98Zv2Q"><span class="glyphicon glyphicon-link" aria-hidden="true"></span></button></h2></div><div class="meta_row"><span class="signatures">ICLR 2019 Conference Paper1203 AnonReviewer3</span></div><div class="clearfix"><div class="meta_row pull-left"><span class="date item">31 Oct 2018 (modified: 07 Nov 2018)</span><span class="item">ICLR 2019 Conference Paper1203 Official Review</span><span class="item">Readers: <span class="readers-icon glyphicon glyphicon-globe"></span> Everyone</span></div><div class="meta_row meta_actions"></div></div><div class="note_contents"><span class="note_content_field">Review: </span><span class="note_content_value">The authors analyse GRUs with hidden sizes of one and two as continuous-time dynamical systems, claiming that the expressive power of the hidden state representation can provide prior knowledge on how well a GRU will perform on a given dataset. Their analysis shows what kind of hidden state dynamics the GRU can approximate in one and two dimensions. In the experimental part, they show how a GRU with two hidden states trained on a multistep prediction task can learn such dynamics.

Although RNNs are important for Machine Learning, the paper seems to contain flaws in the theoretical part, which seem to invalidate some of the claimed results. But we may change our rating in case of a convincing rebuttal.

The Proof of Lemma 2 claims that h(t) achieves all values on the real set, which is false (h(t) assumes values in (-1,1)). Nevertheless, the theorem should hold since there is always at least one intersection between h and tanh(f(h)).

Lemma 1 claims that for any choice of parameters, there exist only finitely many fixed points. However, in the proof the authors only show that the number of fixed points cannot be uncountable, without taking into consideration the possibility that there are countably many fixed points. The proof also omits steps concerning the Taylor expansion which would make the proof clearer: We suggest adding those steps in the appendix. Furthermore, when equation (12) is Taylor-expanded, the authors do not consider the case where the GRU parameters are such that the argument of function “sech” is outside its convergence radius. These might be parameters for which there are infinitely many fixed points, even if we are unable to provide a Taylor expansion. The Lemma may still be correct, but this does not seem to be a complete proof.

The authors claim that an arbitrarily close approximation of a line attractor can be created using two GRUs, but no proof is provided.

The experimental part is difficult to evaluate since there are no learning curves for the three tasks. For instance, it is difficult to judge whether the GRUs are unable to learn the dynamics of a ring attractor because of theoretical limitations or because the model has not been properly trained for the specific task.

The paper is easy to read, except for certain parts where it is not clear if some of the statements are true in general or just have not been proven false by the authors. It is not clear why Figure 3 is representing all possible simple fixed points and bifurcation fixed points: is there a theoretical result stating that these are the only possible topologies, or are these the only ones found? The same question applies for the 36 images in figure 9. The range of the parameters used for finding these configurations is not specified.

Since the hidden state assumes values in (-1,1)^2, why is its range in most of the images (-1.5,1.5)?

We are not familiar with related work on transformations from discrete to continuous dynamical systems: are the dynamics of the discrete time GRU model preserved in the transformation? If so, is there a reference for this? Are the phase portraits in the middle row of figure 8 generated by letting the discrete GRU system evolve, or is the continuous system used with the parameters of the trained GRU?

We would like to see more explanations of why various topologies are useful for the applications mentioned in the paper. Given a generic dataset, how can these results help to understand how well a GRU will perform?

What is the reason behind the belief that the analysis extends to higher dimensions? The effects of a 1D -&gt; 2D extension are far from trivial - why should that be different for higher dimensions?

The problem the authors want to solve seems important, and some of the theoretical results are promising, but we think that this paper has to be further polished before acceptance.

It is possible that we will increase the score if the authors can provide clarifications on the above questions.

Additional comments:

Introduction

-The vanishing gradient problem was not discovered in 1994, but in 1991 by Hochreiter: 

Sepp Hochreiter. Untersuchungen zu dynamischen neuronalen Netzen. Diploma thesis, TU Munich, 1991. Advisor J. Schmidhuber.

- Make clear that GRU is a variant of vanilla LSTM with forget gates (where one gate is missing):

Gers et al. “Learning to Forget: Continual Prediction with LSTM.“ Neural Computation, 12(10):2451-2471, 2000. 

- The intro says that GRU has become widely popular and cites Britz et al 2017, but Britz et al actually show that LSTM consistently outperforms its variant GRU in Neural Machine Translation. Please clarify this. 

- Also mention Weiss et al (“On the Practical Computational Power of Finite Precision RNNs for Language Recognition”) who exhibited basic limitations of GRU when compared to LSTM. 

- Is the result by Weiss et al actually related to the result of the authors who found that 2 GRUS cannot accurately a line attractor without near zero constant curvature in the phase space?


Section 2

-Wrong brackets in equation (4)
-Missing bracket before citing Laurent &amp; Brecht

Section 4

-”We conjecture that the system depicted in figure 2A..” Should be figure 3A
- Lemma1: UZ has capital Z subscript

Section 5.2

-”The added affine transformation allows for a sufficiently long subinterval”: “sufficiently long” is too vague

Section 5.3

“A manifold with without near zero constant curvature”: should be “a manifold without near zero constant curvature”

Appendix A

-Wrong brackets in equation (20)

Appendix B

- In the proof of Theorem 1, the derivative is of (29), not of (12)

Appendix C

Figure 9: “who’s initial conditions” should be “whose initial conditions”</span></div><div class="note_contents"><span class="note_content_field">Rating: </span><span class="note_content_value">4: Ok but not good enough - rejection</span></div><div class="note_contents"><span class="note_content_field">Confidence: </span><span class="note_content_value">4: The reviewer is confident but not absolutely certain that the evaluation is correct</span></div><div class="reply_row clearfix"></div></div><div class="children"></div></div><div class="note_with_children"><div id="note_H1l8eaK1hQ" class="note panel"><div class="title_pdf_row clearfix"><h2 class="note_content_title"><a>Review for the expressive power of GRUs as a continuous dynamical system.</a>  <button class="btn btn-xs btn-default permalink-button" title="Link to this comment" data-permalink-url="https://openreview.net/forum?id=H1eiZnAqKm&amp;noteId=H1l8eaK1hQ"><span class="glyphicon glyphicon-link" aria-hidden="true"></span></button></h2></div><div class="meta_row"><span class="signatures">ICLR 2019 Conference Paper1203 AnonReviewer2</span></div><div class="clearfix"><div class="meta_row pull-left"><span class="date item">26 Oct 2018 (modified: 07 Nov 2018)</span><span class="item">ICLR 2019 Conference Paper1203 Official Review</span><span class="item">Readers: <span class="readers-icon glyphicon glyphicon-globe"></span> Everyone</span></div><div class="meta_row meta_actions"></div></div><div class="note_contents"><span class="note_content_field">Review: </span><span class="note_content_value">Here the authors convert the GRU equations into continuous time and use theory and experiments to study 1- and 2-dimensional GRU networks. The authors showcase every variety of dynamical topology available in these systems and point out that the desirable line and ring attractors are not achievable, except in gross approximation.  The paper is extremely well written.

I am deeply conflicted about this paper.  Is the analysis of 1 or 2 dimensional GRUs interesting or significant? That’s a main question of this paper.  There is no question of quality, or clarity, and I am reasonably certain nobody has analyzed the GRU in this way before.

On the one hand, the authors bring a rigor and language to the discussion of recurrent networks that is both revealing (for these examples) and may to bear fruit in the future.  On the other hand, the paper is exclusively focused on 1- and 2-dimensional examples which have precisely no relevance to the recurrent neural networks as used and studied by machine learning practitioners and researchers, respectively. If the authors have proved something more general for higher dimensional (&gt;2) cases, they should make it as clear as possible.
 
A second, lesser question of relevance is studying a continuous time version.  It is my understanding that discrete time dynamics may exhibit significantly more complex dynamical phenomenon and again practitioners primarily deploy discrete time GRUs.  I understand that theoretical progress often requires retreating to lower dimensionality and (e.g. linearization, etc.) but in this case it is not clear to me that the end justifies the means.  On the other hand, a publication such as this will not only help to change the language of RNNs in the deep learning community, but also potentially bring in more dynamical systems specialists into the deep learning field, which I thoroughly endorse.

Moderate concern

“In order to show this major limitation of GRUs …” but then a 2-gru is used, which means that it’s not a general problem for GRUs with higher dim, right?  Also, won’t approximate slow points would also be fine here? I think this language needs to be more heavily qualified.

Minor

GRU almost always refers to the network, even though it is Gated Recurrrent Unit, this means that when you write ‘two GRUs’, the naive interpretation (to me) is that you are speaking about two networks and not a GRU network with two units.

Side note requiring no response: It might be interesting to study dynamical portrait as a function of training for the two-d GRU.
</span></div><div class="note_contents"><span class="note_content_field">Rating: </span><span class="note_content_value">5: Marginally below acceptance threshold</span></div><div class="note_contents"><span class="note_content_field">Confidence: </span><span class="note_content_value">4: The reviewer is confident but not absolutely certain that the evaluation is correct</span></div><div class="reply_row clearfix"></div></div><div class="children"></div></div></div></main></div>
  </div>
</div>


    <!-- Footer -->
    <footer class="sitemap">
      <div class="container">
        <div class="row hidden-xs">
          <div class="col-sm-4">
            <ul class="list-unstyled">
              <li><a href="/" class="home">Home</a></li>
              <li><a href="/venues">All Venues</a></li>
            </ul>
          </div>
    
          <div class="col-sm-4">
            <ul class="list-unstyled">
              <li><a href="/about">About OpenReview</a></li>
              <li><a href="/contact">Contact</a></li>
              <li><a href="#" data-toggle="modal" data-target="#feedback-modal">Feedback</a></li>
            </ul>
          </div>
    
          <div class="col-sm-4">
            <ul class="list-unstyled">
              <li><a href="/terms">Terms of Service</a></li>
              <li><a href="/privacy">Privacy Policy</a></li>
            </ul>
          </div>
        </div>
    
        <div class="row visible-xs-block">
          <div class="col-xs-6">
            <ul class="list-unstyled">
              <li><a href="/" class="home">Home</a></li>
              <li><a href="/about">About OpenReview</a></li>
              <li><a href="#" data-toggle="modal" data-target="#feedback-modal">Feedback</a></li>
            </ul>
          </div>
    
          <div class="col-xs-6">
            <ul class="list-unstyled">
              <li><a href="/contact">Contact</a></li>
              <li><a href="/terms">Terms of Service</a></li>
              <li><a href="/privacy">Privacy Policy</a></li>
            </ul>
          </div>
        </div>
      </div>
    </footer>
    
    <footer class="sponsor">
      <div class="container">
        <div class="row">
          <div class="col-sm-10 col-sm-offset-1">
            <p class="text-center">
              OpenReview is created by the <a href="http://www.iesl.cs.umass.edu/" target="_blank">Information Extraction and Synthesis Laboratory</a>, College of Information and Computer Science, University of Massachusetts Amherst. We gratefully acknowledge the support of the OpenReview sponsors:  Google,  Facebook, NSF, the University of Massachusetts Amherst Center for Data Science, and Center for Intelligent Information Retrieval, as well as the Google Cloud Platform for donating the computing and networking services on which OpenReview.net runs.
            </p>
          </div>
        </div>
      </div>
    </footer>

  <div id="feedback-modal" class="modal fade" tabindex="-1" role="dialog">
    <div class="modal-dialog">
      <div class="modal-content">
        <div class="modal-header">
          <button type="button" class="close" data-dismiss="modal" aria-label="Close"><span aria-hidden="true">×</span></button>
          <h3 class="modal-title">Send Feedback</h3>
        </div>
        <div class="modal-body">
          <p>Enter your feedback below and we'll get back to you as soon as possible.</p>
          <form action="/feedback" method="POST">
            <div class="form-group">
              <input type="email" name="from" class="form-control" placeholder="Email" />
            </div>
            <div class="form-group">
              <input type="text" name="subject" class="form-control" placeholder="Subject" />
            </div>
            <div class="form-group">
              <textarea name="message" class="form-control feedback-input" rows="5" placeholder="Message" required=""></textarea>
            </div>
          <ul id="ui-id-2" tabindex="0" class="ui-menu ui-widget ui-widget-content" style="display: none;"></ul></form>
        </div>
        <div class="modal-footer">
          <button type="button" class="btn btn-default" data-dismiss="modal">Cancel</button>
          <button type="button" class="btn btn-primary">Send</button>
        </div>
      </div>
    </div>
  </div>

  <script type="text/javascript" async="" src="https://www.google-analytics.com/analytics.js"></script><script src="//cdnjs.cloudflare.com/ajax/libs/jquery/2.2.4/jquery.min.js"></script>
  <script>window.jQuery || document.write('&lt;script src="/static/js/vendor/jquery-2.2.4.min.js"&gt;&lt;\/script&gt;')</script>

  <script src="/static/dist/vendor.min.js"></script>
  <script src="/static/dist/templates.min.js"></script>
  <script src="/static/dist/openreview.min.js"></script>

    <script>window.legacyScripts = true;</script>


    <script async="" src="https://www.googletagmanager.com/gtag/js?id=UA-108703919-1"></script>
    <script>
      window.dataLayer = window.dataLayer || [];
      function gtag() { dataLayer.push(arguments); }
      gtag('js', new Date());
      gtag('config', 'UA-108703919-1');
    </script>


<div role="status" aria-live="assertive" aria-relevant="additions" class="ui-helper-hidden-accessible"></div></body></html>