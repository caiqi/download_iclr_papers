<!DOCTYPE html><html xmlns="http://www.w3.org/1999/xhtml" class="no-js"><head>
  <meta charset="utf-8" />
  <meta http-equiv="X-UA-Compatible" content="IE=edge,chrome=1" />
  <meta name="viewport" content="width=device-width, initial-scale=1" />

  <title>Targeted Adversarial Examples for Black Box Audio Systems | OpenReview</title>
  <meta name="description" content="" />

      <meta name="citation_title" content="Targeted Adversarial Examples for Black Box Audio Systems" />
        <meta name="citation_author" content="Anonymous" />
      <meta name="citation_publication_date" content="2018/09/27" />
      <meta name="citation_online_date" content="2018/09/27" />
      <meta name="citation_pdf_url" content="https://openreview.net/pdf?id=HyGySsAct7" />
      <meta name="twitter:card" content="summary" />
      <meta name="twitter:site" content="@openreviewnet" />
      <meta name="og:title" content="Targeted Adversarial Examples for Black Box Audio Systems" />
      <meta name="og:description" content="The application of deep recurrent networks to audio transcription has led to impressive gains in automatic speech recognition (ASR) systems. Many have demonstrated that small adversarial..." />
      <meta name="og:image" content="https://openreview.net/static/images/openreview_logo_512.png" />

  <link rel="stylesheet" href="https://fonts.googleapis.com/css?family=Noto+Sans:400,400i,700,700i" />

  <link rel="stylesheet" href="/static/css/bootstrap.min.css" />
  <link rel="stylesheet" href="/static/css/jquery-ui.min.css" />
  <link rel="stylesheet" href="/static/css/main.min.css" />
</head>

<body class="forum">
  <nav class="navbar navbar-inverse navbar-fixed-top" role="navigation">
    <div class="container">
  
      <div class="navbar-header">
        <button type="button" class="navbar-toggle collapsed" data-toggle="collapse" data-target="#navbar" aria-expanded="false" aria-controls="navbar">
          <span class="sr-only">Toggle navigation</span>
          <span class="icon-bar"></span>
          <span class="icon-bar"></span>
          <span class="icon-bar"></span>
        </button>
        <a class="navbar-brand home push-link" href="/"><strong>OpenReview</strong>.net</a>
      </div>
  
      <div id="navbar" class="navbar-collapse collapse">
        <form class="navbar-form navbar-left profile-search" role="search">
          <div class="form-group has-feedback">
            <input id="search_input" type="text" class="form-control ui-autocomplete-input" placeholder="Search ICLR 2019 Conference" autocomplete="off" />
            <span class="glyphicon glyphicon-search form-control-feedback" aria-hidden="true"></span>
          </div>
  
          <input id="search_group" type="hidden" value="ICLR.cc/2019/Conference" />
          <input id="search_content" type="hidden" value="all" />
        <ul id="ui-id-1" tabindex="0" class="ui-menu ui-widget ui-widget-content ui-autocomplete ui-front" style="display: none;"></ul></form>
  
        <ul class="nav navbar-nav navbar-right">
        
            <li id="user-menu"><a href="/login">Login</a></li>
        </ul>
      </div>
  
    </div>
  </nav>

  <div id="or-banner" class="banner" style="">
  <div class="container">
    <div class="row">
      <div class="col-xs-12"><a href="/group?id=ICLR.cc/2019/Conference" title="Venue Homepage"><img class="icon" src="/static/images/arrow_left.svg" /> Go to <strong>ICLR 2019 Conference</strong> homepage</a></div>
    </div>
  </div>
</div>
<div id="flash-message-container" class="alert alert-danger" role="alert" style="display: none;">
  <div class="container">
    <div class="row">
      <div class="col-xs-12">
        <div class="alert-content">
          <button type="button" class="close" aria-label="Close"><span aria-hidden="true">×</span></button>
        </div>
      </div>
    </div>
  </div>
</div>

<div class="container">
  <div class="row">
    <div class="col-xs-12">
      <main id="content" class="clearfix openbanner-visible legacy-styles"><div id="note_HyGySsAct7" class="note panel"><div class="title_pdf_row clearfix"><h2 class="note_content_title"><a>Targeted Adversarial Examples for Black Box Audio Systems</a> <a class="note_content_pdf" href="/pdf?id=HyGySsAct7" title="Download PDF" target="_blank"><img src="/static/images/pdf_icon_blue.svg" /></a> </h2></div><div class="meta_row"><span class="signatures">Anonymous</span></div><div class="clearfix"><div class="meta_row pull-left"><span class="date item">28 Sep 2018 (modified: 11 Oct 2018)</span><span class="item">ICLR 2019 Conference Blind Submission</span><span class="item">Readers: <span class="readers-icon glyphicon glyphicon-globe"></span> Everyone</span><span class="item"><a class="action-bibtex-modal" data-bibtex="@inproceedings{    &#10;anonymous2019targeted,    &#10;title={Targeted Adversarial Examples for Black Box Audio Systems},    &#10;author={Anonymous},    &#10;booktitle={Submitted to International Conference on Learning Representations},    &#10;year={2019},    &#10;url={https://openreview.net/forum?id=HyGySsAct7},    &#10;note={under review}    &#10;}">Show Bibtex</a></span></div><div class="meta_row meta_actions"></div></div><div class="note_contents"><span class="note_content_field">Abstract: </span><span class="note_content_value">The application of deep recurrent networks to audio transcription has led to impressive gains in automatic speech recognition (ASR) systems. Many have demonstrated that small adversarial perturbations can fool deep neural networks into incorrectly predicting a specified target with high confidence. Current work on fooling ASR systems have focused on white-box attacks, in which the model architecture and parameters are known. In this paper, we adopt a black-box approach to adversarial generation, combining the approaches of both genetic algorithms and gradient estimation to solve the task. We achieve a 89.25% targeted attack similarity after 3000 generations while maintaining 94.6% audio file similarity.</span></div><div class="note_contents"><span class="note_content_field">Keywords: </span><span class="note_content_value">adversarial attack, adversarial examples, audio processing, speech to text, deep learning, adversarial audio, black box, machine learning</span></div><div class="note_contents"><span class="note_content_field">TL;DR: </span><span class="note_content_value">We present a novel black-box targeted attack on speech to text systems that supports arbitrarily long adversarial transcriptions and achieves state of the art performance.</span></div><div class="reply_row clearfix"><div class="item" id="reply_count">6 Replies</div></div></div><hr class="small" /><div id="note_children"><div class="note_with_children"><div id="note_r1g3c6VgpX" class="note panel"><div class="title_pdf_row clearfix"><h2 class="note_content_title"><a>Evaluation is weak</a>  <button class="btn btn-xs btn-default permalink-button" title="Link to this comment" data-permalink-url="https://openreview.net/forum?id=HyGySsAct7&amp;noteId=r1g3c6VgpX"><span class="glyphicon glyphicon-link" aria-hidden="true"></span></button></h2></div><div class="meta_row"><span class="signatures">ICLR 2019 Conference Paper52 AnonReviewer3</span></div><div class="clearfix"><div class="meta_row pull-left"><span class="date item">07 Nov 2018</span><span class="item">ICLR 2019 Conference Paper52 Official Review</span><span class="item">Readers: <span class="readers-icon glyphicon glyphicon-globe"></span> Everyone</span></div><div class="meta_row meta_actions"></div></div><div class="note_contents"><span class="note_content_field">Review: </span><span class="note_content_value">This paper proposes a black-box attack on multi-word ASR systems.  Most work on black-box attacks have focused on tasks in vision. This work adds to the literature on attac
ks on speech systems. The key novelties are the handling of a loss function over multiple decodings as well as the use of novel genetic algorithms to generate the adversari
al examples.

A weakness of this paper is that they do not compare to the closely related Alzantot et al. work. While the latter is focused on single word settings and is thus solving an
 easier problem, what would happen if the Alzantot et al. method was applied to each


While the idea is interesting but incremental, the evaluation of the approach is weak.

1. Insted of choosing random pairs of words as target phrases, it would be interesting to pick phrases that are likely to occur in English and to ask how success rate varie
s as a function of the initial phrase and target phrase.

2. To confirm that the resulting adversarial examples are similar to audio samples in the original dataset, the authors should do user studies. This is a key component in e
valuating the efficacy of such attacks. The cross correlation is useful but does not get at perceptual similarity.

3. Table 1 is not useful since either the datasets are different or information is not given on the specific white box attacks.

4. Does increasing the iterations lead to a higher success rate as claimed at end of page 7?


Abstract:
1. This sentence is misleading : "Current work..are known" given the Alzantot et al. work focuses on black-box attacks.
</span></div><div class="note_contents"><span class="note_content_field">Rating: </span><span class="note_content_value">4: Ok but not good enough - rejection</span></div><div class="note_contents"><span class="note_content_field">Confidence: </span><span class="note_content_value">3: The reviewer is fairly confident that the evaluation is correct</span></div><div class="reply_row clearfix"></div></div><div class="children"><div class="note_with_children"><div id="note_ryxlDFpLp7" class="note panel"><div class="title_pdf_row clearfix"><h2 class="note_content_title"><a>Author response to reviewer 3</a>  <button class="btn btn-xs btn-default permalink-button" title="Link to this comment" data-permalink-url="https://openreview.net/forum?id=HyGySsAct7&amp;noteId=ryxlDFpLp7"><span class="glyphicon glyphicon-link" aria-hidden="true"></span></button></h2></div><div class="meta_row"><span class="signatures">ICLR 2019 Conference Paper52 Authors</span></div><div class="clearfix"><div class="meta_row pull-left"><span class="date item">12 Nov 2018</span><span class="item">ICLR 2019 Conference Paper52 Official Comment</span><span class="item">Readers: <span class="readers-icon glyphicon glyphicon-globe"></span> Everyone</span></div><div class="meta_row meta_actions"></div></div><div class="note_contents"><span class="note_content_field">Comment: </span><span class="note_content_value">Dear reviewer,

Thank you for your detailed comments on our paper.

== Response to: "ask how success rate varies as a function of the initial phrase and target phrase"

Thank you for this insight; we agree this would be useful to see how the attack performs for phrases likely in the general English language.

== Response to: "the authors should do user studies"

We agree that user studies would most effectively verify the efficacy of the attack; however, this would incur significant costs for the authors. In lieu of a human study, the Carlini &amp; Wagner attack measured attack via cross-correlation, and so we use the measure for similarity.

== Response to: "Table 1 is not useful since either the datasets are different or information is not given on the specific white box attacks."

In Table 1, we attempt to provide a standardized comparison of the previous techniques; naturally, there will be gaps since both are different attack types and attack different models. Since our method is the first to extend black-box attacks to ASR systems, there are no direct previous baselines to compare with. For example, datasets are different since DeepSpeech can accept any input, whereas the classification model can only accept 1-word phrases from the predefined set of classes.

== Response to: "Does increasing the iterations lead to a higher success rate as claimed at end of page 7?"

Yes it does; we will change the wording to make it less ambiguous and make sure to add a couple extra figures in the final version as verification.</span></div><div class="reply_row clearfix"></div></div><div class="children"></div></div></div></div><div class="note_with_children"><div id="note_SkxXsTqT3m" class="note panel"><div class="title_pdf_row clearfix"><h2 class="note_content_title"><a>Targeted adversarial examples for black box audio systems</a>  <button class="btn btn-xs btn-default permalink-button" title="Link to this comment" data-permalink-url="https://openreview.net/forum?id=HyGySsAct7&amp;noteId=SkxXsTqT3m"><span class="glyphicon glyphicon-link" aria-hidden="true"></span></button></h2></div><div class="meta_row"><span class="signatures">ICLR 2019 Conference Paper52 AnonReviewer1</span></div><div class="clearfix"><div class="meta_row pull-left"><span class="date item">05 Nov 2018 (modified: 07 Nov 2018)</span><span class="item">ICLR 2019 Conference Paper52 Official Review</span><span class="item">Readers: <span class="readers-icon glyphicon glyphicon-globe"></span> Everyone</span></div><div class="meta_row meta_actions"></div></div><div class="note_contents"><span class="note_content_field">Review: </span><span class="note_content_value">In "Targeted adversarial examples for black box audio systems" the authors look at an adversarial problem in neural nets for audio processing. There is quite a lot of recent interest in adversarial problems in machine learning. That work is mostly on the image side, and so this work is very topical. The problem is to modify an audio signal without changing how it sounds to the human ear, so that it is interpreted as the attacker wishes by the neural network. In the black box approach, the weights of the neural network are not known by the attacker. The attacker however must be able to present modified audio and learn the network's interpretation as often as the attacker wants. This work is very exciting and topical, and of interest to the ICLR community.

The authors demonstrate a proof of concept using the recent DeepSpeech model, and they connect very well with recent literature on adversarial networks.

The particular algorithm the authors propose is based on genetic algorithms. I thought that this was a weak part of the paper, because genetic algorithms are quite ad hoc and have few theoretical guarantees when compared to SMC, MCMC, nested sampling or herding, which all do basically the same thing as genetic algorithms. This can lead to loose ends, such as the "momentum mutation" introduced by the authors in 2.2, wherein probability of mutation increases as the population fails to adapt. It is true that momentum mutation would avoid local maxima, but it would also take the solution away from global maxima through a sort of "sampling noise" (the global maxima is a point at which the population also "fails to adapt", as there's no more adaptation to be done). It's unclear if this is a problem, but things like annealed importance sampling also deal with the same problem (or effective sample size of SMC), and they have theory to back them up.
</span></div><div class="note_contents"><span class="note_content_field">Rating: </span><span class="note_content_value">6: Marginally above acceptance threshold</span></div><div class="note_contents"><span class="note_content_field">Confidence: </span><span class="note_content_value">4: The reviewer is confident but not absolutely certain that the evaluation is correct</span></div><div class="reply_row clearfix"></div></div><div class="children"><div class="note_with_children"><div id="note_Sklu5t3IaQ" class="note panel"><div class="title_pdf_row clearfix"><h2 class="note_content_title"><a>Author response to reviewer 1</a>  <button class="btn btn-xs btn-default permalink-button" title="Link to this comment" data-permalink-url="https://openreview.net/forum?id=HyGySsAct7&amp;noteId=Sklu5t3IaQ"><span class="glyphicon glyphicon-link" aria-hidden="true"></span></button></h2></div><div class="meta_row"><span class="signatures">ICLR 2019 Conference Paper52 Authors</span></div><div class="clearfix"><div class="meta_row pull-left"><span class="date item">12 Nov 2018</span><span class="item">ICLR 2019 Conference Paper52 Official Comment</span><span class="item">Readers: <span class="readers-icon glyphicon glyphicon-globe"></span> Everyone</span></div><div class="meta_row meta_actions"></div></div><div class="note_contents"><span class="note_content_field">Comment: </span><span class="note_content_value">Dear reviewer,

Thank you for your detailed comments on our paper and finding it of interest. 

The suggestions of various black-box algorithms to use are appreciated, and could promise to generate higher quality adversarial examples. Such extensions would definitely be welcome in future work, as in this paper we attempt to establish a baseline that future methods can compare to.</span></div><div class="reply_row clearfix"></div></div><div class="children"></div></div></div></div><div class="note_with_children"><div id="note_B1eLktzOnm" class="note panel"><div class="title_pdf_row clearfix"><h2 class="note_content_title"><a>The paper is not well-positioned against the existing literature on black-box attack. Its empirical evaluation is somewhat sloppy.</a>  <button class="btn btn-xs btn-default permalink-button" title="Link to this comment" data-permalink-url="https://openreview.net/forum?id=HyGySsAct7&amp;noteId=B1eLktzOnm"><span class="glyphicon glyphicon-link" aria-hidden="true"></span></button></h2></div><div class="meta_row"><span class="signatures">ICLR 2019 Conference Paper52 AnonReviewer2</span></div><div class="clearfix"><div class="meta_row pull-left"><span class="date item">01 Nov 2018 (modified: 07 Nov 2018)</span><span class="item">ICLR 2019 Conference Paper52 Official Review</span><span class="item">Readers: <span class="readers-icon glyphicon glyphicon-globe"></span> Everyone</span></div><div class="meta_row meta_actions"></div></div><div class="note_contents"><span class="note_content_field">Review: </span><span class="note_content_value">PAPER SUMMARY:

This paper introduces a biologically motivated black-box attack algorithm. 
The target model in this case is DNN applied to the ASR context (automatic speech recognition system). 

NOVELTY &amp; SIGNIFICANCE:

The proposed approach extends the previous genetic approach of (Alzantot et al., 2018) to attack a more complicated ASR system (that handles phrases and sentences). The new contribution here is an add-on momentum mutation component on top of the existing genetic programming architecture of (Alzantot et al., 2018) as illustrated in Figure 3.

This however appears very incremental seeing that integrating the mutation component into existing system is straight-forward and that mutation is not even a new concept -- it has always been a vital component in genetic programming paradigm.

It is also unclear how this mutation component improves over the existing work (more on this in the sections below).

Another issue is this work seems to ignore the recent literature on adversarial black-box attacks to DNN model. To list a few:

Chen, P.-Y.; Zhang, H.; Sharma, Y.; Yi, J.; and Hsieh, C.-J. 2017b.
ZOO: Zeroth-order optimization-based  black-box attacks to deepneural networks without training substitute models. 
In Proceedings of the 10th ACM Workshop on Artificial Intelligence and Security (15-26) ACM

Cheng,  M.;  Le,  T.;  Chen,  P.-Y.;  Yi,  J.;  Zhang,  H.;  and  Hsieh,C.-J.2018.
Query-efficient hard-label black-box attack:  An optimization-based approach. arXiv preprint arXiv:1807.04457

While these works have not been used to attacking ASR system, they should be directly applicable to such system since after all, they are black-box attacks. I think the proposed method needs to be compared with these works.

TECHNICAL SOUNDNESS:

I find it surprising that even though the proposed method is claimed to be a black-box attack but in the end, it actually exploits the fact that the target model uses CTC decoder. This pertains specifically to the target model's internal architecture and a black-box attack is not supposed to know this.

CLARITY:

The paper is clearly written.

EMPIRICAL RESULTS:

I do not understand this statement:

"That 35% of random attacks were successful in this respect highlights the fact that black box
adversarial attacks are definitely possible and highly effective at the same time"

Why does 35% successful attack rate is a positive result? The result tends to suggest that this is an attack with low success rate. 

The 2nd paragraph in 3.2 seems to give a vague explanation: "the vast majority of failure cases are only a few edit distances away from the target. 

This suggests that running the algorithm for a few more iterations could produce a higher success rate, although at the cost of correlation similarity".

Given the above statement, I do not see why the authors didn't actually "run the algorithm for a few more iterations" to verify it ...

I am also curious why is the attack success rate of the targeted attack success rate of the proposed method is significantly lower than that of the existing system -- I assume "single word black box" is the work of (Alzantot et al., 2018).

I find the empirical evaluation somewhat sloppy: why are the tested method not compared on the same benchmark? How do we interpret the results then?

REVIEW SUMMARY:

The paper misses the recent literature on black-box attack. The authors need to compare with those to demonstrate the efficiency of their proposed work. I also find the contribution of this paper too incremental &amp; its empirical evaluation appears somewhat sloppy and not convincing (see my specific comments above). </span></div><div class="note_contents"><span class="note_content_field">Rating: </span><span class="note_content_value">3: Clear rejection</span></div><div class="note_contents"><span class="note_content_field">Confidence: </span><span class="note_content_value">4: The reviewer is confident but not absolutely certain that the evaluation is correct</span></div><div class="reply_row clearfix"></div></div><div class="children"><div class="note_with_children"><div id="note_ryeSGI6L67" class="note panel"><div class="title_pdf_row clearfix"><h2 class="note_content_title"><a>Author response to reviewer 2</a>  <button class="btn btn-xs btn-default permalink-button" title="Link to this comment" data-permalink-url="https://openreview.net/forum?id=HyGySsAct7&amp;noteId=ryeSGI6L67"><span class="glyphicon glyphicon-link" aria-hidden="true"></span></button></h2></div><div class="meta_row"><span class="signatures">ICLR 2019 Conference Paper52 Authors</span></div><div class="clearfix"><div class="meta_row pull-left"><span class="date item">12 Nov 2018</span><span class="item">ICLR 2019 Conference Paper52 Official Comment</span><span class="item">Readers: <span class="readers-icon glyphicon glyphicon-globe"></span> Everyone</span></div><div class="meta_row meta_actions"></div></div><div class="note_contents"><span class="note_content_field">Comment: </span><span class="note_content_value">Dear reviewer,

Thank you for your detailed comments on our paper and finding it well written. 

== Response to: "Another issue is this work seems to ignore the recent literature on adversarial black-box attacks to DNN model"

Thank you for providing relevant literature. The first method provided, ZOO [1], is in fact closely related to finite gradient estimation, introduced in [2], which is the method our attack uses in phase 2. As for the second method provided, the paper introduces an attack for hard-label black box settings, where even output logits are not known, and where optimization is much more difficult [3]. In our setting, we assumed output logits are known, and so hard-label methods are not needed, as using output logits make optimization much easier. 

== Response to: "the proposed method is claimed to be a black-box attack but in the end, it actually exploits the fact that the target model uses CTC decoder"

In the black-box setting, all that is required is access to the output logits of the model (as specified in Section 1.3). Any loss function that uses the output logits and target phrase could be applied to our method; we chose CTC loss as it is a well-known loss function suited for this task. Thus, the fact that both the training of the victim model and our attack use CTC loss is mostly coincidence.

== Response to: "The result tends to suggest that this is an attack with low success rate"; "why is the attack success rate of the targeted attack success rate of the proposed method is significantly lower than that of the existing system"

As stated in Section 1.2, the difficulty of this task comes in attempting to apply black-box optimization to a deeply-layered, highly nonlinear decoder model that has the ability to decode phrases of arbitrary length. We would like to clarify that there has not been an existing black box system for targeting the DeepSpeech model; the black box method in [4] attacks a lightweight classification model, where the model uses a softmax loss to classify between 50 words. The DeepSpeech model is much more complex, namely in that it can decode phrases of arbitrary length, and each output state (50 states per second) of the recurrent structure has a softmax layer, whereas in the classification model there was only one softmax.

== Response to: "why are the tested method not compared on the same benchmark"

In Table 1, we attempt to provide a standardized comparison of the previous techniques; naturally, there will be gaps since both are different attack types and attack different models. Our method is the first to extend black-box attacks to ASR systems; thus, we are aiming to be a baseline on this task, and there are no direct previous baselines to compare with. For example, datasets are different since DeepSpeech can accept any input, whereas the classification model can only accept 1-word phrases.

References: 

Chen, P.-Y.; Zhang, H.; Sharma, Y.; Yi, J.; and Hsieh, C.-J. 2017b.
ZOO: Zeroth-order optimization-based  black-box attacks to deepneural networks without training substitute models. 
In Proceedings of the 10th ACM Workshop on Artificial Intelligence and Security (15-26) ACM

A. Nitin Bhagoji, W. He, B. Li, and D. Song. Exploring the Space of Black-box Attacks on Deep
Neural Networks. ArXiv e-prints, December 2017.

Cheng,  M.;  Le,  T.;  Chen,  P.-Y.;  Yi,  J.;  Zhang,  H.;  and  Hsieh,C.-J.2018.
Query-efficient hard-label black-box attack:  An optimization-based approach. arXiv preprint arXiv:1807.04457

M. Alzantot, B. Balaji, and M. Srivastava. Did you hear that? Adversarial Examples Against
Automatic Speech Recognition. ArXiv e-prints, January 2018.
</span></div><div class="reply_row clearfix"></div></div><div class="children"></div></div></div></div></div></main></div>
  </div>
</div>


    <!-- Footer -->
    <footer class="sitemap">
      <div class="container">
        <div class="row hidden-xs">
          <div class="col-sm-4">
            <ul class="list-unstyled">
              <li><a href="/" class="home">Home</a></li>
              <li><a href="/venues">All Venues</a></li>
            </ul>
          </div>
    
          <div class="col-sm-4">
            <ul class="list-unstyled">
              <li><a href="/about">About OpenReview</a></li>
              <li><a href="/contact">Contact</a></li>
              <li><a href="#" data-toggle="modal" data-target="#feedback-modal">Feedback</a></li>
            </ul>
          </div>
    
          <div class="col-sm-4">
            <ul class="list-unstyled">
              <li><a href="/terms">Terms of Service</a></li>
              <li><a href="/privacy">Privacy Policy</a></li>
            </ul>
          </div>
        </div>
    
        <div class="row visible-xs-block">
          <div class="col-xs-6">
            <ul class="list-unstyled">
              <li><a href="/" class="home">Home</a></li>
              <li><a href="/about">About OpenReview</a></li>
              <li><a href="#" data-toggle="modal" data-target="#feedback-modal">Feedback</a></li>
            </ul>
          </div>
    
          <div class="col-xs-6">
            <ul class="list-unstyled">
              <li><a href="/contact">Contact</a></li>
              <li><a href="/terms">Terms of Service</a></li>
              <li><a href="/privacy">Privacy Policy</a></li>
            </ul>
          </div>
        </div>
      </div>
    </footer>
    
    <footer class="sponsor">
      <div class="container">
        <div class="row">
          <div class="col-sm-10 col-sm-offset-1">
            <p class="text-center">
              OpenReview is created by the <a href="http://www.iesl.cs.umass.edu/" target="_blank">Information Extraction and Synthesis Laboratory</a>, College of Information and Computer Science, University of Massachusetts Amherst. We gratefully acknowledge the support of the OpenReview sponsors:  Google,  Facebook, NSF, the University of Massachusetts Amherst Center for Data Science, and Center for Intelligent Information Retrieval, as well as the Google Cloud Platform for donating the computing and networking services on which OpenReview.net runs.
            </p>
          </div>
        </div>
      </div>
    </footer>

  <div id="feedback-modal" class="modal fade" tabindex="-1" role="dialog">
    <div class="modal-dialog">
      <div class="modal-content">
        <div class="modal-header">
          <button type="button" class="close" data-dismiss="modal" aria-label="Close"><span aria-hidden="true">×</span></button>
          <h3 class="modal-title">Send Feedback</h3>
        </div>
        <div class="modal-body">
          <p>Enter your feedback below and we'll get back to you as soon as possible.</p>
          <form action="/feedback" method="POST">
            <div class="form-group">
              <input type="email" name="from" class="form-control" placeholder="Email" />
            </div>
            <div class="form-group">
              <input type="text" name="subject" class="form-control" placeholder="Subject" />
            </div>
            <div class="form-group">
              <textarea name="message" class="form-control feedback-input" rows="5" placeholder="Message" required=""></textarea>
            </div>
          <ul id="ui-id-2" tabindex="0" class="ui-menu ui-widget ui-widget-content" style="display: none;"></ul></form>
        </div>
        <div class="modal-footer">
          <button type="button" class="btn btn-default" data-dismiss="modal">Cancel</button>
          <button type="button" class="btn btn-primary">Send</button>
        </div>
      </div>
    </div>
  </div>

  <script type="text/javascript" async="" src="https://www.google-analytics.com/analytics.js"></script><script src="//cdnjs.cloudflare.com/ajax/libs/jquery/2.2.4/jquery.min.js"></script>
  <script>window.jQuery || document.write('&lt;script src="/static/js/vendor/jquery-2.2.4.min.js"&gt;&lt;\/script&gt;')</script>

  <script src="/static/dist/vendor.min.js"></script>
  <script src="/static/dist/templates.min.js"></script>
  <script src="/static/dist/openreview.min.js"></script>

    <script>window.legacyScripts = true;</script>


    <script async="" src="https://www.googletagmanager.com/gtag/js?id=UA-108703919-1"></script>
    <script>
      window.dataLayer = window.dataLayer || [];
      function gtag() { dataLayer.push(arguments); }
      gtag('js', new Date());
      gtag('config', 'UA-108703919-1');
    </script>


<div role="status" aria-live="assertive" aria-relevant="additions" class="ui-helper-hidden-accessible"></div></body></html>