<!DOCTYPE html><html xmlns="http://www.w3.org/1999/xhtml" class="no-js"><head>
  <meta charset="utf-8" />
  <meta http-equiv="X-UA-Compatible" content="IE=edge,chrome=1" />
  <meta name="viewport" content="width=device-width, initial-scale=1" />

  <title>Adaptive Convolutional ReLUs | OpenReview</title>
  <meta name="description" content="" />

      <meta name="citation_title" content="Adaptive Convolutional ReLUs" />
        <meta name="citation_author" content="Anonymous" />
      <meta name="citation_publication_date" content="2018/09/27" />
      <meta name="citation_online_date" content="2018/09/27" />
      <meta name="citation_pdf_url" content="https://openreview.net/pdf?id=SkgD4jAcYX" />
      <meta name="twitter:card" content="summary" />
      <meta name="twitter:site" content="@openreviewnet" />
      <meta name="og:title" content="Adaptive Convolutional ReLUs" />
      <meta name="og:description" content="Rectified linear units (ReLUs) are currently the most popular activation function used in neural networks. Although ReLUs can solve the gradient vanishing problem and accelerate training..." />
      <meta name="og:image" content="https://openreview.net/static/images/openreview_logo_512.png" />

  <link rel="stylesheet" href="https://fonts.googleapis.com/css?family=Noto+Sans:400,400i,700,700i" />

  <link rel="stylesheet" href="/static/css/bootstrap.min.css" />
  <link rel="stylesheet" href="/static/css/jquery-ui.min.css" />
  <link rel="stylesheet" href="/static/css/main.min.css" />
</head>

<body class="forum">
  <nav class="navbar navbar-inverse navbar-fixed-top" role="navigation">
    <div class="container">
  
      <div class="navbar-header">
        <button type="button" class="navbar-toggle collapsed" data-toggle="collapse" data-target="#navbar" aria-expanded="false" aria-controls="navbar">
          <span class="sr-only">Toggle navigation</span>
          <span class="icon-bar"></span>
          <span class="icon-bar"></span>
          <span class="icon-bar"></span>
        </button>
        <a class="navbar-brand home push-link" href="/"><strong>OpenReview</strong>.net</a>
      </div>
  
      <div id="navbar" class="navbar-collapse collapse">
        <form class="navbar-form navbar-left profile-search" role="search">
          <div class="form-group has-feedback">
            <input id="search_input" type="text" class="form-control ui-autocomplete-input" placeholder="Search ICLR 2019 Conference" autocomplete="off" />
            <span class="glyphicon glyphicon-search form-control-feedback" aria-hidden="true"></span>
          </div>
  
          <input id="search_group" type="hidden" value="ICLR.cc/2019/Conference" />
          <input id="search_content" type="hidden" value="all" />
        <ul id="ui-id-1" tabindex="0" class="ui-menu ui-widget ui-widget-content ui-autocomplete ui-front" style="display: none;"></ul></form>
  
        <ul class="nav navbar-nav navbar-right">
        
            <li id="user-menu"><a href="/login">Login</a></li>
        </ul>
      </div>
  
    </div>
  </nav>

  <div id="or-banner" class="banner" style="">
  <div class="container">
    <div class="row">
      <div class="col-xs-12"><a href="/group?id=ICLR.cc/2019/Conference" title="Venue Homepage"><img class="icon" src="/static/images/arrow_left.svg" /> Go to <strong>ICLR 2019 Conference</strong> homepage</a></div>
    </div>
  </div>
</div>
<div id="flash-message-container" class="alert alert-danger" role="alert" style="display: none;">
  <div class="container">
    <div class="row">
      <div class="col-xs-12">
        <div class="alert-content">
          <button type="button" class="close" aria-label="Close"><span aria-hidden="true">Ã—</span></button>
        </div>
      </div>
    </div>
  </div>
</div>

<div class="container">
  <div class="row">
    <div class="col-xs-12">
      <main id="content" class="clearfix openbanner-visible legacy-styles"><div id="note_SkgD4jAcYX" class="note panel"><div class="title_pdf_row clearfix"><h2 class="note_content_title"><a>Adaptive Convolutional ReLUs</a> <a class="note_content_pdf" href="/pdf?id=SkgD4jAcYX" title="Download PDF" target="_blank"><img src="/static/images/pdf_icon_blue.svg" /></a> </h2></div><div class="meta_row"><span class="signatures">Anonymous</span></div><div class="clearfix"><div class="meta_row pull-left"><span class="date item">28 Sep 2018 (modified: 11 Oct 2018)</span><span class="item">ICLR 2019 Conference Blind Submission</span><span class="item">Readers: <span class="readers-icon glyphicon glyphicon-globe"></span> Everyone</span><span class="item"><a class="action-bibtex-modal" data-bibtex="@inproceedings{    &#10;anonymous2019adaptive,    &#10;title={Adaptive Convolutional ReLUs},    &#10;author={Anonymous},    &#10;booktitle={Submitted to International Conference on Learning Representations},    &#10;year={2019},    &#10;url={https://openreview.net/forum?id=SkgD4jAcYX},    &#10;note={under review}    &#10;}">Show Bibtex</a></span></div><div class="meta_row meta_actions"></div></div><div class="note_contents"><span class="note_content_field">Abstract: </span><span class="note_content_value">Rectified linear units (ReLUs) are currently the most popular activation function used in neural networks. Although ReLUs can solve the gradient vanishing problem and accelerate training convergence, it suffers from the dying ReLU problem in which some neurons are never activated if the weights are not updated properly. In this work, we propose a novel activation function, known as the adaptive convolutional ReLU (ConvReLU), that can better mimic brain neuron activation behaviors and overcome the dying ReLU problem. With our novel parameter sharing scheme, ConvReLUs can be applied to convolution layers that allow each input neuron to be activated by different trainable thresholds without involving a large number of extra parameters. We employ the zero initialization scheme in ConvReLU to encourage trainable thresholds to be close to zero. Finally, we develop a partial replacement strategy that only replaces the ReLUs in the early layers of the network. This resolves the dying ReLU problem and retains sparse representations for linear classifiers. Experimental results demonstrate that our proposed ConvReLU has consistently better performance compared to ReLU, LeakyReLU, and PReLU. In addition, the partial replacement strategy is shown to be effective not only for our ConvReLU but also for LeakyReLU and PReLU.</span></div><div class="note_contents"><span class="note_content_field">Keywords: </span><span class="note_content_value">adaptive, convolutional, ReLUs</span></div><div class="note_contents"><span class="note_content_field">TL;DR: </span><span class="note_content_value">we propose a novel activation function, ConvReLU, that can better mimic brain neuron activation behaviors and overcome the dying ReLU problem.</span></div><div class="reply_row clearfix"><div class="item" id="reply_count">12 Replies</div></div></div><hr class="small" /><div id="note_children"><div class="note_with_children"><div id="note_rJgltbUanQ" class="note panel"><div class="title_pdf_row clearfix"><h2 class="note_content_title"><a>Simple idea, overall well-structured paper, but fairly incremental and unconvincing improvement over alternatives given the extra parameters, little to no exploration of learned thresholds, writing needs work. </a>  <button class="btn btn-xs btn-default permalink-button" title="Link to this comment" data-permalink-url="https://openreview.net/forum?id=SkgD4jAcYX&amp;noteId=rJgltbUanQ"><span class="glyphicon glyphicon-link" aria-hidden="true"></span></button></h2></div><div class="meta_row"><span class="signatures">ICLR 2019 Conference Paper7 AnonReviewer3</span></div><div class="clearfix"><div class="meta_row pull-left"><span class="date item">05 Nov 2018 (modified: 07 Nov 2018)</span><span class="item">ICLR 2019 Conference Paper7 Official Review</span><span class="item">Readers: <span class="readers-icon glyphicon glyphicon-globe"></span> Everyone</span></div><div class="meta_row meta_actions"></div></div><div class="note_contents"><span class="note_content_field">Review: </span><span class="note_content_value">
The most important criticism I have is about the claim that the marginal increase in parameters is negligible. I think it's _probably_ true, but given that the increases in performance are quite small across the board (&lt;0.5 gain in accuracy), and that all of the learned thresholds are so close to 0 (i.e. not much different from pure relu), I think it would have been important and pretty easy just to remove a few thousand parameters from the convrelu architecture to equalize the number of parameters in each network. I think would need to see accuracies for at least one experiment like this to increase my rating, or to be somehow otherwise convinced that the number of parameters does not make the difference. 

The background and related work section covers most of the recent ReLU alternatives, but doesn't not clearly explain what an activation function is, and misses some references. I find the way the biological motivation is presented to be a bit misleading/overstated. It's fine and interesting to say that the idea was inspired by something about biological neurons, but I think it's a stretch to claim to "mimic brain function". The related work section is pretty good, but reads a bit like a list of papers - it would be nice to have a bit more discussion of how related methods are similar/different to the proposed method.

The experimental sections, tables, and plot of learned threshold values are good, but impression from the swish paper and others was that ReLU variants do not perform much differently on smaller datasets, and only make a difference in very deep networks. If my impression is correct, it would be important to do an experiment with a deep resnet or something like that, but I'm not very sure about this impression.
The exploration of the partial replacement strategy is interesting and worthwhile, and again might be even more interesting in much deeper networks.

Most hyperparameter settings are mentioned, but what about the learning rate? Was this tuned for each experiment? I seem to remember there being a paper that showed learning rate made a pretty big difference for different activation functions' performance.
The tinyimagenet numbers seem a little off; I don't have personal recent experience with this dataset but this blog post claims 56.4 with normal ReLUs in a VGGish architecture. <a href="https://learningai.io/projects/2017/06/29/tiny-imagenet.html" target="_blank" rel="nofollow">https://learningai.io/projects/2017/06/29/tiny-imagenet.html</a>

Some other questions:
 - How similar are the distributions of learned thresholds across datasets? 
 - How much do the thresholds change over the course of training (do they quickly reach the values shown in the histogram and stay that way, or are they very noisy?
 - An interesting experiment could be to use fixed thresholds according to the distribution shown in Fig. 6. Especially if the distributions are similar across datasets, it may be that this performs as well as learning the thresholds, without introducing extra parameters to learn. This would also help disentangle the effect of having different thresholds vs. the possible implicit regularization from the noise of the thresholds changing.

Other refs to mention:
Randomized relu (randomly set the leak threshold) https://arxiv.org/pdf/1505.00853.pdf
Noisy activation functions

Quality: Decent (6.5/10), but I think the experiments, level of analysis, and quality of writing are more like a very good blog post than an academic paper (although I think that that line is becoming very blurry)

Clarity: Decent (7/10), but there are many small grammar errors, especially to do with pluralization (e.g. "activation functions .... its importance" should be "their importance") and  incorrect verb tense (e.g. "was used" should be "is used" or "has been used"; past tense implies it no longer is used). I pointed out the ones that were particularly confusing in the "specific comments" section, but the paper should be thoroughly reviewed for writing quality. 
The idea is simple and its implementation and the experiments are well explained. 

Originality: (6.5/10) As far as I know the idea of parameterized thresholds for relus is original, as is the partial replacement strategy, but I am not an expert in this area. There is not much/any originality or creativity in the analysis of the results or probing of why variable thresholds are better; I think this could make the paper much stronger.

Significance: (5.5/10) For anyone looking to squeeze a little extra performance out of a model for some reason, I think even the &lt;0.5 improvement is useful to know about. This seems more like an engineering problem than a research problem though, and I think that the paper is lacking in significant research insight / exploration. The gains might be more significant in deeper networks, but this is difficult to assess without experiments.

Pros: 
 - marginal improvement across the board
 - partial replacement strategy might be useful for other methods

Cons:
 - Writing contains grammatical errors / change of tense / confusing sentence construction which make the paper somewhat hard to get through
 - Not much exploration or insight about the learned functions or the activation function performs well
 - Numbers seem quite incremental to me, and the method is not tested on larger/deeper networks where the gains might be mode significant.


Specific comments/nits (in order reading through paper):
1. "ReLUs can solve the gradient vanishing and accelerate training convergence" &lt;- cite this claim
2. it suffers -&gt; they suffer
3. "suffer from the dying relu problem" &lt;- cite this claim
4. "For the parameters involved in ..." this sentence makes it sound like you don't tryi anything other than L2, and it does not explain why you want the trainable thresholds close to zero.
5. "we propose a ...method known as adaptive ReLUs" -&gt; "method we call adaptive ReLUs" or something like that (it can't be known as it yet; you've just proposed it).
6. "due to its importance in deep neural networks" this is not an explanation of _why_ activations are a popular research field; it just raises the further question of why they are important in DNNs.
7. tanh: describe why the zerocentering property is good if you want to mention it being preferred for that reason
8. mention that motivation for elu etc. is to be soft/differentiable
9. "the use in convolution is not clear"  what do you mean by this? that it hasn't worked well in practice or that they don't work by construction for some reason... explain
10. Not clear what the swish function is or how it is related to ELUs and SELUs.
11. "well known that neural networks mimic computational activities in the brain" I would not say this is well known. It's a contentious claim; "inspired by" or "modeled after" would be more accurate in my opinion.
12. "which is also called" this is confusing/misleadingly worded; the process of the brain inputting and outputting signals is NOT "also called feedforward network with relus"
13. "which can better mimic brain functions" most would consider neuron firing is a process that occurs in the brain, not a "brain function" which would be something like "seeing"
14. "share the same outgoing weight" this is confusing; makes it sound like you dynamically check the value of the weight and group units based on this value.
15. "Glot initialization" -&gt; "Glorot initialization"
16. "zero init was used" -&gt; "has been used" 
17. Cite densenet in Fig.4 caption
18. "with minor adjustments to accomodate diferent datasets" "standard data augmentation" explain what these are (in appendix if necessary)
19. "VGG like network" cite VGG and explain what is different about yours</span></div><div class="note_contents"><span class="note_content_field">Rating: </span><span class="note_content_value">5: Marginally below acceptance threshold</span></div><div class="note_contents"><span class="note_content_field">Confidence: </span><span class="note_content_value">4: The reviewer is confident but not absolutely certain that the evaluation is correct</span></div><div class="reply_row clearfix"></div></div><div class="children"></div></div><div class="note_with_children"><div id="note_S1xrIzyK2X" class="note panel"><div class="title_pdf_row clearfix"><h2 class="note_content_title"><a>Incremental idea; problems in defining the layer; lack of analysis; weak baselines and marginal improvements</a>  <button class="btn btn-xs btn-default permalink-button" title="Link to this comment" data-permalink-url="https://openreview.net/forum?id=SkgD4jAcYX&amp;noteId=S1xrIzyK2X"><span class="glyphicon glyphicon-link" aria-hidden="true"></span></button></h2></div><div class="meta_row"><span class="signatures">ICLR 2019 Conference Paper7 AnonReviewer2</span></div><div class="clearfix"><div class="meta_row pull-left"><span class="date item">02 Nov 2018 (modified: 07 Nov 2018)</span><span class="item">ICLR 2019 Conference Paper7 Official Review</span><span class="item">Readers: <span class="readers-icon glyphicon glyphicon-globe"></span> Everyone</span></div><div class="meta_row meta_actions"></div></div><div class="note_contents"><span class="note_content_field">Review: </span><span class="note_content_value">This paper presents adaptive ReLUs and a convolutional setup termed ConvReLU, which aims to alleviate the dying Relu problem.  To reduce the number of newly introduced parameters by their activation function and avoid overfitting, ConvReLU utilizes a parameter sharing scheme.  Three image classification and three language classification datasets are used to validate the proposed method. 

In general, this paper is well-written and easy to follow. However,  the presented idea of adaptive ReLUs is somewhat trivial.  
One main concern is that the paper lacks some theoretical analysis and there is no any experiment analysis to backup authors' claim on that the proposed activation is better in preventing the problem of dying of ReLu.   At least, authors need to qualitative demonstrate the idea by visualizing the learned filters among different activations.  Deeper discussions on why the proposed activation function is better than the existing ones are needed. 

In addition,  the ConvReLU has some flaws.  The way that authors define a "layer" is strange.  Conventionally, the last component of a Conv layer is the activation/nonlinearity.   However, the first component of a ConvReLU is the AdaReLU function.   At first glance, it seems two setups are equivalent.  However, due to the parameter sharing scheme, I don't know how authors define the values of post-activation (the output after nonlinearity).   Authors also need to provide details on how the ConvReLU is implemented.

The experiment results are not convincing.  Although authors compare their performances to different activation on six datasets, the improvement is very marginal.  Since the proposed ConvReLU introduces new parameters, it is hard to tell, the improved performances come from the method itself or the increasing of the number of parameters.  Moreover, authors also need to report the performance average over multiple runs and the standard division.  This will help to make sure the improvement does not come from the different initialization.   

For the text classification task, authors need to justify the reason for choosing the VGG network structure?   The reported performance is even worse than state-of-the-arts three years ago.  For instead, please see the paper "Convolutional Neural Networks for Sentence Classification" by Yoon Kim from ACL 2014.  The performance of a non-static CNN achieves 81.5 while the best-reported performance here is 80.39.   Similarly, please see "Character-level Convolutional Networks for Text Classification" for AG and Yelp.   

Furthermore, the setting of the partial replacement strategy is also strange.   I am not persuaded that by replacing the activation of the first dense layer only can lead to a consistent performance gain.    Authors also need to report the speed of training of their method compared regular ReLU.  
</span></div><div class="note_contents"><span class="note_content_field">Rating: </span><span class="note_content_value">4: Ok but not good enough - rejection</span></div><div class="note_contents"><span class="note_content_field">Confidence: </span><span class="note_content_value">5: The reviewer is absolutely certain that the evaluation is correct and very familiar with the relevant literature</span></div><div class="reply_row clearfix"></div></div><div class="children"></div></div><div class="note_with_children"><div id="note_rJeH_jcrsX" class="note panel"><div class="title_pdf_row clearfix"><h2 class="note_content_title"><a>Simple, intuitive idea. There are some issues.</a>  <button class="btn btn-xs btn-default permalink-button" title="Link to this comment" data-permalink-url="https://openreview.net/forum?id=SkgD4jAcYX&amp;noteId=rJeH_jcrsX"><span class="glyphicon glyphicon-link" aria-hidden="true"></span></button></h2></div><div class="meta_row"><span class="signatures">ICLR 2019 Conference Paper7 AnonReviewer1</span></div><div class="clearfix"><div class="meta_row pull-left"><span class="date item">18 Oct 2018 (modified: 07 Nov 2018)</span><span class="item">ICLR 2019 Conference Paper7 Official Review</span><span class="item">Readers: <span class="readers-icon glyphicon glyphicon-globe"></span> Everyone</span></div><div class="meta_row meta_actions"></div></div><div class="note_contents"><span class="note_content_field">Review: </span><span class="note_content_value">The authors present a parametrized version of ReLU. It is similar to PReLU except the trainable parameter causes the flat region of ReLU to shift up and down instead of causing it to change its slope. Their method outperforms PReLU and leaky ReLU in their experiments.

Since leaky ReLU / PReLU is seeing some practical use, I believe any improvement upon these methods is welcome. However, there are a significant number of issues regarding this paper. I don't view any of them as major. Nonetheless, they add up.

The experiments should be made stronger in a few ways:
- include at least a few large-scale experiments (ImageNet or larger). Common wisdom suggests that methods that improve performance on CIFAR-sized datasets don't necessarily improve performance on larger datasets 
- your method ConvReLU is composed of three building blocks: (1) the novel parametrization of ReLU (AdaReLU), (2) the parameter sharing scheme, (3) L2 regularization on the trainable parameters. You should run experiments that isolate the contribution of each building block to the performance gains you observed. I.e. you should run AdaReLU with a simpler parameter sharing scheme and ConvReLU without L2 regularization, for example. 
- specifically, I think you should run experiments withe AdaReLU where you only have a single threshold parameter per channel, i.e. x^l_{i,j,d} = \sum_{c=0}^{n_{out}-1} \sum_{a=0}^{k-1} \sum_{b=0}^{k-1} w_{a,b,c,d} \max(x^{l-1}_{i+a,j+b,c}, \theta_c) to use your notation. I would consider this the "default" parameter sharing scheme because this is what is used for the trainable parameters of PReLU, for the trainable biases of convolutional layers, as well as the trainable scaling and bias parameters of batch normalization, layer normalization etc. An important difference between having only a single parameter per channel and using your scheme is that your computation can no longer be efficiently divided into two steps. I.e. if we use a single parameter per channel we can divide the computation x^l_{i,j,d} = \sum_{c=0}^{n_{out}-1} \sum_{a=0}^{k-1} \sum_{b=0}^{k-1} w_{a,b,c,d} \max(x^{l-1}_{i+a,j+b,c}, \theta_c) into two steps: x^l_{i,j,d} = \sum_{c=0}^{n_{out}-1} \sum_{a=0}^{k-1} \sum_{b=0}^{k-1} w_{a,b,c,d} z^{l-1}_{i+a,j+b,c} and z^{l-1}_{i,j,c} = max(z^{l-1}_{i,j,c}, \theta_c). The first step is the "ReLU step" and the second is the "convolutional step". However, the same decomposition is not possible with your parameter sharing scheme without wasting memory. Hence, your scheme is somewhat more complicated than having one threshold per channel. And this additional complexity has to be justified explicitly with experimental gains.
- Looking at table 6, I find that PreLU only has 12 more parameters than ReLU. This looks like PReLU's trainable parameter was shared between every single unit in each layer. Firstly, this is unrealistic as PReLU would ordinarily have a single trainable parameter per channel, not per layer. Second, this makes the comparison to ConvReLU unfair. You should compare AdaReLU and PReLU with the same parameter sharing scheme. Once you have shown that ConvReLU outperforms PReLU in that scenario, then you can compare various parameter sharing schemes. Also allowing ConvReLU the benefit of L2 regularization but not PReLU further waters down the validity of the comparison.
- compare ConvReLU also against ELU/SELU/Swish. The authors claim that "ELU/SELU are only for fully-connected layers", but I don't see a reason that prevents them from being applied to convolutional layers. From the anonymous comment below, it appears SELU has actually been used for convolutional layers with some success. 

Please provide more details on how the hyperparameters are tuned, including the strength of L2 regularization for the thresholds. Are the same hyperparameters used for all methods / architectures? The reviewers should be able to verify that ConvReLU was not tuned excessively relative to the baseline.

Please provide some practical guidance as to which layers ConvReLU should be applied to in an architecture other than the architectures you used. It appears in your experiments that you apply ConvReLU to roughly the bottom third of layers. You should formulate a clear hypothesis regarding which layers ConvReLU should be applied to in an arbitrary network and then test that hypothesis explicitly.

I find figures 2 and 3 unclear and needlessly complicated. What is s/x/y/m/n? Why does the depicted network get narrower with depth? Without the formula you provided in the comment below, I would not be 100% sure how your method works. I would simply replace both figures with the formula you provided below.

I am not entirely clear on what you mean by the phrases "input neuron/unit" and "output neuron/unit". "input neuron" generally refers to neurons in the input layer to which the data is clamped. "output neuron" generally refers to neurons of the output layer which are fed into the loss function. I would suggest using different terms for the concepts you are describing with those phrases, and be more clear on what those concepts are.

I don't think you show that the observed gains of your method are due to solving the "dying ReLU problem". While technically any ReLU where the threshold value is not exactly 0 cannot be "dead", in figure 6 you show that most of your thresholds are in fact very close to 0. Further, you do not show that the "aliveness" of the ReLU units is what causes the performance gain. One could just fix all thresholds at -0.01 to avoid deadness, yet it's not clear whether one would still observe the performance gains you observed. Also, you fail to mention that in practice, the majority of the time, dying ReLU is solved by employing batch normalization, as is done e.g. in the DenseNet architecture you use.

Regarding threshold initialization: First, it's called "Glorot initialization", after the author of the 2010 paper, not "Glot initialization". Second, I'm not sure why you would consider Glorot initialization at all. The goal of Glorot initialization is to stabilize the overall magnitude of neuron activations in the forward pass and backward pass as much as possible. However, Glorot initialization is specifically designed for linear operations (fully-connected / convolutional) based on how those operations impact the magnitude of neuron activations. If you wanted to choose the initial thresholds in order to achieve the same stability, you would have to analyze what impact the threshold values have on the magnitude of neuron activations and then choose the initialization variance accordingly. Simply copying the Glorot variances makes no sense.

What variance do the thresholds have under your "Random initialization"?

"Despite the possibility of resulting in dying neurons in the network, ReLUs have the advantage of inducing higher sparsity for the computed features as compared to other activation functions. This may consequently reduce the risk of over-fitting (Glorot et al., 2011). Although these activation functions can overcome the dying ReLU problem, the sparsity of the outputs is significantly reduced, especially for the final linear classifier layer. This may increase the risk of over-fitting." I have never come across this over-fitting hypothesis and I don't see it in the Glorot paper. Feel free to let me know which papers talk about this over-fitting hypothesis in your rebuttal.

"We believe this marginal increase in parameter number is negligible and will not cause the overfitting problem." Total parameter number is not generally known to be related to overfitting in deep networks.</span></div><div class="note_contents"><span class="note_content_field">Rating: </span><span class="note_content_value">4: Ok but not good enough - rejection</span></div><div class="note_contents"><span class="note_content_field">Confidence: </span><span class="note_content_value">4: The reviewer is confident but not absolutely certain that the evaluation is correct</span></div><div class="reply_row clearfix"></div></div><div class="children"></div></div><div class="note_with_children"><div id="note_SJxkEEi7jm" class="note panel"><div class="title_pdf_row clearfix"><h2 class="note_content_title"><a>I don't understand figure 3</a>  <button class="btn btn-xs btn-default permalink-button" title="Link to this comment" data-permalink-url="https://openreview.net/forum?id=SkgD4jAcYX&amp;noteId=SJxkEEi7jm"><span class="glyphicon glyphicon-link" aria-hidden="true"></span></button></h2></div><div class="meta_row"><span class="signatures">(anonymous)</span></div><div class="clearfix"><div class="meta_row pull-left"><span class="date item">17 Oct 2018</span><span class="item">ICLR 2019 Conference Paper7 Public Comment</span><span class="item">Readers: <span class="readers-icon glyphicon glyphicon-globe"></span> Everyone</span></div><div class="meta_row meta_actions"></div></div><div class="note_contents"><span class="note_content_field">Comment: </span><span class="note_content_value">Could you provide me with an exact formula of how a relu-conv block computes its output from its input under ConvReLU?

Thanks</span></div><div class="reply_row clearfix"></div></div><div class="children"><div class="note_with_children"><div id="note_HkePaKEUsm" class="note panel"><div class="title_pdf_row clearfix"><h2 class="note_content_title"><a>ConvRelu formula</a>  <button class="btn btn-xs btn-default permalink-button" title="Link to this comment" data-permalink-url="https://openreview.net/forum?id=SkgD4jAcYX&amp;noteId=HkePaKEUsm"><span class="glyphicon glyphicon-link" aria-hidden="true"></span></button></h2></div><div class="meta_row"><span class="signatures">ICLR 2019 Conference Paper7 Authors</span></div><div class="clearfix"><div class="meta_row pull-left"><span class="date item">19 Oct 2018</span><span class="item">ICLR 2019 Conference Paper7 Official Comment</span><span class="item">Readers: <span class="readers-icon glyphicon glyphicon-globe"></span> Everyone</span></div><div class="meta_row meta_actions"></div></div><div class="note_contents"><span class="note_content_field">Comment: </span><span class="note_content_value">The unit $x^{l}_{i, j}$ in output feature map is calculated as:

\begin{equation}\label{eq:conv_relu}
x^l_{i, j} = \sum_{a=0}^{k-1} \sum_{b=0}^{k-1} w_{a,b} \max(x^{l-1}_{i+a, j+b}, \theta_{a,b})
\end{equation}

where kernel size $k\times k$ is used in this 2D convolution. $x^{l-1}$ and
$x^{l}$ are the input and output of layer $l$. $\theta$ and $w$ are trainable
thresholds and weights in our ConvRelu layer, respectively. Suppose we have an
input $[ 2, 3, 4 ]$, weights $[ 1, -1 ]$, and thresholds $[ 0, 3.5 ]$. When
applying 1D ConvRelu without padding, the output is $[ -1.5, -1 ]$.

Thanks for pointing out this. We will add this formula to the final version.</span></div><div class="reply_row clearfix"></div></div><div class="children"><div class="note_with_children"><div id="note_SkeJDCiDjX" class="note panel"><div class="title_pdf_row clearfix"><h2 class="note_content_title"><a>Thanks, and what about multiple channels?</a>  <button class="btn btn-xs btn-default permalink-button" title="Link to this comment" data-permalink-url="https://openreview.net/forum?id=SkgD4jAcYX&amp;noteId=SkeJDCiDjX"><span class="glyphicon glyphicon-link" aria-hidden="true"></span></button></h2></div><div class="meta_row"><span class="signatures">(anonymous)</span></div><div class="clearfix"><div class="meta_row pull-left"><span class="date item">20 Oct 2018</span><span class="item">ICLR 2019 Conference Paper7 Public Comment</span><span class="item">Readers: <span class="readers-icon glyphicon glyphicon-globe"></span> Everyone</span></div><div class="meta_row meta_actions"></div></div><div class="note_contents"><span class="note_content_field">Comment: </span><span class="note_content_value">While I have an idea of how to generalize the formula you provided to the multi-channel scenario based on your paper, just to be safe, could you also provide a formula of what this would look like with multiple input channels and multiple output channels?

Thanks</span></div><div class="reply_row clearfix"></div></div><div class="children"><div class="note_with_children"><div id="note_Bkg6w9lusm" class="note panel"><div class="title_pdf_row clearfix"><h2 class="note_content_title"><a>ConvRelu formula for multiple channels</a>  <button class="btn btn-xs btn-default permalink-button" title="Link to this comment" data-permalink-url="https://openreview.net/forum?id=SkgD4jAcYX&amp;noteId=Bkg6w9lusm"><span class="glyphicon glyphicon-link" aria-hidden="true"></span></button></h2></div><div class="meta_row"><span class="signatures">ICLR 2019 Conference Paper7 Authors</span></div><div class="clearfix"><div class="meta_row pull-left"><span class="date item">20 Oct 2018</span><span class="item">ICLR 2019 Conference Paper7 Official Comment</span><span class="item">Readers: <span class="readers-icon glyphicon glyphicon-globe"></span> Everyone</span></div><div class="meta_row meta_actions"></div></div><div class="note_contents"><span class="note_content_field">Comment: </span><span class="note_content_value">Sure, thank you for your interests, which make this paper more precise.

The unit $x^{l}_{i, j}$ in output feature map $d$ is calculated as:

\begin{equation}\label{eq:conv_relu}
x^l_{i, j, d} = \sum_{c=0}^{n_{in}-1} \sum_{a=0}^{k-1} \sum_{b=0}^{k-1} w_{a,b,c,d} \max(x^{l-1}_{i+a, j+b, c}, \theta_{a,b,c})
\end{equation}

where kernel size $k\times k$ is used in this 2D convolution and $n_{in}$ is
the number of input channels. $x^{l-1}$ and $x^{l}$ are the input and output
of layer $l$. $\theta$ and $w$ are trainable thresholds and weights in our
ConvRelu layer, respectively. In our ConvRelu, $\theta$ is shared across output
feature maps.</span></div><div class="reply_row clearfix"></div></div><div class="children"></div></div></div></div></div></div></div></div><div class="note_with_children"><div id="note_H1g84dXntX" class="note panel"><div class="title_pdf_row clearfix"><h2 class="note_content_title"><a>The authors should have read the ELU paper...</a>  <button class="btn btn-xs btn-default permalink-button" title="Link to this comment" data-permalink-url="https://openreview.net/forum?id=SkgD4jAcYX&amp;noteId=H1g84dXntX"><span class="glyphicon glyphicon-link" aria-hidden="true"></span></button></h2></div><div class="meta_row"><span class="signatures">(anonymous)</span></div><div class="clearfix"><div class="meta_row pull-left"><span class="date item">29 Sep 2018</span><span class="item">ICLR 2019 Conference Paper7 Public Comment</span><span class="item">Readers: <span class="readers-icon glyphicon glyphicon-globe"></span> Everyone</span></div><div class="meta_row meta_actions"></div></div><div class="note_contents"><span class="note_content_field">Comment: </span><span class="note_content_value">I can hardly believe, that the authors made this statement:
&gt;&gt;However, both ELUs and SELUs only  fitted fully-connected layers, and the use in convolution layers is not clear&lt;&lt;
</span></div><div class="reply_row clearfix"></div></div><div class="children"><div class="note_with_children"><div id="note_r1l1cgnm9Q" class="note panel"><div class="title_pdf_row clearfix"><h2 class="note_content_title"><a>Public Comments</a>  <button class="btn btn-xs btn-default permalink-button" title="Link to this comment" data-permalink-url="https://openreview.net/forum?id=SkgD4jAcYX&amp;noteId=r1l1cgnm9Q"><span class="glyphicon glyphicon-link" aria-hidden="true"></span></button></h2></div><div class="meta_row"><span class="signatures">ICLR 2019 Conference Program Chairs</span></div><div class="clearfix"><div class="meta_row pull-left"><span class="date item">04 Oct 2018</span><span class="item">ICLR 2019 Conference Paper7 Official Comment</span><span class="item">Readers: <span class="readers-icon glyphicon glyphicon-globe"></span> Everyone</span></div><div class="meta_row meta_actions"></div></div><div class="note_contents"><span class="note_content_field">Comment: </span><span class="note_content_value">Please watch your tone when making anonymous comments. It is fine to point out an issue of a paper but this an academic venue and a comment like "I can hardly believe" is not necessary. The authors are all doing their best to present interesting and relevant work.</span></div><div class="reply_row clearfix"></div></div><div class="children"></div></div><div class="note_with_children"><div id="note_r1eSYnRTF7" class="note panel"><div class="title_pdf_row clearfix"><h2 class="note_content_title"><a>Response</a>  <button class="btn btn-xs btn-default permalink-button" title="Link to this comment" data-permalink-url="https://openreview.net/forum?id=SkgD4jAcYX&amp;noteId=r1eSYnRTF7"><span class="glyphicon glyphicon-link" aria-hidden="true"></span></button></h2></div><div class="meta_row"><span class="signatures">ICLR 2019 Conference Paper7 Authors</span></div><div class="clearfix"><div class="meta_row pull-left"><span class="date item">30 Sep 2018</span><span class="item">ICLR 2019 Conference Paper7 Official Comment</span><span class="item">Readers: <span class="readers-icon glyphicon glyphicon-globe"></span> Everyone</span></div><div class="meta_row meta_actions"></div></div><div class="note_contents"><span class="note_content_field">Comment: </span><span class="note_content_value">Thank you for pointing out this inaccurate statement. We were aware of the fact that ELU can be used in convolutional layers and SELUs is mainly for fully connected layers. We will make this statement more accurate in our final paper.</span></div><div class="reply_row clearfix"></div></div><div class="children"><div class="note_with_children"><div id="note_SkgVTXu4cX" class="note panel"><div class="title_pdf_row clearfix"><h2 class="note_content_title"><a>I don't think SeLUs are "for" fully-connected layers</a>  <button class="btn btn-xs btn-default permalink-button" title="Link to this comment" data-permalink-url="https://openreview.net/forum?id=SkgD4jAcYX&amp;noteId=SkgVTXu4cX"><span class="glyphicon glyphicon-link" aria-hidden="true"></span></button></h2></div><div class="meta_row"><span class="signatures">(anonymous)</span></div><div class="clearfix"><div class="meta_row pull-left"><span class="date item">05 Oct 2018</span><span class="item">ICLR 2019 Conference Paper7 Public Comment</span><span class="item">Readers: <span class="readers-icon glyphicon glyphicon-globe"></span> Everyone</span></div><div class="meta_row meta_actions"></div></div><div class="note_contents"><span class="note_content_field">Comment: </span><span class="note_content_value">The original SeLU paper used fully-connected layers, but there's no reason not to use them in CNNs. See e.g. <a href="https://arxiv.org/pdf/1710.05918.pdf" target="_blank" rel="nofollow">https://arxiv.org/pdf/1710.05918.pdf</a>

Also, I don't think "I can hardly believe" is such an egregious statement.</span></div><div class="reply_row clearfix"></div></div><div class="children"></div></div></div></div></div></div></div></main></div>
  </div>
</div>


    <!-- Footer -->
    <footer class="sitemap">
      <div class="container">
        <div class="row hidden-xs">
          <div class="col-sm-4">
            <ul class="list-unstyled">
              <li><a href="/" class="home">Home</a></li>
              <li><a href="/venues">All Venues</a></li>
            </ul>
          </div>
    
          <div class="col-sm-4">
            <ul class="list-unstyled">
              <li><a href="/about">About OpenReview</a></li>
              <li><a href="/contact">Contact</a></li>
              <li><a href="#" data-toggle="modal" data-target="#feedback-modal">Feedback</a></li>
            </ul>
          </div>
    
          <div class="col-sm-4">
            <ul class="list-unstyled">
              <li><a href="/terms">Terms of Service</a></li>
              <li><a href="/privacy">Privacy Policy</a></li>
            </ul>
          </div>
        </div>
    
        <div class="row visible-xs-block">
          <div class="col-xs-6">
            <ul class="list-unstyled">
              <li><a href="/" class="home">Home</a></li>
              <li><a href="/about">About OpenReview</a></li>
              <li><a href="#" data-toggle="modal" data-target="#feedback-modal">Feedback</a></li>
            </ul>
          </div>
    
          <div class="col-xs-6">
            <ul class="list-unstyled">
              <li><a href="/contact">Contact</a></li>
              <li><a href="/terms">Terms of Service</a></li>
              <li><a href="/privacy">Privacy Policy</a></li>
            </ul>
          </div>
        </div>
      </div>
    </footer>
    
    <footer class="sponsor">
      <div class="container">
        <div class="row">
          <div class="col-sm-10 col-sm-offset-1">
            <p class="text-center">
              OpenReview is created by the <a href="http://www.iesl.cs.umass.edu/" target="_blank">Information Extraction and Synthesis Laboratory</a>, College of Information and Computer Science, University of Massachusetts Amherst. We gratefully acknowledge the support of the OpenReview sponsors:  Google,  Facebook, NSF, the University of Massachusetts Amherst Center for Data Science, and Center for Intelligent Information Retrieval, as well as the Google Cloud Platform for donating the computing and networking services on which OpenReview.net runs.
            </p>
          </div>
        </div>
      </div>
    </footer>

  <div id="feedback-modal" class="modal fade" tabindex="-1" role="dialog">
    <div class="modal-dialog">
      <div class="modal-content">
        <div class="modal-header">
          <button type="button" class="close" data-dismiss="modal" aria-label="Close"><span aria-hidden="true">Ã—</span></button>
          <h3 class="modal-title">Send Feedback</h3>
        </div>
        <div class="modal-body">
          <p>Enter your feedback below and we'll get back to you as soon as possible.</p>
          <form action="/feedback" method="POST">
            <div class="form-group">
              <input type="email" name="from" class="form-control" placeholder="Email" />
            </div>
            <div class="form-group">
              <input type="text" name="subject" class="form-control" placeholder="Subject" />
            </div>
            <div class="form-group">
              <textarea name="message" class="form-control feedback-input" rows="5" placeholder="Message" required=""></textarea>
            </div>
          <ul id="ui-id-2" tabindex="0" class="ui-menu ui-widget ui-widget-content" style="display: none;"></ul></form>
        </div>
        <div class="modal-footer">
          <button type="button" class="btn btn-default" data-dismiss="modal">Cancel</button>
          <button type="button" class="btn btn-primary">Send</button>
        </div>
      </div>
    </div>
  </div>

  <script type="text/javascript" async="" src="https://www.google-analytics.com/analytics.js"></script><script src="//cdnjs.cloudflare.com/ajax/libs/jquery/2.2.4/jquery.min.js"></script>
  <script>window.jQuery || document.write('&lt;script src="/static/js/vendor/jquery-2.2.4.min.js"&gt;&lt;\/script&gt;')</script>

  <script src="/static/dist/vendor.min.js"></script>
  <script src="/static/dist/templates.min.js"></script>
  <script src="/static/dist/openreview.min.js"></script>

    <script>window.legacyScripts = true;</script>


    <script async="" src="https://www.googletagmanager.com/gtag/js?id=UA-108703919-1"></script>
    <script>
      window.dataLayer = window.dataLayer || [];
      function gtag() { dataLayer.push(arguments); }
      gtag('js', new Date());
      gtag('config', 'UA-108703919-1');
    </script>


<div role="status" aria-live="assertive" aria-relevant="additions" class="ui-helper-hidden-accessible"></div></body></html>