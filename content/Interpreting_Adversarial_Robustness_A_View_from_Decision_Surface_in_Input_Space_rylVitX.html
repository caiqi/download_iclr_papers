<!DOCTYPE html><html xmlns="http://www.w3.org/1999/xhtml" class="no-js"><head>
  <meta charset="utf-8" />
  <meta http-equiv="X-UA-Compatible" content="IE=edge,chrome=1" />
  <meta name="viewport" content="width=device-width, initial-scale=1" />

  <title>Interpreting Adversarial Robustness: A View from Decision Surface in Input Space | OpenReview</title>
  <meta name="description" content="" />

      <meta name="citation_title" content="Interpreting Adversarial Robustness: A View from Decision Surface in Input Space" />
        <meta name="citation_author" content="Anonymous" />
      <meta name="citation_publication_date" content="2018/09/27" />
      <meta name="citation_online_date" content="2018/09/27" />
      <meta name="citation_pdf_url" content="https://openreview.net/pdf?id=rylV6i09tX" />
      <meta name="twitter:card" content="summary" />
      <meta name="twitter:site" content="@openreviewnet" />
      <meta name="og:title" content="Interpreting Adversarial Robustness: A View from Decision Surface..." />
      <meta name="og:description" content="One popular hypothesis of neural network generalization is that the flat local minima of loss surface in parameter space leads to good generalization. However, we demonstrate that loss surface in..." />
      <meta name="og:image" content="https://openreview.net/static/images/openreview_logo_512.png" />

  <link rel="stylesheet" href="https://fonts.googleapis.com/css?family=Noto+Sans:400,400i,700,700i" />

  <link rel="stylesheet" href="/static/css/bootstrap.min.css" />
  <link rel="stylesheet" href="/static/css/jquery-ui.min.css" />
  <link rel="stylesheet" href="/static/css/main.min.css" />
</head>

<body class="forum">
  <nav class="navbar navbar-inverse navbar-fixed-top" role="navigation">
    <div class="container">
  
      <div class="navbar-header">
        <button type="button" class="navbar-toggle collapsed" data-toggle="collapse" data-target="#navbar" aria-expanded="false" aria-controls="navbar">
          <span class="sr-only">Toggle navigation</span>
          <span class="icon-bar"></span>
          <span class="icon-bar"></span>
          <span class="icon-bar"></span>
        </button>
        <a class="navbar-brand home push-link" href="/"><strong>OpenReview</strong>.net</a>
      </div>
  
      <div id="navbar" class="navbar-collapse collapse">
        <form class="navbar-form navbar-left profile-search" role="search">
          <div class="form-group has-feedback">
            <input id="search_input" type="text" class="form-control ui-autocomplete-input" placeholder="Search ICLR 2019 Conference" autocomplete="off" />
            <span class="glyphicon glyphicon-search form-control-feedback" aria-hidden="true"></span>
          </div>
  
          <input id="search_group" type="hidden" value="ICLR.cc/2019/Conference" />
          <input id="search_content" type="hidden" value="all" />
        <ul id="ui-id-1" tabindex="0" class="ui-menu ui-widget ui-widget-content ui-autocomplete ui-front" style="display: none;"></ul></form>
  
        <ul class="nav navbar-nav navbar-right">
        
            <li id="user-menu"><a href="/login">Login</a></li>
        </ul>
      </div>
  
    </div>
  </nav>

  <div id="or-banner" class="banner" style="">
  <div class="container">
    <div class="row">
      <div class="col-xs-12"><a href="/group?id=ICLR.cc/2019/Conference" title="Venue Homepage"><img class="icon" src="/static/images/arrow_left.svg" /> Go to <strong>ICLR 2019 Conference</strong> homepage</a></div>
    </div>
  </div>
</div>
<div id="flash-message-container" class="alert alert-danger" role="alert" style="display: none;">
  <div class="container">
    <div class="row">
      <div class="col-xs-12">
        <div class="alert-content">
          <button type="button" class="close" aria-label="Close"><span aria-hidden="true">×</span></button>
        </div>
      </div>
    </div>
  </div>
</div>

<div class="container">
  <div class="row">
    <div class="col-xs-12">
      <main id="content" class="clearfix openbanner-visible legacy-styles"><div id="note_rylV6i09tX" class="note panel"><div class="title_pdf_row clearfix"><h2 class="note_content_title"><a>Interpreting Adversarial Robustness: A View from Decision Surface in Input Space</a> <a class="note_content_pdf" href="/pdf?id=rylV6i09tX" title="Download PDF" target="_blank"><img src="/static/images/pdf_icon_blue.svg" /></a> </h2></div><div class="meta_row"><span class="signatures">Anonymous</span></div><div class="clearfix"><div class="meta_row pull-left"><span class="date item">28 Sep 2018 (modified: 07 Nov 2018)</span><span class="item">ICLR 2019 Conference Blind Submission</span><span class="item">Readers: <span class="readers-icon glyphicon glyphicon-globe"></span> Everyone</span><span class="item"><a class="action-bibtex-modal" data-bibtex="@inproceedings{    &#10;anonymous2019interpreting,    &#10;title={Interpreting Adversarial Robustness: A View from Decision Surface in Input Space},    &#10;author={Anonymous},    &#10;booktitle={Submitted to International Conference on Learning Representations},    &#10;year={2019},    &#10;url={https://openreview.net/forum?id=rylV6i09tX},    &#10;note={under review}    &#10;}">Show Bibtex</a></span><a class="note_content_pdf item" href="/revisions?id=rylV6i09tX" target="_blank">Show Revisions</a></div><div class="meta_row meta_actions"></div></div><div class="note_contents"><span class="note_content_field">Abstract: </span><span class="note_content_value">One popular hypothesis of neural network generalization is that the flat local minima of loss surface in parameter space leads to good generalization. However, we demonstrate that loss surface in parameter space has no obvious relationship with generalization, especially under adversarial settings. Through visualizing decision surfaces in both parameter space and input space, we instead show that the geometry property of decision surface in input space correlates well with the adversarial robustness. We then propose an adversarial robustness indicator, which can evaluate a neural network's intrinsic robustness property without testing its accuracy under adversarial attacks. Guided by it, we further propose our robust training method. Without involving adversarial training, our method could enhance network's intrinsic adversarial robustness against various adversarial attacks.</span></div><div class="note_contents"><span class="note_content_field">Keywords: </span><span class="note_content_value">Adversarial examples, Robustness</span></div><div class="reply_row clearfix"><div class="item" id="reply_count">7 Replies</div></div></div><hr class="small" /><div id="note_children"><div class="note_with_children"><div id="note_SkxL3EDvaQ" class="note panel"><div class="title_pdf_row clearfix"><h2 class="note_content_title"><a>Review of "Interpreting Adversarial Robustness: A View from Decision Surface in Input Space"</a>  <button class="btn btn-xs btn-default permalink-button" title="Link to this comment" data-permalink-url="https://openreview.net/forum?id=rylV6i09tX&amp;noteId=SkxL3EDvaQ"><span class="glyphicon glyphicon-link" aria-hidden="true"></span></button></h2></div><div class="meta_row"><span class="signatures">ICLR 2019 Conference Paper794 AnonReviewer3</span></div><div class="clearfix"><div class="meta_row pull-left"><span class="date item">13 Nov 2018</span><span class="item">ICLR 2019 Conference Paper794 Official Review</span><span class="item">Readers: <span class="readers-icon glyphicon glyphicon-globe"></span> Everyone</span></div><div class="meta_row meta_actions"></div></div><div class="note_contents"><span class="note_content_field">Review: </span><span class="note_content_value">This paper argues that analyzing loss surfaces in parameter space for the purposes of evaluating adversarial robustness and generalization is ineffective, while measuring input loss surfaces is more accurate. By converting loss surfaces to decision surfaces (which denote the difference between the max and second highest logit), the authors show that all adversarial attack methods appear similar wrt the decision surface. This result is then related to the statistics of the input Jacobian and Hessian, which are shown to differ across adversarially sensitive and robust models. Finally, a regularization method based on regularizing the input Jacobian is proposed and evaluated. All of these results are shown through experiments on MNIST and CIFAR-10.

In general, the paper is clear, though there are a number of typos. With respect to novelty, some of the experiments are novel, but others, including the improved training method, have been explored before (see specific comments for references). Finally, regarding significance, many of the insights provided this paper are true by definition, and are therefore unlikely to have a significant impact. 

While I strongly believe that rigorous empirical studies of neural networks are essential, this paper is lacking in several key areas, including framing, experimental insights, and relation to prior work, and is therefore difficult to recommend. Please see the comments below for more detail. 

Major comments:

1) In the beginning of the paper, adversarial robustness and generalization are equated. However, adversarial robustness and generalization are not necessarily equivalent, and in fact, several papers have provided evidence against this notion, showing that adversarial inputs are likely to be present even for very good models [5, 6] and that adversarially sensitive models can often generalize quite well [2]. Moreover, all the experiments within the paper only address adversarial robustness rather than generalization to unperturbed samples.

2) One of the main results of this paper is that the loss surface wrt input space is more sensitive to adversarial perturbations than the loss surface wrt parameter space. Because adversarial inputs are defined in input space, by definition, the loss surface wrt to the input must be sensitive to adversarial examples. This result therefore appears true by definition. Moreover, [3] related the input Jacobian to generalization, finding a similar result, but is not discussed or cited.

3) The main result of Section 3 is that all adversarial attacks “utilize the decision surface geometry properties to cross the decision boundary within least distance.” While to my knowledge the decision surface visualization is novel and might have important uses, this statement is again true by definition, given that adversarial attack methods try to find the smallest perturbation which changes the network decision. As a result, all methods must find directions which are short paths in the decision surface. It is therefore unclear what additional insight this analysis presents. 

4) How does measuring the loss landscape as an indicator for adversarial robustness differ from simply trying to find adversarial examples as is common? If anything, it seems it should be more computationally expensive as points are sampled in a grid search vs optimized for. 

5) The proposed regularizer for adversarial robustness, based on regularizing the input Jacobian, is very similar to what was proposed in [1], yet [1] is not discussed or cited. 

Minor comments:

1) The paper’s first sentence states that “It is commonly believed that a neural network’s generalization is correlated to ...the flatness of the local minima in parameter space.” However, [4] showed several years ago that the local minima flatness can be arbitrarily rescaled and has been fairly influential. While [4] is cited in the paper, it is only cited in the related work section as support for the statement that local minima flatness is related to generalization when this is precisely opposite the point this paper makes. [4] should be discussed in more detail, both in the introduction and the related work section.

2) The paper is quite lengthy, going right up against the hard 10 page limit. While this may be acceptable for papers with large figures or which require the extra space, this paper does not currently meet that threshold. 

3) Throughout the figures, axes should be labeled. 

4) In section 2.2, it is stated that both networks achieve optimal accuracy of ~90% on CIFAR-10. This is not optimal accuracy and hasn’t been for several years [7].

5) Why is equation 2 calculated with respect to the logit layer vs the normalized softmax layer? Using the unnormalized logits may introduce noise due to scaling.

6) In Figure 8, the scales of the Hessian are extremely different. Does this impact the measurement of sparseness? 


Typos:

1) Introduction, second paragraph: “For example, ResNet model usually converges to…” should be “For example, ResNet models usually converge to…”

2) Introduction, second paragraph: “...defected by the adversarial noises...” should be “...defected by adversarial noise…”

3) Introduction, third paragraph: “...introduced by adversarial noises...” should be “...introduced by adversarial noise…”

4) Section 3.1, first paragraph: “cross entropy based loss surface is…” should be “cross entropy based loss surfaces is…”

[1] Jakubovitz, Daniel, and Raja Giryes. "Improving DNN Robustness to Adversarial Attacks using Jacobian Regularization." arXiv preprint arXiv:1803.08680 (2018). ECCV 2018.
[2] Zahavy, Tom, et al. "Ensemble Robustness and Generalization of Stochastic Deep Learning Algorithms." arXiv preprint arXiv:1602.02389 (2016). ICLR Workshop 2018
[3] Novak, Roman, et al. "Sensitivity and generalization in neural networks: an empirical study." arXiv preprint arXiv:1802.08760 (2018). ICLR 2018.
[4] Dinh, Laurent, et al. "Sharp Minima Can Generalize For Deep Nets." International Conference on Machine Learning. 2017.
[5] Fawzi, Alhussein, Hamza Fawzi, and Omar Fawzi. "Adversarial vulnerability for any classifier." arXiv preprint arXiv:1802.08686 (2018). NIPS 2018.
[6] Gilmer, Justin, et al. "Adversarial spheres." arXiv preprint arXiv:1801.02774 (2018). ICLR Workshop 2018.
[7] <a href="http://rodrigob.github.io/are_we_there_yet/build/classification_datasets_results.html" target="_blank" rel="nofollow">http://rodrigob.github.io/are_we_there_yet/build/classification_datasets_results.html</a>
</span></div><div class="note_contents"><span class="note_content_field">Rating: </span><span class="note_content_value">3: Clear rejection</span></div><div class="note_contents"><span class="note_content_field">Confidence: </span><span class="note_content_value">5: The reviewer is absolutely certain that the evaluation is correct and very familiar with the relevant literature</span></div><div class="reply_row clearfix"></div></div><div class="children"></div></div><div class="note_with_children"><div id="note_ryehFslyam" class="note panel"><div class="title_pdf_row clearfix"><h2 class="note_content_title"><a>good visualization for adversarial robustness analysis, unclear loss surface in weight space with adversarial data</a>  <button class="btn btn-xs btn-default permalink-button" title="Link to this comment" data-permalink-url="https://openreview.net/forum?id=rylV6i09tX&amp;noteId=ryehFslyam"><span class="glyphicon glyphicon-link" aria-hidden="true"></span></button></h2></div><div class="meta_row"><span class="signatures">ICLR 2019 Conference Paper794 AnonReviewer1</span></div><div class="clearfix"><div class="meta_row pull-left"><span class="date item">06 Nov 2018 (modified: 07 Nov 2018)</span><span class="item">ICLR 2019 Conference Paper794 Official Review</span><span class="item">Readers: <span class="readers-icon glyphicon glyphicon-globe"></span> Everyone</span></div><div class="meta_row meta_actions"></div></div><div class="note_contents"><span class="note_content_field">Review: </span><span class="note_content_value">The authors demonstrated that the loss surface visualized in parameter space does not reflect its robustness to adversarial examples. By analyzing the geometric properties of the loss surface in both parameter space and input space, they find input space is more appropriate in evaluating the generalization and adversarial robustness of a neural network. Therefore, they extend the loss surface to decision surface. They further visualized the adversarial attack trajectory on decision surfaces in input space, and formalized the adversarial robustness indicator. Finally, a robust training method guided by the indicator is proposed to smooth the decision surface.

This paper is interesting and well organized. The idea of plotting loss surface in input space seems to be a natural extension of the loss surface w.r.t to weight change. The loss surface in input space measures the network’s robustness to the perturbation of inputs, which naturally shows the influence of adversarial examples and is suitable for studying the robustness to adversarial examples. 

Note that loss surface in parameter space measures the network’s robustness to the perturbation of the weights with given inputs, which implicitly assumes the data distribution is not significantly changed so that the loss surface may have similar geometry on the unseen data.  

The claim of “these significant distinctions indicate the ineffectiveness of generalization estimation from the loss surfaces in parameter space” are not well supported as the comparison between Figure 2(a) and Figure 3 seems to be unfair and misleading. Fig 2 are plotted based on input data without any adversarial examples. So it is expected to see that Fig 2(a) and Fig 2(b) have similar contours. However, the loss surface in weight space may still be able to show their difference if the they are both plotted with the adversarial inputs. I believe that models trained by Min-Max robust training will be more stable in comparison with the normally trained models. It would be great if the author provide such plots. I would expect the normal model to have a high and flat surface while the robust model shows reasonable loss with small changes in weight space.

How to choose \alpha and \beta for loss surface of input space for Fig 3 and Fig 4 (first row)?

How are \alpha and \beta normalized for loss surface visualization in weight space as in Eq 1?
</span></div><div class="note_contents"><span class="note_content_field">Rating: </span><span class="note_content_value">6: Marginally above acceptance threshold</span></div><div class="note_contents"><span class="note_content_field">Confidence: </span><span class="note_content_value">4: The reviewer is confident but not absolutely certain that the evaluation is correct</span></div><div class="reply_row clearfix"></div></div><div class="children"><div class="note_with_children"><div id="note_S1licXsy6X" class="note panel"><div class="title_pdf_row clearfix"><h2 class="note_content_title"><a>Reply to "good visualization ..."</a>  <button class="btn btn-xs btn-default permalink-button" title="Link to this comment" data-permalink-url="https://openreview.net/forum?id=rylV6i09tX&amp;noteId=S1licXsy6X"><span class="glyphicon glyphicon-link" aria-hidden="true"></span></button></h2></div><div class="meta_row"><span class="signatures">ICLR 2019 Conference Paper794 Authors</span></div><div class="clearfix"><div class="meta_row pull-left"><span class="date item">07 Nov 2018</span><span class="item">ICLR 2019 Conference Paper794 Official Comment</span><span class="item">Readers: <span class="readers-icon glyphicon glyphicon-globe"></span> Everyone</span></div><div class="meta_row meta_actions"></div></div><div class="note_contents"><span class="note_content_field">Comment: </span><span class="note_content_value">We have updated our submitted paper to address your concerns in Sec.2.2 and in Appendix 8.5. 
Thanks a lot for the reviewer's suggestions!

--------------------------------------------------------------------------------------------------------------------------------------------------------------

We thank the reviewers for liking the visualization idea!

1.	About your first concern that “The claim of ‘these significant distinctions indicate the ineffectiveness of generalization estimation from the loss surfaces in parameter space’ are not well supported”, please let us provide some clarification. We have provided the natural model and robust MinMax model’s contour maps on natural input and adversarial inputs and their Visualizations in the Appendix (the updated version) 8.5, Fig.16.

As expected, in parameter space, natural model's loss surface on adversarial inputs has a larger base height than the robust model, i.e. the average loss values are higher than robust models. But such gap is only obvious on weak attacks, like FGSM. When we use stronger attacks like C\&amp;W attack, the loss surface of the natural model and robust model become similar again: Both models' surfaces demonstrate high cross-entropy loss with no obvious distinction.

Therefore, as mentioned in the review, we can indeed use the loss surface in weight space to show their robustness difference if they are both plotted with weak adversarial inputs. But when we are facing stronger iterative attacks, the loss surface in weight space can no longer show any difference, thus cannot indicate the model robustness. 

By contrast, our input space loss surfaces can explicitly show the model robustness difference with no such restrictions, and the robustness difference can also be more obviously demonstrated, as shown in main paper Fig.3. Therefore, we think this is the advantage of using input space loss surface to indicate the model robustness. We will absolutely update the statement in the main paper. 

2.	In Fig.3, both x-axes (alpha) in (a) and (b) are chosen as random directions with normalization, and both y-axes (beta) are chosen as FGSM attack direction [1].  
In Fig.4, all x-axes (alpha) in (a)-(d) are chosen as random directions with normalization, and y-axes are as following (formulas in Eq.3): (a) random direction, (b) FGSM attack direction [1], (c) Least-likely class attack direction [1], (d) C&amp;W attack direction [2], all with normalization.

3.	The gradients and random noises are normalized as in Fast gradient sign method [1], which is the pixel-wise sign, i.e. beta = sign(beta).

We thank the reviewer for the constructive comments on the first point, and we will absolutely update the statement to be more accurate. 

[1] Adversarial examples in the physical world, Alexey Kurakin et al, 2016.
[2] Towards evaluating the robustness of neural networks. Carlini, Nicholas, and David Wagner. IEEE (SP), 2017.</span></div><div class="reply_row clearfix"></div></div><div class="children"></div></div></div></div><div class="note_with_children"><div id="note_HJejsv4An7" class="note panel"><div class="title_pdf_row clearfix"><h2 class="note_content_title"><a>An interesting visualization paper, but not always so convincing</a>  <button class="btn btn-xs btn-default permalink-button" title="Link to this comment" data-permalink-url="https://openreview.net/forum?id=rylV6i09tX&amp;noteId=HJejsv4An7"><span class="glyphicon glyphicon-link" aria-hidden="true"></span></button></h2></div><div class="meta_row"><span class="signatures">ICLR 2019 Conference Paper794 AnonReviewer2</span></div><div class="clearfix"><div class="meta_row pull-left"><span class="date item">06 Nov 2018 (modified: 07 Nov 2018)</span><span class="item">ICLR 2019 Conference Paper794 Official Review</span><span class="item">Readers: <span class="readers-icon glyphicon glyphicon-globe"></span> Everyone</span></div><div class="meta_row meta_actions"></div></div><div class="note_contents"><span class="note_content_field">Review: </span><span class="note_content_value">This paper uses visualization methods to study how adversarial training methods impact the decision surface of neural networks.  The authors also propose a gradient-based regularizer to improve robustness during training.

Some things I liked about this paper:
The authors are the first to visualize the "decision boundary loss".  I also find this to be a better and more thorough study of loss functions than I have seen in other papers.  The quality of the visualizations is notably higher than I've seen elsewhere on this subject.

I have a few criticisms of this paper that I list below:
1)  I'm not convinced that the decision surface is more informative than the loss surface.  There is indeed a big "hole" in the middle of the plots in Figure 4, but it seems like that is only because the first contour is drawn at too high a level to see what is going on below.  More contours are needed to see what is going on in that central region. 
2) The proposed regularizer is very similar to the method of Ross &amp; Doshi.  It would be good if this similarity was addressed more directly in the paper.  It feels like it's been brushed under the rug.
3) In the MNIST results in Table 1:  These results are much less extensive than the results for CIFAR.  It would especially be nice to see the MinMax results since those of commonly considered to be the state of the art. The fact that they are omitted makes it feel like something is being hidden from the reader.
4) The results of the proposed regularization method aren't super strong.  For CIFAR, the proposed method combined with adversarial training beats MinMax only for small perturbations of size 3, and does worse for larger perturbations.  The original MinMax model is optimized for a perturbation of size 8.  I wonder if a MinMax result with smaller epsilon would be dominant in the regime of small perturbations.  </span></div><div class="note_contents"><span class="note_content_field">Rating: </span><span class="note_content_value">5: Marginally below acceptance threshold</span></div><div class="note_contents"><span class="note_content_field">Confidence: </span><span class="note_content_value">5: The reviewer is absolutely certain that the evaluation is correct and very familiar with the relevant literature</span></div><div class="reply_row clearfix"></div></div><div class="children"><div class="note_with_children"><div id="note_HJgm8ciA3X" class="note panel"><div class="title_pdf_row clearfix"><h2 class="note_content_title"><a>Answers to reviewer's concerns</a>  <button class="btn btn-xs btn-default permalink-button" title="Link to this comment" data-permalink-url="https://openreview.net/forum?id=rylV6i09tX&amp;noteId=HJgm8ciA3X"><span class="glyphicon glyphicon-link" aria-hidden="true"></span></button></h2></div><div class="meta_row"><span class="signatures">ICLR 2019 Conference Paper794 Authors</span></div><div class="clearfix"><div class="meta_row pull-left"><span class="date item">06 Nov 2018 (modified: 07 Nov 2018)</span><span class="item">ICLR 2019 Conference Paper794 Official Comment</span><span class="item">Readers: <span class="readers-icon glyphicon glyphicon-globe"></span> Everyone</span></div><div class="meta_row meta_actions"></div></div><div class="note_contents"><span class="note_content_field">Comment: </span><span class="note_content_value">We have updated our submitted paper and added the MNIST and CIFAR experimental results in Appendix 8.6. 
Thanks a lot for the reviewer's suggestions for our experiments!

-----------------------------------------------------------------------------------------------------------------------------------------------------------------

We thank the reviewer for your interests in our visualization results!

1)	About your first concern on our statement, that decision loss is more informative, there are two reasons: 
a)	About the big hole, we give a simple example and two illustration figures in Appendix 8.1. The “less informative” blank region is caused by the non-linear operations (soft-max and entropy). Specifically, when a neural network has high confidence in correct logits, the cross-entropy could hardly describe the confidence information: 
For example, ten-class model with nine pairs of different logit outputs [1, 1, 1, …, 1, 1], [2, 1, 1, …, 1, 1], …. [9, 1, 1, …, 1, 1];
The corresponding cross-entropy loss is [2.30, 1.46, 0.79, 0.37, 0.15, 0.05, 0.02, 0.01, 0.01];
The corresponding confidence defined in Eq.2 is [0, 1, 2, 3, 4, 5, 6, 7, 8];
With further increasing of logits confidence (model confidence could easily achieve over 20 in common NN models), the cross-entropy loss hardly changes anymore. But the confidence’s change is stable. That’s why the blank region appears in cross-entropy loss surface but not in decision surface. Therefore, we state the decision confidence surfaces could provide more geometry information in the “blank region” of cross-entropy loss surface. 
b)	Second, the decision boundary loss surface contains the explicit decision boundary (contour line L=0), across which the model’s prediction result will be flipped. By contrast, cross-entropy loss surface has no such explicit decision boundary. This is one very important property since this enables us to visualize and evaluate the attack strength needed to conduct a successful adversarial attack against the model.
Therefore with these two points, we claim that decision surface is more informative than cross-entropy loss surface.

2)	About how to distinguish our work from Ross &amp; Doshi [1], the same part is we use the same Loss_ce + Loss_grad idea but different regularizer design in Loss_grad. We choose to penalize decision boundary loss’s (in Eq.2) gradients while Ross &amp; Doshi use common cross-entropy loss’s gradients. The benefit of doing so is related to the problem we mentioned in above part 1(a). Because of the cross-entropy loss involves highly non-linear soft-max and entropy operation compared to the decision loss, the changing of cross-entropy loss is negligible in high confidence cases (as we mentioned before) but decision loss has no such drawbacks. The non-linear operations will also cause the gradient of cross-entropy loss is relatively small, which brings constraints on the gradient penalty effect. As the comparison experiments in Table1 and 2 showed, our decision loss gradient regularizer outperforms cross-entropy loss regularization a lot on both MNIST and CIFAR10. 

3)	About the 3rd and 4th concerns, the reason we omit the MinMax model on MNIST is purely space consideration. Here we comment with the missing part our Ours+Adv Training, and MinMax’s experimental results. We will update our version added with these MNIST experimental results.

Attack | Natural   ----FGSM----              ----BIM----              ----C&amp;W----
Epsilon	|    0      |0.1|0.2|0.3|           0.1|0.2|0.3|           0.1|0.2|0.3
Ours+Adv|  95.9 | 87.6|72.2|44.1|   89.2|67.2|28.4|   89.6|73.2|39.5
MinMax	|  98.4   |97.3|96.3|95.2|   97.2|94.3|92.8|    97.6|96.4|94.5

The MinMax model is absolutely the most robust model, which is the reason why we choose its model to analyze the decision surface geometry. In Fig.6, the (eps &lt; 0.3) region of MinMax model’s decision surface is nearly flat with no downhills under both random noises and adversarial attacks, which makes it near immune to adversarial attacks. 

4)	And about the final concern that on CIFAR10, will original MinMax released model be dominant in small perturbation attacks, below we show the original released model’s (trained with eps=8) performance under the attacks:

Attack	Natural	        ----FGSM----                       ----BIM-----	                           ---C&amp;W---
Epsilon	0	               | 3| 6|8|9|	                    |3|6|8|9|	                          |3|6|8|9|
Acc	       87.3	          |75.3|63.2|56.1|53.4|     |74.2|59.3|48.7|46.2|     |74.2|	59.2|49.8|46.1|

The released original model did not show dominant accuracies on small perturbations above our methods on CIFAR10. The robustness gap is within 10% robustness excluding the model baseline accuracy difference (baseline accuracy difference is about 4% compared to Ours+AdvTrain).  
</span></div><div class="reply_row clearfix"></div></div><div class="children"></div></div><div class="note_with_children"><div id="note_SyxnXcj03m" class="note panel"><div class="title_pdf_row clearfix"><h2 class="note_content_title"><a>Answers to reviewer's concerns</a>  <button class="btn btn-xs btn-default permalink-button" title="Link to this comment" data-permalink-url="https://openreview.net/forum?id=rylV6i09tX&amp;noteId=SyxnXcj03m"><span class="glyphicon glyphicon-link" aria-hidden="true"></span></button></h2></div><div class="meta_row"><span class="signatures">ICLR 2019 Conference Paper794 Authors</span></div><div class="clearfix"><div class="meta_row pull-left"><span class="date item">06 Nov 2018 (modified: 10 Nov 2018)</span><span class="item">ICLR 2019 Conference Paper794 Official Comment</span><span class="item">Readers: <span class="readers-icon glyphicon glyphicon-globe"></span> Everyone</span></div><div class="meta_row meta_actions"></div></div><div class="note_contents"><span class="note_content_field">Comment: </span><span class="note_content_value">5)	And we still need to claim two points about why our method is still meaningful here:
a)	The training cost of MinMax makes it hard to scale. MinMax is commonly known that this method cannot generalize to large-scale datasets [2], e.g. ImageNet, since every training step in MinMax needs to generate PGD adversarial examples through 10-30 backpropagations. This makes training large-scale MinMax robust models impractical. But our method has better scaling ability than MinMax, the time consumption by double-backpropagation per training step is about 2.1 times than normal training, which is thus 5-15 times less than MinMax. 
b)	Meanwhile, on CIFAR10, the gap between MinMax and our method is not that large. Especially, the robustness gap under eps=3 attacks (FGSM, BIM, C&amp;W) is negligible as shown in Table.2.  And about the robustness degradation under larger step attack, our reason analysis is stated in Sec 5.2, that because Taylor Approximation performs well in a small neighborhood but has limitations against larger step attack, which is a limitation of our method which we also talked in the paper.  

6)	Lastly, as our paper named "Interpreting Adversarial Robustness: A View from Decision Surface in Input Space", we sincerely hope that reviewer could also take our paper’s other contributions into consideration, like revealing the nature of adversarial examples and robustness are actually solving NN’s neighborhood underfitting issue, the shared mechanism of various adversarial attacks by decision loss surface visualization and interpretation, proof of the relationship between loss geometry and adversarial robustness by Jacobian and Hessian’s geometry properties, etc., and we believe our paper is a thorough analysis and interpretation work in current interpreting adversarial attack and robustness research. 

Again, we thank the reviewer for the detailed reviews!

[1] Andrew Slavin Ross and Finale Doshi-Velez. Improving the adversarial robustness and interpretability of deep neural networks by regularizing their input gradients. In AAAI, 2018.
[2] Kannan, Harini, Alexey Kurakin, and Ian Goodfellow. "Adversarial Logit Pairing." arXiv preprint arXiv:1803.06373 (2018).</span></div><div class="reply_row clearfix"></div></div><div class="children"></div></div><div class="note_with_children"><div id="note_BylGzcjRh7" class="note panel trashed"><div class="title_pdf_row clearfix"><h2 class="note_content_title">  <button class="btn btn-xs btn-default permalink-button" title="Link to this comment" data-permalink-url="https://openreview.net/forum?id=rylV6i09tX&amp;noteId=BylGzcjRh7"><span class="glyphicon glyphicon-link" aria-hidden="true"></span></button></h2></div><div class="meta_row"><span class="signatures">[Deleted]</span></div><div class="clearfix"><div class="meta_row pull-left"><span class="item">ICLR 2019 Conference Paper794 Official Comment</span><span class="item">Readers: <span class="readers-icon glyphicon glyphicon-globe"></span> Everyone</span></div><div class="meta_row meta_actions"></div></div><div class="reply_row clearfix"></div></div><div class="children"></div></div></div></div></div></main></div>
  </div>
</div>


    <!-- Footer -->
    <footer class="sitemap">
      <div class="container">
        <div class="row hidden-xs">
          <div class="col-sm-4">
            <ul class="list-unstyled">
              <li><a href="/" class="home">Home</a></li>
              <li><a href="/venues">All Venues</a></li>
            </ul>
          </div>
    
          <div class="col-sm-4">
            <ul class="list-unstyled">
              <li><a href="/about">About OpenReview</a></li>
              <li><a href="/contact">Contact</a></li>
              <li><a href="#" data-toggle="modal" data-target="#feedback-modal">Feedback</a></li>
            </ul>
          </div>
    
          <div class="col-sm-4">
            <ul class="list-unstyled">
              <li><a href="/terms">Terms of Service</a></li>
              <li><a href="/privacy">Privacy Policy</a></li>
            </ul>
          </div>
        </div>
    
        <div class="row visible-xs-block">
          <div class="col-xs-6">
            <ul class="list-unstyled">
              <li><a href="/" class="home">Home</a></li>
              <li><a href="/about">About OpenReview</a></li>
              <li><a href="#" data-toggle="modal" data-target="#feedback-modal">Feedback</a></li>
            </ul>
          </div>
    
          <div class="col-xs-6">
            <ul class="list-unstyled">
              <li><a href="/contact">Contact</a></li>
              <li><a href="/terms">Terms of Service</a></li>
              <li><a href="/privacy">Privacy Policy</a></li>
            </ul>
          </div>
        </div>
      </div>
    </footer>
    
    <footer class="sponsor">
      <div class="container">
        <div class="row">
          <div class="col-sm-10 col-sm-offset-1">
            <p class="text-center">
              OpenReview is created by the <a href="http://www.iesl.cs.umass.edu/" target="_blank">Information Extraction and Synthesis Laboratory</a>, College of Information and Computer Science, University of Massachusetts Amherst. We gratefully acknowledge the support of the OpenReview sponsors:  Google,  Facebook, NSF, the University of Massachusetts Amherst Center for Data Science, and Center for Intelligent Information Retrieval, as well as the Google Cloud Platform for donating the computing and networking services on which OpenReview.net runs.
            </p>
          </div>
        </div>
      </div>
    </footer>

  <div id="feedback-modal" class="modal fade" tabindex="-1" role="dialog">
    <div class="modal-dialog">
      <div class="modal-content">
        <div class="modal-header">
          <button type="button" class="close" data-dismiss="modal" aria-label="Close"><span aria-hidden="true">×</span></button>
          <h3 class="modal-title">Send Feedback</h3>
        </div>
        <div class="modal-body">
          <p>Enter your feedback below and we'll get back to you as soon as possible.</p>
          <form action="/feedback" method="POST">
            <div class="form-group">
              <input type="email" name="from" class="form-control" placeholder="Email" />
            </div>
            <div class="form-group">
              <input type="text" name="subject" class="form-control" placeholder="Subject" />
            </div>
            <div class="form-group">
              <textarea name="message" class="form-control feedback-input" rows="5" placeholder="Message" required=""></textarea>
            </div>
          <ul id="ui-id-2" tabindex="0" class="ui-menu ui-widget ui-widget-content" style="display: none;"></ul></form>
        </div>
        <div class="modal-footer">
          <button type="button" class="btn btn-default" data-dismiss="modal">Cancel</button>
          <button type="button" class="btn btn-primary">Send</button>
        </div>
      </div>
    </div>
  </div>

  <script type="text/javascript" async="" src="https://www.google-analytics.com/analytics.js"></script><script src="//cdnjs.cloudflare.com/ajax/libs/jquery/2.2.4/jquery.min.js"></script>
  <script>window.jQuery || document.write('&lt;script src="/static/js/vendor/jquery-2.2.4.min.js"&gt;&lt;\/script&gt;')</script>

  <script src="/static/dist/vendor.min.js"></script>
  <script src="/static/dist/templates.min.js"></script>
  <script src="/static/dist/openreview.min.js"></script>

    <script>window.legacyScripts = true;</script>


    <script async="" src="https://www.googletagmanager.com/gtag/js?id=UA-108703919-1"></script>
    <script>
      window.dataLayer = window.dataLayer || [];
      function gtag() { dataLayer.push(arguments); }
      gtag('js', new Date());
      gtag('config', 'UA-108703919-1');
    </script>


<div role="status" aria-live="assertive" aria-relevant="additions" class="ui-helper-hidden-accessible"></div></body></html>