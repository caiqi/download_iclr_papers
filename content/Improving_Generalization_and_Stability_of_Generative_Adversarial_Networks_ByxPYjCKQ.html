<!DOCTYPE html><html xmlns="http://www.w3.org/1999/xhtml" class="no-js"><head>
  <meta charset="utf-8" />
  <meta http-equiv="X-UA-Compatible" content="IE=edge,chrome=1" />
  <meta name="viewport" content="width=device-width, initial-scale=1" />

  <title>Improving Generalization and Stability of Generative Adversarial Networks | OpenReview</title>
  <meta name="description" content="" />

      <meta name="citation_title" content="Improving Generalization and Stability of Generative Adversarial Networks" />
        <meta name="citation_author" content="Anonymous" />
      <meta name="citation_publication_date" content="2018/09/27" />
      <meta name="citation_online_date" content="2018/09/27" />
      <meta name="citation_pdf_url" content="https://openreview.net/pdf?id=ByxPYjC5KQ" />
      <meta name="twitter:card" content="summary" />
      <meta name="twitter:site" content="@openreviewnet" />
      <meta name="og:title" content="Improving Generalization and Stability of Generative Adversarial..." />
      <meta name="og:description" content="Generative Adversarial Networks (GANs) are one of the most popular tools for learning complex high dimensional distributions. However, generalization properties of GANs have not been well..." />
      <meta name="og:image" content="https://openreview.net/static/images/openreview_logo_512.png" />

  <link rel="stylesheet" href="https://fonts.googleapis.com/css?family=Noto+Sans:400,400i,700,700i" />

  <link rel="stylesheet" href="/static/css/bootstrap.min.css" />
  <link rel="stylesheet" href="/static/css/jquery-ui.min.css" />
  <link rel="stylesheet" href="/static/css/main.min.css" />
</head>

<body class="forum">
  <nav class="navbar navbar-inverse navbar-fixed-top" role="navigation">
    <div class="container">
  
      <div class="navbar-header">
        <button type="button" class="navbar-toggle collapsed" data-toggle="collapse" data-target="#navbar" aria-expanded="false" aria-controls="navbar">
          <span class="sr-only">Toggle navigation</span>
          <span class="icon-bar"></span>
          <span class="icon-bar"></span>
          <span class="icon-bar"></span>
        </button>
        <a class="navbar-brand home push-link" href="/"><strong>OpenReview</strong>.net</a>
      </div>
  
      <div id="navbar" class="navbar-collapse collapse">
        <form class="navbar-form navbar-left profile-search" role="search">
          <div class="form-group has-feedback">
            <input id="search_input" type="text" class="form-control ui-autocomplete-input" placeholder="Search ICLR 2019 Conference" autocomplete="off" />
            <span class="glyphicon glyphicon-search form-control-feedback" aria-hidden="true"></span>
          </div>
  
          <input id="search_group" type="hidden" value="ICLR.cc/2019/Conference" />
          <input id="search_content" type="hidden" value="all" />
        <ul id="ui-id-1" tabindex="0" class="ui-menu ui-widget ui-widget-content ui-autocomplete ui-front" style="display: none;"></ul></form>
  
        <ul class="nav navbar-nav navbar-right">
        
            <li id="user-menu"><a href="/login">Login</a></li>
        </ul>
      </div>
  
    </div>
  </nav>

  <div id="or-banner" class="banner" style="">
  <div class="container">
    <div class="row">
      <div class="col-xs-12"><a href="/group?id=ICLR.cc/2019/Conference" title="Venue Homepage"><img class="icon" src="/static/images/arrow_left.svg" /> Go to <strong>ICLR 2019 Conference</strong> homepage</a></div>
    </div>
  </div>
</div>
<div id="flash-message-container" class="alert alert-danger" role="alert" style="display: none;">
  <div class="container">
    <div class="row">
      <div class="col-xs-12">
        <div class="alert-content">
          <button type="button" class="close" aria-label="Close"><span aria-hidden="true">Ã—</span></button>
        </div>
      </div>
    </div>
  </div>
</div>

<div class="container">
  <div class="row">
    <div class="col-xs-12">
      <main id="content" class="clearfix openbanner-visible legacy-styles"><div id="note_ByxPYjC5KQ" class="note panel"><div class="title_pdf_row clearfix"><h2 class="note_content_title"><a>Improving Generalization and Stability of Generative Adversarial Networks</a> <a class="note_content_pdf" href="/pdf?id=ByxPYjC5KQ" title="Download PDF" target="_blank"><img src="/static/images/pdf_icon_blue.svg" /></a> </h2></div><div class="meta_row"><span class="signatures">Anonymous</span></div><div class="clearfix"><div class="meta_row pull-left"><span class="date item">28 Sep 2018 (modified: 19 Nov 2018)</span><span class="item">ICLR 2019 Conference Blind Submission</span><span class="item">Readers: <span class="readers-icon glyphicon glyphicon-globe"></span> Everyone</span><span class="item"><a class="action-bibtex-modal" data-bibtex="@inproceedings{    &#10;anonymous2019improving,    &#10;title={Improving Generalization and Stability of Generative Adversarial Networks},    &#10;author={Anonymous},    &#10;booktitle={Submitted to International Conference on Learning Representations},    &#10;year={2019},    &#10;url={https://openreview.net/forum?id=ByxPYjC5KQ},    &#10;note={under review}    &#10;}">Show Bibtex</a></span><a class="note_content_pdf item" href="/revisions?id=ByxPYjC5KQ" target="_blank">Show Revisions</a></div><div class="meta_row meta_actions"></div></div><div class="note_contents"><span class="note_content_field">Abstract: </span><span class="note_content_value">Generative Adversarial Networks (GANs) are one of the most popular tools for learning complex high dimensional distributions. However, generalization properties of GANs have not been well understood. In this paper, we analyze the generalization of GANs in practical settings. We show that discriminators trained on discrete datasets with the original GAN loss have poor generalization capability and do not approximate the theoretically optimal discriminator. We propose a zero-centered gradient penalty for improving the generalization of the discriminator by pushing it toward the optimal discriminator. The penalty guarantees the generalization and convergence of GANs. Experiments on synthetic and large scale datasets verify our theoretical analysis.
</span></div><div class="note_contents"><span class="note_content_field">Keywords: </span><span class="note_content_value">GAN, generalization, gradient penalty, zero centered, convergence</span></div><div class="note_contents"><span class="note_content_field">TL;DR: </span><span class="note_content_value">We propose a zero-centered gradient penalty for improving generalization and stability of GANs</span></div><div class="reply_row clearfix"><div class="item" id="reply_count">10 Replies</div></div></div><hr class="small" /><div id="note_children"><div class="note_with_children"><div id="note_rJg7SvH5hQ" class="note panel"><div class="title_pdf_row clearfix"><h2 class="note_content_title"><a>Solid analytical and experimental exploration concerning GAN generalizability</a>  <button class="btn btn-xs btn-default permalink-button" title="Link to this comment" data-permalink-url="https://openreview.net/forum?id=ByxPYjC5KQ&amp;noteId=rJg7SvH5hQ"><span class="glyphicon glyphicon-link" aria-hidden="true"></span></button></h2></div><div class="meta_row"><span class="signatures">ICLR 2019 Conference Paper452 AnonReviewer2</span></div><div class="clearfix"><div class="meta_row pull-left"><span class="date item">03 Nov 2018 (modified: 07 Nov 2018)</span><span class="item">ICLR 2019 Conference Paper452 Official Review</span><span class="item">Readers: <span class="readers-icon glyphicon glyphicon-globe"></span> Everyone</span></div><div class="meta_row meta_actions"></div></div><div class="note_contents"><span class="note_content_field">Review: </span><span class="note_content_value">The primary innovation of this paper seems focused towards increasing the generalization of GANs, while also maintaining convergence and preventing mode collapse.

The authors first discuss common pitfalls concerning the generalization capability of discriminators, providing analytical underpinnings for their later experimental results. Specifically, they address the problem of gradient explosion in discriminators. 

The authors then suggest that a zero-centered gradient penalty (0-GP) can be helpful in addressing this issue. 0-GPs are regularly used in GANs, but the authors point out that the purpose is usually to  provide convergence, not to increase generalizability. Non-zero centered penalties can give a convergence guarantee but, the authors, assert, can allow overfitting. A 0-GP can give the same guarantees but without allowing overfitting to occur.


The authors then verify these assertions through experimentation on synthetic data, as well as MNIST and ImageNet. My only issue here is that very little information was given about the size of the training sets. Did they use all the samples? Some portion? It is not clear from reading. This would be a serious impediment to reproducibility.

All in all, however, the authors provide a convincing  combination of analysis and experimentation. I believe this paper should be accepted into ICLR.

Note: there is an error on page 9, in Figure 3. The paragraph explanation should list that the authors' 0-GP is figure 3(e). They list (d) twice.

</span></div><div class="note_contents"><span class="note_content_field">Rating: </span><span class="note_content_value">7: Good paper, accept</span></div><div class="note_contents"><span class="note_content_field">Confidence: </span><span class="note_content_value">3: The reviewer is fairly confident that the evaluation is correct</span></div><div class="reply_row clearfix"></div></div><div class="children"><div class="note_with_children"><div id="note_ByeSlr4gR7" class="note panel"><div class="title_pdf_row clearfix"><h2 class="note_content_title"><a>Revision uploaded</a>  <button class="btn btn-xs btn-default permalink-button" title="Link to this comment" data-permalink-url="https://openreview.net/forum?id=ByxPYjC5KQ&amp;noteId=ByeSlr4gR7"><span class="glyphicon glyphicon-link" aria-hidden="true"></span></button></h2></div><div class="meta_row"><span class="signatures">ICLR 2019 Conference Paper452 Authors</span></div><div class="clearfix"><div class="meta_row pull-left"><span class="date item">19 Nov 2018</span><span class="item">ICLR 2019 Conference Paper452 Official Comment</span><span class="item">Readers: <span class="readers-icon glyphicon glyphicon-globe"></span> Everyone</span></div><div class="meta_row meta_actions"></div></div><div class="note_contents"><span class="note_content_field">Comment: </span><span class="note_content_value">Thank you for your constructive review. We have updated our paper to address your concerns. The changes are summarized as follow:

1. A background section is added with basic information about GANs and a definition of generalization. A table summarizing the referred gradient penalties is also added.

2. We extended the Related works section to include papers which address the mode collapse problem. The writing of this part and the whole paper was revised.

3. Another MNIST experiment is added to Section 6.1 to further demonstrate the effectiveness of our method in preventing overfitting. Specifically, our GAN-0-GP is the only GAN that could learn to generate realistic samples when the discriminator is updated 5 times per generator update. 

4. WGAN-GP is included to our ImageNet experiment. Our GAN-0-GP outperforms WGAN-GP by a large margin. 

5. Implementation details are added to the appendix. The code for all experiments will be released after the review process.

6. We added the analysis for the 'mode jumping' problem to Section 6.2. We showed that GAN-0-GP-sample suffers from the problem. On the other hand, our GAN-0-GP is robust to the problem and is able to produce better interpolation between modes.

7. A new algorithm for finding a better path between a pair of samples is added to our paper. </span></div><div class="reply_row clearfix"></div></div><div class="children"></div></div><div class="note_with_children"><div id="note_Skgb5H01TQ" class="note panel"><div class="title_pdf_row clearfix"><h2 class="note_content_title"><a>Response to reviewer 2</a>  <button class="btn btn-xs btn-default permalink-button" title="Link to this comment" data-permalink-url="https://openreview.net/forum?id=ByxPYjC5KQ&amp;noteId=Skgb5H01TQ"><span class="glyphicon glyphicon-link" aria-hidden="true"></span></button></h2></div><div class="meta_row"><span class="signatures">ICLR 2019 Conference Paper452 Authors</span></div><div class="clearfix"><div class="meta_row pull-left"><span class="date item">07 Nov 2018</span><span class="item">ICLR 2019 Conference Paper452 Official Comment</span><span class="item">Readers: <span class="readers-icon glyphicon glyphicon-globe"></span> Everyone</span></div><div class="meta_row meta_actions"></div></div><div class="note_contents"><span class="note_content_field">Comment: </span><span class="note_content_value">Thank you for your review. We will revise our paper according to your suggestion. We would like to quickly address your question about the experiment here. For MNIST and ImageNet experiment, the whole dataset was used. For the ImageNet experiment, we used the code from [4]. Details about all experiments will be added to the appendix. We thank you for pointing the typo in Figure 3. 

We will also add an in-depth discussion about our method and other related works to our next revision as suggested by other reviewers. </span></div><div class="reply_row clearfix"></div></div><div class="children"></div></div></div></div><div class="note_with_children"><div id="note_S1lCpY4dn7" class="note panel"><div class="title_pdf_row clearfix"><h2 class="note_content_title"><a>good discussion of generalization and stability of GAN and the gradient penalty method is promising</a>  <button class="btn btn-xs btn-default permalink-button" title="Link to this comment" data-permalink-url="https://openreview.net/forum?id=ByxPYjC5KQ&amp;noteId=S1lCpY4dn7"><span class="glyphicon glyphicon-link" aria-hidden="true"></span></button></h2></div><div class="meta_row"><span class="signatures">ICLR 2019 Conference Paper452 AnonReviewer1</span></div><div class="clearfix"><div class="meta_row pull-left"><span class="date item">01 Nov 2018 (modified: 07 Nov 2018)</span><span class="item">ICLR 2019 Conference Paper452 Official Review</span><span class="item">Readers: <span class="readers-icon glyphicon glyphicon-globe"></span> Everyone</span></div><div class="meta_row meta_actions"></div></div><div class="note_contents"><span class="note_content_field">Review: </span><span class="note_content_value">The paper discusses the generalization capability of GAN especially from the discriminator's perspective. The explanation is clear and the method is promising. The proposed gradient penalty method that penalizes the unseen samples is novel and reasonable from the explanation, although these methods has been proposed before in different forms. 

Pros:
1. Nice explanation of why the training of GAN is not stable and the modes often collapse.
2. Experiments show that the new 0-gradient penalty method seems promising to improve the generalization capability of GAN and helps to resist mode collapsing.

Cons:
1. The paper does not have a clear definition of the generalization capability of the network.
2. The straight line segment between real and fake images seems not a good option as the input images may live on low-dimensional manifolds. 
3. Why samples alpha in (7) uniformly? It seems the sampling rate should relate with its value. Intuitively, the closer to the real image the sampling point is, the larger the penalty should be.
</span></div><div class="note_contents"><span class="note_content_field">Rating: </span><span class="note_content_value">7: Good paper, accept</span></div><div class="note_contents"><span class="note_content_field">Confidence: </span><span class="note_content_value">3: The reviewer is fairly confident that the evaluation is correct</span></div><div class="reply_row clearfix"></div></div><div class="children"><div class="note_with_children"><div id="note_HJgkLVVlCX" class="note panel"><div class="title_pdf_row clearfix"><h2 class="note_content_title"><a>Revision uploaded</a>  <button class="btn btn-xs btn-default permalink-button" title="Link to this comment" data-permalink-url="https://openreview.net/forum?id=ByxPYjC5KQ&amp;noteId=HJgkLVVlCX"><span class="glyphicon glyphicon-link" aria-hidden="true"></span></button></h2></div><div class="meta_row"><span class="signatures">ICLR 2019 Conference Paper452 Authors</span></div><div class="clearfix"><div class="meta_row pull-left"><span class="date item">19 Nov 2018</span><span class="item">ICLR 2019 Conference Paper452 Official Comment</span><span class="item">Readers: <span class="readers-icon glyphicon glyphicon-globe"></span> Everyone</span></div><div class="meta_row meta_actions"></div></div><div class="note_contents"><span class="note_content_field">Comment: </span><span class="note_content_value">Thank you for your review and questions. We have performed additional experiments and analysis to consolidate our finding. The updates are as follows:

1. A background section is added with basic information about GANs and a definition of generalization. A table summarizing the referred gradient penalties is also added.

2. We extended the Related works section to include papers which address the mode collapse problem. The writing of this part and the whole paper was revised.

3. Another MNIST experiment is added to Section 6.1 to further demonstrate the effectiveness of our method in preventing overfitting. Specifically, our GAN-0-GP is the only GAN that could learn to generate realistic samples when the discriminator is updated 5 times per generator update. 

4. WGAN-GP is included to our ImageNet experiment. Our GAN-0-GP outperforms WGAN-GP by a large margin. 

5. Implementation details are added to the appendix. The code for all experiments will be released after the review process.

6. We added the analysis for the 'mode jumping' problem to Section 6.2. We showed that GAN-0-GP-sample suffers from the problem. On the other hand, our GAN-0-GP is robust to the problem and is able to produce better interpolation between modes.

7. A new algorithm for finding a better path between a pair of samples is added to our paper. </span></div><div class="reply_row clearfix"></div></div><div class="children"></div></div><div class="note_with_children"><div id="note_H1gwUJ0yTQ" class="note panel"><div class="title_pdf_row clearfix"><h2 class="note_content_title"><a>Response to reviewer 1</a>  <button class="btn btn-xs btn-default permalink-button" title="Link to this comment" data-permalink-url="https://openreview.net/forum?id=ByxPYjC5KQ&amp;noteId=H1gwUJ0yTQ"><span class="glyphicon glyphicon-link" aria-hidden="true"></span></button></h2></div><div class="meta_row"><span class="signatures">ICLR 2019 Conference Paper452 Authors</span></div><div class="clearfix"><div class="meta_row pull-left"><span class="date item">07 Nov 2018</span><span class="item">ICLR 2019 Conference Paper452 Official Comment</span><span class="item">Readers: <span class="readers-icon glyphicon glyphicon-globe"></span> Everyone</span></div><div class="meta_row meta_actions"></div></div><div class="note_contents"><span class="note_content_field">Comment: </span><span class="note_content_value">Thank you for your constructive comments. We would like to address your concerns as follow:
1. Generalization has been defined in [1, 2, 3]. They were cited in our paper. Because of the space limit, we could not include their definition in the first version of our paper. We will add the definition from [1] to the updated version. The definition in [1] is directly related to our discussion: if the Lipschitz constant is 0, then the network has the maximum generalization capability and no discriminative power. As stated in our paper, our gradient penalty makes the network generalizable while remaining discriminative. 

2. We agree that the straight line is not a good option for real data like images. However, it's the cheapest way to implement our method. We are working on an improved version for the GP, which we briefly describe below. We plan to include the result in the next revision of our paper.

For all interpolated points to be in the same set with the two endpoints, the set must be convex. $supp(p_g) U supp(p_r)$ is generally not convex so linear interpolation in the data space cannot guarantee that every interpolated point to be in the support.  A solution to this problem is to force the set of latent code $z$ to be convex and perform the interpolation in the latent space. This requires an additional encoder E to encode a datapoint $x$ to a latent code $z_x$. The process of sampling a datapoint for regularization is as follow
(i) Sample a noise vector $z ~ p_z$, generate a fake datapoint $y = G(z)$
(ii) Sample a real datapoint $x ~ p_r$, get the latent code of $z_x = E(x)$
(iii) Generate the interpolated latent code: $\tilde(z) = \alpha z_x + (1 - \alpha) z$
(iv) Generate the interpolated datapoint: $\tilde(x) = G(\tilde(z))$
(v) Apply gradient penalty on $\tilde(x)$
$\tilde(x)$ is more likely to lie on the data manifold than the weighted sum of a real and a fake sample. Regularizing the gradient w.r.t. $\tilde(x)$ will allow better generalization and discrimination.

3. We are not sure that your suggestion is correct. As discussed in our paper, gradient exploding tends to happen near the decision boundary, while the gradient near real/fake datapoints tends to vanish. We doubt that increasing the sampling rate near real/fake datapoints will lead to better result. 


[1] Generalization and Equilibrium in Generative Adversarial Nets (GANs). Sanjeev Arora, Rong Ge, Yingyu Liang, Tengyu Ma, Yi Zhang.
[2] Do GANs actually learn the distribution? An empirical study. Sanjeev Arora, Yi Zhang.
[3] On the Discrimination-Generalization Tradeoff in GANs. Pengchuan Zhang, Qiang Liu, Dengyong Zhou, Tao Xu, Xiaodong He.
[4] Which Training Methods for GANs do actually Converge? Lars Mescheder, Andreas Geiger, Sebastian Nowozin.
</span></div><div class="reply_row clearfix"></div></div><div class="children"></div></div></div></div><div class="note_with_children"><div id="note_Bkgv8HHx3Q" class="note panel"><div class="title_pdf_row clearfix"><h2 class="note_content_title"><a>An interesting read on the convergence of GANs with gradient penalties, lacking comparisons to WGAN-GP</a>  <button class="btn btn-xs btn-default permalink-button" title="Link to this comment" data-permalink-url="https://openreview.net/forum?id=ByxPYjC5KQ&amp;noteId=Bkgv8HHx3Q"><span class="glyphicon glyphicon-link" aria-hidden="true"></span></button></h2></div><div class="meta_row"><span class="signatures">ICLR 2019 Conference Paper452 AnonReviewer3</span></div><div class="clearfix"><div class="meta_row pull-left"><span class="date item">26 Oct 2018 (modified: 07 Nov 2018)</span><span class="item">ICLR 2019 Conference Paper452 Official Review</span><span class="item">Readers: <span class="readers-icon glyphicon glyphicon-globe"></span> Everyone</span></div><div class="meta_row meta_actions"></div></div><div class="note_contents"><span class="note_content_field">Review: </span><span class="note_content_value">Summary: 
The paper proposes to add to the original GAN (2014) loss a zero-centered gradient penalty as the one defined in the WGAN-GP paper. It also provides an analysis on the mode collapse and lack of stability of classical GANs. The authors compare results using their penalty on a few synthetic examples and on image net dogs generations to results using the classical GAN loss with or without gradient penalties. 

Positive points:
The paper is interesting to read and well illustrated. 
An experiment on imagenet illustrates the progress that can be achieved by the proposed penalty.

Points to improve: 

If I understood correctly, the main contribution resides in the application of the GP proposed by WGAN-GP to the original setting. Why not compare results to WGAN-GP in this case? Since the proposal of GANs, many papers addressed the mode collapse problem. WGAN-GP, VEEGAN, or Lucas et al arXiv:1806.07185, ICML 2018 to name only a few. 
The related work section looks incomplete with some missing related references as mentioned above, and copy of a segment that appears in the introduction. 
The submission could maybe improved by segmenting the work into intro / related / background (with clear equations presenting the existing GP) / analysis / approach / experiments
The experiments on synthetic data could be improved: for reproducibility, many works on GANs used the same synthetic data as VEEGAN. 
The imagenet experiment lacks details.   </span></div><div class="note_contents"><span class="note_content_field">Rating: </span><span class="note_content_value">5: Marginally below acceptance threshold</span></div><div class="note_contents"><span class="note_content_field">Confidence: </span><span class="note_content_value">4: The reviewer is confident but not absolutely certain that the evaluation is correct</span></div><div class="reply_row clearfix"></div></div><div class="children"><div class="note_with_children"><div id="note_BJgwGQVxRQ" class="note panel"><div class="title_pdf_row clearfix"><h2 class="note_content_title"><a>Revision uploaded</a>  <button class="btn btn-xs btn-default permalink-button" title="Link to this comment" data-permalink-url="https://openreview.net/forum?id=ByxPYjC5KQ&amp;noteId=BJgwGQVxRQ"><span class="glyphicon glyphicon-link" aria-hidden="true"></span></button></h2></div><div class="meta_row"><span class="signatures">ICLR 2019 Conference Paper452 Authors</span></div><div class="clearfix"><div class="meta_row pull-left"><span class="date item">19 Nov 2018</span><span class="item">ICLR 2019 Conference Paper452 Official Comment</span><span class="item">Readers: <span class="readers-icon glyphicon glyphicon-globe"></span> Everyone</span></div><div class="meta_row meta_actions"></div></div><div class="note_contents"><span class="note_content_field">Comment: </span><span class="note_content_value">Thank you again for your suggestions. We have revised our paper to address your concerns as follows:

1. A background section is added with basic information about GANs and a definition of generalization. A table summarizing the referred gradient penalties is also added.

2. We extended the Related works section to include papers which address the mode collapse problem. The writing of this part and the whole paper was revised.

3. Another MNIST experiment is added to Section 6.1 to further demonstrate the effectiveness of our method in preventing overfitting. Specifically, our GAN-0-GP is the only GAN that could learn to generate realistic samples when the discriminator is updated 5 times per generator update. 

4. WGAN-GP is included to our ImageNet experiment. Our GAN-0-GP outperforms WGAN-GP by a large margin. 

5. Implementation details are added to the appendix. The code for all experiments will be released after the review process.

6. We added the analysis for the 'mode jumping' problem to Section 6.2. We showed that GAN-0-GP-sample suffers from the problem. On the other hand, our GAN-0-GP is robust to the problem and is able to produce better interpolation between modes.

7. A new algorithm for finding a better path between a pair of samples is added to our paper. </span></div><div class="reply_row clearfix"></div></div><div class="children"></div></div><div class="note_with_children"><div id="note_HJeB6kCJpQ" class="note panel"><div class="title_pdf_row clearfix"><h2 class="note_content_title"><a>Response to reviewer 3</a>  <button class="btn btn-xs btn-default permalink-button" title="Link to this comment" data-permalink-url="https://openreview.net/forum?id=ByxPYjC5KQ&amp;noteId=HJeB6kCJpQ"><span class="glyphicon glyphicon-link" aria-hidden="true"></span></button></h2></div><div class="meta_row"><span class="signatures">ICLR 2019 Conference Paper452 Authors</span></div><div class="clearfix"><div class="meta_row pull-left"><span class="date item">07 Nov 2018 (modified: 08 Nov 2018)</span><span class="item">ICLR 2019 Conference Paper452 Official Comment</span><span class="item">Readers: <span class="readers-icon glyphicon glyphicon-globe"></span> Everyone</span></div><div class="meta_row meta_actions"></div></div><div class="note_contents"><span class="note_content_field">Comment: </span><span class="note_content_value">Thank you for your comments. We would like to address your concerns as follow.

1. We do not use the gradient penalty in WGAN-GP (1-GP) to improve the original GAN. Our 0-GP, although has a similar form as 1-GP,  is motivated from a very different perspective and produces very different effects. We assume that you find our 0-GP similar to 1-GP because of the use of the straight line from a fake to a real sample. In the response to reviewer 1, we propose a more sophisticated way to find a path from a fake to a real datapoint. The new method highlights the difference between our method and 1-GP.

2. The 0-GP is not the only contribution of our paper. We start by analyzing the generalization of GANs, showing the problem of the original GAN loss. Although generalizability is one of the most desirable properties of generative models, it has not been studied carefully in GAN literature. Based on our analysis, we propose 0-GP to improve the generalization of GANs. On the 8 Gaussian dataset, GAN-0-GP can generate plausible unseen datapoints on the circle, implying better generalization. We show that the original GAN loss makes GAN focuses on generating datapoints in the training dataset. 0-GP-sample proposed in [4] encourages the generator to remember the training samples. That result in the mode jumping behavior: when we perform interpolation between $z_1$ and $z_2$, the output does not smoothly transform from $x_1 = G(z_1)$ to $x_2 = G(z_2)$ but suddenly jump from $x_1$ to $x_2$. The behavior can be seen in figure 8 of BigGAN paper (<a href="https://arxiv.org/abs/1809.11096)." target="_blank" rel="nofollow">https://arxiv.org/abs/1809.11096).</a>

3. We will include WGAN-GP to the baselines for the sake of completeness. However, as discussed in the previous paragraphs and in our paper, WGAN-GP and their 1-GP does not address the same problem as our 0-GP. 

As discussed in our paper, 1-GP does not help improving generalization in GANs. [4] even showed that 1-GP does not help WGAN (and the original GAN as well) to converge to an equilibrium. The phenomenon can be seen in our MNIST experiment where GAN-1-GP fails to produce any realistic samples after 10,000 iterations. It has been observed that WGAN-1-GP does not converge to an equilibrium, the generator continues to map the same noise to different modes as the training continues. In our synthetic experiment, WGAN-1-GP is less robust to change in hyper-parameters than GAN-0-GP. Detailed results will be included in our revision. Please refer to [4] for more in-depth discussion about the non-convergence of WGAN-GP. 

When $p_g$ is the same as $p_r$, the gradient of the optimal discriminator in GAN and the optimal critic in WGAN must be 0. Any non-zero centered GP will not help GANs to converge to the optimal equilibrium.  Our 0-GP helps to improve both generalization and convergence of GANs. Our 0-GP can be applied to WGAN as well. 

Similar to the original GAN, WGAN and WGAN-GP can overfit to the dataset: the distance output by the critic can be larger than the Wasserstein distance between the two distributions. However, overfitting in WGAN and WGAN-GP is not as severe as in GAN. This is partly because the gradient in WGAN and WGAN-GP does not explode so mode collapse is much harder to observe. 

4. We will include more related works to our paper. The vast body of work on GANs makes it difficult to find all related works. We only focus on some key papers on the topic.

Discussion about VEEGAN and Lucas et al. will be added to our next revision. However, we want to emphasize that our work is about improving the generalization of GANs. Reducing mode collapse is related to but is not exactly the same as generalization. As in the 8 Gaussian dataset, a GAN without mode collapse is the one that can generate all 8 modes. A GAN with good generalization should be able to generate unseen datapoints on the circle and to perform smooth interpolation between modes. 

5. We will add more details about the experiments to the appendix. The code for all experiments will be released after the review process. For the imagenet experiment, we used the code from [4] which is available on github. We note that [4] is a state-of-the-art method which is able to help GAN to scale to massive datasets and it is used in BigGAN paper. 

6.Thank you for your suggestion about the paper layout. Adding a table that summarizes referred gradient penalties is a good idea. 

</span></div><div class="reply_row clearfix"></div></div><div class="children"></div></div></div></div></div></main></div>
  </div>
</div>


    <!-- Footer -->
    <footer class="sitemap">
      <div class="container">
        <div class="row hidden-xs">
          <div class="col-sm-4">
            <ul class="list-unstyled">
              <li><a href="/" class="home">Home</a></li>
              <li><a href="/venues">All Venues</a></li>
            </ul>
          </div>
    
          <div class="col-sm-4">
            <ul class="list-unstyled">
              <li><a href="/about">About OpenReview</a></li>
              <li><a href="/contact">Contact</a></li>
              <li><a href="#" data-toggle="modal" data-target="#feedback-modal">Feedback</a></li>
            </ul>
          </div>
    
          <div class="col-sm-4">
            <ul class="list-unstyled">
              <li><a href="/terms">Terms of Service</a></li>
              <li><a href="/privacy">Privacy Policy</a></li>
            </ul>
          </div>
        </div>
    
        <div class="row visible-xs-block">
          <div class="col-xs-6">
            <ul class="list-unstyled">
              <li><a href="/" class="home">Home</a></li>
              <li><a href="/about">About OpenReview</a></li>
              <li><a href="#" data-toggle="modal" data-target="#feedback-modal">Feedback</a></li>
            </ul>
          </div>
    
          <div class="col-xs-6">
            <ul class="list-unstyled">
              <li><a href="/contact">Contact</a></li>
              <li><a href="/terms">Terms of Service</a></li>
              <li><a href="/privacy">Privacy Policy</a></li>
            </ul>
          </div>
        </div>
      </div>
    </footer>
    
    <footer class="sponsor">
      <div class="container">
        <div class="row">
          <div class="col-sm-10 col-sm-offset-1">
            <p class="text-center">
              OpenReview is created by the <a href="http://www.iesl.cs.umass.edu/" target="_blank">Information Extraction and Synthesis Laboratory</a>, College of Information and Computer Science, University of Massachusetts Amherst. We gratefully acknowledge the support of the OpenReview sponsors:  Google,  Facebook, NSF, the University of Massachusetts Amherst Center for Data Science, and Center for Intelligent Information Retrieval, as well as the Google Cloud Platform for donating the computing and networking services on which OpenReview.net runs.
            </p>
          </div>
        </div>
      </div>
    </footer>

  <div id="feedback-modal" class="modal fade" tabindex="-1" role="dialog">
    <div class="modal-dialog">
      <div class="modal-content">
        <div class="modal-header">
          <button type="button" class="close" data-dismiss="modal" aria-label="Close"><span aria-hidden="true">Ã—</span></button>
          <h3 class="modal-title">Send Feedback</h3>
        </div>
        <div class="modal-body">
          <p>Enter your feedback below and we'll get back to you as soon as possible.</p>
          <form action="/feedback" method="POST">
            <div class="form-group">
              <input type="email" name="from" class="form-control" placeholder="Email" />
            </div>
            <div class="form-group">
              <input type="text" name="subject" class="form-control" placeholder="Subject" />
            </div>
            <div class="form-group">
              <textarea name="message" class="form-control feedback-input" rows="5" placeholder="Message" required=""></textarea>
            </div>
          <ul id="ui-id-2" tabindex="0" class="ui-menu ui-widget ui-widget-content" style="display: none;"></ul></form>
        </div>
        <div class="modal-footer">
          <button type="button" class="btn btn-default" data-dismiss="modal">Cancel</button>
          <button type="button" class="btn btn-primary">Send</button>
        </div>
      </div>
    </div>
  </div>

  <script type="text/javascript" async="" src="https://www.google-analytics.com/analytics.js"></script><script src="//cdnjs.cloudflare.com/ajax/libs/jquery/2.2.4/jquery.min.js"></script>
  <script>window.jQuery || document.write('&lt;script src="/static/js/vendor/jquery-2.2.4.min.js"&gt;&lt;\/script&gt;')</script>

  <script src="/static/dist/vendor.min.js"></script>
  <script src="/static/dist/templates.min.js"></script>
  <script src="/static/dist/openreview.min.js"></script>

    <script>window.legacyScripts = true;</script>


    <script async="" src="https://www.googletagmanager.com/gtag/js?id=UA-108703919-1"></script>
    <script>
      window.dataLayer = window.dataLayer || [];
      function gtag() { dataLayer.push(arguments); }
      gtag('js', new Date());
      gtag('config', 'UA-108703919-1');
    </script>


<div role="status" aria-live="assertive" aria-relevant="additions" class="ui-helper-hidden-accessible"></div></body></html>