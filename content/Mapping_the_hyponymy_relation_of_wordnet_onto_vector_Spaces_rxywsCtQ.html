<!DOCTYPE html><html xmlns="http://www.w3.org/1999/xhtml" class="no-js"><head>
  <meta charset="utf-8" />
  <meta http-equiv="X-UA-Compatible" content="IE=edge,chrome=1" />
  <meta name="viewport" content="width=device-width, initial-scale=1" />

  <title>Mapping the hyponymy relation of wordnet onto vector Spaces | OpenReview</title>
  <meta name="description" content="" />

      <meta name="citation_title" content="Mapping the hyponymy relation of wordnet onto vector Spaces" />
        <meta name="citation_author" content="Anonymous" />
      <meta name="citation_publication_date" content="2018/09/27" />
      <meta name="citation_online_date" content="2018/09/27" />
      <meta name="citation_pdf_url" content="https://openreview.net/pdf?id=r1xywsC9tQ" />
      <meta name="twitter:card" content="summary" />
      <meta name="twitter:site" content="@openreviewnet" />
      <meta name="og:title" content="Mapping the hyponymy relation of wordnet onto vector Spaces" />
      <meta name="og:description" content=" In this paper, we investigate mapping the hyponymy relation of&#10;   wordnet to feature vectors.&#10;    We aim to model lexical knowledge in such a way that it can be used as&#10;    input in generic..." />
      <meta name="og:image" content="https://openreview.net/static/images/openreview_logo_512.png" />

  <link rel="stylesheet" href="https://fonts.googleapis.com/css?family=Noto+Sans:400,400i,700,700i" />

  <link rel="stylesheet" href="/static/css/bootstrap.min.css" />
  <link rel="stylesheet" href="/static/css/jquery-ui.min.css" />
  <link rel="stylesheet" href="/static/css/main.min.css" />
</head>

<body class="forum">
  <nav class="navbar navbar-inverse navbar-fixed-top" role="navigation">
    <div class="container">
  
      <div class="navbar-header">
        <button type="button" class="navbar-toggle collapsed" data-toggle="collapse" data-target="#navbar" aria-expanded="false" aria-controls="navbar">
          <span class="sr-only">Toggle navigation</span>
          <span class="icon-bar"></span>
          <span class="icon-bar"></span>
          <span class="icon-bar"></span>
        </button>
        <a class="navbar-brand home push-link" href="/"><strong>OpenReview</strong>.net</a>
      </div>
  
      <div id="navbar" class="navbar-collapse collapse">
        <form class="navbar-form navbar-left profile-search" role="search">
          <div class="form-group has-feedback">
            <input id="search_input" type="text" class="form-control ui-autocomplete-input" placeholder="Search ICLR 2019 Conference" autocomplete="off" />
            <span class="glyphicon glyphicon-search form-control-feedback" aria-hidden="true"></span>
          </div>
  
          <input id="search_group" type="hidden" value="ICLR.cc/2019/Conference" />
          <input id="search_content" type="hidden" value="all" />
        <ul id="ui-id-1" tabindex="0" class="ui-menu ui-widget ui-widget-content ui-autocomplete ui-front" style="display: none;"></ul></form>
  
        <ul class="nav navbar-nav navbar-right">
        
            <li id="user-menu"><a href="/login">Login</a></li>
        </ul>
      </div>
  
    </div>
  </nav>

  <div id="or-banner" class="banner" style="">
  <div class="container">
    <div class="row">
      <div class="col-xs-12"><a href="/group?id=ICLR.cc/2019/Conference" title="Venue Homepage"><img class="icon" src="/static/images/arrow_left.svg" /> Go to <strong>ICLR 2019 Conference</strong> homepage</a></div>
    </div>
  </div>
</div>
<div id="flash-message-container" class="alert alert-danger" role="alert" style="display: none;">
  <div class="container">
    <div class="row">
      <div class="col-xs-12">
        <div class="alert-content">
          <button type="button" class="close" aria-label="Close"><span aria-hidden="true">×</span></button>
        </div>
      </div>
    </div>
  </div>
</div>

<div class="container">
  <div class="row">
    <div class="col-xs-12">
      <main id="content" class="clearfix openbanner-visible legacy-styles"><div id="note_r1xywsC9tQ" class="note panel"><div class="title_pdf_row clearfix"><h2 class="note_content_title"><a>Mapping the hyponymy relation of wordnet onto vector Spaces</a> <a class="note_content_pdf" href="/pdf?id=r1xywsC9tQ" title="Download PDF" target="_blank"><img src="/static/images/pdf_icon_blue.svg" /></a> </h2></div><div class="meta_row"><span class="signatures">Anonymous</span></div><div class="clearfix"><div class="meta_row pull-left"><span class="date item">28 Sep 2018 (modified: 11 Oct 2018)</span><span class="item">ICLR 2019 Conference Blind Submission</span><span class="item">Readers: <span class="readers-icon glyphicon glyphicon-globe"></span> Everyone</span><span class="item"><a class="action-bibtex-modal" data-bibtex="@inproceedings{    &#10;anonymous2019mapping,    &#10;title={Mapping the hyponymy relation of wordnet onto vector Spaces},    &#10;author={Anonymous},    &#10;booktitle={Submitted to International Conference on Learning Representations},    &#10;year={2019},    &#10;url={https://openreview.net/forum?id=r1xywsC9tQ},    &#10;note={under review}    &#10;}">Show Bibtex</a></span></div><div class="meta_row meta_actions"></div></div><div class="note_contents"><span class="note_content_field">Abstract: </span><span class="note_content_value"> In this paper, we investigate mapping the hyponymy relation of
 wordnet to feature vectors.
  We aim to model lexical knowledge in such a way that it can be used as
  input in generic machine-learning models, such as phrase entailment
  predictors.
  We propose two models. The first one leverages an existing mapping of
  words to feature vectors (fasttext), and attempts to classify
  such vectors as within or outside of each class. The second model is fully supervised,
  using solely wordnet as a ground truth. It maps each concept to an
  interval or a disjunction thereof.
  On the first model, we approach, but not quite attain state of the
  art performance. The second model can achieve near-perfect accuracy.
</span></div><div class="note_contents"><span class="note_content_field">Keywords: </span><span class="note_content_value">fasttext, hyponymy, wordnet</span></div><div class="note_contents"><span class="note_content_field">TL;DR: </span><span class="note_content_value">We investigate mapping the hyponymy relation of wordnet to feature vectors</span></div><div class="reply_row clearfix"><div class="item" id="reply_count">4 Replies</div></div></div><hr class="small" /><div id="note_children"><div class="note_with_children"><div id="note_rJxOzhIxaQ" class="note panel"><div class="title_pdf_row clearfix"><h2 class="note_content_title"><a>General response to reviews</a>  <button class="btn btn-xs btn-default permalink-button" title="Link to this comment" data-permalink-url="https://openreview.net/forum?id=r1xywsC9tQ&amp;noteId=rJxOzhIxaQ"><span class="glyphicon glyphicon-link" aria-hidden="true"></span></button></h2></div><div class="meta_row"><span class="signatures">ICLR 2019 Conference Paper226 Authors</span></div><div class="clearfix"><div class="meta_row pull-left"><span class="date item">07 Nov 2018</span><span class="item">ICLR 2019 Conference Paper226 Official Comment</span><span class="item">Readers: <span class="readers-icon glyphicon glyphicon-globe"></span> Everyone</span></div><div class="meta_row meta_actions"></div></div><div class="note_contents"><span class="note_content_field">Comment: </span><span class="note_content_value">Thank you for your reviews. As we see it, the reviewers missed the main point of the paper, hoping that it makes our intent clear. 

We consider the task of mapping wordnet to vector spaces, giving two baselines. 

1. The first baseline is based on dividing fasttext into subspaces corresponding to predicates in wordnet
2. The second baseline computes a simple embedding of predicates to intervals.

The second baseline beats the state of the art, with a much simpler method. Some reviewers claim that it does not generalise --- but we counter that no existing method generalises either (see the paper for an argumentation). So we maintain that we improve on the state of the art.

The first baseline allows for generalisation. Admittedly, its precision is low, but we point out that the same shortcoming is found in state of the art methods.</span></div><div class="reply_row clearfix"></div></div><div class="children"></div></div><div class="note_with_children"><div id="note_ryxzHUBahQ" class="note panel"><div class="title_pdf_row clearfix"><h2 class="note_content_title"><a>An interesting problem, but unclear description and methodological issues </a>  <button class="btn btn-xs btn-default permalink-button" title="Link to this comment" data-permalink-url="https://openreview.net/forum?id=r1xywsC9tQ&amp;noteId=ryxzHUBahQ"><span class="glyphicon glyphicon-link" aria-hidden="true"></span></button></h2></div><div class="meta_row"><span class="signatures">ICLR 2019 Conference Paper226 AnonReviewer3</span></div><div class="clearfix"><div class="meta_row pull-left"><span class="date item">05 Nov 2018 (modified: 07 Nov 2018)</span><span class="item">ICLR 2019 Conference Paper226 Official Review</span><span class="item">Readers: <span class="readers-icon glyphicon glyphicon-globe"></span> Everyone</span></div><div class="meta_row meta_actions"></div></div><div class="note_contents"><span class="note_content_field">Review: </span><span class="note_content_value">* Summary of the paper

This paper studies how hyponymy between words can be mapped to feature representations. To this end, it lists out properties of such mappings and studies two methods from the perspective of how they address these properties.

* Review
The goal of this paper: namely, formalizing the hypernymy relation over vector spaces is not only an interesting one, but also an important one -- being able to do so can help us understand and improve vector representations of words and reason about their quality.

In its execution, however, the paper does not seem ready for publication at this point. Two major issues stand out.

First, several things are unclear about the paper. Here's a partial list:
1. Property 3 is presented as a given. But why is this property necessary or sufficient for defining hyponymy?
2. It is not clear why the measure based definition is introduced. Furthermore, the expression above the statement of property 4 is stated as following from the definition. It may be worth stating why.
3. Section 3.1 is entirely unclear. The plots in Fig 2 are empty. And in the definition of Q on page 4, the predicate that defines the set states P(w, f(x)). But if the range of P is [0, 1], what does it mean as a predicate? Does this mean we restrict it to cases where P(w, f(x)) = 1?

Second, there are methodological concerns about the experiments.
1. In essence, section 3 proposes to create a word-specific linear classifier that decides whether a new vector is a hypernym or not. But this classifier faces huge class imbalance issues, which suggests that simply training a classifier as described can not work (as the authors discovered). So it is not clear what we learn from this section? Especially because the paper says at the just before section 3.1 that "we are ultimately not interested in property 5".
2. Perhaps most importantly, the method in section 4 basically represents a pruned version of WordNet as a collection of intervals. It is not surprising that this gets high recall because the data is explicitly stored in the form of intervals. Unfortunately, however, this means that there is no generalization and the proposed representation for hyponymy essentially remembers WordNet. If we are allowed to do that, then why not just define f(w) to be an indicator for w and P(w, f(w')) to be an indicator for whether the word w is a hyponym of w'. This would give us perfect precision and recall, at the cost of no generalization.

* Minor points   
1. The properties 1 and 2 are essentially saying that the precision and recall respectively are alpha. Is this correct?
2. Should we interpret P as a probability? The paper doesn't explicitly say so, but why not?
3. The paper is written in a somewhat informal style. Some examples:
   - Before introducing property 3, the paper says that it is a "satisfying way". Why/for whom?
   - The part about not admitting defeat (just above section 3.1)
   While these are not bad by themselves, the style tends to be distracting from the point of the paper.

* Missing reference
See: Faruqui, Manaal, Jesse Dodge, Sujay Kumar Jauhar, Chris Dyer, Eduard Hovy, and Noah A. Smith. "Retrofitting Word Vectors to Semantic Lexicons." In Proceedings of the 2015 Conference of the North American Chapter of the Association for Computational Linguistics: Human Language Technologies, pp. 1606-1615. 2015.

This paper and its followup work discusses relationships between word embeddings and relations defined by semantic lexicons, including hyponymy.
</span></div><div class="note_contents"><span class="note_content_field">Rating: </span><span class="note_content_value">3: Clear rejection</span></div><div class="note_contents"><span class="note_content_field">Confidence: </span><span class="note_content_value">4: The reviewer is confident but not absolutely certain that the evaluation is correct</span></div><div class="reply_row clearfix"></div></div><div class="children"></div></div><div class="note_with_children"><div id="note_rJxE1ltc37" class="note panel"><div class="title_pdf_row clearfix"><h2 class="note_content_title"><a>Interesting problem, but contribution (and clarity) currently inadequate</a>  <button class="btn btn-xs btn-default permalink-button" title="Link to this comment" data-permalink-url="https://openreview.net/forum?id=r1xywsC9tQ&amp;noteId=rJxE1ltc37"><span class="glyphicon glyphicon-link" aria-hidden="true"></span></button></h2></div><div class="meta_row"><span class="signatures">ICLR 2019 Conference Paper226 AnonReviewer1</span></div><div class="clearfix"><div class="meta_row pull-left"><span class="date item">03 Nov 2018 (modified: 07 Nov 2018)</span><span class="item">ICLR 2019 Conference Paper226 Official Review</span><span class="item">Readers: <span class="readers-icon glyphicon glyphicon-globe"></span> Everyone</span></div><div class="meta_row meta_actions"></div></div><div class="note_contents"><span class="note_content_field">Review: </span><span class="note_content_value">This paper explores the notion of hyponymy in word vector representations. It tests the capacity of a logistic regression classifier to distinguish words that are and are not hyponyms using fastText embeddings, and it also describes a method of organizing WordNet relations into a tree structure and defining hyponymy based on this structure.

The problem of capturing hyponymy relations within vector space models of word representation is an interesting and important one, but it is not clear to me that this paper has made a substantive contribution to it. The paper seems simply to 1) observe that fastText embeddings are imperfect for hyponymy detection with a linear classifier, and 2) reconstruct the fairly natural interpretation of WordNet relations as a hierarchical tree structure, and to re-extract hyponymy relations from that tree structure. As far as I can tell, the paper's “supervised” model does not use embeddings  (or learning) at all.

Assessing the paper's contribution is made more difficult by an overall lack of clarity. The details of the experiments are not laid out with sufficient explicitness, and the reporting of results is also fairly confusing (I am not clear, for example, on what is depicted in Figure 2). The paper is not organized in a particularly intuitive way, nor has it made clear what the contributions might be. 

Overall, while I think that this is a worthy topic, I do not think that the contribution or the clarity of this paper are currently sufficient for publication. 

Additional comments:

-The PCA plot is too dense to be a useful visual - it would be more useful to plot a smaller number of relevant points.

-Results should be presented more clearly in table form - there seem to be a large number of results that are not reported in any table (for instance, the results described in Section 3).</span></div><div class="note_contents"><span class="note_content_field">Rating: </span><span class="note_content_value">3: Clear rejection</span></div><div class="note_contents"><span class="note_content_field">Confidence: </span><span class="note_content_value">3: The reviewer is fairly confident that the evaluation is correct</span></div><div class="reply_row clearfix"></div></div><div class="children"></div></div><div class="note_with_children"><div id="note_SJefL5lK2m" class="note panel"><div class="title_pdf_row clearfix"><h2 class="note_content_title"><a>An interesting perspective on modeling hyponymy, but doesn't make it over the bar</a>  <button class="btn btn-xs btn-default permalink-button" title="Link to this comment" data-permalink-url="https://openreview.net/forum?id=r1xywsC9tQ&amp;noteId=SJefL5lK2m"><span class="glyphicon glyphicon-link" aria-hidden="true"></span></button></h2></div><div class="meta_row"><span class="signatures">ICLR 2019 Conference Paper226 AnonReviewer2</span></div><div class="clearfix"><div class="meta_row pull-left"><span class="date item">02 Nov 2018 (modified: 07 Nov 2018)</span><span class="item">ICLR 2019 Conference Paper226 Official Review</span><span class="item">Readers: <span class="readers-icon glyphicon glyphicon-globe"></span> Everyone</span></div><div class="meta_row meta_actions"></div></div><div class="note_contents"><span class="note_content_field">Review: </span><span class="note_content_value">This paper reads like the first thoughts and experiments of a physicist or mathematician who has decided to look at word representations and hyponymy. I mean that in both the positive and negative ways that this remark could be construed. On the positive side, it provides an interesting read, with a fresh perspective, willing to poke holes in rather than accepting setups that several previous researchers have used.  On the negative side, though, this paper can have an undue, work-from-scratch mathiness that doesn't really contribute insight or understanding, and the current state of the work is too preliminary. I think another researcher interested in this area could benefit from reading this paper and hearing the perspective it presents. Nevertheless, there just isn't sufficient in the way of strong, non-trivial results in the current paper to justify conference acceptance. 

Quality:

 - Pro
   o Everything is presented in a precise formalized fashion. The paper has interesting remarks and perspectives. I appreciate that the authors not only did find most existing work on modeling hyponymy but provide a detailed and quite insightful discussion of it.  (A related paper from overlapping authors to papers you do cite that maybe should have been included is Chang et al. <a href="https://arxiv.org/abs/1710.00880" target="_blank" rel="nofollow">https://arxiv.org/abs/1710.00880</a> – which is a bit different in trying to learn hyponyms from text not WordNet, but still clearly related.)
  -Con
   o There just isn't enough here in the way of theoretical or experimental results. In the end, two "methods" of hyponymy modeling are presented: one is a simple logistic regression, which is estimated separately PER WORD for words with 10 or more hyponyms. This performs worse than the methods of several recent papers that the author cites. The other is a construction that shows that any tree can be embedded by representing nodes as ranges of the real line. This is true, but trivial. Why don't ML/NLP researchers do this? It's because they want a representation that doesn't only represent the ISA hierarchy but also other aspects of word meaning such as meaning similarity and dimensions of relatedness. Furthermore, in general they would like to learn these representations from data rather than hand-constructing it from an existing source like WordNet. For instance, simply doing that gives no clear way to add other words not in wordnet into the taxonomy. This representation mapping doesn't really give any clear advantage beyond just looking up hyponymy relationships in wordnet when you need them.

Clarity:
 - Pro
   o The paper is in most respects clearly written and enjoyable to read.
 - Con
   o The mathematical style and precision has it's uses, but sometime it just seemed to make things harder to follow. Referring to things throughout as "Property k" – even though some of those properties were given names when first introduced – left me repeatedly flicking up and down through the PDF to refresh myself on what claim was being referred to without any apparent need....

Originality:
 - Pro
   o There is certainly originality of perspective. The authors make some cogent observations on how other prior work has been naive about adopted assumptions and as to what it has achieved (e.g., in the discussion at the start of section 5.1).
 - Con
   o There is not really significant originality of method. The logistic regression model is nothing but straightforward. (It is also highly problematic in learning a separate model for each word with a bunch of hyponyms. This both doesn't give a model that would generalize to novel words or ones with few hyponyms.) Mapping a tree to an interval is fairly trivial, and besides this is just a mapping of representations, it isn't learning a good representation as ML people (or ICLR people) would like. The idea that you can improve recall by using a co-product (disjunction) of intervals is cute, though, I admit. Nice.

Significance 
 - Con
   o I think this work would clearly need more development, and more cognizance of the goals of generalizable representation learning before it would make a significant contribution to the literature. 

Other:
 - p.1: Saying about WordNet etc., "these resources have a fundamentally symbolic representation, which can not be readily used as input to neural NLP models" seems misplaced when there is now a lot of work on producing neural graph embeddings (including node2vec, skip-graphs, deepwalk, etc.). Fundamentally, it is just a bad argument: It is no different to saying that words have a fundamentally symbolic representation which cannot be readily used as input to neural NLP models, but the premise of the whole paper is already that we know how to do that and it isn't hard through the use of word embeddings.
 - p.2: The idea of words and phrases living in subset (and disjointness etc.) relationships according to denotation is the central idea of Natural Logic approaches, and these might be cited here. There are various works, some more philosophical. A good place to start might be: https://nlp.stanford.edu/pubs/natlog-iwcs09.pdf
 - p.2: The notions of Property 1 and 2 are just "precision" and "recall", terms the paper also uses. Do we gain from introducing the names "Property 1" and "Property 2" for them? I also felt that I wouldn't have lost anything if Property 3 was just the idea that hyponymy is represented as vector subspace inclusion.
 - p.2: fn.2: True, but it seems fair to more note that cosine similarity is very standard as a word/document similarity measure, not for modeling hyponymy, for this reason.
 - p.4: Below the equation, shouldn't it be Q(w', w) [not both w'] and then Q(w', w) and not the reverse? If not, I'm misunderstanding.</span></div><div class="note_contents"><span class="note_content_field">Rating: </span><span class="note_content_value">3: Clear rejection</span></div><div class="note_contents"><span class="note_content_field">Confidence: </span><span class="note_content_value">5: The reviewer is absolutely certain that the evaluation is correct and very familiar with the relevant literature</span></div><div class="reply_row clearfix"></div></div><div class="children"></div></div></div></main></div>
  </div>
</div>


    <!-- Footer -->
    <footer class="sitemap">
      <div class="container">
        <div class="row hidden-xs">
          <div class="col-sm-4">
            <ul class="list-unstyled">
              <li><a href="/" class="home">Home</a></li>
              <li><a href="/venues">All Venues</a></li>
            </ul>
          </div>
    
          <div class="col-sm-4">
            <ul class="list-unstyled">
              <li><a href="/about">About OpenReview</a></li>
              <li><a href="/contact">Contact</a></li>
              <li><a href="#" data-toggle="modal" data-target="#feedback-modal">Feedback</a></li>
            </ul>
          </div>
    
          <div class="col-sm-4">
            <ul class="list-unstyled">
              <li><a href="/terms">Terms of Service</a></li>
              <li><a href="/privacy">Privacy Policy</a></li>
            </ul>
          </div>
        </div>
    
        <div class="row visible-xs-block">
          <div class="col-xs-6">
            <ul class="list-unstyled">
              <li><a href="/" class="home">Home</a></li>
              <li><a href="/about">About OpenReview</a></li>
              <li><a href="#" data-toggle="modal" data-target="#feedback-modal">Feedback</a></li>
            </ul>
          </div>
    
          <div class="col-xs-6">
            <ul class="list-unstyled">
              <li><a href="/contact">Contact</a></li>
              <li><a href="/terms">Terms of Service</a></li>
              <li><a href="/privacy">Privacy Policy</a></li>
            </ul>
          </div>
        </div>
      </div>
    </footer>
    
    <footer class="sponsor">
      <div class="container">
        <div class="row">
          <div class="col-sm-10 col-sm-offset-1">
            <p class="text-center">
              OpenReview is created by the <a href="http://www.iesl.cs.umass.edu/" target="_blank">Information Extraction and Synthesis Laboratory</a>, College of Information and Computer Science, University of Massachusetts Amherst. We gratefully acknowledge the support of the OpenReview sponsors:  Google,  Facebook, NSF, the University of Massachusetts Amherst Center for Data Science, and Center for Intelligent Information Retrieval, as well as the Google Cloud Platform for donating the computing and networking services on which OpenReview.net runs.
            </p>
          </div>
        </div>
      </div>
    </footer>

  <div id="feedback-modal" class="modal fade" tabindex="-1" role="dialog">
    <div class="modal-dialog">
      <div class="modal-content">
        <div class="modal-header">
          <button type="button" class="close" data-dismiss="modal" aria-label="Close"><span aria-hidden="true">×</span></button>
          <h3 class="modal-title">Send Feedback</h3>
        </div>
        <div class="modal-body">
          <p>Enter your feedback below and we'll get back to you as soon as possible.</p>
          <form action="/feedback" method="POST">
            <div class="form-group">
              <input type="email" name="from" class="form-control" placeholder="Email" />
            </div>
            <div class="form-group">
              <input type="text" name="subject" class="form-control" placeholder="Subject" />
            </div>
            <div class="form-group">
              <textarea name="message" class="form-control feedback-input" rows="5" placeholder="Message" required=""></textarea>
            </div>
          <ul id="ui-id-2" tabindex="0" class="ui-menu ui-widget ui-widget-content" style="display: none;"></ul></form>
        </div>
        <div class="modal-footer">
          <button type="button" class="btn btn-default" data-dismiss="modal">Cancel</button>
          <button type="button" class="btn btn-primary">Send</button>
        </div>
      </div>
    </div>
  </div>

  <script type="text/javascript" async="" src="https://www.google-analytics.com/analytics.js"></script><script src="//cdnjs.cloudflare.com/ajax/libs/jquery/2.2.4/jquery.min.js"></script>
  <script>window.jQuery || document.write('&lt;script src="/static/js/vendor/jquery-2.2.4.min.js"&gt;&lt;\/script&gt;')</script>

  <script src="/static/dist/vendor.min.js"></script>
  <script src="/static/dist/templates.min.js"></script>
  <script src="/static/dist/openreview.min.js"></script>

    <script>window.legacyScripts = true;</script>


    <script async="" src="https://www.googletagmanager.com/gtag/js?id=UA-108703919-1"></script>
    <script>
      window.dataLayer = window.dataLayer || [];
      function gtag() { dataLayer.push(arguments); }
      gtag('js', new Date());
      gtag('config', 'UA-108703919-1');
    </script>


<div role="status" aria-live="assertive" aria-relevant="additions" class="ui-helper-hidden-accessible"></div></body></html>