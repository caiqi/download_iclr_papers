<!DOCTYPE html><html xmlns="http://www.w3.org/1999/xhtml" class="no-js"><head>
  <meta charset="utf-8" />
  <meta http-equiv="X-UA-Compatible" content="IE=edge,chrome=1" />
  <meta name="viewport" content="width=device-width, initial-scale=1" />

  <title>Boosting Trust Region Policy Optimization by Normalizing flows Policy | OpenReview</title>
  <meta name="description" content="" />

      <meta name="citation_title" content="Boosting Trust Region Policy Optimization by Normalizing flows Policy" />
        <meta name="citation_author" content="Anonymous" />
      <meta name="citation_publication_date" content="2018/09/27" />
      <meta name="citation_online_date" content="2018/09/27" />
      <meta name="citation_pdf_url" content="https://openreview.net/pdf?id=SJNRHiAcYX" />
      <meta name="twitter:card" content="summary" />
      <meta name="twitter:site" content="@openreviewnet" />
      <meta name="og:title" content="Boosting Trust Region Policy Optimization by Normalizing flows Policy" />
      <meta name="og:description" content="We propose to improve trust region policy search with normalizing flows policy. We illustrate that when the trust region is constructed by KL divergence constraint, normalizing flows policy can..." />
      <meta name="og:image" content="https://openreview.net/static/images/openreview_logo_512.png" />

  <link rel="stylesheet" href="https://fonts.googleapis.com/css?family=Noto+Sans:400,400i,700,700i" />

  <link rel="stylesheet" href="/static/css/bootstrap.min.css" />
  <link rel="stylesheet" href="/static/css/jquery-ui.min.css" />
  <link rel="stylesheet" href="/static/css/main.min.css" />
</head>

<body class="forum">
  <nav class="navbar navbar-inverse navbar-fixed-top" role="navigation">
    <div class="container">
  
      <div class="navbar-header">
        <button type="button" class="navbar-toggle collapsed" data-toggle="collapse" data-target="#navbar" aria-expanded="false" aria-controls="navbar">
          <span class="sr-only">Toggle navigation</span>
          <span class="icon-bar"></span>
          <span class="icon-bar"></span>
          <span class="icon-bar"></span>
        </button>
        <a class="navbar-brand home push-link" href="/"><strong>OpenReview</strong>.net</a>
      </div>
  
      <div id="navbar" class="navbar-collapse collapse">
        <form class="navbar-form navbar-left profile-search" role="search">
          <div class="form-group has-feedback">
            <input id="search_input" type="text" class="form-control ui-autocomplete-input" placeholder="Search ICLR 2019 Conference" autocomplete="off" />
            <span class="glyphicon glyphicon-search form-control-feedback" aria-hidden="true"></span>
          </div>
  
          <input id="search_group" type="hidden" value="ICLR.cc/2019/Conference" />
          <input id="search_content" type="hidden" value="all" />
        <ul id="ui-id-1" tabindex="0" class="ui-menu ui-widget ui-widget-content ui-autocomplete ui-front" style="display: none;"></ul></form>
  
        <ul class="nav navbar-nav navbar-right">
        
            <li id="user-menu"><a href="/login">Login</a></li>
        </ul>
      </div>
  
    </div>
  </nav>

  <div id="or-banner" class="banner" style="">
  <div class="container">
    <div class="row">
      <div class="col-xs-12"><a href="/group?id=ICLR.cc/2019/Conference" title="Venue Homepage"><img class="icon" src="/static/images/arrow_left.svg" /> Go to <strong>ICLR 2019 Conference</strong> homepage</a></div>
    </div>
  </div>
</div>
<div id="flash-message-container" class="alert alert-danger" role="alert" style="display: none;">
  <div class="container">
    <div class="row">
      <div class="col-xs-12">
        <div class="alert-content">
          <button type="button" class="close" aria-label="Close"><span aria-hidden="true">×</span></button>
        </div>
      </div>
    </div>
  </div>
</div>

<div class="container">
  <div class="row">
    <div class="col-xs-12">
      <main id="content" class="clearfix openbanner-visible legacy-styles"><div id="note_SJNRHiAcYX" class="note panel"><div class="title_pdf_row clearfix"><h2 class="note_content_title"><a>Boosting Trust Region Policy Optimization by Normalizing flows Policy</a> <a class="note_content_pdf" href="/pdf?id=SJNRHiAcYX" title="Download PDF" target="_blank"><img src="/static/images/pdf_icon_blue.svg" /></a> </h2></div><div class="meta_row"><span class="signatures">Anonymous</span></div><div class="clearfix"><div class="meta_row pull-left"><span class="date item">28 Sep 2018 (modified: 12 Nov 2018)</span><span class="item">ICLR 2019 Conference Blind Submission</span><span class="item">Readers: <span class="readers-icon glyphicon glyphicon-globe"></span> Everyone</span><span class="item"><a class="action-bibtex-modal" data-bibtex="@inproceedings{    &#10;anonymous2019boosting,    &#10;title={Boosting Trust Region Policy Optimization by Normalizing flows Policy},    &#10;author={Anonymous},    &#10;booktitle={Submitted to International Conference on Learning Representations},    &#10;year={2019},    &#10;url={https://openreview.net/forum?id=SJNRHiAcYX},    &#10;note={under review}    &#10;}">Show Bibtex</a></span><a class="note_content_pdf item" href="/revisions?id=SJNRHiAcYX" target="_blank">Show Revisions</a></div><div class="meta_row meta_actions"></div></div><div class="note_contents"><span class="note_content_field">Abstract: </span><span class="note_content_value">We propose to improve trust region policy search with normalizing flows policy. We illustrate that when the trust region is constructed by KL divergence constraint, normalizing flows policy can generate samples far from the 'center' of the previous policy iterate, which potentially enables better exploration and helps avoid bad local optima. We show that normalizing flows policy significantly improves upon factorized Gaussian policy baseline, with both TRPO and ACKTR, especially on tasks with complex dynamics such as Humanoid.</span></div><div class="note_contents"><span class="note_content_field">Keywords: </span><span class="note_content_value">Reinforcement Learning, Normalizing Flows</span></div><div class="note_contents"><span class="note_content_field">TL;DR: </span><span class="note_content_value">Normalizing flows policy to improve TRPO and ACKTR</span></div><div class="reply_row clearfix"><div class="item" id="reply_count">6 Replies</div></div></div><hr class="small" /><div id="note_children"><div class="note_with_children"><div id="note_rkeZNVXihX" class="note panel"><div class="title_pdf_row clearfix"><h2 class="note_content_title"><a>Simple modification with relatively robust gains</a>  <button class="btn btn-xs btn-default permalink-button" title="Link to this comment" data-permalink-url="https://openreview.net/forum?id=SJNRHiAcYX&amp;noteId=rkeZNVXihX"><span class="glyphicon glyphicon-link" aria-hidden="true"></span></button></h2></div><div class="meta_row"><span class="signatures">ICLR 2019 Conference Paper135 AnonReviewer3</span></div><div class="clearfix"><div class="meta_row pull-left"><span class="date item">03 Nov 2018 (modified: 07 Nov 2018)</span><span class="item">ICLR 2019 Conference Paper135 Official Review</span><span class="item">Readers: <span class="readers-icon glyphicon glyphicon-globe"></span> Everyone</span></div><div class="meta_row meta_actions"></div></div><div class="note_contents"><span class="note_content_field">Review: </span><span class="note_content_value">The authors in this work present an approach to policy optimization that relies on an alternative policy formulation based on normalizing flows. This is a relatively simple modification (this is no criticism) that essentially uses the same TRPO algorithm as previous approaches, but a different mechanism for generating the distribution over actions. The crux of the authors’ approach is detailed in equations (6) and (7), although it could have been useful to see more of the discussion of the architecture from appendix B in the actual text of the paper.

The authors then go on to analyze the properties and expressiveness of the resulting properties and show that it is more capable of capturing complex interactions than a simple Gaussian. It was somewhat unclear, however, in section 4.2 what the exact form of the policies being compared are. Is this a simple example with only the parameters of the Gaussian, or was the Gaussian parameterized by a multi-layer model? Further, one thing I would also have liked to see the authors question more is, for the problems they attack, whether this expressiveness is more useful “during exploration” or for the ultimate performance of the final policy.

The authors, finally, show that this approach is able to out-perform the alternative Gaussian policy. Ultimately this approach seems to be a simple modification (or replacement) of the standard policy formulation, and one that seems to lead to good performance gains. </span></div><div class="note_contents"><span class="note_content_field">Rating: </span><span class="note_content_value">6: Marginally above acceptance threshold</span></div><div class="note_contents"><span class="note_content_field">Confidence: </span><span class="note_content_value">4: The reviewer is confident but not absolutely certain that the evaluation is correct</span></div><div class="reply_row clearfix"></div></div><div class="children"><div class="note_with_children"><div id="note_HklshWdUp7" class="note panel"><div class="title_pdf_row clearfix"><h2 class="note_content_title"><a>Thank you for your review!</a>  <button class="btn btn-xs btn-default permalink-button" title="Link to this comment" data-permalink-url="https://openreview.net/forum?id=SJNRHiAcYX&amp;noteId=HklshWdUp7"><span class="glyphicon glyphicon-link" aria-hidden="true"></span></button></h2></div><div class="meta_row"><span class="signatures">ICLR 2019 Conference Paper135 Authors</span></div><div class="clearfix"><div class="meta_row pull-left"><span class="date item">12 Nov 2018</span><span class="item">ICLR 2019 Conference Paper135 Official Comment</span><span class="item">Readers: <span class="readers-icon glyphicon glyphicon-globe"></span> Everyone</span></div><div class="meta_row meta_actions"></div></div><div class="note_contents"><span class="note_content_field">Comment: </span><span class="note_content_value">Thank you for your time and efforts in reviewing our paper! We have made edits to the original manuscript based on your valuable feedbacks, and would like to make a few clarifications.

&gt;&gt;The crux of the authors’ approach is detailed in equations (6) and (7), although it could have been useful to see more of the discussion of the architecture from appendix B in the actual text of the paper.

Thank you for pointing this out! As also mentioned by the other two reviewers, we agree that the properties of the NF policy depends on the architectures. In this our we focus our attention on the architectures in Dinh et al (2016) and aim to illustrate how the expressive policy derived from such specific architecture benefits trust region policy search. To better clarify the scope, we have added in Section 3 and 4 that we limit our attention to Dinh et al (2016) architecture only and other recent advances might benefit the policy search in other ways.

&gt;&gt;The authors then go on to analyze the properties and expressiveness of the resulting properties and show that it is more capable of capturing complex interactions than a simple Gaussian. It was somewhat unclear, however, in section 4.2 what the exact form of the policies being compared are. Is this a simple example with only the parameters of the Gaussian, or was the Gaussian parameterized by a multi-layer model?

It is a Gaussian with mean/std parameterized by multi-layer network. We will clarify this in our final manuscript. We thank the reviewer for pointing out studying the actual effect of the policy class on exploration. We actually tried to illustrate this issue a bit with the ant example in Section 4, where we can infer that NF explores better by observing its diverse trajectories. On the other hand, we see that Gaussian is stuck (both in rewards and trajectories), hence we infer that Gaussian explores less efficiently. In general, we think it is quite hard to disentangle the effect of exploration from the final performance of the policy. This is definitely one way to further explore how expressive policy interacts with the learning procedure, and we will pursue this as future work.</span></div><div class="reply_row clearfix"></div></div><div class="children"></div></div></div></div><div class="note_with_children"><div id="note_SJxd2NNKhX" class="note panel"><div class="title_pdf_row clearfix"><h2 class="note_content_title"><a>simple idea but not totally clear</a>  <button class="btn btn-xs btn-default permalink-button" title="Link to this comment" data-permalink-url="https://openreview.net/forum?id=SJNRHiAcYX&amp;noteId=SJxd2NNKhX"><span class="glyphicon glyphicon-link" aria-hidden="true"></span></button></h2></div><div class="meta_row"><span class="signatures">ICLR 2019 Conference Paper135 AnonReviewer1</span></div><div class="clearfix"><div class="meta_row pull-left"><span class="date item">02 Nov 2018 (modified: 07 Nov 2018)</span><span class="item">ICLR 2019 Conference Paper135 Official Review</span><span class="item">Readers: <span class="readers-icon glyphicon glyphicon-globe"></span> Everyone</span></div><div class="meta_row meta_actions"></div></div><div class="note_contents"><span class="note_content_field">Review: </span><span class="note_content_value">This paper generalizes basic policy gradient methods by replacing the original Gaussian or Gaussian mixture policy with a normalizing flow policy, which is defined by a sequence of invertible transformations from a base policy.

Although the concept of normalizing flow is simple, and it has been applied to other models such as VAE, there seems no work on applying it for policy optimization. Thus I think this method is itself interesting.

However, I find the paper written in a way assuming readers very familiar with related concept and algorithms in reinforcement learning. Thus although one can get the general idea on how the method works, it might be difficult to get a deeper understanding on some details.

For example, normalizing flows are defined in Section 4, and then it is directly claimed that normalizing flows can be applied to policy optimization, without giving details on how it is actually applied, e.g., what is the objective function? and why one needs to compute gradients of the entropy (Section 4.1)?

Also, in the experiments, it is said that one can combing normalizing flows with TRPO without describing the details. I can't get how exactly normalizing flows + TRPO works.

The experiments also talk about 2D bandit problem, and again, without any descriptions. BTW, in the Section 4.3, what does [-1, 1]^2 mean? (I have seen {-1, 1}^2, but not [-1, 1]^2).

It seems that the authors only use the basic normalizing flow structures studied in Rezende&amp;Mohamed (2015) and Dinh et al (2016). However, there are more powerful variants of normalizing flows such as the Multiplicative Normalizing Flows or the Glow. I wonder how good the results are if these more advanced versions are used. Maybe they can uniformly outperform Gaussian policy?</span></div><div class="note_contents"><span class="note_content_field">Rating: </span><span class="note_content_value">4: Ok but not good enough - rejection</span></div><div class="note_contents"><span class="note_content_field">Confidence: </span><span class="note_content_value">4: The reviewer is confident but not absolutely certain that the evaluation is correct</span></div><div class="reply_row clearfix"></div></div><div class="children"><div class="note_with_children"><div id="note_H1epdS-UT7" class="note panel trashed"><div class="title_pdf_row clearfix"><h2 class="note_content_title">  <button class="btn btn-xs btn-default permalink-button" title="Link to this comment" data-permalink-url="https://openreview.net/forum?id=SJNRHiAcYX&amp;noteId=H1epdS-UT7"><span class="glyphicon glyphicon-link" aria-hidden="true"></span></button></h2></div><div class="meta_row"><span class="signatures">[Deleted]</span></div><div class="clearfix"><div class="meta_row pull-left"><span class="item">ICLR 2019 Conference Paper135 Official Comment</span><span class="item">Readers: <span class="readers-icon glyphicon glyphicon-globe"></span> Everyone</span></div><div class="meta_row meta_actions"></div></div><div class="reply_row clearfix"></div></div><div class="children"></div></div><div class="note_with_children"><div id="note_HJe1jGZ86m" class="note panel"><div class="title_pdf_row clearfix"><h2 class="note_content_title"><a>Thank you for your review!</a>  <button class="btn btn-xs btn-default permalink-button" title="Link to this comment" data-permalink-url="https://openreview.net/forum?id=SJNRHiAcYX&amp;noteId=HJe1jGZ86m"><span class="glyphicon glyphicon-link" aria-hidden="true"></span></button></h2></div><div class="meta_row"><span class="signatures">ICLR 2019 Conference Paper135 Authors</span></div><div class="clearfix"><div class="meta_row pull-left"><span class="date item">12 Nov 2018</span><span class="item">ICLR 2019 Conference Paper135 Official Comment</span><span class="item">Readers: <span class="readers-icon glyphicon glyphicon-globe"></span> Everyone</span></div><div class="meta_row meta_actions"></div></div><div class="note_contents"><span class="note_content_field">Comment: </span><span class="note_content_value">Thank you for your time and efforts in reviewing our paper! We have made edits to the original manuscript based on your valuable feedbacks, and would like to make a few clarifications.

&gt;&gt;However, I find the paper written in a way assuming readers very familiar with related concept and algorithms in reinforcement learning. Thus although one can get the general idea on how the method works, it might be difficult to get a deeper understanding on some details.

We think we have made the presentation relatively clear. In terms of the algorithmic procedure, we have concisely yet comprehensively illustrated how TRPO works; in terms of architectures, we have briefly explained how NF works and detail architectures in the Appendix. We also think replacing Gaussian by NF in the context of TRPO is fairly straightforward as explained below.

&gt;&gt;For example, normalizing flows are defined in Section 4, and then it is directly claimed that normalizing flows can be applied to policy optimization, without giving details on how it is actually applied, e.g., what is the objective function? 

Indeed we have assumed that transferring the policy from Gaussian to Normalizing flows (NF) is quite straightforward, in that both assume a distribution \pi_\theta(a|s) parameterized by some policy \theta. NF and Gaussian only differ in this parameterization and their interface with the algorithms is exactly the same. The objective is still the old surrogate loss of the policy optimization procedure.

&gt;&gt;and why one needs to compute gradients of the entropy (Section 4.1)?

We have mentioned that sometimes entropy regularization is needed, and for that we need to be able to compute gradient of the policy entropy.

&gt;&gt;Also, in the experiments, it is said that one can combing normalizing flows with TRPO without describing the details. I can't get how exactly normalizing flows + TRPO works.

As mentioned above, we have assumed that replacing Gaussian by NF in the case of TRPO is quite straightforward: NF and Gaussian have different parameterizations for \pi_\theta(a|s), but other than that everything else if pretty much the same. We have added a concise explanation in Appendix A.

&gt;&gt;The experiments also talk about 2D bandit problem, and again, without any descriptions.

We have described the setup for the 2D bandit problem in Section 4.3. In particular, we interpret 2D bandit problem as a state-less one-step MDP where we can more clearly compare the properties of various policy classes.

&gt;&gt;BTW, in the Section 4.3, what does [-1, 1]^2 mean? 

By [-1,1]^2 we mean a 2-dimensional box: the Cartesian product of two [-1,1] intervals.

&gt;&gt;It seems that the authors only use the basic normalizing flow structures studied in Rezende&amp;Mohamed (2015) and Dinh et al (2016). However, there are more powerful variants of normalizing flows such as the Multiplicative Normalizing Flows or the Glow. I wonder how good the results are if these more advanced versions are used. Maybe they can uniformly outperform Gaussian policy?

Thank you for pointing this out. These more advanced flows architectures are definitely worth experimenting with. In this work we only consider the architecture adopted in Dinh et al (2016) and hope to illustrate how an expressive policy improves upon trust region policy search baselines. We also think that NF policies enjoy more advantages on tasks with complex dynamics (e.g. to better escape locally optimal solutions). For tasks with simple dynamics, Gaussian might be already good enough and a complex distribution will even take longer to converge. 
In addition, we have added in Section 3 and 4 that we limit our attention to the architectures in Dnih et al (2016) while more recent advances in NF can provide more benefits.</span></div><div class="reply_row clearfix"></div></div><div class="children"></div></div></div></div><div class="note_with_children"><div id="note_B1eWSaTuhX" class="note panel"><div class="title_pdf_row clearfix"><h2 class="note_content_title"><a>Combining normalizing flows and Gaussian policies is relatively new, but justification is very limited</a>  <button class="btn btn-xs btn-default permalink-button" title="Link to this comment" data-permalink-url="https://openreview.net/forum?id=SJNRHiAcYX&amp;noteId=B1eWSaTuhX"><span class="glyphicon glyphicon-link" aria-hidden="true"></span></button></h2></div><div class="meta_row"><span class="signatures">ICLR 2019 Conference Paper135 AnonReviewer2</span></div><div class="clearfix"><div class="meta_row pull-left"><span class="date item">02 Nov 2018 (modified: 07 Nov 2018)</span><span class="item">ICLR 2019 Conference Paper135 Official Review</span><span class="item">Readers: <span class="readers-icon glyphicon glyphicon-globe"></span> Everyone</span></div><div class="meta_row meta_actions"></div></div><div class="note_contents"><span class="note_content_field">Review: </span><span class="note_content_value">The papers proposed to use normalizing flow policies instead of Gaussian policies to improve exploration and achieve better sample complexity in practice. While I believe this idea has not specifically been tried in previous literature and the vague intuition that NF leads to more exploration that helps learning a better policy, the novelty of combining these two seems limited, and the paper does not seem to provide enough justification to using NF policies instead of alternative policy distributions both in theory and in the experiments.

1. About Section 4.2. I believe that the normalizing flow in question would transform the volume of a Gaussian? So there would exist some parameter setting for a flow model that also shrinks volume, thereby resulting in lower variance policies? The arguments would thereby depend heavily on the specific architecture and initialization of the flow model, which is not discussed in detail. 

Also, why is finding a high variance policy better in terms of the trust region argument? Isn't the whole point of using trust region that the new policy should be closer to old policy to prevent performance degradation? I also think that a fair comparison would be compare KL between normalizing flow policies, instead of KL between NF and Gaussian.

2. The TRPO experiments seem wrong -- at least the results don't match what is reported in the ACKTR paper for Reacher and InverseDoublePendulum envs -- there the TRPO policy at least learns something. Also TRPO in general does not perform as bad as it may seem, see "Deep RL that matters" paper by Henderson et al. Maybe this is because of using OpenAI baselines code which seems to have worse TRPO performance.

There is also no experiments on ACKTR on the small Mujoco tasks (even in the Appendix), which seems to be a rather big oversight given the authors have already done even harder tasks for ACKTR + NF.

Moreover I think a fair comparison is to use almost the same architecture for implicit and gaussian, where the only difference is where you sample the noise. For Gaussian with flows, you can first use an MLP to produce deterministic outputs and then use flow to generate the mean actions. Otherwise it is impossible to say whether the architecture or the implicit distribution contributes more to the success.

One could also use truncated Gaussian distributions / Beta distributions / Gaussian + tanh, since Mujoco actions beyond (-1, 1) is treated as -1 or 1, so Gaussian should already be bad. It is unclear whether NF is able to outperform these settings. 

Minor points:

- Fix citations. Please use \citep throughout.
- Is Equation (6) correct? Seems like \Sigma_i should be the inverse of g_i(\epsilon)? Also this is the "change of variables formula" not "chain rule".
- Why is normalizing flow not part of the background?
- Add legends in Figure (1)
- Figure 2(c), I believe with max entropy you could already obtain diverse ant trajectories?
- I believe in the context of generative models, "implicit" typically means the case where likelihood is not tractable? Here the likelihood is perfectly tractable.</span></div><div class="note_contents"><span class="note_content_field">Rating: </span><span class="note_content_value">4: Ok but not good enough - rejection</span></div><div class="note_contents"><span class="note_content_field">Confidence: </span><span class="note_content_value">4: The reviewer is confident but not absolutely certain that the evaluation is correct</span></div><div class="reply_row clearfix"></div></div><div class="children"><div class="note_with_children"><div id="note_rkxmRGW86X" class="note panel"><div class="title_pdf_row clearfix"><h2 class="note_content_title"><a>Thank you for your review!</a>  <button class="btn btn-xs btn-default permalink-button" title="Link to this comment" data-permalink-url="https://openreview.net/forum?id=SJNRHiAcYX&amp;noteId=rkxmRGW86X"><span class="glyphicon glyphicon-link" aria-hidden="true"></span></button></h2></div><div class="meta_row"><span class="signatures">ICLR 2019 Conference Paper135 Authors</span></div><div class="clearfix"><div class="meta_row pull-left"><span class="date item">12 Nov 2018</span><span class="item">ICLR 2019 Conference Paper135 Official Comment</span><span class="item">Readers: <span class="readers-icon glyphicon glyphicon-globe"></span> Everyone</span></div><div class="meta_row meta_actions"></div></div><div class="note_contents"><span class="note_content_field">Comment: </span><span class="note_content_value">Thank you for your time and efforts in reviewing our paper! We have made edits to the original manuscript based on your valuable feedbacks, and would like to make a few clarifications.

&gt;&gt;About Section 4.2. I believe that the normalizing flow in question would transform the volume of a Gaussian? So there would exist some parameter setting for a flow model that also shrinks volume, thereby resulting in lower variance policies? The arguments would thereby depend heavily on the specific architecture and initialization of the flow model, which is not discussed in detail.

Thank you for pointing this out! We agree that the properties of the NF model depends on the architecture, and that using other flows architecture might result in more extensive benefits (expressive policy plus lower variance gradient). In this work we limit our attention to the architecture proposed in Dinh et al (2016) and we have made it clear in both Section 3 and 4 that we limit the attention to Dinh et al (2016) while other flows architecture might provide additional benefits.

&gt;&gt;Also, why is finding a high variance policy better in terms of the trust region argument? Isn't the whole point of using trust region that the new policy should be closer to old policy to prevent performance degradation? I also think that a fair comparison would be compare KL between normalizing flow policies, instead of KL between NF and Gaussian.

Schulman et al (2015) shows the benefit of explicitly imposing KL divergence constraint on consecutive updates, hence proposing TRPO. In our paper, we argue that while NF and Gaussian can both enforce the same constraints on KL divergence, the induced constraints on the sampled action space is different. This is critical since though the constraint is placed on KL divergence, it is the sampled actions that lead to exploration. We argue that under the same KL constraint, NF can sample actions that potentially lead to better exploration, this is also the motivation behind the comparison of sampled actions between Gaussian and NF under the same KL constraint.

&gt;&gt;The TRPO experiments seem wrong -- at least the results don't match what is reported in the ACKTR paper for Reacher and InverseDoublePendulum envs -- there the TRPO policy at least learns something. Also TRPO in general does not perform as bad as it may seem, see "Deep RL that matters" paper by Henderson et al. Maybe this is because of using OpenAI baselines code which seems to have worse TRPO performance.

Thank you for pointing this out. We are aware that deep RL experiments depend heavily on implementations. For fair comparison, throughout the paper we use OpenAI baseline implementation: all hyper-parameters are shared across various policy classes, except for their individual parameterization of pi_\theta(a|s). It is possible that baseline implementation gives poor performance on reacher and inverted-pendulum, however, due to the uniform implementation we think our comparison is fair. 

&gt;&gt;There is also no experiments on ACKTR on the small Mujoco tasks (even in the Appendix), which seems to be a rather big oversight given the authors have already done even harder tasks for ACKTR + NF.

We have run similar experiments on smaller tasks but find that ACKTR + NF does not yield significant gains compared to on complex tasks. This is also consistent with our argument that complex distributions lead to performance gains on tasks with complex dynamics. 

&gt;&gt;One could also use truncated Gaussian distributions / Beta distributions / Gaussian + tanh, since Mujoco actions beyond (-1, 1) is treated as -1 or 1, so Gaussian should already be bad. It is unclear whether NF is able to outperform these settings. 

We have included new experiments on Beta distribution and Gaussian distribution + tanh. The motivations for these distributions are different from ours, in that they aim to bound the actions to the rang [-1,1]. We have added their results for clarity. We have also experimented with NF + tanh (at the last layer) to bound actions in [-1,1] but found it not to be helpful.

&gt;&gt;Minor points

We have fixed citation and symbol issues addressed by the reviewer. 

&gt;&gt;Why is normalizing flow not part of the background?

We have introduced the generative process of NF in Section 4, which we think can serve as part of the background.

&gt;&gt;Figure 2(c), I believe with max entropy you could already obtain diverse ant trajectories?

We try to argue that NF can do better exploration than Gaussian for this task, as a result the trajectories of NF are more diverse. It is possible that adding entropy to Gaussian can lead to slightly more entropic trajectories, but we suspect that it significantly helps to escape the locally optimal gait. </span></div><div class="reply_row clearfix"></div></div><div class="children"></div></div></div></div></div></main></div>
  </div>
</div>


    <!-- Footer -->
    <footer class="sitemap">
      <div class="container">
        <div class="row hidden-xs">
          <div class="col-sm-4">
            <ul class="list-unstyled">
              <li><a href="/" class="home">Home</a></li>
              <li><a href="/venues">All Venues</a></li>
            </ul>
          </div>
    
          <div class="col-sm-4">
            <ul class="list-unstyled">
              <li><a href="/about">About OpenReview</a></li>
              <li><a href="/contact">Contact</a></li>
              <li><a href="#" data-toggle="modal" data-target="#feedback-modal">Feedback</a></li>
            </ul>
          </div>
    
          <div class="col-sm-4">
            <ul class="list-unstyled">
              <li><a href="/terms">Terms of Service</a></li>
              <li><a href="/privacy">Privacy Policy</a></li>
            </ul>
          </div>
        </div>
    
        <div class="row visible-xs-block">
          <div class="col-xs-6">
            <ul class="list-unstyled">
              <li><a href="/" class="home">Home</a></li>
              <li><a href="/about">About OpenReview</a></li>
              <li><a href="#" data-toggle="modal" data-target="#feedback-modal">Feedback</a></li>
            </ul>
          </div>
    
          <div class="col-xs-6">
            <ul class="list-unstyled">
              <li><a href="/contact">Contact</a></li>
              <li><a href="/terms">Terms of Service</a></li>
              <li><a href="/privacy">Privacy Policy</a></li>
            </ul>
          </div>
        </div>
      </div>
    </footer>
    
    <footer class="sponsor">
      <div class="container">
        <div class="row">
          <div class="col-sm-10 col-sm-offset-1">
            <p class="text-center">
              OpenReview is created by the <a href="http://www.iesl.cs.umass.edu/" target="_blank">Information Extraction and Synthesis Laboratory</a>, College of Information and Computer Science, University of Massachusetts Amherst. We gratefully acknowledge the support of the OpenReview sponsors:  Google,  Facebook, NSF, the University of Massachusetts Amherst Center for Data Science, and Center for Intelligent Information Retrieval, as well as the Google Cloud Platform for donating the computing and networking services on which OpenReview.net runs.
            </p>
          </div>
        </div>
      </div>
    </footer>

  <div id="feedback-modal" class="modal fade" tabindex="-1" role="dialog">
    <div class="modal-dialog">
      <div class="modal-content">
        <div class="modal-header">
          <button type="button" class="close" data-dismiss="modal" aria-label="Close"><span aria-hidden="true">×</span></button>
          <h3 class="modal-title">Send Feedback</h3>
        </div>
        <div class="modal-body">
          <p>Enter your feedback below and we'll get back to you as soon as possible.</p>
          <form action="/feedback" method="POST">
            <div class="form-group">
              <input type="email" name="from" class="form-control" placeholder="Email" />
            </div>
            <div class="form-group">
              <input type="text" name="subject" class="form-control" placeholder="Subject" />
            </div>
            <div class="form-group">
              <textarea name="message" class="form-control feedback-input" rows="5" placeholder="Message" required=""></textarea>
            </div>
          <ul id="ui-id-2" tabindex="0" class="ui-menu ui-widget ui-widget-content" style="display: none;"></ul></form>
        </div>
        <div class="modal-footer">
          <button type="button" class="btn btn-default" data-dismiss="modal">Cancel</button>
          <button type="button" class="btn btn-primary">Send</button>
        </div>
      </div>
    </div>
  </div>

  <script type="text/javascript" async="" src="https://www.google-analytics.com/analytics.js"></script><script src="//cdnjs.cloudflare.com/ajax/libs/jquery/2.2.4/jquery.min.js"></script>
  <script>window.jQuery || document.write('&lt;script src="/static/js/vendor/jquery-2.2.4.min.js"&gt;&lt;\/script&gt;')</script>

  <script src="/static/dist/vendor.min.js"></script>
  <script src="/static/dist/templates.min.js"></script>
  <script src="/static/dist/openreview.min.js"></script>

    <script>window.legacyScripts = true;</script>


    <script async="" src="https://www.googletagmanager.com/gtag/js?id=UA-108703919-1"></script>
    <script>
      window.dataLayer = window.dataLayer || [];
      function gtag() { dataLayer.push(arguments); }
      gtag('js', new Date());
      gtag('config', 'UA-108703919-1');
    </script>


<div role="status" aria-live="assertive" aria-relevant="additions" class="ui-helper-hidden-accessible"></div></body></html>