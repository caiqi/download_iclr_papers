<!DOCTYPE html><html xmlns="http://www.w3.org/1999/xhtml" class="no-js"><head>
  <meta charset="utf-8" />
  <meta http-equiv="X-UA-Compatible" content="IE=edge,chrome=1" />
  <meta name="viewport" content="width=device-width, initial-scale=1" />

  <title>Variational Autoencoders with Jointly Optimized Latent Dependency Structure | OpenReview</title>
  <meta name="description" content="" />

      <meta name="citation_title" content="Variational Autoencoders with Jointly Optimized Latent Dependency Structure" />
        <meta name="citation_author" content="Anonymous" />
      <meta name="citation_publication_date" content="2018/09/27" />
      <meta name="citation_online_date" content="2018/09/27" />
      <meta name="citation_pdf_url" content="https://openreview.net/pdf?id=SJgsCjCqt7" />
      <meta name="twitter:card" content="summary" />
      <meta name="twitter:site" content="@openreviewnet" />
      <meta name="og:title" content="Variational Autoencoders with Jointly Optimized Latent Dependency..." />
      <meta name="og:description" content="We propose a method for learning the dependency structure between latent variables in deep latent variable models.  Our general modeling and inference framework combines the complementary strengths..." />
      <meta name="og:image" content="https://openreview.net/static/images/openreview_logo_512.png" />

  <link rel="stylesheet" href="https://fonts.googleapis.com/css?family=Noto+Sans:400,400i,700,700i" />

  <link rel="stylesheet" href="/static/css/bootstrap.min.css" />
  <link rel="stylesheet" href="/static/css/jquery-ui.min.css" />
  <link rel="stylesheet" href="/static/css/main.min.css" />
</head>

<body class="forum">
  <nav class="navbar navbar-inverse navbar-fixed-top" role="navigation">
    <div class="container">
  
      <div class="navbar-header">
        <button type="button" class="navbar-toggle collapsed" data-toggle="collapse" data-target="#navbar" aria-expanded="false" aria-controls="navbar">
          <span class="sr-only">Toggle navigation</span>
          <span class="icon-bar"></span>
          <span class="icon-bar"></span>
          <span class="icon-bar"></span>
        </button>
        <a class="navbar-brand home push-link" href="/"><strong>OpenReview</strong>.net</a>
      </div>
  
      <div id="navbar" class="navbar-collapse collapse">
        <form class="navbar-form navbar-left profile-search" role="search">
          <div class="form-group has-feedback">
            <input id="search_input" type="text" class="form-control ui-autocomplete-input" placeholder="Search ICLR 2019 Conference" autocomplete="off" />
            <span class="glyphicon glyphicon-search form-control-feedback" aria-hidden="true"></span>
          </div>
  
          <input id="search_group" type="hidden" value="ICLR.cc/2019/Conference" />
          <input id="search_content" type="hidden" value="all" />
        <ul id="ui-id-1" tabindex="0" class="ui-menu ui-widget ui-widget-content ui-autocomplete ui-front" style="display: none;"></ul></form>
  
        <ul class="nav navbar-nav navbar-right">
        
            <li id="user-menu"><a href="/login">Login</a></li>
        </ul>
      </div>
  
    </div>
  </nav>

  <div id="or-banner" class="banner" style="">
  <div class="container">
    <div class="row">
      <div class="col-xs-12"><a href="/group?id=ICLR.cc/2019/Conference" title="Venue Homepage"><img class="icon" src="/static/images/arrow_left.svg" /> Go to <strong>ICLR 2019 Conference</strong> homepage</a></div>
    </div>
  </div>
</div>
<div id="flash-message-container" class="alert alert-danger" role="alert" style="display: none;">
  <div class="container">
    <div class="row">
      <div class="col-xs-12">
        <div class="alert-content">
          <button type="button" class="close" aria-label="Close"><span aria-hidden="true">×</span></button>
        </div>
      </div>
    </div>
  </div>
</div>

<div class="container">
  <div class="row">
    <div class="col-xs-12">
      <main id="content" class="clearfix openbanner-visible legacy-styles"><div id="note_SJgsCjCqt7" class="note panel"><div class="title_pdf_row clearfix"><h2 class="note_content_title"><a>Variational Autoencoders with Jointly Optimized Latent Dependency Structure</a> <a class="note_content_pdf" href="/pdf?id=SJgsCjCqt7" title="Download PDF" target="_blank"><img src="/static/images/pdf_icon_blue.svg" /></a> </h2></div><div class="meta_row"><span class="signatures">Anonymous</span></div><div class="clearfix"><div class="meta_row pull-left"><span class="date item">28 Sep 2018 (modified: 11 Oct 2018)</span><span class="item">ICLR 2019 Conference Blind Submission</span><span class="item">Readers: <span class="readers-icon glyphicon glyphicon-globe"></span> Everyone</span><span class="item"><a class="action-bibtex-modal" data-bibtex="@inproceedings{    &#10;anonymous2019variational,    &#10;title={Variational Autoencoders with Jointly Optimized Latent Dependency Structure},    &#10;author={Anonymous},    &#10;booktitle={Submitted to International Conference on Learning Representations},    &#10;year={2019},    &#10;url={https://openreview.net/forum?id=SJgsCjCqt7},    &#10;note={under review}    &#10;}">Show Bibtex</a></span></div><div class="meta_row meta_actions"></div></div><div class="note_contents"><span class="note_content_field">Abstract: </span><span class="note_content_value">We propose a method for learning the dependency structure between latent variables in deep latent variable models.  Our general modeling and inference framework combines the complementary strengths of deep generative models and probabilistic graphical models. In particular, we express the latent variable space of a variational autoencoder (VAE) in terms of a Bayesian network with a learned, flexible dependency structure.  The network parameters, variational parameters as well as the latent topology are optimized simultaneously with a single variational objective.  Inference is formulated via a sampling procedure that produces expectations over latent variable structures and incorporates top-down and bottom-up reasoning over latent variable values.  We validate our framework in extensive experiments on MNIST, Omniglot, and CIFAR-10. Comparisons to state-of-the-art structured variational autoencoder baselines show improvements in terms of the expressiveness of the learned model.</span></div><div class="note_contents"><span class="note_content_field">Keywords: </span><span class="note_content_value">deep generative models, structure learning</span></div><div class="note_contents"><span class="note_content_field">TL;DR: </span><span class="note_content_value">We propose a method for learning latent dependency structure in variational autoencoders.</span></div><div class="reply_row clearfix"><div class="item" id="reply_count">3 Replies</div></div></div><hr class="small" /><div id="note_children"><div class="note_with_children"><div id="note_BygZQun5nQ" class="note panel"><div class="title_pdf_row clearfix"><h2 class="note_content_title"><a>Interesting idea: use a matrix of binary random variables to capture dependencies between latent variables in a hierarchical deep generative model. </a>  <button class="btn btn-xs btn-default permalink-button" title="Link to this comment" data-permalink-url="https://openreview.net/forum?id=SJgsCjCqt7&amp;noteId=BygZQun5nQ"><span class="glyphicon glyphicon-link" aria-hidden="true"></span></button></h2></div><div class="meta_row"><span class="signatures">ICLR 2019 Conference Paper924 AnonReviewer1</span></div><div class="clearfix"><div class="meta_row pull-left"><span class="date item">03 Nov 2018 (modified: 07 Nov 2018)</span><span class="item">ICLR 2019 Conference Paper924 Official Review</span><span class="item">Readers: <span class="readers-icon glyphicon glyphicon-globe"></span> Everyone</span></div><div class="meta_row meta_actions"></div></div><div class="note_contents"><span class="note_content_field">Review: </span><span class="note_content_value">Often in a deep generative model with multiple latent variables, the structure amongst the latent variables is pre-specified before parameter estimation. This work aims to learn the structure as part of the parameters. To do so, this work represents all possible dependencies amongst the latent random variables via a learned binary adjacency matrix, c, where a 1 denotes each parent child relationship.

Each setting of c defines a latent variable as the root and subsequent parent-child relationships amongst the others. To be able to support (up to N-1) parents, the paper proposes a neural architecture where the sample from each parent is multiplied by the corresponding value of c_ij (0'd out if the edge does not exist in c), concatenated and fed into an MLP that predicts a distribution over the child node. The inference network shares parameters with the generative model (as in Sonderby et. al). Given any setting of c, one can define the variational lower-bound of the data. This work performs parameter estimation by sampling c and then performing gradient ascent on the resulting lower-bound.

The model is evaluated on MNIST, Omniglot and CIFAR where it is found to outperform a VAE with a single latent variable (with the same number of latent dimensions as the proposed graphVAE), the LadderVAE and the FCVAE (VAE with a fully connected graph). An ablation study is conducted to study the effect of number of nodes and their dimensionality.

Overall, the paper is (a) well written, (b) proposes a new, interesting idea and (c) shows that the choice to parameterize structure via the use of auxillary random variables improves the quality of results on some standard benchmarks.

Comments and questions for the authors:
* Clarity
It might be instructive to describe in detail how the inference network is structured for different settings of c (for example, via a scenario with three latent variables) rather than via reference to Sonderby et. al.

What prior distribution was used for c?

For the baseline comparing to the LadderVAE, what dimensionalities were used for the latent variables in the LadderVAE (which has a chain structured dependence amongst its latent variables)? The experimental setup keeps fixed the latent dimensionality to 80 -- the original paper recommends a different dimensionality for each latent variables in the chain [<a href="https://arxiv.org/pdf/1602.02282.pdf," target="_blank" rel="nofollow">https://arxiv.org/pdf/1602.02282.pdf,</a> Table 2] -- was this tried? Did the ladderVAE do better if each latent variable in the chain was allowed to have a dimensionality?

* Related work
There is related work which leverages Bayesian non-parametric models to learn hierarchical priors for deep generative models. It is worth discussing for putting this line of work into context. For example:
http://openaccess.thecvf.com/content_ICCV_2017/papers/Goyal_Nonparametric_Variational_Auto-Encoders_ICCV_2017_paper.pdf
and more recently: https://arxiv.org/pdf/1810.06891.pdf

In the context of defining inference networks for generative models where the latent variables have structure, Webb et. al [https://arxiv.org/abs/1712.00287] describe how inference networks should be setup in order to invert the generative process.

* Qualitative study
Notable in its absence is a qualitative analysis of what happens to the data sampled from the model when the various nodes in the learned hierarchy are perturbed holding fixed their parents. Have you attempted this experiment? Are the edge relationships sensible or interesting?

Is there a relationship between the complexity of each conditional distribution in the generative model and the learned latent structure? Specifically, have you experimented to see what happens to the learned structure amongst the latent variables if each conditional density is a linear function of its parents?
</span></div><div class="note_contents"><span class="note_content_field">Rating: </span><span class="note_content_value">6: Marginally above acceptance threshold</span></div><div class="note_contents"><span class="note_content_field">Confidence: </span><span class="note_content_value">4: The reviewer is confident but not absolutely certain that the evaluation is correct</span></div><div class="reply_row clearfix"></div></div><div class="children"></div></div><div class="note_with_children"><div id="note_rJgSaaLv3m" class="note panel"><div class="title_pdf_row clearfix"><h2 class="note_content_title"><a>An idea with potential, but weakly developped and tested</a>  <button class="btn btn-xs btn-default permalink-button" title="Link to this comment" data-permalink-url="https://openreview.net/forum?id=SJgsCjCqt7&amp;noteId=rJgSaaLv3m"><span class="glyphicon glyphicon-link" aria-hidden="true"></span></button></h2></div><div class="meta_row"><span class="signatures">ICLR 2019 Conference Paper924 AnonReviewer3</span></div><div class="clearfix"><div class="meta_row pull-left"><span class="date item">01 Nov 2018 (modified: 07 Nov 2018)</span><span class="item">ICLR 2019 Conference Paper924 Official Review</span><span class="item">Readers: <span class="readers-icon glyphicon glyphicon-globe"></span> Everyone</span></div><div class="meta_row meta_actions"></div></div><div class="note_contents"><span class="note_content_field">Review: </span><span class="note_content_value">The authors propose to augment the latent space of a Variational AutoEncoder [1] with an auto-regressive structure, to improve the expressiveness of both the inference network and the latent prior, making them into a general DAG of latent variables. This works goes further in the same direction as the Ladder VAE [2]. This paper introduces a mechanism for the latent model to directly learn its DAG structure by first considering the fully-connected DAG of latent variables, and adding Bernoulli variables controlling the presence or absence of each edge. The authors derive a new ELBO taking these variables into account, and use it to train the model. The gradients of the parameters of the Bernoulli variables are computed using the Gumbel-Softmax approach [3] and annealing the temperature.

The authors observe with they experiments that the Bernoulli variables converge relatively quickly towards 0 or 1 during the training, fixing the structure of the DAG for the rest of the training. They test their model against a VAE, a Ladder VAE and an alternative to their model were the DAG is fixed to remain fully-connected (FC-VAE), and observe improvements in terms of the ELBO values and log-likelihood estimations.

The main addition of this paper is the introduction of the gating mechanism to reduce the latent DAG from its fully-connected state. It is motivated by the tendency of latent models to fall into local optima.

However, it is not clear to me what this mechanism as it is now adds to the model:

- The reported results shows the improvements of Graph-VAE over FC-VAE to be quite small, making their relevance dubious in the absence of measurement of variance accross different trainings. Additionally, the reported performances for Ladder VAE are inferior to what [2] reports. Actually the performance of Ladder-VAE reported in [2] is better than the one reported for Graph-VAE in this paper, both on the MNIST and Omniglot datasets.

- The authors observe that the Bernoulli variables have converged after around ~200 epochs. At this time, according to their reported experimental setup, the Gumbel-Softmax temperature is 0.999^200 ~= 0.82, which is still quite near 1.0, meaning the model is still pretty far from a real Bernoulli-like behavior. And actually, equation 9 is not a proper description of the Gumbel-Softmax as described by [3] : there should be only 2 samples from the Gumbel distribution, not 3. Given these two issues, I can't believe that the c_ij coefficients behave like Bernoulli variables in this experiment. As such, It seems to me that Graph-VAE is nothing more than a special reparametrization of FC-VAE that tends to favor saturating behavior for the c_ij variables.

- On figure 3b, the learned structure is very symmetrical (z2, z3, z4 play an identical role in the final DAG). In my opinion, this begs for the introduction of a regulatory mechanism regarding the gating variable to push the model towards sparsity. I was honestly surprised to see this gating mechanism introduced without anything guiding the convergence of the c_ij variables.

I like the idea of learning a latent structure DAG for VAEs, but this paper introduces a rather weak way to try to achieve this, and the experimental results are not convincing.

[1] <a href="https://arxiv.org/abs/1312.6114" target="_blank" rel="nofollow">https://arxiv.org/abs/1312.6114</a>
[2] https://arxiv.org/abs/1602.02282
[3] https://arxiv.org/abs/1611.01144</span></div><div class="note_contents"><span class="note_content_field">Rating: </span><span class="note_content_value">4: Ok but not good enough - rejection</span></div><div class="note_contents"><span class="note_content_field">Confidence: </span><span class="note_content_value">3: The reviewer is fairly confident that the evaluation is correct</span></div><div class="reply_row clearfix"></div></div><div class="children"></div></div><div class="note_with_children"><div id="note_H1eOmiKiom" class="note panel"><div class="title_pdf_row clearfix"><h2 class="note_content_title"><a>Interesting paper but with technical issues that need addressing</a>  <button class="btn btn-xs btn-default permalink-button" title="Link to this comment" data-permalink-url="https://openreview.net/forum?id=SJgsCjCqt7&amp;noteId=H1eOmiKiom"><span class="glyphicon glyphicon-link" aria-hidden="true"></span></button></h2></div><div class="meta_row"><span class="signatures">ICLR 2019 Conference Paper924 AnonReviewer2</span></div><div class="clearfix"><div class="meta_row pull-left"><span class="date item">23 Oct 2018 (modified: 07 Nov 2018)</span><span class="item">ICLR 2019 Conference Paper924 Official Review</span><span class="item">Readers: <span class="readers-icon glyphicon glyphicon-globe"></span> Everyone</span></div><div class="meta_row meta_actions"></div></div><div class="note_contents"><span class="note_content_field">Review: </span><span class="note_content_value">This paper presents a VAE approach in which a dependency structure on the latent variable is learned during training.  Specifically, a lower-triangular random binary matrix c is introduced, where c_{i,j} = 1 for i&gt;j, indicates that z_i depends on z_j, where z is the latent vector.  Each element of c is separately parametrized by a Bernoulli distribution whose means are optimized for during training, using the target \mathbb{E}_{p(c)}[\mathcal{L}_c] where \mathcal{L}_c indicates the ELBO for a particular instance of c.  The resulting "Graph-VAE" scheme is shown to train models with improved marginal likelihood than a number of baselines for MNIST, Omniglot, and CIFAR-10.

The core concept for this paper is good, the results are impressive, and the paper is, for the most part, easy to follow.  Though I think a lot of people have been thinking about how to learn dependency structures in VAEs, I think this work is the first to clearly lay out a concrete approach for doing so.  I thus think that even though this is not the most novel of papers, it is work which will be of significant interest to the ICLR community.  However, the paper has a number of technical issues and I do not believe the paper is suitable for publication unless they are addressed, or at the vest least acknowledged. I further have some misgivings with the experiments and the explanations of some key elements of the method.  Because of these issues, I think the paper falls below the acceptance threshold in its current form, but I think they could potentially be correctable during the rebuttal period and I will be very happy to substantially increase my score if they are; I feel this has the potential to be a very good paper that I would ultimately like to see published.

%%% Lower bound %%%

My first major concern is in the justification of the final approach (Eq 8), namely using a lower bound argument to move the p(c) term outside of the log.  A target being a lower bound on something we care about is never in itself a justification for that target -- it just says that the resulting estimator is provably negatively biased.  The arguments behind the use of lower bounds in conventional ELBOs are based on much more subtle arguments in terms of the bound becoming tight if we have good posterior approximations and implicit assumptions that the bound will behave similarly to the true marginal.  The bound derived in A.1 of the current paper is instead almost completely useless and serves little purpose other than adding "mathiness" of the type discussed in <a href="https://arxiv.org/abs/1807.0334." target="_blank" rel="nofollow">https://arxiv.org/abs/1807.0334.</a> Eq 8 is not a variational end-to-end target like you claim.  It is never tight and will demonstrably behave very differently to the original target.

To see why it will behave very differently, consider how the original and bound would combine two instances of c for the MNIST experiment, one corresponding to the MAP values of c in the final trained system, the other a value of c that has an ELBO which is, say, 10 nats lower.  Using Eq 8, these will have similar contributions to the overall expectation and so a good network setup (i.e. theta and phi) is one which produces a decent ELBO for both.  Under the original expectation, on the other hand, the MAP value of c corresponds to a setup that has many orders of magnitude higher probability and so the best network setup is the one that does well for the MAP value of c, with the other instance being of little importance.  We thus see that the original target and the lower bound behave very differently for a given p(c).

Thankfully, the target in Eq 8 is a potentially reasonable thing to do in its own right (maybe actually more so that the original formulation), because the averaging over c is somewhat spurious given you are optimizing its mean parameters anyway.  It is easy to show that the "optimum" p(c) for a given (\theta,\phi) is always a delta function on the value of c which has the highest ELBO_c.  As Fig 3 shows, the optimization of the parameters of p(c) practically leads to such a collapse.  This is effectively desirable behavior given the overall aims and so averaging over values of c is from a modeling perspective actually a complete red herring anyway.  It is very much possible that the training procedure represented by Eq 8 is (almost by chance) a good approach in terms of learning the optimal configuration for c, but if this is the case it needs to be presented as such, instead of using the current argument about putting a prior on c and constructing a second lower bound, which is a best dubious and misleading, and at worst complete rubbish.  Ideally, the current explanations would be replaced by a more principled justification, but even just saying you tried Eq 8 and it worked well empirically would be a lot better than what is there at the moment.

%%% Encoder dependency structure does not match the generative model %%%

My second major concern is that the dependency structure used for the encoder is incorrect from the point of view of the generative model.  Namely, a dependency structure on the prior does not induce the same dependency structure on the posterior.  In general, just because z_1 and z_2 are independent, doesn't mean that z_1 and z_2 are independent given x (see e.g. Bishop).  Consequently, the encoder in your setup will be incapable of correctly representing the posterior implied by the generative model.  This has a number of serious practical and theoretical knock-on effects, such as prohibiting the bound becoming tight, causing the encoder to indirectly impact the expressivity of the generative model etc.  Note that this problem is not shared with the Ladder VAE, as there the Markovian dependency structure means produces a special case where the posterior and prior dependency structure is shared.

As shown in https://arxiv.org/abs/1712.00287 (a critical missing reference more generally), it is actually possible to derive the dependency structure of the posterior from that of the prior.  I think in your case their results imply that the encoder needs to be fully connected as the decoder can induce arbitrary dependencies between the latent variables.  I am somewhat surprised that this has not had more of an apparent negative impact on the empirical results and I think at the very very least the paper needs to acknowledge this issue.  I would recommend the authors run experiments using a fully connected encoder and the Graph-VAE decoder (and potentially also vice verse).  Should this approach perform well, it would represent a more principled approach to replace the old on from a generative model perspective.  Should it not, it would provide an empirical justification for what is, in essence, a different restriction to that of the learned prior structure: it is conceivably actually the case that these encoder restrictions induce the desired decoder behavior, but this is distinct to learning a particular dependency structure in the generative model.

%%% Specifics of model and experiments %%%

Though the paper is generally very easy to read, there as some key areas where the explanations are overly terse.  In particular, the explanation surrounding the encoding was difficult to follow and it took me a while to establish exactly what was going on; I am still unsure how \tilde{\psi} and \hat{\psi} are combined.  I think a more careful explanation here and a section giving more detail in the appendices would both help massively.

I was not clear on exactly what was meant by the FC-VAE.  I do not completely agree with the assertion that a standard VAE has independent latents.  Though the typical choice that the prior is N(0,I) obviously causes the prior to have independent latents, as explained earlier, this does not mean the latents are independent in the posterior.  Furthermore, the encoder implicitly incorporates these dependencies through its mean vector, even if it uses a diagonal covariance (which is usually rather small anyway).  What is actually changed from this by the FC-VAE?  Are you doing some kind of normalizing flow approach here?  If so this needs proper explanation.

Relatedly, I am also far from convinced by the arguments presented about why the FC-VAE does worse at the end of the experiments.  VAEs attempt to maximize a marginal likelihood (through a surrogate target) and a model which makes no structural assumptions will generally have a lower marginal likelihood than one which makes the correct structural assumptions.  It is thus perfectly reasonable that when you learn dependency structures, you will get a higher marginal likelihood than if you presume none.  I thus find your arguments about local optima somewhat speculative and further investigation is required.

%%% Experiments %%%

Though certainly not terrible, I felt that the experimental evaluation of the work could have been better.  The biggest issue I have is that no error bars are given for the results, so it is difficult to assess the robustness of the Graph-VAE.  I think it would be good to add convergence plots with error bars to see how the performance varies with time and provide an idea of variability.  More generally, the experiment section overall feels more terse and rushed than the rest of the paper, with some details difficult to find or potentially even straight up missing.

Though Fig 3 is very nice, it would be nice to have additional plots seeing qualitatively what happens with the latent space.  E.g. on average what proportion of the c tend to zero?  Is the same dependency structure always learned?  What do the dataset encodings look like?  Are there noticeable qualitative changes in samples generated from the learned models?  I would be perfectly happy for the paper to extend over the 8 pages to allow more results addressing these questions.</span></div><div class="note_contents"><span class="note_content_field">Rating: </span><span class="note_content_value">5: Marginally below acceptance threshold</span></div><div class="note_contents"><span class="note_content_field">Confidence: </span><span class="note_content_value">4: The reviewer is confident but not absolutely certain that the evaluation is correct</span></div><div class="reply_row clearfix"></div></div><div class="children"></div></div></div></main></div>
  </div>
</div>


    <!-- Footer -->
    <footer class="sitemap">
      <div class="container">
        <div class="row hidden-xs">
          <div class="col-sm-4">
            <ul class="list-unstyled">
              <li><a href="/" class="home">Home</a></li>
              <li><a href="/venues">All Venues</a></li>
            </ul>
          </div>
    
          <div class="col-sm-4">
            <ul class="list-unstyled">
              <li><a href="/about">About OpenReview</a></li>
              <li><a href="/contact">Contact</a></li>
              <li><a href="#" data-toggle="modal" data-target="#feedback-modal">Feedback</a></li>
            </ul>
          </div>
    
          <div class="col-sm-4">
            <ul class="list-unstyled">
              <li><a href="/terms">Terms of Service</a></li>
              <li><a href="/privacy">Privacy Policy</a></li>
            </ul>
          </div>
        </div>
    
        <div class="row visible-xs-block">
          <div class="col-xs-6">
            <ul class="list-unstyled">
              <li><a href="/" class="home">Home</a></li>
              <li><a href="/about">About OpenReview</a></li>
              <li><a href="#" data-toggle="modal" data-target="#feedback-modal">Feedback</a></li>
            </ul>
          </div>
    
          <div class="col-xs-6">
            <ul class="list-unstyled">
              <li><a href="/contact">Contact</a></li>
              <li><a href="/terms">Terms of Service</a></li>
              <li><a href="/privacy">Privacy Policy</a></li>
            </ul>
          </div>
        </div>
      </div>
    </footer>
    
    <footer class="sponsor">
      <div class="container">
        <div class="row">
          <div class="col-sm-10 col-sm-offset-1">
            <p class="text-center">
              OpenReview is created by the <a href="http://www.iesl.cs.umass.edu/" target="_blank">Information Extraction and Synthesis Laboratory</a>, College of Information and Computer Science, University of Massachusetts Amherst. We gratefully acknowledge the support of the OpenReview sponsors:  Google,  Facebook, NSF, the University of Massachusetts Amherst Center for Data Science, and Center for Intelligent Information Retrieval, as well as the Google Cloud Platform for donating the computing and networking services on which OpenReview.net runs.
            </p>
          </div>
        </div>
      </div>
    </footer>

  <div id="feedback-modal" class="modal fade" tabindex="-1" role="dialog">
    <div class="modal-dialog">
      <div class="modal-content">
        <div class="modal-header">
          <button type="button" class="close" data-dismiss="modal" aria-label="Close"><span aria-hidden="true">×</span></button>
          <h3 class="modal-title">Send Feedback</h3>
        </div>
        <div class="modal-body">
          <p>Enter your feedback below and we'll get back to you as soon as possible.</p>
          <form action="/feedback" method="POST">
            <div class="form-group">
              <input type="email" name="from" class="form-control" placeholder="Email" />
            </div>
            <div class="form-group">
              <input type="text" name="subject" class="form-control" placeholder="Subject" />
            </div>
            <div class="form-group">
              <textarea name="message" class="form-control feedback-input" rows="5" placeholder="Message" required=""></textarea>
            </div>
          <ul id="ui-id-2" tabindex="0" class="ui-menu ui-widget ui-widget-content" style="display: none;"></ul></form>
        </div>
        <div class="modal-footer">
          <button type="button" class="btn btn-default" data-dismiss="modal">Cancel</button>
          <button type="button" class="btn btn-primary">Send</button>
        </div>
      </div>
    </div>
  </div>

  <script type="text/javascript" async="" src="https://www.google-analytics.com/analytics.js"></script><script src="//cdnjs.cloudflare.com/ajax/libs/jquery/2.2.4/jquery.min.js"></script>
  <script>window.jQuery || document.write('&lt;script src="/static/js/vendor/jquery-2.2.4.min.js"&gt;&lt;\/script&gt;')</script>

  <script src="/static/dist/vendor.min.js"></script>
  <script src="/static/dist/templates.min.js"></script>
  <script src="/static/dist/openreview.min.js"></script>

    <script>window.legacyScripts = true;</script>


    <script async="" src="https://www.googletagmanager.com/gtag/js?id=UA-108703919-1"></script>
    <script>
      window.dataLayer = window.dataLayer || [];
      function gtag() { dataLayer.push(arguments); }
      gtag('js', new Date());
      gtag('config', 'UA-108703919-1');
    </script>


<div role="status" aria-live="assertive" aria-relevant="additions" class="ui-helper-hidden-accessible"></div></body></html>