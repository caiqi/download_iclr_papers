<!DOCTYPE html><html xmlns="http://www.w3.org/1999/xhtml" class="no-js"><head>
  <meta charset="utf-8" />
  <meta http-equiv="X-UA-Compatible" content="IE=edge,chrome=1" />
  <meta name="viewport" content="width=device-width, initial-scale=1" />

  <title>Learning to Represent Edits | OpenReview</title>
  <meta name="description" content="" />

      <meta name="citation_title" content="Learning to Represent Edits" />
        <meta name="citation_author" content="Anonymous" />
      <meta name="citation_publication_date" content="2018/09/27" />
      <meta name="citation_online_date" content="2018/09/27" />
      <meta name="citation_pdf_url" content="https://openreview.net/pdf?id=BJl6AjC5F7" />
      <meta name="twitter:card" content="summary" />
      <meta name="twitter:site" content="@openreviewnet" />
      <meta name="og:title" content="Learning to Represent Edits" />
      <meta name="og:description" content="We introduce the problem of learning distributed representations of edits. By combining a&#10;  " neural="" editor"="" with="" an="" "edit="" encoder",="" our="" models="" learn="" to="" represent="" the="" salient="" information="" of="" edit..."="" />
      <meta name="og:image" content="https://openreview.net/static/images/openreview_logo_512.png" />

  <link rel="stylesheet" href="https://fonts.googleapis.com/css?family=Noto+Sans:400,400i,700,700i" />

  <link rel="stylesheet" href="/static/css/bootstrap.min.css" />
  <link rel="stylesheet" href="/static/css/jquery-ui.min.css" />
  <link rel="stylesheet" href="/static/css/main.min.css" />
</head>

<body class="forum">
  <nav class="navbar navbar-inverse navbar-fixed-top" role="navigation">
    <div class="container">
  
      <div class="navbar-header">
        <button type="button" class="navbar-toggle collapsed" data-toggle="collapse" data-target="#navbar" aria-expanded="false" aria-controls="navbar">
          <span class="sr-only">Toggle navigation</span>
          <span class="icon-bar"></span>
          <span class="icon-bar"></span>
          <span class="icon-bar"></span>
        </button>
        <a class="navbar-brand home push-link" href="/"><strong>OpenReview</strong>.net</a>
      </div>
  
      <div id="navbar" class="navbar-collapse collapse">
        <form class="navbar-form navbar-left profile-search" role="search">
          <div class="form-group has-feedback">
            <input id="search_input" type="text" class="form-control ui-autocomplete-input" placeholder="Search ICLR 2019 Conference" autocomplete="off" />
            <span class="glyphicon glyphicon-search form-control-feedback" aria-hidden="true"></span>
          </div>
  
          <input id="search_group" type="hidden" value="ICLR.cc/2019/Conference" />
          <input id="search_content" type="hidden" value="all" />
        <ul id="ui-id-1" tabindex="0" class="ui-menu ui-widget ui-widget-content ui-autocomplete ui-front" style="display: none;"></ul></form>
  
        <ul class="nav navbar-nav navbar-right">
        
            <li id="user-menu"><a href="/login">Login</a></li>
        </ul>
      </div>
  
    </div>
  </nav>

  <div id="or-banner" class="banner" style="">
  <div class="container">
    <div class="row">
      <div class="col-xs-12"><a href="/group?id=ICLR.cc/2019/Conference" title="Venue Homepage"><img class="icon" src="/static/images/arrow_left.svg" /> Go to <strong>ICLR 2019 Conference</strong> homepage</a></div>
    </div>
  </div>
</div>
<div id="flash-message-container" class="alert alert-danger" role="alert" style="display: none;">
  <div class="container">
    <div class="row">
      <div class="col-xs-12">
        <div class="alert-content">
          <button type="button" class="close" aria-label="Close"><span aria-hidden="true">×</span></button>
        </div>
      </div>
    </div>
  </div>
</div>

<div class="container">
  <div class="row">
    <div class="col-xs-12">
      <main id="content" class="clearfix openbanner-visible legacy-styles"><div id="note_BJl6AjC5F7" class="note panel"><div class="title_pdf_row clearfix"><h2 class="note_content_title"><a>Learning to Represent Edits</a> <a class="note_content_pdf" href="/pdf?id=BJl6AjC5F7" title="Download PDF" target="_blank"><img src="/static/images/pdf_icon_blue.svg" /></a> </h2></div><div class="meta_row"><span class="signatures">Anonymous</span></div><div class="clearfix"><div class="meta_row pull-left"><span class="date item">28 Sep 2018 (modified: 11 Oct 2018)</span><span class="item">ICLR 2019 Conference Blind Submission</span><span class="item">Readers: <span class="readers-icon glyphicon glyphicon-globe"></span> Everyone</span><span class="item"><a class="action-bibtex-modal" data-bibtex="@inproceedings{    &#10;anonymous2019learning,    &#10;title={Learning to Represent Edits},    &#10;author={Anonymous},    &#10;booktitle={Submitted to International Conference on Learning Representations},    &#10;year={2019},    &#10;url={https://openreview.net/forum?id=BJl6AjC5F7},    &#10;note={under review}    &#10;}">Show Bibtex</a></span></div><div class="meta_row meta_actions"></div></div><div class="note_contents"><span class="note_content_field">Abstract: </span><span class="note_content_value">We introduce the problem of learning distributed representations of edits. By combining a
"neural editor" with an "edit encoder", our models learn to represent the salient
information of an edit and can be used to apply edits to new inputs.
We experiment on natural language and source code edit data. Our evaluation yields
promising results that suggest that our neural network models learn to capture
the structure and semantics of edits. We hope that this interesting task and
data source will inspire other researchers to work further on this problem.</span></div><div class="note_contents"><span class="note_content_field">Keywords: </span><span class="note_content_value">Representation Learning, Source Code, Natural Language, edit</span></div><div class="reply_row clearfix"><div class="item" id="reply_count">7 Replies</div></div></div><hr class="small" /><div id="note_children"><div class="note_with_children"><div id="note_H1gbD8dQ6Q" class="note panel"><div class="title_pdf_row clearfix"><h2 class="note_content_title"><a>To All Reviewers: Regarding Data Annotation</a>  <button class="btn btn-xs btn-default permalink-button" title="Link to this comment" data-permalink-url="https://openreview.net/forum?id=BJl6AjC5F7&amp;noteId=H1gbD8dQ6Q"><span class="glyphicon glyphicon-link" aria-hidden="true"></span></button></h2></div><div class="meta_row"><span class="signatures">ICLR 2019 Conference Paper935 Authors</span></div><div class="clearfix"><div class="meta_row pull-left"><span class="date item">10 Nov 2018</span><span class="item">ICLR 2019 Conference Paper935 Official Comment</span><span class="item">Readers: <span class="readers-icon glyphicon glyphicon-globe"></span> Everyone</span></div><div class="meta_row meta_actions"></div></div><div class="note_contents"><span class="note_content_field">Comment: </span><span class="note_content_value">To all reviewers:

We thank all reviewers for their insightful comments!

**Regarding Data Annotation**

We apologize for not detailing the annotation rubric and will make this clearer. We will update the main text to clarify the most important points, and provide the instructions and examples for the rating system in the supplementary material.

As also noted by Reviewer-#1, we realized that it is difficult to come up with a fine-grained rating system (e.g., using a 5-element scale) for characterizing semantic/syntactic similarity between edits, especially for free-form natural language data. We believe this problem alone would be an interesting research issue, reminiscent of studies in categorizing syntactic transformations in natural language (e.g., He et al., 2015). 

Therefore, we chose to use a simpler 3-element scale (semantically/syntactically equivalent edits, related edits, unrelated). For both natural language and code data, we designed detailed annotation instructions with illustrative examples (will be included in the supplementary material of the next version of our paper). Admittedly, this grading scheme is not perfect, as the category of “relevant edits” could be further divided, and it does not distinguish semantically similar edits from syntactically similar ones. However, we found no way to exactly define how to do such finer-grained annotations, and thus used our simple scheme. Note that this simple grading system is already effective in comparing the performance of different models. For example, we observe a clear win of Seq2Seq models over the bag-of-words baseline in both natural language and code datasets (Tables 1 and 2), and Graph2Tree with sequential edit encoder over Seq2Seq (Table 2), especially in Acc@1. 

The annotation was carried out by three of the authors, and we anonymized the source of systems that generated the output. Due to time limits we assigned different sampled edits to different annotators.  We will provide inter-rater agreement score shortly.
</span></div><div class="reply_row clearfix"></div></div><div class="children"></div></div><div class="note_with_children"><div id="note_HylSf_0a27" class="note panel"><div class="title_pdf_row clearfix"><h2 class="note_content_title"><a>This paper looks at learning to represent edits for text revisions and code changes. The main contribution is in defining a new task, providing a new dataset, and building simple neural network models that show good performance.</a>  <button class="btn btn-xs btn-default permalink-button" title="Link to this comment" data-permalink-url="https://openreview.net/forum?id=BJl6AjC5F7&amp;noteId=HylSf_0a27"><span class="glyphicon glyphicon-link" aria-hidden="true"></span></button></h2></div><div class="meta_row"><span class="signatures">ICLR 2019 Conference Paper935 AnonReviewer1</span></div><div class="clearfix"><div class="meta_row pull-left"><span class="date item">05 Nov 2018 (modified: 07 Nov 2018)</span><span class="item">ICLR 2019 Conference Paper935 Official Review</span><span class="item">Readers: <span class="readers-icon glyphicon glyphicon-globe"></span> Everyone</span></div><div class="meta_row meta_actions"></div></div><div class="note_contents"><span class="note_content_field">Review: </span><span class="note_content_value">This paper looks at learning to represent edits for text revisions and code changes. The main contributions are as follows:
* They define a new task of representing and predicting textual and code changes 
* They make available a new dataset of code changes (text edit dataset was already available) with labels of the type of change
* They try simple neural network models that show good performance in representing and predicting the changes

The NLP community has recently defined the problem of predicting atomic edits for text data (Faraqui, et al. EMNLP 2018, cited in the paper), and that is the source of their Wikipedia revision dataset. Although it is an interesting problem, it is not immediately clear from the Introduction of this paper what would be enabled by accurate prediction of atomic edits (i.e. simple insertions and deletions), and I hope the next version would elaborate on the motivation and significance for this new task. 

The "Fixer" dataset that they created is interesting. Those edits supposedly make the code better, so modeling those edits could lead to "better" code. Having that as labeled data enables a clean and convincing evaluation task of predicting similar edits.

The paper focuses on the novelty of the task and the dataset, so the models are simple variations of the existing bidirectional LSTM and the gated graph neural network. Because much of the input text (or code) does not change, the decoder gets to directly copy parts of the input. For code data, the AST is used instead of flat text of the code. These small changes seem reasonable and work well for this problem.

Evaluation is not easy for this task. For the task of representing the edits, they show visualizations of the clusters of similar edits and conduct a human evaluation to see how similar these edits actually are. This human evaluation is not described in detail, as they do not say how many people rated the similarity, who they were (how they were recruited), how they were instructed, and what the inter-rater agreement was. The edit prediction evaluation is done well, but it is not clear what it means when they say better prediction performance does not necessarily mean it generalizes better. That may be true, but then without another metric for better generalization, one cannot say that better performance means worse generalization. 

Despite these minor issues, the paper contributes significantly novel task, dataset, and results. I believe it will lead to interesting future research in representing text and code changes.</span></div><div class="note_contents"><span class="note_content_field">Rating: </span><span class="note_content_value">7: Good paper, accept</span></div><div class="note_contents"><span class="note_content_field">Confidence: </span><span class="note_content_value">3: The reviewer is fairly confident that the evaluation is correct</span></div><div class="reply_row clearfix"></div></div><div class="children"><div class="note_with_children"><div id="note_r1g6GvdXp7" class="note panel"><div class="title_pdf_row clearfix"><h2 class="note_content_title"><a>Thanks for your comments!</a>  <button class="btn btn-xs btn-default permalink-button" title="Link to this comment" data-permalink-url="https://openreview.net/forum?id=BJl6AjC5F7&amp;noteId=r1g6GvdXp7"><span class="glyphicon glyphicon-link" aria-hidden="true"></span></button></h2></div><div class="meta_row"><span class="signatures">ICLR 2019 Conference Paper935 Authors</span></div><div class="clearfix"><div class="meta_row pull-left"><span class="date item">10 Nov 2018</span><span class="item">ICLR 2019 Conference Paper935 Official Comment</span><span class="item">Readers: <span class="readers-icon glyphicon glyphicon-globe"></span> Everyone</span></div><div class="meta_row meta_actions"></div></div><div class="note_contents"><span class="note_content_field">Comment: </span><span class="note_content_value">Question: “what would be enabled by accurate prediction of atomic edits … elaborate on the motivation and significance for this new task”

Response: Our work focuses on developing a generic approach to represent and apply edits. On the WikiAtomicEdits data, one interesting application of our model would be facilitating the development of data exploration toolkits that cluster and visualizes semantically and syntactically similar edits (e.g., the example clusters shown in Table 9). Since our proposed approach is relatively general, we believe we could explore more interesting applications given access to parallel data of other forms of natural language edits. For example, our model could be used to represent and apply syntactic transfer given parallel corpora of sentences with different syntactic structures.

On the source code domain, our work enjoys more intriguing and immediate applications like learning to represent and apply code fixes from commit data, similar to the one-shot learning task we present in Section 4.4. Our work could also enable human-in-loop machine learning applications like clustering commit streams on GitHub at large-scale and helping users identify emerging “best practices” or bug fixes. Indeed, the initial motivation for our research was to automatically identify common improvements to source code that are not covered by existing tools.

Question: "human evaluation is not described in detail..."

Response: please refer to our general response regarding data annotation.

Question: “what it means when they say better prediction performance does not necessarily mean it generalizes better...”

Response: This observation is grounded in the comparison of the results displayed in Tables 4 and 5 in our end-to-end experiment on GitHubEdits data (Section 4.4). Table 4 indicates that given the encoding of an edit (x-, x+), the Seq2Seq editor is most precise in generating x+ from x-, (slightly) outperforming the Graph2Tree editor. We evaluate the generality of edit representations in our “one-shot” experiment, where we use the encoding of a related edit (x-, x+) to reconstruct x+’ from x-’. There, the Graph2Tree editor performs significantly better than the Seq2Seq editor. The latter experiment serves as a good proxy in evaluating the generalization ability of different system configurations, from whose result we derive the hypothesis that better performance with gold-standard edit encodings might not imply better performance with noisy edit encodings.

We apologize for the confusion and will update the text of the paper to clarify what we mean by generalizable and how we draw that conclusion from our experiments.
</span></div><div class="reply_row clearfix"></div></div><div class="children"></div></div></div></div><div class="note_with_children"><div id="note_H1eeIsRo3Q" class="note panel"><div class="title_pdf_row clearfix"><h2 class="note_content_title"><a>This work introduces a new learning task of automated edits for text/code, a learning framework for it, a dataset, and some evaluations but we found mostly the latter lacked, reducing our enthusiasm.</a>  <button class="btn btn-xs btn-default permalink-button" title="Link to this comment" data-permalink-url="https://openreview.net/forum?id=BJl6AjC5F7&amp;noteId=H1eeIsRo3Q"><span class="glyphicon glyphicon-link" aria-hidden="true"></span></button></h2></div><div class="meta_row"><span class="signatures">ICLR 2019 Conference Paper935 AnonReviewer3</span></div><div class="clearfix"><div class="meta_row pull-left"><span class="date item">04 Nov 2018 (modified: 07 Nov 2018)</span><span class="item">ICLR 2019 Conference Paper935 Official Review</span><span class="item">Readers: <span class="readers-icon glyphicon glyphicon-globe"></span> Everyone</span></div><div class="meta_row meta_actions"></div></div><div class="note_contents"><span class="note_content_field">Review: </span><span class="note_content_value">The authors state nicely and clearly the main contributions they see in their work (Intro, last paragraph). Specifically the state the paper: 1) present a new and important machine learning task, 2) present a family of models that capture the structure of edits and compute efficient representations, 3) create a new source code edit dataset, 4) perform a set of experiments on the learned edit representations and present promising empirical evidence that the models succeed in capturing the semantics of edits. 

We decided to organize this review by commenting on the above-stated contributions one at a time:

“A new and important machine learning task”

Regarding “new task”:

PRO: We are unfamiliar with past work which presents this precise task; the task is new. Section 5 makes a good case for the novelty of this work.

CON: None.


Regarding “important task”:

PRO: The authors motivate the task with tantalizing prospective applications-- automatically editing text and code, e.g. for grammar, clarity, and style. Conceptualizing edits as NLP objects of interest that can be concretely represented, clustered, and used for prediction is an advance.

CON: Many text editors, office suites, and coding IDEs already include features which automatically suggest or apply edits for grammar, clarity, and style. The authors do not describe shortcomings in existing tools that might be better addressed using distributed representations of edits. Consequently, the significance of the proposed contribution is unclear.


“A family of models that capture the structure of edits and compute efficient representations”

Regarding “a family of models”:

PRO: The family of models presented by the authors clearly generalizes: such models may be utilized for computational experiments on datasets and edit types beyond those specifically utilized in this evaluation. The authors apply well-utilized neural network architectures that may be trained and applied to large datasets. The architecture of the neural editor permits evaluation of the degree to which the editor successfully predicts the correct edit given a pre-edit input and a known representation of a similar edit.

CON: The authors do not propose any scheme under which edit representations might be utilized for automatically editing text or code when an edit very similar to the desired edit is not already known and its representation available as input. Hence, we find the authors do not sufficiently motivate the input scheme of their neural editor. The input scheme of the neural editor makes trivial the case in which no edit is needed, as the editor would learn during training that the output x+ should be the same as the input x- when the representation of the “zero edit” is given as input. While the authors discuss the importance of “bottlenecking” the edit encoder so that it does not simply learn to encode the desired output x+, they do not concretely demonstrate that the edit encoder has done otherwise in the final experiments. Related to that: If the authors aimed to actually solve automated edits in text/code then it seems crucial their data contained "negative examples" i.e. segments which require no edits. In such an evaluation one would test also when the algorithm introduces unnecessary/erroneous edits. 


Regarding “capture structure of edits”:

PRO: The authors present evidence that edit encoders tightly cluster relatively simple edits which involve adding or removing common tokens. The authors present evidence that relatively simple edits completed automatically by a “fixer” often cluster together, i.e. a known signal is retained in clustering. The authors present evidence that the nearest neighbors of edits in an edit-representation space often are semantically or structurally similar, as judged by human annotators. Section 4.3 includes interesting observations comparing edit patterns better captured by the graph or seq edit encoders. 

CON: The details of the human annotation tasks which generated the numerical results in Tables 1 and 2 are unclear: were unbiased third parties utilized? Were the edits stripped of their source-encoder label when evaluated? Objectively, what separates an “unrelated” from a “similar” edit, and what separates a “similar” from a “same” edit? Did multiple human annotators undertake this task in parallel, and what was their overall concordance (e.g. “intercoder reliability”)? Without concrete answers to these questions, the validity and significance of the DCG/NDCG results reported in Tables 1 and 2 are unclear. It is not clear from the two examples given in Table 1 that the three nearest neighbors embedded by the Seq encoder are “better”, i.e. overall more semantically and/or syntactically similar to the example edit, than those embedded by the Bag of Words model. It is unclear which specific aspects of “edit structure” are better captured by the Seq encoder than the Bag of Words model. The overall structure of Tables 1 and 2 is awkward, with concrete numerical results dominated by a spatially large section containing a small number of examples.


“create a new source code edit dataset”

PRO: The authors create a new source code edit dataset, an important contribution to the study of this new task.

CON: Minor: is the provided dataset large enough to do more than simple experiments? See note below on sample size.


“present promising empirical evidence that the models succeed in capturing the semantics of edits”

PRO: The experiment results show how frequently the end-to-end system successfully predicted the correct edit given a pre-edit input and a known representation of a similar edit. Gold standard accuracies of more than 70%, and averaged transfer learning accuracies of more than 30%, suggest that this system shows promise for capturing the semantics of edits.

CON: Due to concerns expressed above about the model design and evaluation of the edit representations, it remains unclear to what degree the models succeed in capturing the semantics of edits. Table 11 shows dramatic variation in success levels across fixer ID in the transfer learning task, yet the authors do not propose ways their end-to-end system might be adjusted to address areas of weak performance. The authors do not discuss the impact of training set size on their evaluation metrics. The authors do not discuss the degree to which their model training task would scale to larger language datasets such as those needed for the motivating applications.
</span></div><div class="note_contents"><span class="note_content_field">Rating: </span><span class="note_content_value">4: Ok but not good enough - rejection</span></div><div class="note_contents"><span class="note_content_field">Confidence: </span><span class="note_content_value">3: The reviewer is fairly confident that the evaluation is correct</span></div><div class="reply_row clearfix"></div></div><div class="children"><div class="note_with_children"><div id="note_rJeUUtum6X" class="note panel"><div class="title_pdf_row clearfix"><h2 class="note_content_title"><a>Thanks for your comments!</a>  <button class="btn btn-xs btn-default permalink-button" title="Link to this comment" data-permalink-url="https://openreview.net/forum?id=BJl6AjC5F7&amp;noteId=rJeUUtum6X"><span class="glyphicon glyphicon-link" aria-hidden="true"></span></button></h2></div><div class="meta_row"><span class="signatures">ICLR 2019 Conference Paper935 Authors</span></div><div class="clearfix"><div class="meta_row pull-left"><span class="date item">10 Nov 2018</span><span class="item">ICLR 2019 Conference Paper935 Official Comment</span><span class="item">Readers: <span class="readers-icon glyphicon glyphicon-globe"></span> Everyone</span></div><div class="meta_row meta_actions"></div></div><div class="note_contents"><span class="note_content_field">Comment: </span><span class="note_content_value">Thank you for the careful reading of the paper (including the lengthy appendices!), and elucidating concerns about validity of the task and method. We believe that several of these were due to a lack of clarity in our exposition, that can be resolved. We have attempted to clarify these below and will revise the paper to make things more clear before the end of the review period.

* Regarding "important task"

Response: existing editing systems, like the the grammar checker in MS Word and code refactoring module in IDEs, often use heavily engineered, domain-specific, manually crafted rules to perform editing. Our proposed learning-based model is a data-driven approach that automatically **learns** to extract, represent and apply edits from large-scale edit data, and it is also a **generic** system that could be applied to heterogeneous domains like text and source code. 

Additionally, using distributed representations also facilitates visualization (Figure 2) and clustering (Appendix B) of semantically similar edits. These novel applications open possibilities to develop human-assistance toolkits for discovering and extracting emerging edit patterns (e.g., new bug fixes from GitHub commits) for rule-based systems from large-scale edit data. For example, this could be used to drive the development of new rules for existing edit tools, by identifying common patterns not covered by existing capabilities. We apologize and will make this clearer. 

* Regarding "a family of models"

Response: We agree that our current system is not able to identify places where an edit should be performed, and that this is important future work. In this work, we have focused on (1) computing representations of edits that allow us to group similar changes, and (2) applying such representations in a new context. Both of these scenarios are already useful in human-in-the-loop scenarios. For example, a good solution to problem (1) can inform the development of new edit and refactoring tools (by observing common changes), whereas (2) can be used to propose changes that can be accepted/rejected by a human.

We will make this aspect of future work clearer in the next version of our paper.

* Regarding “capture structure of edits” and Human Evaluation

Response: please refer to our response regarding annotation.

* Regarding “present promising empirical evidence that the models succeed in capturing the semantics of edits” and Results in Table 11

Response: we thank the reviewer for his effort in analyzing the many statistics we present in Table 11! We remark that this task is a transfer learning task is indeed non-trivial. For instance, some fixer categories cover many different types of edits (e.g., RCS1077 (<a href="https://github.com/JosefPihrt/Roslynator/blob/master/docs/analyzers/RCS1077.md" target="_blank" rel="nofollow">https://github.com/JosefPihrt/Roslynator/blob/master/docs/analyzers/RCS1077.md</a> handles 12 differents ways of optimizing LINQ expressions). In these cases, edits are semantically related (“improving a LINQ expression”), but this relationship only exists at a high level and is not directly reflected to the syntactic transformations required by the fixer.

Other categories contain complex refactoring rules that require reasoning about a chain of expressions (e.g., RCS1197 (https://github.com/JosefPihrt/Roslynator/blob/master/docs/analyzers/RCS1197.md turns sb.Append(s1 + s2 + … + sN) into sb.Append(s1).Append(s2).[...]Append(sN)), which our current models are unable to reason about. We believe that further advances in (general) learning from source code are required to correctly handle theses cases.

We will expand Appendix C with a more fine-grained analysis of the results in Table 11, providing more background on categories whose results deviate substantially from the average.

[Impact of training set size and scalability]: Thanks for the comments! We will discuss this in our final version.


</span></div><div class="reply_row clearfix"></div></div><div class="children"></div></div></div></div><div class="note_with_children"><div id="note_HkeCSi493X" class="note panel"><div class="title_pdf_row clearfix"><h2 class="note_content_title"><a>review</a>  <button class="btn btn-xs btn-default permalink-button" title="Link to this comment" data-permalink-url="https://openreview.net/forum?id=BJl6AjC5F7&amp;noteId=HkeCSi493X"><span class="glyphicon glyphicon-link" aria-hidden="true"></span></button></h2></div><div class="meta_row"><span class="signatures">ICLR 2019 Conference Paper935 AnonReviewer2</span></div><div class="clearfix"><div class="meta_row pull-left"><span class="date item">03 Nov 2018 (modified: 07 Nov 2018)</span><span class="item">ICLR 2019 Conference Paper935 Official Review</span><span class="item">Readers: <span class="readers-icon glyphicon glyphicon-globe"></span> Everyone</span></div><div class="meta_row meta_actions"></div></div><div class="note_contents"><span class="note_content_field">Review: </span><span class="note_content_value">The main contributions of the paper are an edit encoder model similar to (Guu et al. 2017 <a href="http://aclweb.org/anthology/Q18-1031)," target="_blank" rel="nofollow">http://aclweb.org/anthology/Q18-1031),</a> a new dataset of tree-structured source code edits, and thorough and well thought-out analysis of the edit encodings. The paper is clearly written, and provides clear support for each of their main claims.

I think this would be of interest to NLP researchers and others working on sequence- and graph-transduction models, but I think the authors could have gone further to demonstrate the robustness of their edit encodings and their applicability to other tasks. This would also benefit greatly from a more direct comparison to Guu et al. 2017, which presents a very similar "neural editor" model.

Some more specific points:

- I really like the idea of transferring edits from one context to another. The one-shot experiment is well-designed, however it would benefit from also having a lower bound to get a better sense of how good the encodings are.

- If I'm reading it correctly, the edit encoder has access to the full sequences x- and x+, in addition to the alignment symbols. I wonder if this hurts the quality of the representations, since it's possible (albeit not efficient) to memorize the output sequence x+ and decode it directly from the 512-dimensional vector. Have you explored more constrained versions of the edit encoder (such as the bag-of-edits from Guu et al. 2017) or alternate learning objectives to control for this?

- The WikiAtomicEdits corpus has 13.7 million English insertions - why did you subsample this to only 1M? There is also a human-annotated subset of that you might use as evaluation data, similar to the C#Fixers set.

- On the human evaluation: Who were the annotators? The categories "similar edit", and "semantically or syntactically same edit" seem to leave a lot to interpretation; were more specific instructions given? It also might be interesting, if possible, to separately classify syntactically similar and semantically similar edits.

- On the automatic evaluation: accuracy seems brittle for evaluating sequence output. Did you consider reporting BLEU, ROUGE, or another "soft" sequence metric?

- It would be worth citing existing literature on classification of Wikipedia edits, for example Yang et al. 2017 (https://www.cs.cmu.edu/~diyiy/docs/emnlp17.pdf). An interesting experiment would be to correlate your edit encodings with their taxonomy.</span></div><div class="note_contents"><span class="note_content_field">Rating: </span><span class="note_content_value">6: Marginally above acceptance threshold</span></div><div class="note_contents"><span class="note_content_field">Confidence: </span><span class="note_content_value">4: The reviewer is confident but not absolutely certain that the evaluation is correct</span></div><div class="reply_row clearfix"></div></div><div class="children"><div class="note_with_children"><div id="note_Hyg1fVtXaQ" class="note panel"><div class="title_pdf_row clearfix"><h2 class="note_content_title"><a>Thanks for the comments</a>  <button class="btn btn-xs btn-default permalink-button" title="Link to this comment" data-permalink-url="https://openreview.net/forum?id=BJl6AjC5F7&amp;noteId=Hyg1fVtXaQ"><span class="glyphicon glyphicon-link" aria-hidden="true"></span></button></h2></div><div class="meta_row"><span class="signatures">ICLR 2019 Conference Paper935 Authors</span></div><div class="clearfix"><div class="meta_row pull-left"><span class="date item">10 Nov 2018</span><span class="item">ICLR 2019 Conference Paper935 Official Comment</span><span class="item">Readers: <span class="readers-icon glyphicon glyphicon-globe"></span> Everyone</span></div><div class="meta_row meta_actions"></div></div><div class="note_contents"><span class="note_content_field">Comment: </span><span class="note_content_value">*robustness of edit encodings*: Thanks for the comment! Directly measuring the robustness of edit encodings is non-trivial, but our one-shot learning experiments (Sec. 4.4) serve as a good proxy by testing the editing accuracy using the edit encoding from a similar example.

*applicability to other tasks*: Our proposed method is general and could be applied to other structured transduction tasks. We perform experiment on natural language edits (sequential) and source code commit data (tree-structured), since these are two commonly occurring sources of edits. We leave applying our model to other data sources as interesting future work.

*comparison with Guu et al., 2017*: Thanks for pointing out the related work by Guu et al! As discussed in Section 5, we remark that our motivation and research issues are very different, and these two models are not directly comparable --- Guu et al. focus on learning a generative language model by marginalizing over latent edits, while our work focuses on discriminative learning of (1) representing edits given the original (x-) and edited (x+) data, and (2) applying the learned edit to new input data. We therefore directly evaluate the quality of neighboring edit representations via human annotation, and the end-to-end performance of applying edits to both parallel data and in a novel one-shot learning scenario, which are not covered in Guu et al.

Nevertheless, our model architecture shares a similar spirit with Guu et al. For example, the model in Guu et al. also has an edit encoder based on “Bag-of-Edits” (i.e., the posterior distribution $q(z|x-, x+)$) and a seq2seq generation (reconstruction) model of x+ given x- and the edit representation z. In some sense, our seq2seq editor with a “Bag-of-Edits” edit encoder would be similar as the “discriminative” version of Guu et al. We will make the difference between this research and Guu et al clearer in an updated version of the paper. Please also refer to below for our response to the “Bag-of-Edits” edit encoder. 

Response to your specific questions:

*lower-bounding transfer learning results*: Thanks for the comments! Having a lower-bound is helpful in understanding the relative advantage of our proposed method, however it is not clear what a reasonable lower-bounding baseline would be. One baseline would be an editor model (e.g., Graph2Tree with sequential edit encoder) that doesn’t use edit encodings. 

*constrained versions of the edit encoder*: First, we remark that our Bag-of-Word edit encoder (Table 1 and 2) is similar to a “Bag-of-Edits” model, where the representation of an edit is modeled by a vector of added/deleted tokens (we use different vocabularies for added and deleted words). Our neural edit encoders have access to the full sequences x- and x+. 

We also tried a distributional bag-of-edits model like the one used in Guu et al., using an LSTM to summarize only changed tokens. This model had worse performance in our end-to-end experiment (Table 4) and we therefore we did not include the results. Through error analysis we found that many edits are **context and positional sensitive**, and encoding context (i.e., full sequences) is important. For instance, the WikiAtomicEdits examples we present in Table 9 clearly indicate that semantically similar insertions also share similar editing positions, which cannot be captured by the bag-of-edits encoder as in Guu et al. This might be more obvious for structured data source like code edits (c.f., Table 10). For instance, in the first example in Table 10, `Equal()` can be changed to `Empty()` **only** in the `Assert` namespace (i.e., the context). We apologize for the confusion and will include more results and analysis in the final version, facilitating more direct comparison with the editor encoder in Guu et al. Nevertheless, we remark that as discussed above, our work is not directly comparable with Guu et al.

*subsampling WikiAtomicEdits*: At the time of submission the WikiAtomicEdits dataset could not be downloaded in full, due to an error with the zip file provided. We managed to extract the first 1M edits from the dataset. We believe that the full corpus would not present significantly different statistical properties from the 1M samples we used.

*human evaluation*: please refer to our response regarding annotation. The idea of separating syntactically and semantically similar edits is also very interesting, which we will explore in our final version.

*soft metric*: Thanks for the comment! We can definitely do BLEU evaluation on WikiAtomEdits. For source code data, a sensible “soft” metric on source code still remains an open research issue (Yin and Neubig, 2017). We will include more discussion in our final version.

*classifying Wikipedia edits*: This is a very great idea, thanks for suggesting this. Given the time constraints, we will examine the feasibility of doing something like this for the final version of the paper.


</span></div><div class="reply_row clearfix"></div></div><div class="children"></div></div></div></div></div></main></div>
  </div>
</div>


    <!-- Footer -->
    <footer class="sitemap">
      <div class="container">
        <div class="row hidden-xs">
          <div class="col-sm-4">
            <ul class="list-unstyled">
              <li><a href="/" class="home">Home</a></li>
              <li><a href="/venues">All Venues</a></li>
            </ul>
          </div>
    
          <div class="col-sm-4">
            <ul class="list-unstyled">
              <li><a href="/about">About OpenReview</a></li>
              <li><a href="/contact">Contact</a></li>
              <li><a href="#" data-toggle="modal" data-target="#feedback-modal">Feedback</a></li>
            </ul>
          </div>
    
          <div class="col-sm-4">
            <ul class="list-unstyled">
              <li><a href="/terms">Terms of Service</a></li>
              <li><a href="/privacy">Privacy Policy</a></li>
            </ul>
          </div>
        </div>
    
        <div class="row visible-xs-block">
          <div class="col-xs-6">
            <ul class="list-unstyled">
              <li><a href="/" class="home">Home</a></li>
              <li><a href="/about">About OpenReview</a></li>
              <li><a href="#" data-toggle="modal" data-target="#feedback-modal">Feedback</a></li>
            </ul>
          </div>
    
          <div class="col-xs-6">
            <ul class="list-unstyled">
              <li><a href="/contact">Contact</a></li>
              <li><a href="/terms">Terms of Service</a></li>
              <li><a href="/privacy">Privacy Policy</a></li>
            </ul>
          </div>
        </div>
      </div>
    </footer>
    
    <footer class="sponsor">
      <div class="container">
        <div class="row">
          <div class="col-sm-10 col-sm-offset-1">
            <p class="text-center">
              OpenReview is created by the <a href="http://www.iesl.cs.umass.edu/" target="_blank">Information Extraction and Synthesis Laboratory</a>, College of Information and Computer Science, University of Massachusetts Amherst. We gratefully acknowledge the support of the OpenReview sponsors:  Google,  Facebook, NSF, the University of Massachusetts Amherst Center for Data Science, and Center for Intelligent Information Retrieval, as well as the Google Cloud Platform for donating the computing and networking services on which OpenReview.net runs.
            </p>
          </div>
        </div>
      </div>
    </footer>

  <div id="feedback-modal" class="modal fade" tabindex="-1" role="dialog">
    <div class="modal-dialog">
      <div class="modal-content">
        <div class="modal-header">
          <button type="button" class="close" data-dismiss="modal" aria-label="Close"><span aria-hidden="true">×</span></button>
          <h3 class="modal-title">Send Feedback</h3>
        </div>
        <div class="modal-body">
          <p>Enter your feedback below and we'll get back to you as soon as possible.</p>
          <form action="/feedback" method="POST">
            <div class="form-group">
              <input type="email" name="from" class="form-control" placeholder="Email" />
            </div>
            <div class="form-group">
              <input type="text" name="subject" class="form-control" placeholder="Subject" />
            </div>
            <div class="form-group">
              <textarea name="message" class="form-control feedback-input" rows="5" placeholder="Message" required=""></textarea>
            </div>
          <ul id="ui-id-2" tabindex="0" class="ui-menu ui-widget ui-widget-content" style="display: none;"></ul></form>
        </div>
        <div class="modal-footer">
          <button type="button" class="btn btn-default" data-dismiss="modal">Cancel</button>
          <button type="button" class="btn btn-primary">Send</button>
        </div>
      </div>
    </div>
  </div>

  <script type="text/javascript" async="" src="https://www.google-analytics.com/analytics.js"></script><script src="//cdnjs.cloudflare.com/ajax/libs/jquery/2.2.4/jquery.min.js"></script>
  <script>window.jQuery || document.write('&lt;script src="/static/js/vendor/jquery-2.2.4.min.js"&gt;&lt;\/script&gt;')</script>

  <script src="/static/dist/vendor.min.js"></script>
  <script src="/static/dist/templates.min.js"></script>
  <script src="/static/dist/openreview.min.js"></script>

    <script>window.legacyScripts = true;</script>


    <script async="" src="https://www.googletagmanager.com/gtag/js?id=UA-108703919-1"></script>
    <script>
      window.dataLayer = window.dataLayer || [];
      function gtag() { dataLayer.push(arguments); }
      gtag('js', new Date());
      gtag('config', 'UA-108703919-1');
    </script>


<div role="status" aria-live="assertive" aria-relevant="additions" class="ui-helper-hidden-accessible"></div></body></html>