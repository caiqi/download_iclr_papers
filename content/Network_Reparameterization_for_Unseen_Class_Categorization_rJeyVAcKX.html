<!DOCTYPE html><html xmlns="http://www.w3.org/1999/xhtml" class="no-js"><head>
  <meta charset="utf-8" />
  <meta http-equiv="X-UA-Compatible" content="IE=edge,chrome=1" />
  <meta name="viewport" content="width=device-width, initial-scale=1" />

  <title>Network Reparameterization for Unseen Class Categorization | OpenReview</title>
  <meta name="description" content="" />

      <meta name="citation_title" content="Network Reparameterization for Unseen Class Categorization" />
        <meta name="citation_author" content="Anonymous" />
      <meta name="citation_publication_date" content="2018/09/27" />
      <meta name="citation_online_date" content="2018/09/27" />
      <meta name="citation_pdf_url" content="https://openreview.net/pdf?id=rJeyV2AcKX" />
      <meta name="twitter:card" content="summary" />
      <meta name="twitter:site" content="@openreviewnet" />
      <meta name="og:title" content="Network Reparameterization for Unseen Class Categorization" />
      <meta name="og:description" content="Many problems with large-scale labeled training data have been impressively solved by deep learning. However, Unseen Class Categorization (UCC) with minimal information provided about target..." />
      <meta name="og:image" content="https://openreview.net/static/images/openreview_logo_512.png" />

  <link rel="stylesheet" href="https://fonts.googleapis.com/css?family=Noto+Sans:400,400i,700,700i" />

  <link rel="stylesheet" href="/static/css/bootstrap.min.css" />
  <link rel="stylesheet" href="/static/css/jquery-ui.min.css" />
  <link rel="stylesheet" href="/static/css/main.min.css" />
</head>

<body class="forum">
  <nav class="navbar navbar-inverse navbar-fixed-top" role="navigation">
    <div class="container">
  
      <div class="navbar-header">
        <button type="button" class="navbar-toggle collapsed" data-toggle="collapse" data-target="#navbar" aria-expanded="false" aria-controls="navbar">
          <span class="sr-only">Toggle navigation</span>
          <span class="icon-bar"></span>
          <span class="icon-bar"></span>
          <span class="icon-bar"></span>
        </button>
        <a class="navbar-brand home push-link" href="/"><strong>OpenReview</strong>.net</a>
      </div>
  
      <div id="navbar" class="navbar-collapse collapse">
        <form class="navbar-form navbar-left profile-search" role="search">
          <div class="form-group has-feedback">
            <input id="search_input" type="text" class="form-control ui-autocomplete-input" placeholder="Search ICLR 2019 Conference" autocomplete="off" />
            <span class="glyphicon glyphicon-search form-control-feedback" aria-hidden="true"></span>
          </div>
  
          <input id="search_group" type="hidden" value="ICLR.cc/2019/Conference" />
          <input id="search_content" type="hidden" value="all" />
        <ul id="ui-id-1" tabindex="0" class="ui-menu ui-widget ui-widget-content ui-autocomplete ui-front" style="display: none;"></ul></form>
  
        <ul class="nav navbar-nav navbar-right">
        
            <li id="user-menu"><a href="/login">Login</a></li>
        </ul>
      </div>
  
    </div>
  </nav>

  <div id="or-banner" class="banner" style="">
  <div class="container">
    <div class="row">
      <div class="col-xs-12"><a href="/group?id=ICLR.cc/2019/Conference" title="Venue Homepage"><img class="icon" src="/static/images/arrow_left.svg" /> Go to <strong>ICLR 2019 Conference</strong> homepage</a></div>
    </div>
  </div>
</div>
<div id="flash-message-container" class="alert alert-danger" role="alert" style="display: none;">
  <div class="container">
    <div class="row">
      <div class="col-xs-12">
        <div class="alert-content">
          <button type="button" class="close" aria-label="Close"><span aria-hidden="true">Ã—</span></button>
        </div>
      </div>
    </div>
  </div>
</div>

<div class="container">
  <div class="row">
    <div class="col-xs-12">
      <main id="content" class="clearfix openbanner-visible legacy-styles"><div id="note_rJeyV2AcKX" class="note panel"><div class="title_pdf_row clearfix"><h2 class="note_content_title"><a>Network Reparameterization for Unseen Class Categorization</a> <a class="note_content_pdf" href="/pdf?id=rJeyV2AcKX" title="Download PDF" target="_blank"><img src="/static/images/pdf_icon_blue.svg" /></a> </h2></div><div class="meta_row"><span class="signatures">Anonymous</span></div><div class="clearfix"><div class="meta_row pull-left"><span class="date item">28 Sep 2018 (modified: 11 Oct 2018)</span><span class="item">ICLR 2019 Conference Blind Submission</span><span class="item">Readers: <span class="readers-icon glyphicon glyphicon-globe"></span> Everyone</span><span class="item"><a class="action-bibtex-modal" data-bibtex="@inproceedings{    &#10;anonymous2019network,    &#10;title={Network Reparameterization for Unseen Class Categorization},    &#10;author={Anonymous},    &#10;booktitle={Submitted to International Conference on Learning Representations},    &#10;year={2019},    &#10;url={https://openreview.net/forum?id=rJeyV2AcKX},    &#10;note={under review}    &#10;}">Show Bibtex</a></span></div><div class="meta_row meta_actions"></div></div><div class="note_contents"><span class="note_content_field">Abstract: </span><span class="note_content_value">Many problems with large-scale labeled training data have been impressively solved by deep learning. However, Unseen Class Categorization (UCC) with minimal information provided about target classes is the most commonly encountered setting in industry, which remains a challenging research problem in machine learning. Previous approaches to UCC either fail to generate a powerful discriminative feature extractor or fail to learn a flexible classifier that can be easily adapted to unseen classes. In this paper, we propose to address these issues through network reparameterization, \textit{i.e.}, reparametrizing the learnable weights of a network as a function of other variables, by which we decouple the feature extraction part and the classification part of a deep classification model to suit the special setting of UCC, securing both strong discriminability and excellent adaptability. Extensive experiments for UCC on several widely-used benchmark datasets in the settings of zero-shot and few-shot learning demonstrate that, our method with network reparameterization achieves state-of-the-art performance.</span></div><div class="note_contents"><span class="note_content_field">Keywords: </span><span class="note_content_value">Unseen class categorization, network reparameterization, few-shot learning, zero-shot learning</span></div><div class="note_contents"><span class="note_content_field">TL;DR: </span><span class="note_content_value">A unified frame for both few-shot learning and zero-shot learning based on network reparameterization</span></div><div class="reply_row clearfix"><div class="item" id="reply_count">12 Replies</div></div></div><hr class="small" /><div id="note_children"><div class="note_with_children"><div id="note_rke1slj93X" class="note panel"><div class="title_pdf_row clearfix"><h2 class="note_content_title"><a>This paper proposes a framework to fully utilize the training data for zero/few-shot learning. The improvement is notable and the motivation for few-shot learning is great. The reference needs improvement.</a>  <button class="btn btn-xs btn-default permalink-button" title="Link to this comment" data-permalink-url="https://openreview.net/forum?id=rJeyV2AcKX&amp;noteId=rke1slj93X"><span class="glyphicon glyphicon-link" aria-hidden="true"></span></button></h2></div><div class="meta_row"><span class="signatures">ICLR 2019 Conference Paper1415 AnonReviewer2</span></div><div class="clearfix"><div class="meta_row pull-left"><span class="date item">03 Nov 2018 (modified: 07 Nov 2018)</span><span class="item">ICLR 2019 Conference Paper1415 Official Review</span><span class="item">Readers: <span class="readers-icon glyphicon glyphicon-globe"></span> Everyone</span></div><div class="meta_row meta_actions"></div></div><div class="note_contents"><span class="note_content_field">Review: </span><span class="note_content_value">Pros:

1) The paper points out the issue of learning feature extractor in existing few-shot learning. Such an observation is crucial and the proposed method leads to significant improvement. The authors also include the ablation studies w/ and w/o training features using the dataset-wise information to support the claim.

Cons:

1) While the authors tend to sell the idea for both zero-shot (ZSL) and few-shot learning, the novelty to ZSL is actually much small. First of all, the features are pre-trained from ImageNet, so the episode-based issue is not applied. Second, learning a mapping from attributes to classifier weights have already been proposed but not cited or discussed at all. The authors should compare to them and adjust the claim.

Jimmy Ba et al., "Predicting deep zero-shot convolutional neural networks using textual descriptions," CVPR 2015
Xiaolong Wang et al., "Zero-shot Recognition via Semantic Embeddings and Knowledge Graphs," CVPR 2018
(Changpinyo et al., 2016)

Third, the issue of generalized ZSL is raised in the following paper, not in (Romera-Paredes 2015). The following paper has also pointed out a framework for zero-shot and few-shot learning similar to what the paper proposed.

Wei-Lun Chao et al., "An empirical study and analysis of generalized zero-shot learning for object recognition in the wild," ECCV, 2016

2) Some implementation details seem to be missing:
2-1) What is the architecture of g_\phi? for both ZSL and FSL.
2-2) Do the authors fine-tune the resnet features for ZSL?
2-3) Do the authors use the same architecture of f_\theta as other methods in Table 3? If not, the comparison might be unfair. The authors should explicitly point out what architectures have been used for different methods.
2-4) The authors should include Ours with episode-based training in Table 3.

3) Improvement on ZSL needs more analysis. Specifically, many zero-shot learning methods do not use the training procedure mentioned in Section 4.1. The authors should thus compare the proposed method/architecture w/o this training procedure to show the difference. This study would benefit the ZSL community.

Other comments:
I would suggest the authors focusing on FSL.</span></div><div class="note_contents"><span class="note_content_field">Rating: </span><span class="note_content_value">5: Marginally below acceptance threshold</span></div><div class="note_contents"><span class="note_content_field">Confidence: </span><span class="note_content_value">5: The reviewer is absolutely certain that the evaluation is correct and very familiar with the relevant literature</span></div><div class="reply_row clearfix"></div></div><div class="children"></div></div><div class="note_with_children"><div id="note_HylZhe-9h7" class="note panel"><div class="title_pdf_row clearfix"><h2 class="note_content_title"><a>Little novelty, there are prior works that have the same idea, but this paper fails to discuss any of them</a>  <button class="btn btn-xs btn-default permalink-button" title="Link to this comment" data-permalink-url="https://openreview.net/forum?id=rJeyV2AcKX&amp;noteId=HylZhe-9h7"><span class="glyphicon glyphicon-link" aria-hidden="true"></span></button></h2></div><div class="meta_row"><span class="signatures">ICLR 2019 Conference Paper1415 AnonReviewer3</span></div><div class="clearfix"><div class="meta_row pull-left"><span class="date item">03 Nov 2018 (modified: 07 Nov 2018)</span><span class="item">ICLR 2019 Conference Paper1415 Official Review</span><span class="item">Readers: <span class="readers-icon glyphicon glyphicon-globe"></span> Everyone</span></div><div class="meta_row meta_actions"></div></div><div class="note_contents"><span class="note_content_field">Review: </span><span class="note_content_value">Summary
This paper proposes to learn a neural network that predicts classifier weights of unseen classes from attribute embeddings in zero-shot learning or from averaged feature embeddings of  available samples in few-shot learning. The approch is validated on  established zero-shot and few-shot learning benchmarks and results show some significant improvement over the baselines.  

Pros
-Well written
-Good empirical results

Cons
-Little novelty. This paper fails to discuss the following highly similar related works. [1] learns a nueral network (MLP) to predict classifier weights and convolutional filters from text features in the context of zero-shot learning. [2] predicts the classifier weights of unseen classes by doing graph convolution on top of the semantic knowledge graph and word embedding. [3] tackles few-shot learning by learning a MLP to predict classifier weights of novel classes from image features. In my point of view, the key idea of this paper is highly close to [1] and [3]. Unfortunately, this paper seems to be not aware of those important prior works. 

[1] Predicting Deep Zero-Shot Convolutional Neural Networks using Textual Descriptions.  Ba et al.  ICCV'15
[2] Zero-shot Recognition via Semantic Embeddings and Knowledge Graphs. Wang et al. CVPR'18
[3] Few-Shot Image Recognition by Predicting Parameters from Activations. Qiao et al. CVPR'18

-Stronger zero-shot learning baselines are missing. The numbers in GBU have been improved significantly by some CVPR'18 papers, i.e. [4]. I suspect Table 2 is not current the state-of-the-art.
[4] Generalized Zero-Shot Learning via Synthesized Examples. Verma et al. CVPR'18

Based on the above analysis, I would rate this paper as "Clear rejection". In the rebuttal,  I suggest the authors to discuss the main differences between their approach and [1], [3].</span></div><div class="note_contents"><span class="note_content_field">Rating: </span><span class="note_content_value">3: Clear rejection</span></div><div class="note_contents"><span class="note_content_field">Confidence: </span><span class="note_content_value">5: The reviewer is absolutely certain that the evaluation is correct and very familiar with the relevant literature</span></div><div class="reply_row clearfix"></div></div><div class="children"></div></div><div class="note_with_children"><div id="note_BygYkmp-2m" class="note panel"><div class="title_pdf_row clearfix"><h2 class="note_content_title"><a>Hard to evaluate the experimental results.</a>  <button class="btn btn-xs btn-default permalink-button" title="Link to this comment" data-permalink-url="https://openreview.net/forum?id=rJeyV2AcKX&amp;noteId=BygYkmp-2m"><span class="glyphicon glyphicon-link" aria-hidden="true"></span></button></h2></div><div class="meta_row"><span class="signatures">ICLR 2019 Conference Paper1415 AnonReviewer1</span></div><div class="clearfix"><div class="meta_row pull-left"><span class="date item">27 Oct 2018 (modified: 07 Nov 2018)</span><span class="item">ICLR 2019 Conference Paper1415 Official Review</span><span class="item">Readers: <span class="readers-icon glyphicon glyphicon-globe"></span> Everyone</span></div><div class="meta_row meta_actions"></div></div><div class="note_contents"><span class="note_content_field">Review: </span><span class="note_content_value">The paper addresses both zero-shot and few-shot learning in a unified framework. The key idea is to separate the deep network model into two networks. The first, aimed at learning a feature representation, is trained using standard multi-class but using cosine instead of dot product . The second, is trained using episode-based meta learning. 
The paper suggests that by training the features in a standard way, the representation is more rich and provides better space for learning the ZS and FS classes. 

The idea is very reasonable, and the experimental evaluations show  large improvement over existing baselines. 

My main concern is that evaluating ZSL and FSL, is often tricky, and one should be careful about the details such that no information is leaked to the unseen classes. The paper is written in a way that makes it hard to evaluate if all the experiments were done in a proper way. Some of gains claimed, like the accuracy improvement for GZSL are very large, reporting by +70% or even +150%(aPY). Sometimes this is a sign that the evaluations are not done in the same way as the baselines, or other issues. More careful analysis is expected to convince that the gains are due to the proposed method. 

I would have liked to see (1) more detailed analysis of why the approach works (2) Ablation experiments and more convincing evidence that the improvement is indeed due to the way the representation is trained. (3) Evidence that the feature representation is better. (4) more details about the evaluation procedure. 

For example these can include  (1) repeat the experiments with the same setup, but with a feature representation learned in an episodic way. (2) Evaluate the quality of the representation on another task (3) For the GZSL experiments,  report separately the accuracy on seen and unseen classes. (4) qualitative analysis linking wins and losses to changes in the representation. (5) tuning the size of episodic training to support the claim that small episodes lead to worse representation. 

Other comments: 
Some more recent baselines are not included,  e.g. better performance on CUB (57.8%) was reported by 
Atzmon et al, Probabilitis AND-OR Attribute Grouping for Zero-Shot Learning, UAI 2018.  Other improvement by 
Zhang and Koniusz. Zero-shot kernel learning. CVPR, 2018.


Minor comments :
-- clarify which classes were used in train, validation, test
-- Eq 6: L is defined as a function of phi. not used. 



</span></div><div class="note_contents"><span class="note_content_field">Rating: </span><span class="note_content_value">5: Marginally below acceptance threshold</span></div><div class="note_contents"><span class="note_content_field">Confidence: </span><span class="note_content_value">3: The reviewer is fairly confident that the evaluation is correct</span></div><div class="reply_row clearfix"></div></div><div class="children"></div></div><div class="note_with_children"><div id="note_S1gQL_Lfc7" class="note panel"><div class="title_pdf_row clearfix"><h2 class="note_content_title"><a>Minor clarifications about pre-training</a>  <button class="btn btn-xs btn-default permalink-button" title="Link to this comment" data-permalink-url="https://openreview.net/forum?id=rJeyV2AcKX&amp;noteId=S1gQL_Lfc7"><span class="glyphicon glyphicon-link" aria-hidden="true"></span></button></h2></div><div class="meta_row"><span class="signatures">(anonymous)</span></div><div class="clearfix"><div class="meta_row pull-left"><span class="date item">03 Oct 2018</span><span class="item">ICLR 2019 Conference Paper1415 Public Comment</span><span class="item">Readers: <span class="readers-icon glyphicon glyphicon-globe"></span> Everyone</span></div><div class="meta_row meta_actions"></div></div><div class="note_contents"><span class="note_content_field">Comment: </span><span class="note_content_value">Could you please specify exactly how many steps were used to pre-train features for both ZSL and FSL? The paper doesn't explicitly give the number of steps in an epoch and neither does the cited ResNet paper? Also, could you please clarify how many images and classes were used for pre-training in both cases? Thank you!
</span></div><div class="reply_row clearfix"></div></div><div class="children"><div class="note_with_children"><div id="note_BkgXfuE45Q" class="note panel"><div class="title_pdf_row clearfix"><h2 class="note_content_title"><a>Feature pre-training is performed using standard multi-class classification procedures</a>  <button class="btn btn-xs btn-default permalink-button" title="Link to this comment" data-permalink-url="https://openreview.net/forum?id=rJeyV2AcKX&amp;noteId=BkgXfuE45Q"><span class="glyphicon glyphicon-link" aria-hidden="true"></span></button></h2></div><div class="meta_row"><span class="signatures">ICLR 2019 Conference Paper1415 Authors</span></div><div class="clearfix"><div class="meta_row pull-left"><span class="date item">05 Oct 2018 (modified: 16 Oct 2018)</span><span class="item">ICLR 2019 Conference Paper1415 Official Comment</span><span class="item">Readers: <span class="readers-icon glyphicon glyphicon-globe"></span> Everyone</span></div><div class="meta_row meta_actions"></div></div><div class="note_contents"><span class="note_content_field">Comment: </span><span class="note_content_value">Thanks for the comments. We train our feature extraction network by following standard multi-class classification training procedures, i.e., randomly sampling a batch of images, feeding them to the network, calculating the cross-entropy loss, and doing the back-propagation.

The number of steps in each epoch is dependent on the dataset.  It is the number of randomly sampled batches that are required to cover the whole dataset. It is not a hyper-parameter set by us in our PyTorch implementation, but determined automatically by the provided data loader.  For example, for the mini-imagenet dataset, we found that the number of batches required in a epoch is 1535 with batch size 25.

As to the number of images used for training the feature extraction network, we follow strictly the previous methods in both tasks to make fair comparisons. For FSL, we use 64 classes, 600 images for each class, to train the feature extraction network on the mini-imagenet dataset. For the CUB dataset, we use the 100 classes  for training, where each class has about 40 images. For ZSL,  we use the ResNet101 model provided by Xian et al. (2018) to get a 2048-dimension vector for each input image, following the compared methods. </span></div><div class="reply_row clearfix"></div></div><div class="children"><div class="note_with_children"><div id="note_rkeY1eyGom" class="note panel"><div class="title_pdf_row clearfix"><h2 class="note_content_title"><a>More details required to reproduce work</a>  <button class="btn btn-xs btn-default permalink-button" title="Link to this comment" data-permalink-url="https://openreview.net/forum?id=rJeyV2AcKX&amp;noteId=rkeY1eyGom"><span class="glyphicon glyphicon-link" aria-hidden="true"></span></button></h2></div><div class="meta_row"><span class="signatures">(anonymous)</span></div><div class="clearfix"><div class="meta_row pull-left"><span class="date item">15 Oct 2018</span><span class="item">ICLR 2019 Conference Paper1415 Public Comment</span><span class="item">Readers: <span class="readers-icon glyphicon glyphicon-globe"></span> Everyone</span></div><div class="meta_row meta_actions"></div></div><div class="note_contents"><span class="note_content_field">Comment: </span><span class="note_content_value">Dear authors, in trying to reproduce your results I have encountered a few difficulties. Could you please clarify the points below? Thank you!
- It is not clear whether data augmentation was used to train the embedding. For example, when training with inception-like data augmentation I get 70.28% 64-way accuracy on the meta-training set, and without it I get 99.55%. Which one was used for your final miniImageNet results? Could you please report your accuracy for that model to get an idea whether the embedding is similar?
- What kind of image resizing did you use? Does it involve several crops as it is standard in evaluation of ResNet models?
- For meta-learning I gather that the meta-training set was used for the final results, and meta-validation was used only for choosing hyper-parameters. Could you please confirm?
- Were the 60,000 tasks used one at a time or was the meta-gradient averaged over several tasks in parallel?
- Your generator network is a 2-layer MLP with ReLU nonlinearities at the output layer too, or is it a 3-layer MLP?</span></div><div class="reply_row clearfix"></div></div><div class="children"><div class="note_with_children"><div id="note_HylT1TKMsm" class="note panel"><div class="title_pdf_row clearfix"><h2 class="note_content_title"><a>Details to reproduce the work</a>  <button class="btn btn-xs btn-default permalink-button" title="Link to this comment" data-permalink-url="https://openreview.net/forum?id=rJeyV2AcKX&amp;noteId=HylT1TKMsm"><span class="glyphicon glyphicon-link" aria-hidden="true"></span></button></h2></div><div class="meta_row"><span class="signatures">ICLR 2019 Conference Paper1415 Authors</span></div><div class="clearfix"><div class="meta_row pull-left"><span class="date item">16 Oct 2018</span><span class="item">ICLR 2019 Conference Paper1415 Official Comment</span><span class="item">Readers: <span class="readers-icon glyphicon glyphicon-globe"></span> Everyone</span></div><div class="meta_row meta_actions"></div></div><div class="note_contents"><span class="note_content_field">Comment: </span><span class="note_content_value">Thanks for the questions.

1. We only used common data augmentation techniques that are available in the standard PyTorch library to train the embedding, namely, random resized cropping (transforms.RandomResizedCrop in PyTorch) and random horizontal flipping (transforms.RandomHorizontalFlip). No extra data augmentation technique was used. We reached 83.66% for the 64-way accuracy on the meta-training set with the used data augmentation techniques.

2. We followed the standard evaluation protocols for ResNet models and resized images to 256x256 (transforms.Resize(256) in PyTorch), and used center cropping (transforms.CenterCrop(224)) for the evaluation.

3. Yes, only the meta-training set was used for final training and the validation set was only used for tuning the parameters.

4. The 60,000 tasks were sequentially sampled and processed. No parallel technique was used.

5. Our weight generation network is a 2-layer MLP. For each MLP layer, ReLU was used as the nonlinearity function. More precisely, the structure of the weight generation network is FC-ReLU-FC-ReLU.

We hope the above explanations make things clear and let us know if you have further questions.
</span></div><div class="reply_row clearfix"></div></div><div class="children"><div class="note_with_children"><div id="note_r1xZZopSsQ" class="note panel"><div class="title_pdf_row clearfix"><h2 class="note_content_title"><a>Cannot reproduce the FSL results on miniImageNet</a>  <button class="btn btn-xs btn-default permalink-button" title="Link to this comment" data-permalink-url="https://openreview.net/forum?id=rJeyV2AcKX&amp;noteId=r1xZZopSsQ"><span class="glyphicon glyphicon-link" aria-hidden="true"></span></button></h2></div><div class="meta_row"><span class="signatures">(anonymous)</span></div><div class="clearfix"><div class="meta_row pull-left"><span class="date item">18 Oct 2018</span><span class="item">ICLR 2019 Conference Paper1415 Public Comment</span><span class="item">Readers: <span class="readers-icon glyphicon glyphicon-globe"></span> Everyone</span></div><div class="meta_row meta_actions"></div></div><div class="note_contents"><span class="note_content_field">Comment: </span><span class="note_content_value">Thanks for the final details required to reproduce the feature embedding. We get a meta-training set classification accuracy of 83.28%, which is very similar to your reported accuracy of 83.66%. We followed the procedure explained here to export center crops of size 224x244 and we used the one-but-last layer (17) after average pooling as the feature embedding in evaluation mode. This means that batch norm used training statistics to export embeddings for all 3 sets.

We followed all the details presented here and in the paper; however, after much effort we could not reproduce the FSL results on miniImageNet.

Our results with paper reported hyper-parameters and setup:

meta-validation 1-shot: 67.95%
meta-validation 5-shot: 81.48%

meta-test 1-shot: 56.76%  (paper: 62.95%)
meta-test 5-shot: 71.72%  (paper: 79.17%)

Obviously, the differences are very large, despite our best efforts. For example, we ran a hyper-parameter sweep using meta-validation accuracy, but the best parameters were similar to the ones reported in the paper. Could you please report your meta-training and meta-validation accuracies for the miniImageNet models? What was the value of the learned scaler controlling the softmax peakiness?

We welcome any feedback and are willing to adjust our implementation with any extra details you provide!

</span></div><div class="reply_row clearfix"></div></div><div class="children"><div class="note_with_children"><div id="note_Byl6p0K8om" class="note panel"><div class="title_pdf_row clearfix"><h2 class="note_content_title"><a>Some technical details to reach our reported performance</a>  <button class="btn btn-xs btn-default permalink-button" title="Link to this comment" data-permalink-url="https://openreview.net/forum?id=rJeyV2AcKX&amp;noteId=Byl6p0K8om"><span class="glyphicon glyphicon-link" aria-hidden="true"></span></button></h2></div><div class="meta_row"><span class="signatures">ICLR 2019 Conference Paper1415 Authors</span></div><div class="clearfix"><div class="meta_row pull-left"><span class="date item">19 Oct 2018</span><span class="item">ICLR 2019 Conference Paper1415 Official Comment</span><span class="item">Readers: <span class="readers-icon glyphicon glyphicon-globe"></span> Everyone</span></div><div class="meta_row meta_actions"></div></div><div class="note_contents"><span class="note_content_field">Comment: </span><span class="note_content_value">We appreciate your great effort of re-implementing our method.

Your current results have already been on par with the state of the art, which suggests that the majority of your implementation is consistent with ours. The following details might be missing in your implementation and should be considered and adopted.

1. We use the original images from the miniImageNet for experiments, rather than the resized  ones (84x84). Images of the miniImageNet dataset are selected directly from the ImageNet dataset. Many previous methods resized them to 84x84 offline and used the resized images as input. You might have downloaded the pre-processed images and then resized them again from 84x84 to 224x224, which degenerates the image quality. Therefore, please make sure to take the original images from the miniImageNet as input. We believe that 3-4% performance gains should be obtained with images of better quality as input.

2. For few-shot evaluations, we use ten crops (transforms.TenCrop() in pyTorch) to generate 10 feature embeddings for each test image and average the 10 embeddings to get the final embedding for the image. This should produce 2-3% performance gains on the miniImageNet dataset. (We use center crops for pre-training feature embedding evaluations but ten crops for few-shot evaluations.)

We hope that you can reach our reported performance by incorporating the above two possibly missing details. We are glad to provide more intermediate results to validate your implementation if your performance still cannot reach it.</span></div><div class="reply_row clearfix"></div></div><div class="children"><div class="note_with_children"><div id="note_ryeYPc66im" class="note panel"><div class="title_pdf_row clearfix"><h2 class="note_content_title"><a>Claims should be tempered; using data augmentation renders results not comparable to baselines</a>  <button class="btn btn-xs btn-default permalink-button" title="Link to this comment" data-permalink-url="https://openreview.net/forum?id=rJeyV2AcKX&amp;noteId=ryeYPc66im"><span class="glyphicon glyphicon-link" aria-hidden="true"></span></button></h2></div><div class="meta_row"><span class="signatures">(anonymous)</span></div><div class="clearfix"><div class="meta_row pull-left"><span class="date item">24 Oct 2018</span><span class="item">ICLR 2019 Conference Paper1415 Public Comment</span><span class="item">Readers: <span class="readers-icon glyphicon glyphicon-globe"></span> Everyone</span></div><div class="meta_row meta_actions"></div></div><div class="note_contents"><span class="note_content_field">Comment: </span><span class="note_content_value">Thanks for the clarifications. We did not use sub-sampled images or any data augmentation during embedding export.

We would like to point the authors to the very similar work of [Qiao et al. 2017]. Furthermore, that paper also published "multi-view" results which are equivalent to the "TenCrop"preprocessing used in your paper, see [Qiao et al. 2017 (github)]. Since the works are so similar, it is somewhat surprising that their numbers are slightly higher. Nevertheless, we believe claims about state-of-the-art should be tempered in view of their results, which also went online in 2017. However, your table of results basically ignores about a year or work from June 2017 when Prototypical Networks were published.

We agree with your previous post; using such data augmentation lifts results by about 3% for many methods, not just your approach. What we find mesmerising is that we had to spend 3 weeks of work to uncover this fact, which should have been specified in the paper, since using this type of test set pre-processing makes a much larger difference than the  progress from novel algorithms, e.g. [Munkhdalai et al. 2017], [Oreshkin et al. 2018], [Rusu et al 2018]. Thus, it is very hard to judge from the paper whether your approach is superior or data augmentation is superior.

References:
[Qiao et al. 2017] Few-Shot Image Recognition by Predicting Parameters from Activations. <a href="https://arxiv.org/abs/1706.03466" target="_blank" rel="nofollow">https://arxiv.org/abs/1706.03466</a> [CVPR]
[Qiao et al. 2017 (github)]: https://github.com/joe-siyuan-qiao/FewShot-CVPR/tree/master/MiniImageNet
[Munkhdalai et al. 2017] Learning rapid-temporal adaptations.  http://arxiv.org/abs/
1712.09926.
[Oreshkin et al. 2018] TADAM: Task dependent adaptive metric for improved few-shot learning. ArXiv e-prints, May 2018.
[Rusu et al. 2018] Meta-Learning with Latent Embedding Optimization. https://arxiv.org/abs/1807.05960
</span></div><div class="reply_row clearfix"></div></div><div class="children"><div class="note_with_children"><div id="note_ryx1Tmf427" class="note panel"><div class="title_pdf_row clearfix"><h2 class="note_content_title"><a>The proposed method outperforms the state-of-the-art even without additional data augmentation.</a>  <button class="btn btn-xs btn-default permalink-button" title="Link to this comment" data-permalink-url="https://openreview.net/forum?id=rJeyV2AcKX&amp;noteId=ryx1Tmf427"><span class="glyphicon glyphicon-link" aria-hidden="true"></span></button></h2></div><div class="meta_row"><span class="signatures">ICLR 2019 Conference Paper1415 Authors</span></div><div class="clearfix"><div class="meta_row pull-left"><span class="date item">29 Oct 2018</span><span class="item">ICLR 2019 Conference Paper1415 Official Comment</span><span class="item">Readers: <span class="readers-icon glyphicon glyphicon-globe"></span> Everyone</span></div><div class="meta_row meta_actions"></div></div><div class="note_contents"><span class="note_content_field">Comment: </span><span class="note_content_value">Thank you for pointing out the valuable information. In this response, we would like to assure you that our proposed method outperforms the state-of-the-art even without ten-crop data  augmentation and it is essentially different from the method pointed out ([Qiao et al. 2017]). 

As we posted in the last response, we used ten-crop augmentation to boost the performance on the miniImageNet dataset according to validation set performance and it brought 2-3% performance gains. However, contrary to what you believed, using ten-crop augmentation does not always brings performance gains and it sometimes even harms the performance. For example, for the CUB dataset, we get 85.21% classification accuracy for the 5-shot case without data augmentation, but the accuracy turns to 84.70% when data augmentation is added. We will add both types (with and without ten-crop augmentation) of results in the future version of our paper. 

In fact, even without data augmentation, our method outperforms the state-of-the-art (posted in arxiv in the last several months) on the commonly used miniImageNet dataset.


[Munkhdalai et al. 2017] 1-shot: 57.10, 5-shot: 70.04 
[Qiao, et al. 2017] 1-shot: 59.60, 5-shot: 73.74
[Oreshkin et al. 2018]	1-shot: 58.50, 	5-shot: 76.70			
[Rusu et al 2018, v1] (before ICLR deadline) 1-shot: 60.06,  5-shot: 75.72
[Rusu et al 2018, v2] (after ICLR deadline) 1-shot: 61.76,  5-shot: 77.59 
Ours (WRN-28-10) 1-shot: 62.33,  5-shot: 79.17
Ours (ResNet18) 1-shot: 60.51,  5-shot: 77.08
-------
[Qiao, et al. 2017] (WRN-28-10+TenCrop) 1-shot: 63.62ï¼Œ5-shot: 78.83
Ours (WRN-28-10+TenCrop) 1-shot: 63.40,  5-shot: 81.72 
Ours (ResNet18+TenCrop) 1-shot: 62.95,  5-shot: 79.32


We can observe that our method, when using ResNet18 as the feature extraction network, beats the-state-the-art [Rusu et al 2018, v1], for both the 1-shot and 5-shot cases, and is only slightly inferior to the results in the very recently (after ICLR deadline) updated version [Rusu et al 2018, v2]. However, we noticed that both [Rusu et al 2018, v1 &amp; v2] and [Qiao, et al. 2017] use WRN-28-10 as the feature extraction network, which had been proved to be more powerful than ResNet18 for standard classification tasks. To make fair comparisons, we replace our ResNet18 with WRN-28-10, train it exactly in the same way as the previous methods, and evaluate our FSL performance. We observe that with a better feature extraction network, the accuracy of our method for both 1-shot and 5-shot is significantly boosted, raising from 60.51 to 62.33 for 1-shot and from 77.08 to 79.17 for 5-shot. Remarkably, with the same feature extraction network, our method outperforms even the very recently posted results [Rusu et al 2018, v2].

As to the ten-crop data augmentation, it seems that it has a more significant impact for [Qiao, et al. 2017] than for our method: it brings 5-6 point gains for [Qiao, et al. 2017] while only about 2 point gains for our method in both ResNet18 and WRN-28-10 cases. Even with a larger boost, [Qiao, et al. 2017]'s method is inferior to our method when using the same feature extraction network (WRN-28-10).
</span></div><div class="reply_row clearfix"></div></div><div class="children"><div class="note_with_children"><div id="note_Byx-X4M4nX" class="note panel"><div class="title_pdf_row clearfix"><h2 class="note_content_title"><a>Continue to the above</a>  <button class="btn btn-xs btn-default permalink-button" title="Link to this comment" data-permalink-url="https://openreview.net/forum?id=rJeyV2AcKX&amp;noteId=Byx-X4M4nX"><span class="glyphicon glyphicon-link" aria-hidden="true"></span></button></h2></div><div class="meta_row"><span class="signatures">ICLR 2019 Conference Paper1415 Authors</span></div><div class="clearfix"><div class="meta_row pull-left"><span class="date item">29 Oct 2018</span><span class="item">ICLR 2019 Conference Paper1415 Official Comment</span><span class="item">Readers: <span class="readers-icon glyphicon glyphicon-globe"></span> Everyone</span></div><div class="meta_row meta_actions"></div></div><div class="note_contents"><span class="note_content_field">Comment: </span><span class="note_content_value">We read carefully the paper [Qiao et al. 2017] and would like to point out that our method significantly differs from it in the following several aspects,

1. While training the weight generation network, [Qiao et al. 2017] requires samples in each mini-batch covers all classes in the meta-training set, which limits its scalability to deal with datasets with a large number of classes. Besides, it is acknowledged that training an FSL model in the episode-based fashion is key to ensure the adaptability of the model to unseen classes, since such a way exposes the model with numerous (faked) new FSL tasks during the training so that it can handle well (true) new FSL tasks in the testing. However, [Qiao et al. 2017] fails to secure this key factor because the whole and same classes are sampled in each mini-batch, such that the weight generation network is not trained to adapt to new FSL tasks. Our method avoids this limitation. We train our network flexibly by continuously sampling new FSL tasks comprising of images of some randomly sampled classes. The weight generation network is trained to well handle various (faked) new FSL tasks, therefore, better model prediction with better adaptability is obtained when real new FSL tasks come during the testing. Meanwhile, our method is scalable to datasets with a huge number of classes.

2. The loss functions are different. While [Qiao et al. 2017] uses dot product to calculate the similarity, we use a variant of cosine similarity, which bounds and reduces the variance of the classification weights and thus result in models of better generalization.

3. We extend our method to the ZSL task and obtain dramatic performance improvements especially for the generalized ZSL setting. 

By the way, we were unaware of some papers archived recently and the paper  [Qiao et al. 2017]. [Qiao et al. 2017]'s latest results with ten-crop data augmentation was not in their paper but added to their GitHub page in late June 2018 (we checked their GitHub revision history). Thank you for pointing out these papers and we will reference them in the revision of our paper.

We are glad to provide further assistance for replicating our results and will release our code upon paper acceptance. </span></div><div class="reply_row clearfix"></div></div><div class="children"></div></div></div></div></div></div></div></div></div></div></div></div></div></div></div></div></div></div></div></main></div>
  </div>
</div>


    <!-- Footer -->
    <footer class="sitemap">
      <div class="container">
        <div class="row hidden-xs">
          <div class="col-sm-4">
            <ul class="list-unstyled">
              <li><a href="/" class="home">Home</a></li>
              <li><a href="/venues">All Venues</a></li>
            </ul>
          </div>
    
          <div class="col-sm-4">
            <ul class="list-unstyled">
              <li><a href="/about">About OpenReview</a></li>
              <li><a href="/contact">Contact</a></li>
              <li><a href="#" data-toggle="modal" data-target="#feedback-modal">Feedback</a></li>
            </ul>
          </div>
    
          <div class="col-sm-4">
            <ul class="list-unstyled">
              <li><a href="/terms">Terms of Service</a></li>
              <li><a href="/privacy">Privacy Policy</a></li>
            </ul>
          </div>
        </div>
    
        <div class="row visible-xs-block">
          <div class="col-xs-6">
            <ul class="list-unstyled">
              <li><a href="/" class="home">Home</a></li>
              <li><a href="/about">About OpenReview</a></li>
              <li><a href="#" data-toggle="modal" data-target="#feedback-modal">Feedback</a></li>
            </ul>
          </div>
    
          <div class="col-xs-6">
            <ul class="list-unstyled">
              <li><a href="/contact">Contact</a></li>
              <li><a href="/terms">Terms of Service</a></li>
              <li><a href="/privacy">Privacy Policy</a></li>
            </ul>
          </div>
        </div>
      </div>
    </footer>
    
    <footer class="sponsor">
      <div class="container">
        <div class="row">
          <div class="col-sm-10 col-sm-offset-1">
            <p class="text-center">
              OpenReview is created by the <a href="http://www.iesl.cs.umass.edu/" target="_blank">Information Extraction and Synthesis Laboratory</a>, College of Information and Computer Science, University of Massachusetts Amherst. We gratefully acknowledge the support of the OpenReview sponsors:  Google,  Facebook, NSF, the University of Massachusetts Amherst Center for Data Science, and Center for Intelligent Information Retrieval, as well as the Google Cloud Platform for donating the computing and networking services on which OpenReview.net runs.
            </p>
          </div>
        </div>
      </div>
    </footer>

  <div id="feedback-modal" class="modal fade" tabindex="-1" role="dialog">
    <div class="modal-dialog">
      <div class="modal-content">
        <div class="modal-header">
          <button type="button" class="close" data-dismiss="modal" aria-label="Close"><span aria-hidden="true">Ã—</span></button>
          <h3 class="modal-title">Send Feedback</h3>
        </div>
        <div class="modal-body">
          <p>Enter your feedback below and we'll get back to you as soon as possible.</p>
          <form action="/feedback" method="POST">
            <div class="form-group">
              <input type="email" name="from" class="form-control" placeholder="Email" />
            </div>
            <div class="form-group">
              <input type="text" name="subject" class="form-control" placeholder="Subject" />
            </div>
            <div class="form-group">
              <textarea name="message" class="form-control feedback-input" rows="5" placeholder="Message" required=""></textarea>
            </div>
          <ul id="ui-id-2" tabindex="0" class="ui-menu ui-widget ui-widget-content" style="display: none;"></ul></form>
        </div>
        <div class="modal-footer">
          <button type="button" class="btn btn-default" data-dismiss="modal">Cancel</button>
          <button type="button" class="btn btn-primary">Send</button>
        </div>
      </div>
    </div>
  </div>

  <script type="text/javascript" async="" src="https://www.google-analytics.com/analytics.js"></script><script src="//cdnjs.cloudflare.com/ajax/libs/jquery/2.2.4/jquery.min.js"></script>
  <script>window.jQuery || document.write('&lt;script src="/static/js/vendor/jquery-2.2.4.min.js"&gt;&lt;\/script&gt;')</script>

  <script src="/static/dist/vendor.min.js"></script>
  <script src="/static/dist/templates.min.js"></script>
  <script src="/static/dist/openreview.min.js"></script>

    <script>window.legacyScripts = true;</script>


    <script async="" src="https://www.googletagmanager.com/gtag/js?id=UA-108703919-1"></script>
    <script>
      window.dataLayer = window.dataLayer || [];
      function gtag() { dataLayer.push(arguments); }
      gtag('js', new Date());
      gtag('config', 'UA-108703919-1');
    </script>


<div role="status" aria-live="assertive" aria-relevant="additions" class="ui-helper-hidden-accessible"></div></body></html>