<!DOCTYPE html><html xmlns="http://www.w3.org/1999/xhtml" class="no-js"><head>
  <meta charset="utf-8" />
  <meta http-equiv="X-UA-Compatible" content="IE=edge,chrome=1" />
  <meta name="viewport" content="width=device-width, initial-scale=1" />

  <title>DVOLVER: Efficient Pareto-Optimal Neural Network Architecture Search | OpenReview</title>
  <meta name="description" content="" />

      <meta name="citation_title" content="DVOLVER: Efficient Pareto-Optimal Neural Network Architecture Search" />
        <meta name="citation_author" content="Anonymous" />
      <meta name="citation_publication_date" content="2018/09/27" />
      <meta name="citation_online_date" content="2018/09/27" />
      <meta name="citation_pdf_url" content="https://openreview.net/pdf?id=Bkf1tjR9KQ" />
      <meta name="twitter:card" content="summary" />
      <meta name="twitter:site" content="@openreviewnet" />
      <meta name="og:title" content="DVOLVER: Efficient Pareto-Optimal Neural Network Architecture Search" />
      <meta name="og:description" content="Automatic search of neural network architectures is a standing research topic. In addition to the fact that it presents a faster alternative to hand-designed architectures, it can improve their..." />
      <meta name="og:image" content="https://openreview.net/static/images/openreview_logo_512.png" />

  <link rel="stylesheet" href="https://fonts.googleapis.com/css?family=Noto+Sans:400,400i,700,700i" />

  <link rel="stylesheet" href="/static/css/bootstrap.min.css" />
  <link rel="stylesheet" href="/static/css/jquery-ui.min.css" />
  <link rel="stylesheet" href="/static/css/main.min.css" />
</head>

<body class="forum">
  <nav class="navbar navbar-inverse navbar-fixed-top" role="navigation">
    <div class="container">
  
      <div class="navbar-header">
        <button type="button" class="navbar-toggle collapsed" data-toggle="collapse" data-target="#navbar" aria-expanded="false" aria-controls="navbar">
          <span class="sr-only">Toggle navigation</span>
          <span class="icon-bar"></span>
          <span class="icon-bar"></span>
          <span class="icon-bar"></span>
        </button>
        <a class="navbar-brand home push-link" href="/"><strong>OpenReview</strong>.net</a>
      </div>
  
      <div id="navbar" class="navbar-collapse collapse">
        <form class="navbar-form navbar-left profile-search" role="search">
          <div class="form-group has-feedback">
            <input id="search_input" type="text" class="form-control ui-autocomplete-input" placeholder="Search ICLR 2019 Conference" autocomplete="off" />
            <span class="glyphicon glyphicon-search form-control-feedback" aria-hidden="true"></span>
          </div>
  
          <input id="search_group" type="hidden" value="ICLR.cc/2019/Conference" />
          <input id="search_content" type="hidden" value="all" />
        <ul id="ui-id-1" tabindex="0" class="ui-menu ui-widget ui-widget-content ui-autocomplete ui-front" style="display: none;"></ul></form>
  
        <ul class="nav navbar-nav navbar-right">
        
            <li id="user-menu"><a href="/login">Login</a></li>
        </ul>
      </div>
  
    </div>
  </nav>

  <div id="or-banner" class="banner" style="">
  <div class="container">
    <div class="row">
      <div class="col-xs-12"><a href="/group?id=ICLR.cc/2019/Conference" title="Venue Homepage"><img class="icon" src="/static/images/arrow_left.svg" /> Go to <strong>ICLR 2019 Conference</strong> homepage</a></div>
    </div>
  </div>
</div>
<div id="flash-message-container" class="alert alert-danger" role="alert" style="display: none;">
  <div class="container">
    <div class="row">
      <div class="col-xs-12">
        <div class="alert-content">
          <button type="button" class="close" aria-label="Close"><span aria-hidden="true">×</span></button>
        </div>
      </div>
    </div>
  </div>
</div>

<div class="container">
  <div class="row">
    <div class="col-xs-12">
      <main id="content" class="clearfix openbanner-visible legacy-styles"><div id="note_Bkf1tjR9KQ" class="note panel"><div class="title_pdf_row clearfix"><h2 class="note_content_title"><a>DVOLVER: Efficient Pareto-Optimal Neural Network Architecture Search</a> <a class="note_content_pdf" href="/pdf?id=Bkf1tjR9KQ" title="Download PDF" target="_blank"><img src="/static/images/pdf_icon_blue.svg" /></a> </h2></div><div class="meta_row"><span class="signatures">Anonymous</span></div><div class="clearfix"><div class="meta_row pull-left"><span class="date item">28 Sep 2018 (modified: 17 Nov 2018)</span><span class="item">ICLR 2019 Conference Blind Submission</span><span class="item">Readers: <span class="readers-icon glyphicon glyphicon-globe"></span> Everyone</span><span class="item"><a class="action-bibtex-modal" data-bibtex="@inproceedings{    &#10;anonymous2019dvolver:,    &#10;title={DVOLVER: Efficient Pareto-Optimal Neural Network Architecture Search},    &#10;author={Anonymous},    &#10;booktitle={Submitted to International Conference on Learning Representations},    &#10;year={2019},    &#10;url={https://openreview.net/forum?id=Bkf1tjR9KQ},    &#10;note={under review}    &#10;}">Show Bibtex</a></span><a class="note_content_pdf item" href="/revisions?id=Bkf1tjR9KQ" target="_blank">Show Revisions</a></div><div class="meta_row meta_actions"></div></div><div class="note_contents"><span class="note_content_field">Abstract: </span><span class="note_content_value">Automatic search of neural network architectures is a standing research topic. In addition to the fact that it presents a faster alternative to hand-designed architectures, it can improve their efficiency and for instance generate Convolutional Neural Networks (CNN) adapted for mobile devices. In this paper, we present a multi-objective neural architecture search method to find a family of CNN models with the best accuracy and computational resources tradeoffs, in a search space inspired by the state-of-the-art findings in neural search. Our work, called Dvolver, evolves a population of architectures and iteratively improves an approximation of the optimal Pareto front. Applying Dvolver on the model accuracy and on the number of floating points operations as objective functions, we are able to find, in only 2.5 days 1 , a set of competitive mobile models on ImageNet. Amongst these models one architecture has the same Top-1 accuracy on ImageNet as NASNet-A mobile with 8% less floating point operations and another one has a Top-1 accuracy of 75.28% on ImageNet exceeding by 0.28% the best MobileNetV2 model for the same computational resources.</span></div><div class="note_contents"><span class="note_content_field">Keywords: </span><span class="note_content_value">architecture search, Pareto optimality, multi-objective, optimization, cnn, deep learning</span></div><div class="note_contents"><span class="note_content_field">TL;DR: </span><span class="note_content_value">Multi-objective Neural architecture search as an efficient way to find fast and accurate architecture for mobile devices.</span></div><div class="reply_row clearfix"><div class="item" id="reply_count">6 Replies</div></div></div><hr class="small" /><div id="note_children"><div class="note_with_children"><div id="note_ryeL_QZ9hm" class="note panel"><div class="title_pdf_row clearfix"><h2 class="note_content_title"><a>Good results but limited novelty. Experimental comparison could be improved.</a>  <button class="btn btn-xs btn-default permalink-button" title="Link to this comment" data-permalink-url="https://openreview.net/forum?id=Bkf1tjR9KQ&amp;noteId=ryeL_QZ9hm"><span class="glyphicon glyphicon-link" aria-hidden="true"></span></button></h2></div><div class="meta_row"><span class="signatures">ICLR 2019 Conference Paper407 AnonReviewer1</span></div><div class="clearfix"><div class="meta_row pull-left"><span class="date item">03 Nov 2018 (modified: 07 Nov 2018)</span><span class="item">ICLR 2019 Conference Paper407 Official Review</span><span class="item">Readers: <span class="readers-icon glyphicon glyphicon-globe"></span> Everyone</span></div><div class="meta_row meta_actions"></div></div><div class="note_contents"><span class="note_content_field">Review: </span><span class="note_content_value">The paper proposes a multi-objective search algorithm that designs resource-efficient convolutional architectures. The key idea is to maintain a population of networks and to iteratively approach the Pareto front through evolution. The normal &amp; reduction cells are searched on CIFAR-10 and then transferred to ImageNet. The resulting architectures empirically lead to better trade-offs than other baselines.

Pros:
The paper is well-written and easy to comprehend.
Results are competitive against strong baselines such as NASNet.
Resource budgets are handled in a principled manner with multi-objective optimization. 

Cons:

My main concerns are on the technical novelty and experimental comparison.

Technical novelty:

The proposed algorithm seems highly similar to the existing multi-objective NAS algorithms, especially the ones based on Pareto optimality [1,2,3]. In Sect 2, the authors state that the main difference from prior works such as [2] and [3] is the usage of a different and larger search space and large-scale experiments. However, both aspects are of limited technical novelty.

Experimental comparison:

In sect 3.3, the authors say “we noticed that the original NASNet search space can greatly benefit from extra connections from any given block”. If the proposed algorithm was investigated in an enhanced version of the NASNet space, it would be unclear whether we should attribute the reported performance to the proposed multi-objective evolution or this additional search space engineering. It would be better to report the results using the original space as well for fair comparison. 

The main claimed contribution is a multi-objective evolutionary algorithm. To demonstrate its effectiveness, it would be necessary to compare against existent multi-objective NAS strategies in the literature. Most of those strategies (e.g., scalarization, weighted product method) should be straightforward to implement on top of the current search space. The current results are less convincing since the authors only compared their method against single-objective baselines (e.g. NASNet, PNAS, AmoebaNet) which are completely unaware of additional dimensions of the desired objectives. 

The networks are searched on CIFAR-10 and then transferred to ImageNet. Unlike most prior works (including the ones focusing on resource-constrained NAS), the authors did not the final performance of their architecture on CIFAR-10. It would be informative to report the CIFAR-10 results as well.

Other suggestions &amp; questions:
The authors did not report their training setup for ImageNet. It would be good to include those details to ensure the readers are informed should there are any additional augmentations.

“uniform mutation and a crossover probability of 0.1” (sect 4.1)
It would be better to included more details on these evolution forces for reproducibility. These are also important component of the proposed algorithm.

“We manually select 3 architectures that we will be fully train on ImageNet in Section 4.2” (sect 4.1)
I believe this part needs more clarifications since there can be a large number of architectures on the Pareto front. What’s the criteria for manual selection?

[1] Elsken, Thomas, Jan Hendrik Metzen, and Frank Hutter. "Multi-objective architecture search for cnns." arXiv preprint arXiv:1804.09081 (2018).
[2] Kim Ye-Hoon, Reddy Bhargava, Yun Sojung, and Seo Chanwon. NEMO: Neuro-Evolution with Multiobjective Optimization of Deep Neural Network for Speed and Accuracy. ICML’17 AutoML Workshop, 2017.
[3] Dong, Jin-Dong, et al. "DPP-Net: Device-aware Progressive Search for Pareto-optimal Neural Architectures." arXiv preprint arXiv:1806.08198 (2018).</span></div><div class="note_contents"><span class="note_content_field">Rating: </span><span class="note_content_value">4: Ok but not good enough - rejection</span></div><div class="note_contents"><span class="note_content_field">Confidence: </span><span class="note_content_value">4: The reviewer is confident but not absolutely certain that the evaluation is correct</span></div><div class="reply_row clearfix"></div></div><div class="children"><div class="note_with_children"><div id="note_ryeNabaaa7" class="note panel"><div class="title_pdf_row clearfix"><h2 class="note_content_title"><a>Experimental update and review answer</a>  <button class="btn btn-xs btn-default permalink-button" title="Link to this comment" data-permalink-url="https://openreview.net/forum?id=Bkf1tjR9KQ&amp;noteId=ryeNabaaa7"><span class="glyphicon glyphicon-link" aria-hidden="true"></span></button></h2></div><div class="meta_row"><span class="signatures">ICLR 2019 Conference Paper407 Authors</span></div><div class="clearfix"><div class="meta_row pull-left"><span class="date item">18 Nov 2018</span><span class="item">ICLR 2019 Conference Paper407 Official Comment</span><span class="item">Readers: <span class="readers-icon glyphicon glyphicon-globe"></span> Everyone</span></div><div class="meta_row meta_actions"></div></div><div class="note_contents"><span class="note_content_field">Comment: </span><span class="note_content_value">Thanks for your time and comments. Below we pull quotes from the review followed by responses.

"The proposed algorithm seems highly similar to the existing multi-objective NAS algorithms, especially […] However, both aspects are of limited technical novelty."

Response: We argue that to prove that a search technique is efficient it has to be done on a large search space where exhaustive and random search are not tractable.
          Moreover, Our paper prove that on a given large search space, multi-objective search can be more efficient than single objective search with extra value when designing neural architecture for mobile device.
          To our knowledge, no previous work has stated that point because they do not search in a search space already studied with a single objective method.

          Our comments on the given references:
          * [1]: build networks incrementally with a limited set of possible operations. In theory, their search space is infinite but in practice it is quiet small.
                 objective functions are arbitrary separated into 2 categories: expensive and cheap. We show that it is not necessary.
          * [2]: Their method initializes the search process from a given baseline network which is a strong prior and only explore the vicinity of already defined network instead of exploring the whole search space.
                 They do not study transferability to ImageNet.
          * [3]: We argue that their search space is too small (See Table 4 of our paper). It is centered around efficient handcrafted architectures. For 4 layers, there are only 324 possible cell architectures of which they evaluate around 200.
                 With small increase in computation, it is possible to perform exhaustive search.


" If the proposed algorithm was investigated […] to the proposed multi-objective evolution or this additional search space engineering."

Response: Thanks for pointing the ambiguity, we fixed the paper with more explanations: NASNet, AmoebaNet and PNAS all have additional connections in the provided checkpoints and codes by the authors. They are not described in their respective papers.
          We did an experiment with no extra connections and compared with NASNet without additional connections and found that Dvolver outperforms NASNet.


"The main claimed contribution is a multi-objective evolutionary algorithm. To demonstrate its effectiveness, it would be […]  completely unaware of additional dimensions of the desired objectives.
"

Response: Our main goal is to compare single objective vs multi-objective search for neural network but we think your are right to ask for broader comparison and we updated the paper with results from DPP-Net (only paper with results on ImageNet).
          We also made it clearer that when we compare with single objective method, only accuracy is to be considered.
          One of our conclusion is that multi-objective search is still relevant even when looking for accuracy only.

"It would be informative to report the CIFAR-10 results as well."

Response: Again, thank you for pointing that out. We updated the paper with results for CIFAR-10.


"The authors did not report their training setup for ImageNet. It would be good to include those details to ensure the readers are informed should there are any additional augmentations."

and

"It would be better to included more details on these evolution forces for reproducibility. These are also important component of the proposed algorithm"

Response: We added more details in the paper and we provide the full source code for search and train on CIFAR-10 and also the code for train and evaluation on ImageNet (GPU and TPU) with the final checkpoints for all networks we presented on ImageNet.
          We hope it will be enough to clear the shadow details in our setup.


"What’s the criteria for manual selection?"

Response: In general, interpreting the Pareto front is context sensitive. Ideally, we should train all the architectures in the Pareto front but it is very expensive.
          Our selection process is as follow:
          * take the cell with the best accuracy (DVOLVER-A), then take 2 more on Pareto front knees (where small drop in accuracy leads to great improvement in speed): DVOLVER-B &amp; C.
          * Then for all N, F and cells compute MACs and we select the few with MACs comparable with existing networks with want to compare with.

          Our goal is to easy comparison with competitive architectures.
</span></div><div class="reply_row clearfix"></div></div><div class="children"></div></div></div></div><div class="note_with_children"><div id="note_HJxO1b4Yhm" class="note panel"><div class="title_pdf_row clearfix"><h2 class="note_content_title"><a>The results are competitive, but not too much innovation</a>  <button class="btn btn-xs btn-default permalink-button" title="Link to this comment" data-permalink-url="https://openreview.net/forum?id=Bkf1tjR9KQ&amp;noteId=HJxO1b4Yhm"><span class="glyphicon glyphicon-link" aria-hidden="true"></span></button></h2></div><div class="meta_row"><span class="signatures">ICLR 2019 Conference Paper407 AnonReviewer3</span></div><div class="clearfix"><div class="meta_row pull-left"><span class="date item">02 Nov 2018 (modified: 07 Nov 2018)</span><span class="item">ICLR 2019 Conference Paper407 Official Review</span><span class="item">Readers: <span class="readers-icon glyphicon glyphicon-globe"></span> Everyone</span></div><div class="meta_row meta_actions"></div></div><div class="note_contents"><span class="note_content_field">Review: </span><span class="note_content_value">The paper is easy to read. The authors did a job in describing the problem, concepts, and the proposed multi-objective optimization method. The computational results are on par with NASNet-A mobile. 

It is good to know that we can use standard multi-objective method for neural architecture search. The implementation seems to be straightforward. The paper mainly uses existing ideas, but with some incremental improvements. It lacks novelty.  

The time reduction of this method on ImageNet comes from transfer learning by training on CIFAR-10 first. As the paper admits this is not going to generalizing well. How good the method is if just using a single dateset? For CIFAR-10, is this method comparable with ENAS(<a href="https://arxiv.org/pdf/1802.03268.pdf)?" target="_blank" rel="nofollow">https://arxiv.org/pdf/1802.03268.pdf)?</a>

</span></div><div class="note_contents"><span class="note_content_field">Rating: </span><span class="note_content_value">5: Marginally below acceptance threshold</span></div><div class="note_contents"><span class="note_content_field">Confidence: </span><span class="note_content_value">3: The reviewer is fairly confident that the evaluation is correct</span></div><div class="reply_row clearfix"></div></div><div class="children"><div class="note_with_children"><div id="note_rkgrzzaT67" class="note panel"><div class="title_pdf_row clearfix"><h2 class="note_content_title"><a>Review answers and extended experimental results</a>  <button class="btn btn-xs btn-default permalink-button" title="Link to this comment" data-permalink-url="https://openreview.net/forum?id=Bkf1tjR9KQ&amp;noteId=rkgrzzaT67"><span class="glyphicon glyphicon-link" aria-hidden="true"></span></button></h2></div><div class="meta_row"><span class="signatures">ICLR 2019 Conference Paper407 Authors</span></div><div class="clearfix"><div class="meta_row pull-left"><span class="date item">18 Nov 2018</span><span class="item">ICLR 2019 Conference Paper407 Official Comment</span><span class="item">Readers: <span class="readers-icon glyphicon glyphicon-globe"></span> Everyone</span></div><div class="meta_row meta_actions"></div></div><div class="note_contents"><span class="note_content_field">Comment: </span><span class="note_content_value">Thanks for your time and comments. Below we pull quotes from the review followed by responses.

" It lacks novelty"

Response: Our paper proves that multi-objective search can be more efficient than single objective method with extra value relevant for mobile neural network architecture design.
          To our knowledge, no previous work has proved it.


"As the paper admits this is not going to generalizing well. How good the method is if just using a single dateset?"

Response: The main goal is to prove multi-objective search efficiency and for fast iteration, it is faster to perform the search on CIFAR-10 but it is not a requierement.
          * We did the search on CIFAR-10 and then fully train on CIFAR-10 see Table 1.
          * Searching on ImageNet can be done with our method but it is significantly more expensive. One of the future leads is to downscale Imagenet to make the search faster.


"For CIFAR-10, is this method comparable with ENAS"

Response: ENAS uses a clever approach to reuse weights and eliminate the need to retrain from scratch.
          ENAS is a method to improve search efficiency. Our work is more centered on multi objective search. One of our future leads, is to use weight sharing with multi-objective search. </span></div><div class="reply_row clearfix"></div></div><div class="children"></div></div></div></div><div class="note_with_children"><div id="note_r1xaACsOhm" class="note panel"><div class="title_pdf_row clearfix"><h2 class="note_content_title"><a>Interesting but incremental work</a>  <button class="btn btn-xs btn-default permalink-button" title="Link to this comment" data-permalink-url="https://openreview.net/forum?id=Bkf1tjR9KQ&amp;noteId=r1xaACsOhm"><span class="glyphicon glyphicon-link" aria-hidden="true"></span></button></h2></div><div class="meta_row"><span class="signatures">ICLR 2019 Conference Paper407 AnonReviewer2</span></div><div class="clearfix"><div class="meta_row pull-left"><span class="date item">02 Nov 2018 (modified: 07 Nov 2018)</span><span class="item">ICLR 2019 Conference Paper407 Official Review</span><span class="item">Readers: <span class="readers-icon glyphicon glyphicon-globe"></span> Everyone</span></div><div class="meta_row meta_actions"></div></div><div class="note_contents"><span class="note_content_field">Review: </span><span class="note_content_value">The authors propose a multi-objective neural architecture search based on an evolutionary algorithm. The contradicting objective functions are optimized by ranking the candidates by Pareto-dominance, replace the bottom 50% with new candidates generated by the top 50% candidates through random mutations. The multi-objective function considers classification accuracy and an approximation of the inference speed. The method is compared to MobileNet and Mobile NASNet on ImageNet indicating an improvement with respect to search time.

The authors admit that their work is incremental and a combination of existing work. Furthermore, they admit that Dong et al. (2018) is the closest related work, however, they do not compare to them in the experimental section. The method by Dong et al. requires only 8 GPU days (Dvolver requires 50) yielding very similar results. Why this has been ignored remains unclear.

The paper is not self-contained, important methodological aspects of the method are insufficiently described. I recommend at least to formally define the crowding distance. It would be also reasonable to define your objective functions already in Section 3 instead of mentioning them in the caption of Figure 3 and its axis labels.

I think it's fair to call your approach evolutionary but you might want to discuss its relationship to beam search and in this scope discuss [A].

The comparison in Table 2 is not fair. You use the swift activation function and do not report the corresponding numbers for MobileNet or Mobile NASNet. Ramachandran et al. (2017) report these (75% and 74.2% for NASNet and MobileNet).
Comparing the Dvolver architecture with ReLU activations to MobileNet does not indicate any improvements.

You mention that most previous approaches are only keeping track of the best solution while you evolve over a population. Maybe this sentence is not well written and something else is meant but now this statement is wrong.

[A] Thomas Elsken, Jan Hendrik Metzen, Frank Hutter: Simple And Efficient Architecture Search for Convolutional Neural Networks. CoRR abs/1711.04528 (2017)</span></div><div class="note_contents"><span class="note_content_field">Rating: </span><span class="note_content_value">4: Ok but not good enough - rejection</span></div><div class="note_contents"><span class="note_content_field">Confidence: </span><span class="note_content_value">4: The reviewer is confident but not absolutely certain that the evaluation is correct</span></div><div class="reply_row clearfix"></div></div><div class="children"><div class="note_with_children"><div id="note_rJeMDMpTT7" class="note panel"><div class="title_pdf_row clearfix"><h2 class="note_content_title"><a>Arguments in favor of original work instead of just incremental</a>  <button class="btn btn-xs btn-default permalink-button" title="Link to this comment" data-permalink-url="https://openreview.net/forum?id=Bkf1tjR9KQ&amp;noteId=rJeMDMpTT7"><span class="glyphicon glyphicon-link" aria-hidden="true"></span></button></h2></div><div class="meta_row"><span class="signatures">ICLR 2019 Conference Paper407 Authors</span></div><div class="clearfix"><div class="meta_row pull-left"><span class="date item">18 Nov 2018</span><span class="item">ICLR 2019 Conference Paper407 Official Comment</span><span class="item">Readers: <span class="readers-icon glyphicon glyphicon-globe"></span> Everyone</span></div><div class="meta_row meta_actions"></div></div><div class="note_contents"><span class="note_content_field">Comment: </span><span class="note_content_value">Thanks for your time and comments. Below we pull quotes from the review followed by responses.

"The authors admit that their work is incremental and a combination of existing work. Furthermore, they admit that Dong et al. (2018) is the closest related work, however, they do not compare to them in the experimental section. The method by Dong et al. requires only 8 GPU days (Dvolver requires 50) yielding very similar results. Why this has been ignored remains unclear."

Response: Thanks for pointing the missing comparison with DPP-Net, we updated the paper with their results.
          We argue that the search space of DPP-Net is too small to prove that their method is efficient. There are only 324 possible architectures with 4 layers which can be searched exhaustively.
          DPP-Net is an extension of Progressive NAS with multi-objective optimization but their search space is different. We cannot conclude on their effectiveness in these conditions.


"The paper is not self-contained, important methodological aspects of the method are insufficiently described. I recommend at least to formally define the crowding distance. It would be also reasonable to define your objective functions already in Section 3 instead of mentioning them in the caption of Figure 3 and its axis labels."

Response: Thanks for pointing the missing information. We update the paper with more details and an appendix with algorithm details.
          We also added the source code and checkpoints for easy reproduction.

"I think it's fair to call your approach evolutionary but you might want to discuss its relationship to beam search and in this scope discuss [A]."

Response: [A] is a single objective method that evolves a single architecture from a simple to a more complex one.
          It is a very different evolutionary approach than ours. We work with a population of individuals (architectures), then breed and mutate them to create new generations of architectures better than their parents.
          It is the interactions between the individuals that is important in our method.


"The comparison in Table 2 is not fair. You use the swift activation function and do not report the corresponding numbers for MobileNet or Mobile NASNet. Ramachandran et al. (2017) report these (75% and 74.2% for NASNet and MobileNet).
Comparing the Dvolver architecture with ReLU activations to MobileNet does not indicate any improvements."

Response: First we compare NASNet and DVOLVER without swish to prove that single objective search is less efficient than multi-objective method.
          Then, in table 3, We want to push our cells to the maximum and compare with the best results found in previous works. We added NASNet swish and showed that DVOLVER still has better accuracy than NASNet.
          We added the best results found in state-of-the-art works for mobile architecture and found that DVOLVER is very competitive.
          Ramachandran et al. 2017(<a href="https://arxiv.org/pdf/1710.05941.pdf)," target="_blank" rel="nofollow">https://arxiv.org/pdf/1710.05941.pdf),</a> state that MobileNet can benefit from swish and the publication date implies that it is MobileNet V1.
          Their paper states that MobileNet with RELU is at 72% (Table 8) but we could not find any reference for MobileNet V1 (https://arxiv.org/abs/1704.04861) with that accuracy. Without more details, we cannot add this values in our paper but will if we can find more details.
          We think it is fair to compare with the best results found in previous works and it is not the subject of our paper to optimize other architectures.


"You mention that most previous approaches are only keeping track of the best solution while you evolve over a population. Maybe this sentence is not well written and something else is meant but now this statement is wrong."

Response: We remove that statement, thanks for the catch.
</span></div><div class="reply_row clearfix"></div></div><div class="children"></div></div></div></div></div></main></div>
  </div>
</div>


    <!-- Footer -->
    <footer class="sitemap">
      <div class="container">
        <div class="row hidden-xs">
          <div class="col-sm-4">
            <ul class="list-unstyled">
              <li><a href="/" class="home">Home</a></li>
              <li><a href="/venues">All Venues</a></li>
            </ul>
          </div>
    
          <div class="col-sm-4">
            <ul class="list-unstyled">
              <li><a href="/about">About OpenReview</a></li>
              <li><a href="/contact">Contact</a></li>
              <li><a href="#" data-toggle="modal" data-target="#feedback-modal">Feedback</a></li>
            </ul>
          </div>
    
          <div class="col-sm-4">
            <ul class="list-unstyled">
              <li><a href="/terms">Terms of Service</a></li>
              <li><a href="/privacy">Privacy Policy</a></li>
            </ul>
          </div>
        </div>
    
        <div class="row visible-xs-block">
          <div class="col-xs-6">
            <ul class="list-unstyled">
              <li><a href="/" class="home">Home</a></li>
              <li><a href="/about">About OpenReview</a></li>
              <li><a href="#" data-toggle="modal" data-target="#feedback-modal">Feedback</a></li>
            </ul>
          </div>
    
          <div class="col-xs-6">
            <ul class="list-unstyled">
              <li><a href="/contact">Contact</a></li>
              <li><a href="/terms">Terms of Service</a></li>
              <li><a href="/privacy">Privacy Policy</a></li>
            </ul>
          </div>
        </div>
      </div>
    </footer>
    
    <footer class="sponsor">
      <div class="container">
        <div class="row">
          <div class="col-sm-10 col-sm-offset-1">
            <p class="text-center">
              OpenReview is created by the <a href="http://www.iesl.cs.umass.edu/" target="_blank">Information Extraction and Synthesis Laboratory</a>, College of Information and Computer Science, University of Massachusetts Amherst. We gratefully acknowledge the support of the OpenReview sponsors:  Google,  Facebook, NSF, the University of Massachusetts Amherst Center for Data Science, and Center for Intelligent Information Retrieval, as well as the Google Cloud Platform for donating the computing and networking services on which OpenReview.net runs.
            </p>
          </div>
        </div>
      </div>
    </footer>

  <div id="feedback-modal" class="modal fade" tabindex="-1" role="dialog">
    <div class="modal-dialog">
      <div class="modal-content">
        <div class="modal-header">
          <button type="button" class="close" data-dismiss="modal" aria-label="Close"><span aria-hidden="true">×</span></button>
          <h3 class="modal-title">Send Feedback</h3>
        </div>
        <div class="modal-body">
          <p>Enter your feedback below and we'll get back to you as soon as possible.</p>
          <form action="/feedback" method="POST">
            <div class="form-group">
              <input type="email" name="from" class="form-control" placeholder="Email" />
            </div>
            <div class="form-group">
              <input type="text" name="subject" class="form-control" placeholder="Subject" />
            </div>
            <div class="form-group">
              <textarea name="message" class="form-control feedback-input" rows="5" placeholder="Message" required=""></textarea>
            </div>
          <ul id="ui-id-2" tabindex="0" class="ui-menu ui-widget ui-widget-content" style="display: none;"></ul></form>
        </div>
        <div class="modal-footer">
          <button type="button" class="btn btn-default" data-dismiss="modal">Cancel</button>
          <button type="button" class="btn btn-primary">Send</button>
        </div>
      </div>
    </div>
  </div>

  <script type="text/javascript" async="" src="https://www.google-analytics.com/analytics.js"></script><script src="//cdnjs.cloudflare.com/ajax/libs/jquery/2.2.4/jquery.min.js"></script>
  <script>window.jQuery || document.write('&lt;script src="/static/js/vendor/jquery-2.2.4.min.js"&gt;&lt;\/script&gt;')</script>

  <script src="/static/dist/vendor.min.js"></script>
  <script src="/static/dist/templates.min.js"></script>
  <script src="/static/dist/openreview.min.js"></script>

    <script>window.legacyScripts = true;</script>


    <script async="" src="https://www.googletagmanager.com/gtag/js?id=UA-108703919-1"></script>
    <script>
      window.dataLayer = window.dataLayer || [];
      function gtag() { dataLayer.push(arguments); }
      gtag('js', new Date());
      gtag('config', 'UA-108703919-1');
    </script>


<div role="status" aria-live="assertive" aria-relevant="additions" class="ui-helper-hidden-accessible"></div></body></html>