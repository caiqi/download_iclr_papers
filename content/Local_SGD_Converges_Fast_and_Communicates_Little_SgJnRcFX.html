<!DOCTYPE html><html xmlns="http://www.w3.org/1999/xhtml" class="no-js"><head>
  <meta charset="utf-8" />
  <meta http-equiv="X-UA-Compatible" content="IE=edge,chrome=1" />
  <meta name="viewport" content="width=device-width, initial-scale=1" />

  <title>Local SGD Converges Fast and Communicates Little | OpenReview</title>
  <meta name="description" content="" />

      <meta name="citation_title" content="Local SGD Converges Fast and Communicates Little" />
        <meta name="citation_author" content="Anonymous" />
      <meta name="citation_publication_date" content="2018/09/27" />
      <meta name="citation_online_date" content="2018/09/27" />
      <meta name="citation_pdf_url" content="https://openreview.net/pdf?id=S1g2JnRcFX" />
      <meta name="twitter:card" content="summary" />
      <meta name="twitter:site" content="@openreviewnet" />
      <meta name="og:title" content="Local SGD Converges Fast and Communicates Little" />
      <meta name="og:description" content="Mini-batch stochastic gradient descent (SGD) is state of the art in large scale distributed training. The scheme can reach a linear speed-up with respect to the number of workers, but this is..." />
      <meta name="og:image" content="https://openreview.net/static/images/openreview_logo_512.png" />

  <link rel="stylesheet" href="https://fonts.googleapis.com/css?family=Noto+Sans:400,400i,700,700i" />

  <link rel="stylesheet" href="/static/css/bootstrap.min.css" />
  <link rel="stylesheet" href="/static/css/jquery-ui.min.css" />
  <link rel="stylesheet" href="/static/css/main.min.css" />
</head>

<body class="forum">
  <nav class="navbar navbar-inverse navbar-fixed-top" role="navigation">
    <div class="container">
  
      <div class="navbar-header">
        <button type="button" class="navbar-toggle collapsed" data-toggle="collapse" data-target="#navbar" aria-expanded="false" aria-controls="navbar">
          <span class="sr-only">Toggle navigation</span>
          <span class="icon-bar"></span>
          <span class="icon-bar"></span>
          <span class="icon-bar"></span>
        </button>
        <a class="navbar-brand home push-link" href="/"><strong>OpenReview</strong>.net</a>
      </div>
  
      <div id="navbar" class="navbar-collapse collapse">
        <form class="navbar-form navbar-left profile-search" role="search">
          <div class="form-group has-feedback">
            <input id="search_input" type="text" class="form-control ui-autocomplete-input" placeholder="Search ICLR 2019 Conference" autocomplete="off" />
            <span class="glyphicon glyphicon-search form-control-feedback" aria-hidden="true"></span>
          </div>
  
          <input id="search_group" type="hidden" value="ICLR.cc/2019/Conference" />
          <input id="search_content" type="hidden" value="all" />
        <ul id="ui-id-1" tabindex="0" class="ui-menu ui-widget ui-widget-content ui-autocomplete ui-front" style="display: none;"></ul></form>
  
        <ul class="nav navbar-nav navbar-right">
        
            <li id="user-menu"><a href="/login">Login</a></li>
        </ul>
      </div>
  
    </div>
  </nav>

  <div id="or-banner" class="banner" style="">
  <div class="container">
    <div class="row">
      <div class="col-xs-12"><a href="/group?id=ICLR.cc/2019/Conference" title="Venue Homepage"><img class="icon" src="/static/images/arrow_left.svg" /> Go to <strong>ICLR 2019 Conference</strong> homepage</a></div>
    </div>
  </div>
</div>
<div id="flash-message-container" class="alert alert-danger" role="alert" style="display: none;">
  <div class="container">
    <div class="row">
      <div class="col-xs-12">
        <div class="alert-content">
          <button type="button" class="close" aria-label="Close"><span aria-hidden="true">×</span></button>
        </div>
      </div>
    </div>
  </div>
</div>

<div class="container">
  <div class="row">
    <div class="col-xs-12">
      <main id="content" class="clearfix openbanner-visible legacy-styles"><div id="note_S1g2JnRcFX" class="note panel"><div class="title_pdf_row clearfix"><h2 class="note_content_title"><a>Local SGD Converges Fast and Communicates Little</a> <a class="note_content_pdf" href="/pdf?id=S1g2JnRcFX" title="Download PDF" target="_blank"><img src="/static/images/pdf_icon_blue.svg" /></a> </h2></div><div class="meta_row"><span class="signatures">Anonymous</span></div><div class="clearfix"><div class="meta_row pull-left"><span class="date item">28 Sep 2018 (modified: 11 Oct 2018)</span><span class="item">ICLR 2019 Conference Blind Submission</span><span class="item">Readers: <span class="readers-icon glyphicon glyphicon-globe"></span> Everyone</span><span class="item"><a class="action-bibtex-modal" data-bibtex="@inproceedings{    &#10;anonymous2019local,    &#10;title={Local SGD Converges Fast and Communicates Little},    &#10;author={Anonymous},    &#10;booktitle={Submitted to International Conference on Learning Representations},    &#10;year={2019},    &#10;url={https://openreview.net/forum?id=S1g2JnRcFX},    &#10;note={under review}    &#10;}">Show Bibtex</a></span></div><div class="meta_row meta_actions"></div></div><div class="note_contents"><span class="note_content_field">Abstract: </span><span class="note_content_value">Mini-batch stochastic gradient descent (SGD) is state of the art in large scale distributed training. The scheme can reach a linear speed-up with respect to the number of workers, but this is rarely seen in practice as the scheme often suffers from large network delays and bandwidth limits. To overcome this communication bottleneck recent works propose to reduce the communication frequency. An algorithm of this type is local SGD that runs SGD independently in parallel on different workers and averages the sequences only once in a while. This scheme shows promising results in practice, but eluded thorough theoretical analysis.
    
We prove concise convergence rates for local SGD on convex problems and show that it converges at the same rate as mini-batch SGD in terms of number of evaluated gradients, that is, the scheme achieves linear speed-up in the number of workers and mini-batch size. The number of  communication rounds can be reduced up to a factor of T^{1/2}---where T denotes the number of total steps---compared to mini-batch SGD. This also holds for asynchronous implementations.

Local SGD can also be used for large scale training of deep learning models. The results shown here aim serving as a guideline to further explore the theoretical and practical aspects of local SGD in these applications.</span></div><div class="note_contents"><span class="note_content_field">Keywords: </span><span class="note_content_value">optimization, communication, theory, stochastic gradient descent, SGD, mini-batch, local SGD, parallel restart SGD, distributed training</span></div><div class="note_contents"><span class="note_content_field">TL;DR: </span><span class="note_content_value">We prove that parallel local SGD achieves linear speedup with much lesser communication than parallel mini-batch SGD.</span></div><div class="reply_row clearfix"><div class="item" id="reply_count">7 Replies</div></div></div><hr class="small" /><div id="note_children"><div class="note_with_children"><div id="note_SJxvQJ4WT7" class="note panel"><div class="title_pdf_row clearfix"><h2 class="note_content_title"><a>Several References are missed</a>  <button class="btn btn-xs btn-default permalink-button" title="Link to this comment" data-permalink-url="https://openreview.net/forum?id=S1g2JnRcFX&amp;noteId=SJxvQJ4WT7"><span class="glyphicon glyphicon-link" aria-hidden="true"></span></button></h2></div><div class="meta_row"><span class="signatures">(anonymous)</span></div><div class="clearfix"><div class="meta_row pull-left"><span class="date item">08 Nov 2018</span><span class="item">ICLR 2019 Conference Paper1023 Public Comment</span><span class="item">Readers: <span class="readers-icon glyphicon glyphicon-globe"></span> Everyone</span></div><div class="meta_row meta_actions"></div></div><div class="note_contents"><span class="note_content_field">Comment: </span><span class="note_content_value">Since you are talking about local SGD, or model averaging, these two papers should be cited:
"Improving deep neural network acoustic models using generalized maxout networks" The training method in this paper is exactly same with Local SGD

and this paper
"Scalable Training of Deep Learning Machines by Incremental Block Training with Intra-block Parallel Optimization and Blockwise Model-Update Filtering" This paper propose to build connections between successive synchronizations of "local sgd"




</span></div><div class="reply_row clearfix"></div></div><div class="children"></div></div><div class="note_with_children"><div id="note_HkxcmSCa2m" class="note panel"><div class="title_pdf_row clearfix"><h2 class="note_content_title"><a>A convergence proof for local SGD is provided. Local SGD (averaging local SGD models, once in a while) can provably provide the same speedup gains as minibatch, but may be able to communicate significantly less.</a>  <button class="btn btn-xs btn-default permalink-button" title="Link to this comment" data-permalink-url="https://openreview.net/forum?id=S1g2JnRcFX&amp;noteId=HkxcmSCa2m"><span class="glyphicon glyphicon-link" aria-hidden="true"></span></button></h2></div><div class="meta_row"><span class="signatures">ICLR 2019 Conference Paper1023 AnonReviewer2</span></div><div class="clearfix"><div class="meta_row pull-left"><span class="date item">05 Nov 2018 (modified: 07 Nov 2018)</span><span class="item">ICLR 2019 Conference Paper1023 Official Review</span><span class="item">Readers: <span class="readers-icon glyphicon glyphicon-globe"></span> Everyone</span></div><div class="meta_row meta_actions"></div></div><div class="note_contents"><span class="note_content_field">Review: </span><span class="note_content_value">The authors of this paper analyze a well known technique for parallel training, where each compute node locally trains a model with SGD, and once in a while the K compute nodes average their models. Local SGD, although not as widely used as mini-batch SGD, can provide some gains in terms of the cost of communication. This can be achieved by decreasing the frequency of synchronization, while locally also increasing the minibatch. 

To the best of my knowledge, the authors are the first to provide a complete theoretical analysis of local SGD for strongly convex functions. They prove that under strong convexity, and the bounded gradients assumption, local SGD will (in the worst case) achieve a linear speedup over vanilla SGD, as long as the parallel models are averaged frequently enough. They show that although frequent averaging is important for speedup, the overall communication cost can be lower than minibatch SGD that may require smaller batches and hence more frequent communication. 

The authors extend their results to the asynchronous case, where a similar convergence bound is derived. The overall theory seems to be partly inspired by the perturbed iterates framework of Mania et al., however the application is novel and interesting.

The authors include some limited experimental results that validate their bounds.

This is a well-written paper, that will certainly be of interest to researchers working on stochastic optimization, and distributed learning. The results are interesting and clearly stated. The proofs seem complete and correct, and are easy to follow. 

I have two minor comments:
1) In a recent paper, Dong et al. [1] suggest that for any problem (convex or nonconvex), the largest possible batch size in minibatch SGD that allows for linear speedups will be proportional to “gradient diversity”, i.e., a measure of similarity between the concurrently processed gradients. For example, when all gradient are identical, there is no speedup to be extracted. This diversity term does not seem to appear in the main theorem, as one may expect. For example, the presented bounds still seem to provide speedup gains for the case where all individual n functions are identical (eg minimum grad. diversity). This should not be possible, as there are no parallel speedups to be extracted in this case. I’m wondering how that fact is reflected in the presented bounds (maybe it’s one of the extreme parameter cases that are not covered by the main theorem).

2) The authors do not provide details of their experimental setup. For example it would be useful to know what hardware they implemented their algorithms on. It seems that they run experiments for up to 1K workers. Are these individual cores, or was this the result of hyper-threading? Finally, it’s unclear if Fig 1 is a theoretical, or an experimental curve.



[1] <a href="http://proceedings.mlr.press/v84/yin18a/yin18a.pdf" target="_blank" rel="nofollow">http://proceedings.mlr.press/v84/yin18a/yin18a.pdf</a>
</span></div><div class="note_contents"><span class="note_content_field">Rating: </span><span class="note_content_value">8: Top 50% of accepted papers, clear accept</span></div><div class="note_contents"><span class="note_content_field">Confidence: </span><span class="note_content_value">5: The reviewer is absolutely certain that the evaluation is correct and very familiar with the relevant literature</span></div><div class="reply_row clearfix"></div></div><div class="children"><div class="note_with_children"><div id="note_S1gTViwlRQ" class="note panel"><div class="title_pdf_row clearfix"><h2 class="note_content_title"><a>Response to Reviewer 2 - we did not consider gradient diversity as we provide data-independent bounds</a>  <button class="btn btn-xs btn-default permalink-button" title="Link to this comment" data-permalink-url="https://openreview.net/forum?id=S1g2JnRcFX&amp;noteId=S1gTViwlRQ"><span class="glyphicon glyphicon-link" aria-hidden="true"></span></button></h2></div><div class="meta_row"><span class="signatures">ICLR 2019 Conference Paper1023 Authors</span></div><div class="clearfix"><div class="meta_row pull-left"><span class="date item">20 Nov 2018</span><span class="item">ICLR 2019 Conference Paper1023 Official Comment</span><span class="item">Readers: <span class="readers-icon glyphicon glyphicon-globe"></span> Everyone</span></div><div class="meta_row meta_actions"></div></div><div class="note_contents"><span class="note_content_field">Comment: </span><span class="note_content_value">Dear Reviewer. We thank you for your time and effort that you invested into reviewing our manuscript and your favorable assessment of our work. Thanks for pointing us to (Yin et al.), we will add short remark on gradient diversity in the revision (see answer to 1)). We think that decreasing the communication frequency in local SGD is especially helpful (to save communication), when the alternative strategy of increasing the batch size does not give a linear speedup (due to low gradient diversity).

Concerning your minor comments:
1) Let us consider mini-batch SGD as an example: a *data-independent* worst case analysis, assuming just a bound on the variance on each sample of the stochastic gradient, predicts a linear speed up with respect to batch size. However, in practice, the variance of a single sample (or a batch) could be much lower than predicted by these bounds. Gradient diversity is a *data-dependent” quantity that measures this discrepancy and explains why mini-batch SGD does only enjoy linear speedup for small batch sizes for if the gradient diversity is small (Yin et al.).

Our analysis is *data-independent*, that is why gradient diversity does not appear in our bounds. However, we note that the variance term explicitly appears in our Theorem and allows to extract more fine-grained results for special cases. For instance, the special case mentioned by the reviewer (all functions identical, so no speed-up possible), implies \sigma = 0, and Corollary 2.3 shows convergence at rate 1/T^2 (which is better than 1/T for the general case). However, for this example a rate of e^(-T) could be obtained with a *constant* stepsize. Our proof only covers decreasing stepsize, thus the suboptimal result is expected.

2) All experiments are conducted for the synchronous versions of the algorithm, for more details on the protocol see also Section D. Besides the randomness used to pick the stochastic gradients for each thread/local sequence, the aggregation of the local sequences after H steps is deterministic. Thus, we (as stated) simulate the number of workers by running the corresponding threads in sequence. This does not change the output of the algorithm. We will make this more precise. Figure 1 shows the theoretical expected behavior, Figure 3 the measured behavior.
</span></div><div class="reply_row clearfix"></div></div><div class="children"></div></div></div></div><div class="note_with_children"><div id="note_Byl1lBwchX" class="note panel"><div class="title_pdf_row clearfix"><h2 class="note_content_title"><a>Interesting direction</a>  <button class="btn btn-xs btn-default permalink-button" title="Link to this comment" data-permalink-url="https://openreview.net/forum?id=S1g2JnRcFX&amp;noteId=Byl1lBwchX"><span class="glyphicon glyphicon-link" aria-hidden="true"></span></button></h2></div><div class="meta_row"><span class="signatures">ICLR 2019 Conference Paper1023 AnonReviewer3</span></div><div class="clearfix"><div class="meta_row pull-left"><span class="date item">03 Nov 2018 (modified: 07 Nov 2018)</span><span class="item">ICLR 2019 Conference Paper1023 Official Review</span><span class="item">Readers: <span class="readers-icon glyphicon glyphicon-globe"></span> Everyone</span></div><div class="meta_row meta_actions"></div></div><div class="note_contents"><span class="note_content_field">Review: </span><span class="note_content_value">This paper presents an analysis of "local SGD", which averages estimators obtained by running SGD in separate machines once in a while. The paper presents bounds on "how frequent" the estimators required to be averaged in order to yield linear parallelization speedups. This is an interesting paper, but I have some concerns that I will elaborate on below:

[1] This paper's assumption of bounded variance of Stochastic Gradients and drawing conclusions about frequency of averaging does not reflect practical implementations of SGD for Machine Learning contexts. For example, note that in this oracle model, there exists bound on batch size (T^alpha, alpha\in[1/3,1/2]) that yield linear parallelization speedups (for example, see Dekel et al. (2012)); however, as Dekel et al (2012) note, such bounds are fairly crude estimates on a per-problem basis for practical purposes. These issues naturally continue to exist with regards to the upperbound on the frequency of communication as argued by this paper. 

[2] Furthermore, the claim that such a bound on frequency of communication for local SGD which is not known before is not really true. In the convex case, the paper of Jain et al. (2016) presents a precise characterization of when to average of iterates across machines to obtain linear parallelization speedups, and this is a problem dependent quantity that works without assumptions such as bounded variance of stochastic gradients for the least squares problem. Note that, as reflective in practice, this result conveys that averaging the solutions of multiple independent runs of SGD does not help anything when the bias (initial error) dominates the variance. 

[3] Note that local SGD has been known for a while and is referred to as Iterative Parameter Mixing in the literature. An example of this is the thesis of Greg Coppola (2015). A more careful literature search can provide more references/results on this topic.

[4] This paper claims that (in page 2) in order to "improve computation versus communication tradeoff, one can increase the batch size or increase communication interval". This appears to be an imprecise statement. For example, if I kept increasing batchsize without any limit, and the bias in my problem is much larger than the variance (where bias and variance follows definitions from Bach and Moulines (2011,2013)), this does not lead to any parallelization speedup. This is in contrast to when the variance dominates the bias, wherein, model averaging/increasing batch size helps. What is the reason for the authors to conclude that increasing batch size is equivalent to increasing communication interval?</span></div><div class="note_contents"><span class="note_content_field">Rating: </span><span class="note_content_value">5: Marginally below acceptance threshold</span></div><div class="note_contents"><span class="note_content_field">Confidence: </span><span class="note_content_value">5: The reviewer is absolutely certain that the evaluation is correct and very familiar with the relevant literature</span></div><div class="reply_row clearfix"></div></div><div class="children"><div class="note_with_children"><div id="note_Sklci2Dg07" class="note panel"><div class="title_pdf_row clearfix"><h2 class="note_content_title"><a>Response to Reviewer 3 - our result captures the bias/variance tradeoff; but we will be more precise with the simplified statements in the introduction</a>  <button class="btn btn-xs btn-default permalink-button" title="Link to this comment" data-permalink-url="https://openreview.net/forum?id=S1g2JnRcFX&amp;noteId=Sklci2Dg07"><span class="glyphicon glyphicon-link" aria-hidden="true"></span></button></h2></div><div class="meta_row"><span class="signatures">ICLR 2019 Conference Paper1023 Authors</span></div><div class="clearfix"><div class="meta_row pull-left"><span class="date item">20 Nov 2018</span><span class="item">ICLR 2019 Conference Paper1023 Official Comment</span><span class="item">Readers: <span class="readers-icon glyphicon glyphicon-globe"></span> Everyone</span></div><div class="meta_row meta_actions"></div></div><div class="note_contents"><span class="note_content_field">Comment: </span><span class="note_content_value">Dear Reviewer. We thank you for your time and effort that you invested into reviewing our manuscript and your thoughtful comments. We hope that our answers will settle your concerns.

1) We agree with the reviewer on this comment. The bounds on H derived here have a somewhat theoretical flavor, like the asymptotic bounds derived in (Dekel et al., 2012). Our results show that H=O(sqrt(T)) is asymptotically optimal, however, as the “O” notation hides (problem specific) constants, this might not be the best choice for practical purposes, similar as discussed for the batch size in (Dekel et al., 2012).

2) We like to note a few fundamental differences from (Jain et al, 2016, <a href="https://arxiv.org/pdf/1610.03774v4.pdf" target="_blank" rel="nofollow">https://arxiv.org/pdf/1610.03774v4.pdf</a> ). 
(Jain et al. 2016) consider the stochastic approximation problem of Least Squares Regression, under strong convexity and bounded fourth moment assumption (we consider general strongly convex functions, but with more restrictive bounded second moment assumption). They provide analysis for SGD with constant stepsize (we consider decreasing stepsizes) for mini-batch SGD and SGD with tail-averaging. Theorem 6 discusses the averaging of *independent* runs of SGD with tail averaging, however the averaging only happens at the end (i.e. H=T, one-shot averaging). In local SGD the sequences are averaged more often, and after averaging, the sequences become correlated. This algorithm is not addressed in (Jain et al.).

 (Jain et al.) show that averaging the solutions of multiple independent runs of SGD does not help when the bias (initial error) dominates the variance. Besides the already stated differences of the algorithm in (Jain et al.) and local SGD, we like to remark that a similar observation can be made in our case: In Corollary 2.3 only the variance terms enjoy a linear speedup, whereas the bias terms do not. We will add a remark on this observation and will be more careful when stating the results (see also 4) below).

3) (Coppola, 2015) show that Iterative Parameter Mixing (IPM) converges, but no speedup from parallelization has been shown (cf. pg. 94, Coppola, 2015, “In fact, a O(M) penalty is occurred”, where M=H in our notation). We like to thank the reviewer for pointing us to this reference, and to IPM in general (McDonald et al. (2010)). We will update the related work section with those appearances of “local SGD” in the literature.

4) We agree with the reviewer, that “one can increase the batch size […] to improve the computation versus communication tradeoff” is an imprecise statement and does only hold when increasing the batch size gives faster convergence (i.e. when variance dominates the bias). When we claim “one can increase the batch size or increase communication interval […] to improve the computation versus communication tradeoff” we mean the following:

Clearly, [Increasing the batch size/decreasing the communication frequency] results in less communication *per iteration*. However, both strategies can also *reduce the total communication* when the variance dominates the bias term (cf. the Theorem, and Corollary 2.3). This was known for mini-batch SGD, and we prove this fact for local SGD in this paper. We will clarify the statement on page 2.</span></div><div class="reply_row clearfix"></div></div><div class="children"></div></div></div></div><div class="note_with_children"><div id="note_BJeqkEXq3m" class="note panel"><div class="title_pdf_row clearfix"><h2 class="note_content_title"><a>Review: Local SGD converges fast and communicates little</a>  <button class="btn btn-xs btn-default permalink-button" title="Link to this comment" data-permalink-url="https://openreview.net/forum?id=S1g2JnRcFX&amp;noteId=BJeqkEXq3m"><span class="glyphicon glyphicon-link" aria-hidden="true"></span></button></h2></div><div class="meta_row"><span class="signatures">ICLR 2019 Conference Paper1023 AnonReviewer1</span></div><div class="clearfix"><div class="meta_row pull-left"><span class="date item">03 Nov 2018 (modified: 07 Nov 2018)</span><span class="item">ICLR 2019 Conference Paper1023 Official Review</span><span class="item">Readers: <span class="readers-icon glyphicon glyphicon-globe"></span> Everyone</span></div><div class="meta_row meta_actions"></div></div><div class="note_contents"><span class="note_content_field">Review: </span><span class="note_content_value">The authors analyze the local SGD algorithm, where $K$ parallel chains of SGD are run, and the iterates are occasionally synchronized across machines by averaging. For sufficiently short intervals between synchronization, the algorithm achieves the same convergence rate in terms of the number of gradient evaluations as parallel minibatch SGD, but with the advantage that communication can be significantly reduced.

The algorithm is simple and practical, and the analysis is concise and seems like it could be applicable more generally to other parallel SGD variants.

I am curious about what happens for the analysis of the algorithm when $H$ becomes large. As the authors point out, when $H=T$, this is one-shot averaging which is known to converge. The authors mention not working too hard to optimize the bounds for extreme values of $H$, which is fine, but I wonder if this is possible using their analysis technique, or whether new tools would be necessary.</span></div><div class="note_contents"><span class="note_content_field">Rating: </span><span class="note_content_value">8: Top 50% of accepted papers, clear accept</span></div><div class="note_contents"><span class="note_content_field">Confidence: </span><span class="note_content_value">4: The reviewer is confident but not absolutely certain that the evaluation is correct</span></div><div class="reply_row clearfix"></div></div><div class="children"><div class="note_with_children"><div id="note_r1ePa5DeR7" class="note panel"><div class="title_pdf_row clearfix"><h2 class="note_content_title"><a>Response to Reviewer 1 - recovering the rates for one-shot averaging requires modifications to the proof</a>  <button class="btn btn-xs btn-default permalink-button" title="Link to this comment" data-permalink-url="https://openreview.net/forum?id=S1g2JnRcFX&amp;noteId=r1ePa5DeR7"><span class="glyphicon glyphicon-link" aria-hidden="true"></span></button></h2></div><div class="meta_row"><span class="signatures">ICLR 2019 Conference Paper1023 Authors</span></div><div class="clearfix"><div class="meta_row pull-left"><span class="date item">20 Nov 2018</span><span class="item">ICLR 2019 Conference Paper1023 Official Comment</span><span class="item">Readers: <span class="readers-icon glyphicon glyphicon-globe"></span> Everyone</span></div><div class="meta_row meta_actions"></div></div><div class="note_contents"><span class="note_content_field">Comment: </span><span class="note_content_value">Dear Reviewer. We thank you for your time and effort that you invested into reviewing our manuscript and your favorable assessment of our work.

Indeed, when H -&gt; T, we do not recover the results for one shot averaging. In our proof we leverage the fact that less frequent averaging does not hurt the convergence as long as the local sequences are close (Lemma 3.3). However, when H=T, then Lemma 3.3 is not tight enough. Perhaps it is possible to trade-off the error introduced in Lemma 3.3 for a slower rate (e.g. only sublinear speed up), but right now we do not see how this could be achieved without significant changes to the proof.</span></div><div class="reply_row clearfix"></div></div><div class="children"></div></div></div></div></div></main></div>
  </div>
</div>


    <!-- Footer -->
    <footer class="sitemap">
      <div class="container">
        <div class="row hidden-xs">
          <div class="col-sm-4">
            <ul class="list-unstyled">
              <li><a href="/" class="home">Home</a></li>
              <li><a href="/venues">All Venues</a></li>
            </ul>
          </div>
    
          <div class="col-sm-4">
            <ul class="list-unstyled">
              <li><a href="/about">About OpenReview</a></li>
              <li><a href="/contact">Contact</a></li>
              <li><a href="#" data-toggle="modal" data-target="#feedback-modal">Feedback</a></li>
            </ul>
          </div>
    
          <div class="col-sm-4">
            <ul class="list-unstyled">
              <li><a href="/terms">Terms of Service</a></li>
              <li><a href="/privacy">Privacy Policy</a></li>
            </ul>
          </div>
        </div>
    
        <div class="row visible-xs-block">
          <div class="col-xs-6">
            <ul class="list-unstyled">
              <li><a href="/" class="home">Home</a></li>
              <li><a href="/about">About OpenReview</a></li>
              <li><a href="#" data-toggle="modal" data-target="#feedback-modal">Feedback</a></li>
            </ul>
          </div>
    
          <div class="col-xs-6">
            <ul class="list-unstyled">
              <li><a href="/contact">Contact</a></li>
              <li><a href="/terms">Terms of Service</a></li>
              <li><a href="/privacy">Privacy Policy</a></li>
            </ul>
          </div>
        </div>
      </div>
    </footer>
    
    <footer class="sponsor">
      <div class="container">
        <div class="row">
          <div class="col-sm-10 col-sm-offset-1">
            <p class="text-center">
              OpenReview is created by the <a href="http://www.iesl.cs.umass.edu/" target="_blank">Information Extraction and Synthesis Laboratory</a>, College of Information and Computer Science, University of Massachusetts Amherst. We gratefully acknowledge the support of the OpenReview sponsors:  Google,  Facebook, NSF, the University of Massachusetts Amherst Center for Data Science, and Center for Intelligent Information Retrieval, as well as the Google Cloud Platform for donating the computing and networking services on which OpenReview.net runs.
            </p>
          </div>
        </div>
      </div>
    </footer>

  <div id="feedback-modal" class="modal fade" tabindex="-1" role="dialog">
    <div class="modal-dialog">
      <div class="modal-content">
        <div class="modal-header">
          <button type="button" class="close" data-dismiss="modal" aria-label="Close"><span aria-hidden="true">×</span></button>
          <h3 class="modal-title">Send Feedback</h3>
        </div>
        <div class="modal-body">
          <p>Enter your feedback below and we'll get back to you as soon as possible.</p>
          <form action="/feedback" method="POST">
            <div class="form-group">
              <input type="email" name="from" class="form-control" placeholder="Email" />
            </div>
            <div class="form-group">
              <input type="text" name="subject" class="form-control" placeholder="Subject" />
            </div>
            <div class="form-group">
              <textarea name="message" class="form-control feedback-input" rows="5" placeholder="Message" required=""></textarea>
            </div>
          <ul id="ui-id-2" tabindex="0" class="ui-menu ui-widget ui-widget-content" style="display: none;"></ul></form>
        </div>
        <div class="modal-footer">
          <button type="button" class="btn btn-default" data-dismiss="modal">Cancel</button>
          <button type="button" class="btn btn-primary">Send</button>
        </div>
      </div>
    </div>
  </div>

  <script type="text/javascript" async="" src="https://www.google-analytics.com/analytics.js"></script><script src="//cdnjs.cloudflare.com/ajax/libs/jquery/2.2.4/jquery.min.js"></script>
  <script>window.jQuery || document.write('&lt;script src="/static/js/vendor/jquery-2.2.4.min.js"&gt;&lt;\/script&gt;')</script>

  <script src="/static/dist/vendor.min.js"></script>
  <script src="/static/dist/templates.min.js"></script>
  <script src="/static/dist/openreview.min.js"></script>

    <script>window.legacyScripts = true;</script>


    <script async="" src="https://www.googletagmanager.com/gtag/js?id=UA-108703919-1"></script>
    <script>
      window.dataLayer = window.dataLayer || [];
      function gtag() { dataLayer.push(arguments); }
      gtag('js', new Date());
      gtag('config', 'UA-108703919-1');
    </script>


<div role="status" aria-live="assertive" aria-relevant="additions" class="ui-helper-hidden-accessible"></div></body></html>