<!DOCTYPE html><html xmlns="http://www.w3.org/1999/xhtml" class="no-js"><head>
  <meta charset="utf-8" />
  <meta http-equiv="X-UA-Compatible" content="IE=edge,chrome=1" />
  <meta name="viewport" content="width=device-width, initial-scale=1" />

  <title>Learning Implicitly Recurrent CNNs Through Parameter Sharing | OpenReview</title>
  <meta name="description" content="" />

      <meta name="citation_title" content="Learning Implicitly Recurrent CNNs Through Parameter Sharing" />
        <meta name="citation_author" content="Anonymous" />
      <meta name="citation_publication_date" content="2018/09/27" />
      <meta name="citation_online_date" content="2018/09/27" />
      <meta name="citation_pdf_url" content="https://openreview.net/pdf?id=rJgYxn09Fm" />
      <meta name="twitter:card" content="summary" />
      <meta name="twitter:site" content="@openreviewnet" />
      <meta name="og:title" content="Learning Implicitly Recurrent CNNs Through Parameter Sharing" />
      <meta name="og:description" content="We introduce a parameter sharing scheme, in which different layers of a convolutional neural network (CNN) are defined by a learned linear combination of parameter tensors from a global bank of..." />
      <meta name="og:image" content="https://openreview.net/static/images/openreview_logo_512.png" />

  <link rel="stylesheet" href="https://fonts.googleapis.com/css?family=Noto+Sans:400,400i,700,700i" />

  <link rel="stylesheet" href="/static/css/bootstrap.min.css" />
  <link rel="stylesheet" href="/static/css/jquery-ui.min.css" />
  <link rel="stylesheet" href="/static/css/main.min.css" />
</head>

<body class="forum">
  <nav class="navbar navbar-inverse navbar-fixed-top" role="navigation">
    <div class="container">
  
      <div class="navbar-header">
        <button type="button" class="navbar-toggle collapsed" data-toggle="collapse" data-target="#navbar" aria-expanded="false" aria-controls="navbar">
          <span class="sr-only">Toggle navigation</span>
          <span class="icon-bar"></span>
          <span class="icon-bar"></span>
          <span class="icon-bar"></span>
        </button>
        <a class="navbar-brand home push-link" href="/"><strong>OpenReview</strong>.net</a>
      </div>
  
      <div id="navbar" class="navbar-collapse collapse">
        <form class="navbar-form navbar-left profile-search" role="search">
          <div class="form-group has-feedback">
            <input id="search_input" type="text" class="form-control ui-autocomplete-input" placeholder="Search ICLR 2019 Conference" autocomplete="off" />
            <span class="glyphicon glyphicon-search form-control-feedback" aria-hidden="true"></span>
          </div>
  
          <input id="search_group" type="hidden" value="ICLR.cc/2019/Conference" />
          <input id="search_content" type="hidden" value="all" />
        <ul id="ui-id-1" tabindex="0" class="ui-menu ui-widget ui-widget-content ui-autocomplete ui-front" style="display: none;"></ul></form>
  
        <ul class="nav navbar-nav navbar-right">
        
            <li id="user-menu"><a href="/login">Login</a></li>
        </ul>
      </div>
  
    </div>
  </nav>

  <div id="or-banner" class="banner" style="">
  <div class="container">
    <div class="row">
      <div class="col-xs-12"><a href="/group?id=ICLR.cc/2019/Conference" title="Venue Homepage"><img class="icon" src="/static/images/arrow_left.svg" /> Go to <strong>ICLR 2019 Conference</strong> homepage</a></div>
    </div>
  </div>
</div>
<div id="flash-message-container" class="alert alert-danger" role="alert" style="display: none;">
  <div class="container">
    <div class="row">
      <div class="col-xs-12">
        <div class="alert-content">
          <button type="button" class="close" aria-label="Close"><span aria-hidden="true">×</span></button>
        </div>
      </div>
    </div>
  </div>
</div>

<div class="container">
  <div class="row">
    <div class="col-xs-12">
      <main id="content" class="clearfix openbanner-visible legacy-styles"><div id="note_rJgYxn09Fm" class="note panel"><div class="title_pdf_row clearfix"><h2 class="note_content_title"><a>Learning Implicitly Recurrent CNNs Through Parameter Sharing</a> <a class="note_content_pdf" href="/pdf?id=rJgYxn09Fm" title="Download PDF" target="_blank"><img src="/static/images/pdf_icon_blue.svg" /></a> </h2></div><div class="meta_row"><span class="signatures">Anonymous</span></div><div class="clearfix"><div class="meta_row pull-left"><span class="date item">28 Sep 2018 (modified: 11 Oct 2018)</span><span class="item">ICLR 2019 Conference Blind Submission</span><span class="item">Readers: <span class="readers-icon glyphicon glyphicon-globe"></span> Everyone</span><span class="item"><a class="action-bibtex-modal" data-bibtex="@inproceedings{    &#10;anonymous2019learning,    &#10;title={Learning Implicitly Recurrent CNNs Through Parameter Sharing},    &#10;author={Anonymous},    &#10;booktitle={Submitted to International Conference on Learning Representations},    &#10;year={2019},    &#10;url={https://openreview.net/forum?id=rJgYxn09Fm},    &#10;note={under review}    &#10;}">Show Bibtex</a></span></div><div class="meta_row meta_actions"></div></div><div class="note_contents"><span class="note_content_field">Abstract: </span><span class="note_content_value">We introduce a parameter sharing scheme, in which different layers of a convolutional neural network (CNN) are defined by a learned linear combination of parameter tensors from a global bank of templates.  Restricting the number of templates yields a flexible hybridization of traditional CNNs and recurrent networks.  Compared to traditional CNNs, we demonstrate substantial parameter savings on standard image classification tasks, while maintaining accuracy.

Our simple parameter sharing scheme, though defined via soft weights, in practice yields trained networks with near strict recurrent structure; with negligible side effects, they convert into networks with actual recurrent loops.  Training these networks thus implicitly involves discovery of suitable recurrent architectures.  As a consequence, our hybrid networks are not only more parameter efficient, but also learn some tasks faster.  Specifically, on synthetic tasks which are algorithmic in nature, our hybrid networks both train faster and extrapolate better on test examples outside the span of the training set.</span></div><div class="note_contents"><span class="note_content_field">Keywords: </span><span class="note_content_value">deep learning, architecture search, computer vision</span></div><div class="note_contents"><span class="note_content_field">TL;DR: </span><span class="note_content_value">We propose a method that enables CNN folding to create recurrent connections</span></div><div class="reply_row clearfix"><div class="item" id="reply_count">3 Replies</div></div></div><hr class="small" /><div id="note_children"><div class="note_with_children"><div id="note_BygndrCp3m" class="note panel"><div class="title_pdf_row clearfix"><h2 class="note_content_title"><a>Interesting approach to weight sharing among CNN layers via shared weight templates, well written, convincing results.</a>  <button class="btn btn-xs btn-default permalink-button" title="Link to this comment" data-permalink-url="https://openreview.net/forum?id=rJgYxn09Fm&amp;noteId=BygndrCp3m"><span class="glyphicon glyphicon-link" aria-hidden="true"></span></button></h2></div><div class="meta_row"><span class="signatures">ICLR 2019 Conference Paper1098 AnonReviewer3</span></div><div class="clearfix"><div class="meta_row pull-left"><span class="date item">05 Nov 2018 (modified: 07 Nov 2018)</span><span class="item">ICLR 2019 Conference Paper1098 Official Review</span><span class="item">Readers: <span class="readers-icon glyphicon glyphicon-globe"></span> Everyone</span></div><div class="meta_row meta_actions"></div></div><div class="note_contents"><span class="note_content_field">Review: </span><span class="note_content_value">The manuscript introduces a novel and interesting approach to weight sharing among CNNs layers, by learning linear combinations of shared weight templates. This allows parameter reduction, better sample efficiency. Furthermore, the authors propose a very simple way to inspect which layers choose similar combinations of template, as well as to push the network toward using similar combinations at each layer. This regularization term has a clear potential for computation reuse on dedicated hardware. The paper is well written, the method is interesting, the results are convincing and thoroughly conducted. I recommend acceptance.

1) It would be interesting to explore how often the layer parameters converge to similar weights and how similar. To this end I suggest to plot a 2d heatmap representing the similarity matrices between every pair of layers.

2) Figure 1 is not of immediate interpretability. Especially for the middle figure, what does the dotted box represent? What is the difference between weights and templates? Also it’s unclear which of the three options corresponds to the proposed method. I would have thought the middle one, but the text seems to indicate it is the rightmost one instead.

3) How are the alphas initialized? How fast are their transitions? Do they change smoothly over training? Do they evolve rapidly and plateau to a fixed value or keep changing during training? It would be really interesting to plot their value and discuss their evolution.

4) While the number of learned parameters is indeed reduced when the templates are shared among layers - which could lead to better sample efficiency - I am not sure whether the memory footprint on GPU would change (i.e., I believe that current frameworks would allocate the same kernel n-times if the same template was shared by n layers, but I am not certain). Although the potential reduction of the number of trainable parameters is an important result by itself, I wonder if what you propose would also allow to run bigger models on the same device or not, without heavy modifications of the inner machineries of pyTorch or Tensorflow.  Can you comment on this? Also note that the soft sharing regularization scheme that you propose can be of great interest for FPGA hardware implementations, that benefit a lot from module reuse. You could mention that in the paper.

5) Sec 4.1, the number of layers in one group is defined as (L-4)/3. It’s unclear to me where the 4 comes from. Also on page 6, k = (L-2)/3 - 2 is said to set one template per layer. I thought the two formulas would be the same in that case. What am I missing? Is it possible that one of the two formulas contain a typo (I believe that at the very least it should be either (L-2) or (L-4) in both cases)?

6) Sec 4.1, I find the notation SWRN-L-w-k and SWRN-L-w confusing. My suggestion is to set k to be the total number of templates (as opposed to the number of templates *per group of layers*), which makes it much easier to relate to, and most importantly allows for an immediate comparison with the capacity of the vanilla model. As a side effect, it also makes it very easy to spot the configuration with one template per layer (SWRN-L-w-L) thus eliminating the need for an ad-hoc notation to distinguish it.

7) The authors inspect how similar the template combination weights alphas are among layers. It would also be interesting to look into what is learned in the templates. CNN layers are known to learn peculiar and somewhat interpretable template matching filters. It would be really interesting to compare the filters learned by a vanilla network and its template sharing alternative. Also, I would welcome an analysis of which templates gets chosen the most at each layer in the hierarchy. It would be compelling if some kind of pattern of reuse emerged from learning.

8) Sec 4.4, it is unclear to me what can be the contribution of the 1x1 initial convolution, since it will see no context and all the information at the pixel level can be represented by a binary bit. Also, are the 3x3 convolutions “same” convolutions? If not, how are the last feature maps upscaled to be able to predict at the original resolution?

9) At the end of sec 4.4 the authors claim that the SCNN is “also advantaged over a more RNN-like model”. I fail to understand how to process this sentence, but I have a feeling that it’s incorrect to make any claims to the performance of “RNN-like models” as such a model was not used as a baseline in the experiments in any way. Similarly, in the conclusions I find it a bit stretched to claim that you can “gain a more flexible form of behavior typically attributed to RNNs”. While it’s true that the proposed network can in theory learn to reuse the same combination of templates, which can be mapped to a network with recursion, the results in this direction don’t seem strong enough to draw any conclusion and a more in-depth comparison against RNN performance would be in order before making any claim in this direction.


MINOR
- Sec3: I wouldn’t say the parameters are shared among layers in LSTMs, but rather among time unrolls.
- One drawback of the proposed method is that the layers are constrained to have the same shape. This is not a major disadvantage, but is still a constraint that would be good to make more explicit in the description of the model.
- Sec3, end of page 3: does the network reach the same accuracy as the vanilla model when k=L? Also, does the network use all the templates? How is the distribution of the alpha weights across layers in this case?
- Sec3.1, the V notation makes the narrative unnecessarily heavy. I suggest to drop it and refer directly to the templates T. Also the second part of the section, with examples of templates, doesn’t add much in my opinion and would be better depicted with a figure.
- Sec3.1, the e^(i) notation can be confused with an exp. I suggest to replace it with the much more common 1_{i=j}.
- Figure 2 depicts the relation between the LSM matrix and the topology of the network. This should be declared more clearly in the caption, in place of the ambiguous “capturing implicit recurrencies”. Also, the caption should explain what black/white stand for as well, and possibly quickly describe what the LSM matrix is. Also, it would be more clear that the network in the middle is equivalent to that on the right if the two were somehow connected in the figure. To this end they could, e.g., share a single LSM matrix among them. Finally, if possible try and put the LSM matrices on top of the related network, so that it’s clear which network they refer to. Sec 3.2 should also refer to Fig2 I believe.
- Table 1: I suggest to leave the comment on the results out of the caption, since it’s already in the main text.
- Table 2: rather than using blue, I suggest to underline the overall best results, so that it’s visible even if the paper is printed in B&amp;W.
- Fig 3, I would specify that it’s better viewed in color
- Discussion: I feel the discussion of Table 1 is a bit difficult to follow. It could be made easier by reporting the difference in test error against the corresponding vanilla model (e.g., “improves the error rate on CIFAR10 by 0.26%”, rather than reporting the performance of both models)
- Fig 4, are all the stages the same and is the network in the left one such stages? If so, update the caption to make it clear please.
- Fig 4, which lambda has been used? Is it the same for all stages?
- Fig 5, specify that the one on the right is the target grid. Also, I believe that merging the two figures would make it easier to understand (e.g., some of the structure in the target comes from how the obstacles are placed, which requires to move back and forth from input to target several times to understand)
- Sec 4.4, space permitting, I would like to see at least one sample of what kind of shortest path prediction the network can come up with.



A few typos:
    * End of 3.2: the closer elements -&gt; the closer the elements
    * Parameter efficiency: the period before re-parametrizing should probably be a comma?
    * Fig 4, illustration of stages -&gt; illustration of the stages
    * End of pag7, an syntetic -&gt; a syntetic</span></div><div class="note_contents"><span class="note_content_field">Rating: </span><span class="note_content_value">8: Top 50% of accepted papers, clear accept</span></div><div class="note_contents"><span class="note_content_field">Confidence: </span><span class="note_content_value">4: The reviewer is confident but not absolutely certain that the evaluation is correct</span></div><div class="reply_row clearfix"></div></div><div class="children"></div></div><div class="note_with_children"><div id="note_Hke2lCb92Q" class="note panel"><div class="title_pdf_row clearfix"><h2 class="note_content_title"><a>a promising proposal that exploits the over-parameterization nature of neural nets to reduce the model size</a>  <button class="btn btn-xs btn-default permalink-button" title="Link to this comment" data-permalink-url="https://openreview.net/forum?id=rJgYxn09Fm&amp;noteId=Hke2lCb92Q"><span class="glyphicon glyphicon-link" aria-hidden="true"></span></button></h2></div><div class="meta_row"><span class="signatures">ICLR 2019 Conference Paper1098 AnonReviewer1</span></div><div class="clearfix"><div class="meta_row pull-left"><span class="date item">03 Nov 2018 (modified: 07 Nov 2018)</span><span class="item">ICLR 2019 Conference Paper1098 Official Review</span><span class="item">Readers: <span class="readers-icon glyphicon glyphicon-globe"></span> Everyone</span></div><div class="meta_row meta_actions"></div></div><div class="note_contents"><span class="note_content_field">Review: </span><span class="note_content_value">This work is motivated by the widely recognized issue of over-parameterization in modern neural nets, and proposes a clever template sharing design to reduce the model size. The design is sound, and the experiments are valid and thorough. The writing is clear and fluent. 

The reviewer is not entirely sure of the originality of this work. According to the sparse 'related work' section, the contribution is novel, but I will leave it to the consensus of others who are more versed in this regard.

The part that I find most interesting is the fact that template sharing helps with the optimization without even reducing the number of parameters, as illustrated in CIFAR from Table 1. The trade-off of accuracy and parameter-efficiency is overall well-studied in CIFAR and ImageNet, although results on ImageNet is not as impressive. 

Regarding the coefficient alpha, I'm not sure how cosine similarity is computed. I have the impression that each layer has its own alpha, which is a scalar. How is cosine similarity computed on scalars?

In the experiments, there's no mentioning of the regularization terms for alpha, which makes me think it is perhaps not important? What is the generic setup?

In summary, I find this work interesting, and with sufficient experiments to backup its claim. On the other hand, I'm not entirely sure of its novelty/originality, leaving this part open to others.</span></div><div class="note_contents"><span class="note_content_field">Rating: </span><span class="note_content_value">7: Good paper, accept</span></div><div class="note_contents"><span class="note_content_field">Confidence: </span><span class="note_content_value">3: The reviewer is fairly confident that the evaluation is correct</span></div><div class="reply_row clearfix"></div></div><div class="children"></div></div><div class="note_with_children"><div id="note_S1gMfAgqh7" class="note panel"><div class="title_pdf_row clearfix"><h2 class="note_content_title"><a>Interesting idea of using parameter sharing scheme to explore network structure; experiments can be stronger </a>  <button class="btn btn-xs btn-default permalink-button" title="Link to this comment" data-permalink-url="https://openreview.net/forum?id=rJgYxn09Fm&amp;noteId=S1gMfAgqh7"><span class="glyphicon glyphicon-link" aria-hidden="true"></span></button></h2></div><div class="meta_row"><span class="signatures">ICLR 2019 Conference Paper1098 AnonReviewer2</span></div><div class="clearfix"><div class="meta_row pull-left"><span class="date item">03 Nov 2018 (modified: 07 Nov 2018)</span><span class="item">ICLR 2019 Conference Paper1098 Official Review</span><span class="item">Readers: <span class="readers-icon glyphicon glyphicon-globe"></span> Everyone</span></div><div class="meta_row meta_actions"></div></div><div class="note_contents"><span class="note_content_field">Review: </span><span class="note_content_value">Authors propose a parameter sharing scheme by allowing parameters to be reused across layers. It further makes connection between traditional CNNs with RNNs by adding additional regularization and using hard sharing scheme.

The way of parameter sharing is similar to the filter prediction method proposed in Rebuff et al’s work, where they model a convolutional layer’s parameters as a linear combination of a bank of filters and use that to address difference among multiple domains.

Sylvestre-Alvise Rebuffi, Hakan Bilen, Andrea Vedaldi, Learning multiple visual domains with residual adapters, NIPS 2017.

The discussion on the connection between coefficients for different layers and a network’s structure and visualization of layer similarity matrix is interesting. Additional regularization can further encourage a recurrent neural network to be learned. 

However, they only experiment with one or two templates and advantage on accuracy and model size  over other methods is not very clear.</span></div><div class="note_contents"><span class="note_content_field">Rating: </span><span class="note_content_value">6: Marginally above acceptance threshold</span></div><div class="note_contents"><span class="note_content_field">Confidence: </span><span class="note_content_value">4: The reviewer is confident but not absolutely certain that the evaluation is correct</span></div><div class="reply_row clearfix"></div></div><div class="children"></div></div></div></main></div>
  </div>
</div>


    <!-- Footer -->
    <footer class="sitemap">
      <div class="container">
        <div class="row hidden-xs">
          <div class="col-sm-4">
            <ul class="list-unstyled">
              <li><a href="/" class="home">Home</a></li>
              <li><a href="/venues">All Venues</a></li>
            </ul>
          </div>
    
          <div class="col-sm-4">
            <ul class="list-unstyled">
              <li><a href="/about">About OpenReview</a></li>
              <li><a href="/contact">Contact</a></li>
              <li><a href="#" data-toggle="modal" data-target="#feedback-modal">Feedback</a></li>
            </ul>
          </div>
    
          <div class="col-sm-4">
            <ul class="list-unstyled">
              <li><a href="/terms">Terms of Service</a></li>
              <li><a href="/privacy">Privacy Policy</a></li>
            </ul>
          </div>
        </div>
    
        <div class="row visible-xs-block">
          <div class="col-xs-6">
            <ul class="list-unstyled">
              <li><a href="/" class="home">Home</a></li>
              <li><a href="/about">About OpenReview</a></li>
              <li><a href="#" data-toggle="modal" data-target="#feedback-modal">Feedback</a></li>
            </ul>
          </div>
    
          <div class="col-xs-6">
            <ul class="list-unstyled">
              <li><a href="/contact">Contact</a></li>
              <li><a href="/terms">Terms of Service</a></li>
              <li><a href="/privacy">Privacy Policy</a></li>
            </ul>
          </div>
        </div>
      </div>
    </footer>
    
    <footer class="sponsor">
      <div class="container">
        <div class="row">
          <div class="col-sm-10 col-sm-offset-1">
            <p class="text-center">
              OpenReview is created by the <a href="http://www.iesl.cs.umass.edu/" target="_blank">Information Extraction and Synthesis Laboratory</a>, College of Information and Computer Science, University of Massachusetts Amherst. We gratefully acknowledge the support of the OpenReview sponsors:  Google,  Facebook, NSF, the University of Massachusetts Amherst Center for Data Science, and Center for Intelligent Information Retrieval, as well as the Google Cloud Platform for donating the computing and networking services on which OpenReview.net runs.
            </p>
          </div>
        </div>
      </div>
    </footer>

  <div id="feedback-modal" class="modal fade" tabindex="-1" role="dialog">
    <div class="modal-dialog">
      <div class="modal-content">
        <div class="modal-header">
          <button type="button" class="close" data-dismiss="modal" aria-label="Close"><span aria-hidden="true">×</span></button>
          <h3 class="modal-title">Send Feedback</h3>
        </div>
        <div class="modal-body">
          <p>Enter your feedback below and we'll get back to you as soon as possible.</p>
          <form action="/feedback" method="POST">
            <div class="form-group">
              <input type="email" name="from" class="form-control" placeholder="Email" />
            </div>
            <div class="form-group">
              <input type="text" name="subject" class="form-control" placeholder="Subject" />
            </div>
            <div class="form-group">
              <textarea name="message" class="form-control feedback-input" rows="5" placeholder="Message" required=""></textarea>
            </div>
          <ul id="ui-id-2" tabindex="0" class="ui-menu ui-widget ui-widget-content" style="display: none;"></ul></form>
        </div>
        <div class="modal-footer">
          <button type="button" class="btn btn-default" data-dismiss="modal">Cancel</button>
          <button type="button" class="btn btn-primary">Send</button>
        </div>
      </div>
    </div>
  </div>

  <script type="text/javascript" async="" src="https://www.google-analytics.com/analytics.js"></script><script src="//cdnjs.cloudflare.com/ajax/libs/jquery/2.2.4/jquery.min.js"></script>
  <script>window.jQuery || document.write('&lt;script src="/static/js/vendor/jquery-2.2.4.min.js"&gt;&lt;\/script&gt;')</script>

  <script src="/static/dist/vendor.min.js"></script>
  <script src="/static/dist/templates.min.js"></script>
  <script src="/static/dist/openreview.min.js"></script>

    <script>window.legacyScripts = true;</script>


    <script async="" src="https://www.googletagmanager.com/gtag/js?id=UA-108703919-1"></script>
    <script>
      window.dataLayer = window.dataLayer || [];
      function gtag() { dataLayer.push(arguments); }
      gtag('js', new Date());
      gtag('config', 'UA-108703919-1');
    </script>


<div role="status" aria-live="assertive" aria-relevant="additions" class="ui-helper-hidden-accessible"></div></body></html>