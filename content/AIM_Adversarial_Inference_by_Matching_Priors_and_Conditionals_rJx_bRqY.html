<!DOCTYPE html><html xmlns="http://www.w3.org/1999/xhtml" class="no-js"><head>
  <meta charset="utf-8" />
  <meta http-equiv="X-UA-Compatible" content="IE=edge,chrome=1" />
  <meta name="viewport" content="width=device-width, initial-scale=1" />

  <title>AIM: Adversarial Inference by Matching Priors and Conditionals | OpenReview</title>
  <meta name="description" content="" />

      <meta name="citation_title" content="AIM: Adversarial Inference by Matching Priors and Conditionals" />
        <meta name="citation_author" content="Anonymous" />
      <meta name="citation_publication_date" content="2018/09/27" />
      <meta name="citation_online_date" content="2018/09/27" />
      <meta name="citation_pdf_url" content="https://openreview.net/pdf?id=rJx_b3RqY7" />
      <meta name="twitter:card" content="summary" />
      <meta name="twitter:site" content="@openreviewnet" />
      <meta name="og:title" content="AIM: Adversarial Inference by Matching Priors and Conditionals" />
      <meta name="og:description" content="Effective inference for a generative adversarial model remains an important and challenging problem. We propose a novel approach, Adversarial Inference by Matching priors and conditionals (AIM)..." />
      <meta name="og:image" content="https://openreview.net/static/images/openreview_logo_512.png" />

  <link rel="stylesheet" href="https://fonts.googleapis.com/css?family=Noto+Sans:400,400i,700,700i" />

  <link rel="stylesheet" href="/static/css/bootstrap.min.css" />
  <link rel="stylesheet" href="/static/css/jquery-ui.min.css" />
  <link rel="stylesheet" href="/static/css/main.min.css" />
</head>

<body class="forum">
  <nav class="navbar navbar-inverse navbar-fixed-top" role="navigation">
    <div class="container">
  
      <div class="navbar-header">
        <button type="button" class="navbar-toggle collapsed" data-toggle="collapse" data-target="#navbar" aria-expanded="false" aria-controls="navbar">
          <span class="sr-only">Toggle navigation</span>
          <span class="icon-bar"></span>
          <span class="icon-bar"></span>
          <span class="icon-bar"></span>
        </button>
        <a class="navbar-brand home push-link" href="/"><strong>OpenReview</strong>.net</a>
      </div>
  
      <div id="navbar" class="navbar-collapse collapse">
        <form class="navbar-form navbar-left profile-search" role="search">
          <div class="form-group has-feedback">
            <input id="search_input" type="text" class="form-control ui-autocomplete-input" placeholder="Search ICLR 2019 Conference" autocomplete="off" />
            <span class="glyphicon glyphicon-search form-control-feedback" aria-hidden="true"></span>
          </div>
  
          <input id="search_group" type="hidden" value="ICLR.cc/2019/Conference" />
          <input id="search_content" type="hidden" value="all" />
        <ul id="ui-id-1" tabindex="0" class="ui-menu ui-widget ui-widget-content ui-autocomplete ui-front" style="display: none;"></ul></form>
  
        <ul class="nav navbar-nav navbar-right">
        
            <li id="user-menu"><a href="/login">Login</a></li>
        </ul>
      </div>
  
    </div>
  </nav>

  <div id="or-banner" class="banner" style="">
  <div class="container">
    <div class="row">
      <div class="col-xs-12"><a href="/group?id=ICLR.cc/2019/Conference" title="Venue Homepage"><img class="icon" src="/static/images/arrow_left.svg" /> Go to <strong>ICLR 2019 Conference</strong> homepage</a></div>
    </div>
  </div>
</div>
<div id="flash-message-container" class="alert alert-danger" role="alert" style="display: none;">
  <div class="container">
    <div class="row">
      <div class="col-xs-12">
        <div class="alert-content">
          <button type="button" class="close" aria-label="Close"><span aria-hidden="true">×</span></button>
        </div>
      </div>
    </div>
  </div>
</div>

<div class="container">
  <div class="row">
    <div class="col-xs-12">
      <main id="content" class="clearfix openbanner-visible legacy-styles"><div id="note_rJx_b3RqY7" class="note panel"><div class="title_pdf_row clearfix"><h2 class="note_content_title"><a>AIM: Adversarial Inference by Matching Priors and Conditionals</a> <a class="note_content_pdf" href="/pdf?id=rJx_b3RqY7" title="Download PDF" target="_blank"><img src="/static/images/pdf_icon_blue.svg" /></a> </h2></div><div class="meta_row"><span class="signatures">Anonymous</span></div><div class="clearfix"><div class="meta_row pull-left"><span class="date item">28 Sep 2018 (modified: 19 Nov 2018)</span><span class="item">ICLR 2019 Conference Blind Submission</span><span class="item">Readers: <span class="readers-icon glyphicon glyphicon-globe"></span> Everyone</span><span class="item"><a class="action-bibtex-modal" data-bibtex="@inproceedings{    &#10;anonymous2019aim:,    &#10;title={AIM: Adversarial Inference by Matching Priors and Conditionals},    &#10;author={Anonymous},    &#10;booktitle={Submitted to International Conference on Learning Representations},    &#10;year={2019},    &#10;url={https://openreview.net/forum?id=rJx_b3RqY7},    &#10;note={under review}    &#10;}">Show Bibtex</a></span><a class="note_content_pdf item" href="/revisions?id=rJx_b3RqY7" target="_blank">Show Revisions</a></div><div class="meta_row meta_actions"></div></div><div class="note_contents"><span class="note_content_field">Abstract: </span><span class="note_content_value">Effective inference for a generative adversarial model remains an important and challenging problem. We propose a novel approach, Adversarial Inference by Matching priors and conditionals (AIM), which explicitly matches prior and conditional distributions in both data and code spaces, and puts a direct constraint on the dependency structure of the generative model. We derive an equivalent form of the prior and conditional matching objective that can be optimized efficiently without any parametric assumption on the data. We validate the effectiveness of AIM on the MNIST, CIFAR-10, and CelebA datasets by conducting quantitative and qualitative evaluations. Results demonstrate that AIM significantly improves both reconstruction and generation as compared to other adversarial inference models.</span></div><div class="note_contents"><span class="note_content_field">Keywords: </span><span class="note_content_value">Generative adversarial network, inference, generative model</span></div><div class="reply_row clearfix"><div class="item" id="reply_count">6 Replies</div></div></div><hr class="small" /><div id="note_children"><div class="note_with_children"><div id="note_HkxoDveR2m" class="note panel"><div class="title_pdf_row clearfix"><h2 class="note_content_title"><a>A Review on Adversarial Inference by Matching Priors and Conditionals</a>  <button class="btn btn-xs btn-default permalink-button" title="Link to this comment" data-permalink-url="https://openreview.net/forum?id=rJx_b3RqY7&amp;noteId=HkxoDveR2m"><span class="glyphicon glyphicon-link" aria-hidden="true"></span></button></h2></div><div class="meta_row"><span class="signatures">ICLR 2019 Conference Paper1184 AnonReviewer3</span></div><div class="clearfix"><div class="meta_row pull-left"><span class="date item">06 Nov 2018 (modified: 07 Nov 2018)</span><span class="item">ICLR 2019 Conference Paper1184 Official Review</span><span class="item">Readers: <span class="readers-icon glyphicon glyphicon-globe"></span> Everyone</span></div><div class="meta_row meta_actions"></div></div><div class="note_contents"><span class="note_content_field">Review: </span><span class="note_content_value">The goal this is work is to develop a generative model that enjoys the strengths of both GAN and VAE without their inherent weaknesses. The paper proposes a learning framework, in which a generating process p is modeled by a neural network called generator, and an inference process q by another neural network encoder. The ultimate goal is to match the joint distributions, p(x, z) and q(x, z), and this is done by attempting to match the priors  p(z) and q(z) and matching the conditionals p(x|z) and q(x|z). As both q(z) and q(x|z) are impossible to sample from, the authors mathematically expand this objective criterion and rewrite to be dependent only on p(x|z), q(x) and q(z|x), that can be easily sampled from. In the main part of the work, the authors use the f-divergence theory (Nowozin et al., 2016) to present the optimization problem as minmax optimization problem, that is learned using an adversarial game, using training and inference algorithms that are proposed by the authors. In experiments, the authors consider both reconstruction and generation tasks using the MNIST, CIFAR10 and CelebA datasets. Results show that the proposed method yields better MSE reconstruction error as better as a higher inception scores for the generated examples, compared to a standard GAN and a few other methods. 

This work establishes an important bridge between the VAE and GAN framework, and has a a good combination of theoretical and experimental aspects. Experiments results are encouraging, even though only relatively simple and small datasets were used. Overall, I would recommend accepting the paper for presentation in the conference. </span></div><div class="note_contents"><span class="note_content_field">Rating: </span><span class="note_content_value">7: Good paper, accept</span></div><div class="note_contents"><span class="note_content_field">Confidence: </span><span class="note_content_value">4: The reviewer is confident but not absolutely certain that the evaluation is correct</span></div><div class="reply_row clearfix"></div></div><div class="children"><div class="note_with_children"><div id="note_H1gK50mgAX" class="note panel"><div class="title_pdf_row clearfix"><h2 class="note_content_title"><a>Author Response to AnonReviewer3</a>  <button class="btn btn-xs btn-default permalink-button" title="Link to this comment" data-permalink-url="https://openreview.net/forum?id=rJx_b3RqY7&amp;noteId=H1gK50mgAX"><span class="glyphicon glyphicon-link" aria-hidden="true"></span></button></h2></div><div class="meta_row"><span class="signatures">ICLR 2019 Conference Paper1184 Authors</span></div><div class="clearfix"><div class="meta_row pull-left"><span class="date item">19 Nov 2018</span><span class="item">ICLR 2019 Conference Paper1184 Official Comment</span><span class="item">Readers: <span class="readers-icon glyphicon glyphicon-globe"></span> Everyone</span></div><div class="meta_row meta_actions"></div></div><div class="note_contents"><span class="note_content_field">Comment: </span><span class="note_content_value">We thank Reviewer 3 for the encouraging feedback and the precise summary of our work. 

For a fair comparison, we only conduct experiments on the same datasets used in the related papers. But the architecture of our model (especially the generator and discriminator) can be easily replaced by more advanced state-of-the-art GANs for larger and more complicated datasets.

FYI, we have added another Section 4.3 to explain the interesting relation between our method and VAE. And we have also added more experiments to Section 5.3.</span></div><div class="reply_row clearfix"></div></div><div class="children"></div></div></div></div><div class="note_with_children"><div id="note_r1l9TGz637" class="note panel"><div class="title_pdf_row clearfix"><h2 class="note_content_title"><a>Ok paper, some nice comparisons, but too similar to existing models</a>  <button class="btn btn-xs btn-default permalink-button" title="Link to this comment" data-permalink-url="https://openreview.net/forum?id=rJx_b3RqY7&amp;noteId=r1l9TGz637"><span class="glyphicon glyphicon-link" aria-hidden="true"></span></button></h2></div><div class="meta_row"><span class="signatures">ICLR 2019 Conference Paper1184 AnonReviewer1</span></div><div class="clearfix"><div class="meta_row pull-left"><span class="date item">05 Nov 2018 (modified: 07 Nov 2018)</span><span class="item">ICLR 2019 Conference Paper1184 Official Review</span><span class="item">Readers: <span class="readers-icon glyphicon glyphicon-globe"></span> Everyone</span></div><div class="meta_row meta_actions"></div></div><div class="note_contents"><span class="note_content_field">Review: </span><span class="note_content_value">This paper presents a variant of the adversarial generative modeling
framework, allowing it to incorporate an inference mechanism. As such it is
very much in the same spirit as existing methods such as ALI/BiGAN. The
authors go through an information theoretic motivation but end up with the
standard GAN objective function plus a latent space (z) reconstruction
term. The z-space reconstruction is accomplished by first sampling z from
its standard normal prior and pushing that sample through the generator to
get sample in the data space (x), then x is propagated through an encoder
to get a new latent-space sample z'. Reconstruction is done to reduce the
error between z' and z.

Novelty: The space of adversarially trained latent variable models has
grown quite crowded in recent years. In light of the existing literature,
this paper's contribution can be seen as incremental, with relatively low novelty. 

In the end, the training paradigm is basically the same as InfoGAN, with
the difference being that, in the proposed model,  all the latent
variables are inferred (in InfoGAN, only a subset of the latent
variables are inferred) . This difference was a design decision on the part of the InfoGAN
authors and, in my opinion, does not represent a significantly novel
contribution on the part of this paper.  

Experiments: The experiments show that the proposed method is
better able to reconstruct examples than does ALI -- a result is not
necessarily surprising, but is interesting and worth further
investigation. I would like to understand better why it is that latent
variable (z) reconstruction gives rise to better x-space reconstruction.

I did not find the claims of better sample quality of AIM over ALI to be
well supported by the data. In this context, it is not entirely clear what
the significant difference in inception scores represents, though on this, the
results are consistent with those previously published

I really liked the experiment shown in Figure 4 (esp. 4b), it makes the
differences between AIM and ALI very clear. It shows that relative to ALI,
AIM sacrifices coherence between the "marginal" posterior (the distribution
of latent variables encoded from data samples) and the latent space
prior, in favor of superior reconstructions. AIM's choice of trade-off is
one that, in many contexts, one would happy to take as it ensures that
information about x is not lost -- as discussed elsewhere in the paper.
I view this aspect of the paper by far the most interesting. 

Summary,
Overall, the proposed AIM model is interesting and shows promise, but I'm
not sure how much impact it will have in light of the existing literature
in this area. Perhaps more ambitious applications would really show off the
power of the model and make it standout from the existing crowd. 
</span></div><div class="note_contents"><span class="note_content_field">Rating: </span><span class="note_content_value">4: Ok but not good enough - rejection</span></div><div class="note_contents"><span class="note_content_field">Confidence: </span><span class="note_content_value">5: The reviewer is absolutely certain that the evaluation is correct and very familiar with the relevant literature</span></div><div class="reply_row clearfix"></div></div><div class="children"><div class="note_with_children"><div id="note_HygMkrMgAm" class="note panel"><div class="title_pdf_row clearfix"><h2 class="note_content_title"><a>Author Response to AnonReviewer1</a>  <button class="btn btn-xs btn-default permalink-button" title="Link to this comment" data-permalink-url="https://openreview.net/forum?id=rJx_b3RqY7&amp;noteId=HygMkrMgAm"><span class="glyphicon glyphicon-link" aria-hidden="true"></span></button></h2></div><div class="meta_row"><span class="signatures">ICLR 2019 Conference Paper1184 Authors</span></div><div class="clearfix"><div class="meta_row pull-left"><span class="date item">19 Nov 2018</span><span class="item">ICLR 2019 Conference Paper1184 Official Comment</span><span class="item">Readers: <span class="readers-icon glyphicon glyphicon-globe"></span> Everyone</span></div><div class="meta_row meta_actions"></div></div><div class="note_contents"><span class="note_content_field">Comment: </span><span class="note_content_value">We thank reviewer 1 for the deep and insightful review. Here is our point-to-point response to the comments and questions raised in the review:

1. “The space of adversarially trained latent variable models has grown quite crowded in recent years.”

Although there has been a large improvement in the topic of adversarial inference in recent years, some big issues are still not well addressed and limit the effectiveness of the inference mechanism in adversarial frameworks.

Firstly, to the best of our knowledge, all of the works that attempt to incorporate the inference mechanism into GAN suffer from deteriorating the generation performance. This is supported by the paper [1], in which they conducted extensive experiments to compare many state-of-the-art models with DCGAN. The result shows that GAN variants with inference network perform worse than the standard DCGAN on image generation.

Secondly, the inference performance is also very limited, and as the data distribution becomes more complicated, this issue will be more severe. For example, ALICE’s reconstruction performance on CIFAR-10 is much worse than that on MNIST.

To the best of our knowledge, we are the first to successfully handle these two issues simultaneously. For generation performance, our model AIM does not deteriorate the generation performance but actually further improve it compared with GAN with the same architecture. For the inference, AIM consistently achieves better results on even complicated distributions. 

2. “I would like to understand better why it is that latent variable (z) reconstruction gives rise to better x-space reconstruction.”

We have added a new Section 4.3 to demonstrate the connection between our model and VAE. Specifically, the objective of VAE can be understood as the equation (5), which is like the “reverse version” of equation (3).  By "reverse", we mean that VAE can be explained as performing marginal distribution matching in the data space and conditional distribution matching in the latent space, while our model performs marginal distribution matching in the latent space and conditional distribution matching in the data space. 

Note that the latent z reconstruction alone does not guarantee a better data space reconstruction, just like a stand-alone x reconstruction of VAE will not work without the help of regularization on z. Our method has two simultaneous conditions on the generator, encoder, and discriminator: the generator has to generate samples that can fool the discriminator and the encoder has to bring these generated samples back to their latent codes. So the generator needs to not only generate samples that look real, but also map the latent codes to the “correct” locations (e.g. modes). Otherwise, the encoder will have a hard time to map the samples back (more precisely in our case, will have a low likelihood).

Another interesting property of (5) is that it actually provides a new perspective on VAE’s objective, different from the maximum likelihood point of view. Note that there is also no inequality in (5), unlike the ELBO approach. We can get VAE’s objective by decomposing the summation of the KL-divergence between posteriors on z and between marginals on x.

3. I did not find the claims of better sample quality of AIM over ALI to be well supported by the data. In this context, it is not entirely clear what the significant difference in inception scores represents.

Probably the 2D Gaussian mixture result will provide an insight here. From Table 2, we can see that ALI’s generated samples only cover on average 16 (out of 25) modes while our method’s can cover 25 every time. The ALI’s result we show in Figure 3 is the best-covering result they report, and we include it only to give more insights on the difference between our method and the joint distribution matching scheme.

From the quantitative results in Table 1, we also observe that AIM gives a higher inception score than ALI, and this happens when AIM also has a much lower reconstruction error. The main takeaway message is that, compared to the joint distribution matching of ALI, the separate marginal and conditional matching of AIM leads to better reconstruction and generation. Actually, from Figure 2, the reconstructions of ALI are not always faithful even on the MNIST dataset. We think this is because that it is still very hard for the adversarial game to discover the dependency relation between x and z.

Reference:
[1] Distribution matching in variational inference.
</span></div><div class="reply_row clearfix"></div></div><div class="children"></div></div></div></div><div class="note_with_children"><div id="note_BygpS0C527" class="note panel"><div class="title_pdf_row clearfix"><h2 class="note_content_title"><a>Interesting idea, but needs more work</a>  <button class="btn btn-xs btn-default permalink-button" title="Link to this comment" data-permalink-url="https://openreview.net/forum?id=rJx_b3RqY7&amp;noteId=BygpS0C527"><span class="glyphicon glyphicon-link" aria-hidden="true"></span></button></h2></div><div class="meta_row"><span class="signatures">ICLR 2019 Conference Paper1184 AnonReviewer2</span></div><div class="clearfix"><div class="meta_row pull-left"><span class="date item">03 Nov 2018 (modified: 07 Nov 2018)</span><span class="item">ICLR 2019 Conference Paper1184 Official Review</span><span class="item">Readers: <span class="readers-icon glyphicon glyphicon-globe"></span> Everyone</span></div><div class="meta_row meta_actions"></div></div><div class="note_contents"><span class="note_content_field">Review: </span><span class="note_content_value">The paper proposes a new loss for training deep latent variable models. The novelty seems a bit limited, and the proposed method does not consistently seem to outperform existing methods in the experiments. I'd encourage the authors to add more experiments (see below for suggestions) and resubmit to a different venue.

Section 4:
- q(z) seems to be undefined. Is it the aggregated posterior?
- How is equation (1) related to ELBO that is used for training VAEs?

Some relevant references are missing: I’d love to see a discussion of how this loss relates to other VAE-GAN hybrids.

VEEGAN: Reducing mode collapse in GANs using implicit variational learning
<a href="https://arxiv.org/pdf/1705.07761.pdf" target="_blank" rel="nofollow">https://arxiv.org/pdf/1705.07761.pdf</a>

Distribution Matching in Variational Inference
https://arxiv.org/pdf/1802.06847.pdf


Section 5.1:
- The quantitative comparison measures MSE in pixel space and inception score, neither of which are particularly good measures for measuring the quality of how well the conditionals match. I’d encourage the authors to consider other metrics such as log-likelihood.

- It might be worth evaluating the usefulness of the method on higher-dimensional examples where the analytic forms of q(x|z) and q(z) are known, e.g. plot KL between true and estimated distributions as a function of the number of dimensions.

Section 5.4: 
- The error bars seem quite high. Is there a reason why the method cannot reliably reduce mode collapse?

Minor issues:
- CIFAT-10 -&gt; CIFAR-10
</span></div><div class="note_contents"><span class="note_content_field">Rating: </span><span class="note_content_value">5: Marginally below acceptance threshold</span></div><div class="note_contents"><span class="note_content_field">Confidence: </span><span class="note_content_value">4: The reviewer is confident but not absolutely certain that the evaluation is correct</span></div><div class="reply_row clearfix"></div></div><div class="children"><div class="note_with_children"><div id="note_S1lGhOQlC7" class="note panel"><div class="title_pdf_row clearfix"><h2 class="note_content_title"><a>Author Response to AnonReviewer2</a>  <button class="btn btn-xs btn-default permalink-button" title="Link to this comment" data-permalink-url="https://openreview.net/forum?id=rJx_b3RqY7&amp;noteId=S1lGhOQlC7"><span class="glyphicon glyphicon-link" aria-hidden="true"></span></button></h2></div><div class="meta_row"><span class="signatures">ICLR 2019 Conference Paper1184 Authors</span></div><div class="clearfix"><div class="meta_row pull-left"><span class="date item">19 Nov 2018 (modified: 20 Nov 2018)</span><span class="item">ICLR 2019 Conference Paper1184 Official Comment</span><span class="item">Readers: <span class="readers-icon glyphicon glyphicon-globe"></span> Everyone</span></div><div class="meta_row meta_actions"></div></div><div class="note_contents"><span class="note_content_field">Comment: </span><span class="note_content_value">We thank Reviewer 2 for the constructive feedback. Here is our point-to-point response to the comments and questions raised in this review:

1. Section 4:
- q(z) seems to be undefined. Is it the aggregated posterior?

We are sorry for the confusion. Yes, q(z) is the aggregated posterior. In this paper, p() stands for the distribution on the generator, and q() stands for the distribution on the encoder.

- How is equation (1) related to ELBO that is used for training VAEs?

To better explain the relation between (1) and VAE, we have added a new Section 4.3. 

To summarize, equation (1) is our method’s objective. It means that AIM performs marginal distribution matching in the latent space and conditional distribution matching in the data space. But this objective cannot be optimized directly, so we transfer the problem using (3). It turns out VAE can be derived in a similar manner. Specifically, the objective of VAE can be understood as the equation (5), which is like the “reverse version” of equation (3).  By reverse, we mean that VAE can be explained as performing marginal distribution matching in the data space and conditional distribution matching in the latent space. Note that the RHS of (5) is the well-known VAE form, i.e. a regularization on z (I_vae) plus a reconstruction term on x (II_vae). But we actually get it from a perspective different from ELBO. ELBO is a lower bound of the log-likelihood of the data, and we maximize ELBO in order to maximize the log-likelihood. However, in equation (5), we do not have any inequality and are not directly trying to increase the likelihood. Instead, the LHS of (5) is the summation of KL-divergence between the conditionals on z and between the marginals on x. This is the quantity that VAE tries to minimize, from our perspective motivated by (3).

2. Some relevant references are missing: I’d love to see a discussion of how this loss relates to other VAE-GAN hybrids.

Thank you for bringing these work to our attention. We have cited and discussed them in our updated draft. We also added another adversarial inference paper (adversarial variational Bayes). They are discussed in the second and fourth paragraph in the Related Work section. Empirical comparison with VEEGAN has also been added to Section 5.3.

3. Section 5.1

We use MSE as the measure of how well our model reconstructs the samples. While there may not exist an absolutely perfect measure, we think the relative improvement on one measure still provides lots of information. For example, the best baseline model in Section 5.1 has MSE 0.080 on MNIST, while our model has only 0.026. On CIFAR-10, the best baseline MSE is 0.416, while ours is only 0.019. Moreover, all these improvements on MSE do not come with any compromise on generation. In fact, our model even improves the generating performance over GAN with the same architecture.

But per the reviewer’s request, we add another measure called “percentage of high-quality samples” to our mode-collapse experiment, motivated by the experiments in VEEGAN. The results are summarized in Table 2. We observe that the best baseline model covers about 24.6 modes with 40% generated samples to be of high quality, while our model can cover all the 25 modes with more than 80% high-quality samples. This together with the inception score provides strong evidence that AIM can generate higher-quality samples.

4. Section 5.4

We are not very sure which error bar was the reviewer referring to. But for better illustration, we have summarized the results in a table (Table 2). And from that, we can see that our model covers all of the 25 modes every time with high-quality samples, and can indeed reliably reduce the mode collapse.

5. Minor issue

Thank you for pointing out this typo =) We have corrected it!
</span></div><div class="reply_row clearfix"></div></div><div class="children"></div></div></div></div></div></main></div>
  </div>
</div>


    <!-- Footer -->
    <footer class="sitemap">
      <div class="container">
        <div class="row hidden-xs">
          <div class="col-sm-4">
            <ul class="list-unstyled">
              <li><a href="/" class="home">Home</a></li>
              <li><a href="/venues">All Venues</a></li>
            </ul>
          </div>
    
          <div class="col-sm-4">
            <ul class="list-unstyled">
              <li><a href="/about">About OpenReview</a></li>
              <li><a href="/contact">Contact</a></li>
              <li><a href="#" data-toggle="modal" data-target="#feedback-modal">Feedback</a></li>
            </ul>
          </div>
    
          <div class="col-sm-4">
            <ul class="list-unstyled">
              <li><a href="/terms">Terms of Service</a></li>
              <li><a href="/privacy">Privacy Policy</a></li>
            </ul>
          </div>
        </div>
    
        <div class="row visible-xs-block">
          <div class="col-xs-6">
            <ul class="list-unstyled">
              <li><a href="/" class="home">Home</a></li>
              <li><a href="/about">About OpenReview</a></li>
              <li><a href="#" data-toggle="modal" data-target="#feedback-modal">Feedback</a></li>
            </ul>
          </div>
    
          <div class="col-xs-6">
            <ul class="list-unstyled">
              <li><a href="/contact">Contact</a></li>
              <li><a href="/terms">Terms of Service</a></li>
              <li><a href="/privacy">Privacy Policy</a></li>
            </ul>
          </div>
        </div>
      </div>
    </footer>
    
    <footer class="sponsor">
      <div class="container">
        <div class="row">
          <div class="col-sm-10 col-sm-offset-1">
            <p class="text-center">
              OpenReview is created by the <a href="http://www.iesl.cs.umass.edu/" target="_blank">Information Extraction and Synthesis Laboratory</a>, College of Information and Computer Science, University of Massachusetts Amherst. We gratefully acknowledge the support of the OpenReview sponsors:  Google,  Facebook, NSF, the University of Massachusetts Amherst Center for Data Science, and Center for Intelligent Information Retrieval, as well as the Google Cloud Platform for donating the computing and networking services on which OpenReview.net runs.
            </p>
          </div>
        </div>
      </div>
    </footer>

  <div id="feedback-modal" class="modal fade" tabindex="-1" role="dialog">
    <div class="modal-dialog">
      <div class="modal-content">
        <div class="modal-header">
          <button type="button" class="close" data-dismiss="modal" aria-label="Close"><span aria-hidden="true">×</span></button>
          <h3 class="modal-title">Send Feedback</h3>
        </div>
        <div class="modal-body">
          <p>Enter your feedback below and we'll get back to you as soon as possible.</p>
          <form action="/feedback" method="POST">
            <div class="form-group">
              <input type="email" name="from" class="form-control" placeholder="Email" />
            </div>
            <div class="form-group">
              <input type="text" name="subject" class="form-control" placeholder="Subject" />
            </div>
            <div class="form-group">
              <textarea name="message" class="form-control feedback-input" rows="5" placeholder="Message" required=""></textarea>
            </div>
          <ul id="ui-id-2" tabindex="0" class="ui-menu ui-widget ui-widget-content" style="display: none;"></ul></form>
        </div>
        <div class="modal-footer">
          <button type="button" class="btn btn-default" data-dismiss="modal">Cancel</button>
          <button type="button" class="btn btn-primary">Send</button>
        </div>
      </div>
    </div>
  </div>

  <script type="text/javascript" async="" src="https://www.google-analytics.com/analytics.js"></script><script src="//cdnjs.cloudflare.com/ajax/libs/jquery/2.2.4/jquery.min.js"></script>
  <script>window.jQuery || document.write('&lt;script src="/static/js/vendor/jquery-2.2.4.min.js"&gt;&lt;\/script&gt;')</script>

  <script src="/static/dist/vendor.min.js"></script>
  <script src="/static/dist/templates.min.js"></script>
  <script src="/static/dist/openreview.min.js"></script>

    <script>window.legacyScripts = true;</script>


    <script async="" src="https://www.googletagmanager.com/gtag/js?id=UA-108703919-1"></script>
    <script>
      window.dataLayer = window.dataLayer || [];
      function gtag() { dataLayer.push(arguments); }
      gtag('js', new Date());
      gtag('config', 'UA-108703919-1');
    </script>


<div role="status" aria-live="assertive" aria-relevant="additions" class="ui-helper-hidden-accessible"></div></body></html>