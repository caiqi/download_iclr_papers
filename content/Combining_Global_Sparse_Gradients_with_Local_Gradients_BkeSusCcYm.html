<!DOCTYPE html><html xmlns="http://www.w3.org/1999/xhtml" class="no-js"><head>
  <meta charset="utf-8" />
  <meta http-equiv="X-UA-Compatible" content="IE=edge,chrome=1" />
  <meta name="viewport" content="width=device-width, initial-scale=1" />

  <title>Combining Global Sparse Gradients with Local Gradients | OpenReview</title>
  <meta name="description" content="" />

      <meta name="citation_title" content="Combining Global Sparse Gradients with Local Gradients" />
        <meta name="citation_author" content="Alham Fikri Aji" />
        <meta name="citation_author" content="Kenneth Heafield" />
      <meta name="citation_publication_date" content="2018/09/27" />
      <meta name="citation_online_date" content="2018/09/27" />
      <meta name="citation_pdf_url" content="https://openreview.net/pdf?id=BkeSusCcYm" />
      <meta name="twitter:card" content="summary" />
      <meta name="twitter:site" content="@openreviewnet" />
      <meta name="og:title" content="Combining Global Sparse Gradients with Local Gradients" />
      <meta name="og:description" content="Data-parallel neural network training is network-intensive, so gradient dropping was designed to exchange only large gradients.  However, gradient dropping has been shown to slow convergence.  We..." />
      <meta name="og:image" content="https://openreview.net/static/images/openreview_logo_512.png" />

  <link rel="stylesheet" href="https://fonts.googleapis.com/css?family=Noto+Sans:400,400i,700,700i" />

  <link rel="stylesheet" href="/static/css/bootstrap.min.css" />
  <link rel="stylesheet" href="/static/css/jquery-ui.min.css" />
  <link rel="stylesheet" href="/static/css/main.min.css" />
</head>

<body class="forum">
  <nav class="navbar navbar-inverse navbar-fixed-top" role="navigation">
    <div class="container">
  
      <div class="navbar-header">
        <button type="button" class="navbar-toggle collapsed" data-toggle="collapse" data-target="#navbar" aria-expanded="false" aria-controls="navbar">
          <span class="sr-only">Toggle navigation</span>
          <span class="icon-bar"></span>
          <span class="icon-bar"></span>
          <span class="icon-bar"></span>
        </button>
        <a class="navbar-brand home push-link" href="/"><strong>OpenReview</strong>.net</a>
      </div>
  
      <div id="navbar" class="navbar-collapse collapse">
        <form class="navbar-form navbar-left profile-search" role="search">
          <div class="form-group has-feedback">
            <input id="search_input" type="text" class="form-control ui-autocomplete-input" placeholder="Search ICLR 2019 Conference" autocomplete="off" />
            <span class="glyphicon glyphicon-search form-control-feedback" aria-hidden="true"></span>
          </div>
  
          <input id="search_group" type="hidden" value="ICLR.cc/2019/Conference" />
          <input id="search_content" type="hidden" value="all" />
        <ul id="ui-id-1" tabindex="0" class="ui-menu ui-widget ui-widget-content ui-autocomplete ui-front" style="display: none;"></ul></form>
  
        <ul class="nav navbar-nav navbar-right">
        
            <li id="user-menu"><a href="/login">Login</a></li>
        </ul>
      </div>
  
    </div>
  </nav>

  <div id="or-banner" class="banner" style="">
  <div class="container">
    <div class="row">
      <div class="col-xs-12"><a href="/group?id=ICLR.cc/2019/Conference" title="Venue Homepage"><img class="icon" src="/static/images/arrow_left.svg" /> Go to <strong>ICLR 2019 Conference</strong> homepage</a></div>
    </div>
  </div>
</div>
<div id="flash-message-container" class="alert alert-danger" role="alert" style="display: none;">
  <div class="container">
    <div class="row">
      <div class="col-xs-12">
        <div class="alert-content">
          <button type="button" class="close" aria-label="Close"><span aria-hidden="true">×</span></button>
        </div>
      </div>
    </div>
  </div>
</div>

<div class="container">
  <div class="row">
    <div class="col-xs-12">
      <main id="content" class="clearfix openbanner-visible legacy-styles"><div id="note_BkeSusCcYm" class="note panel"><div class="title_pdf_row clearfix"><h2 class="note_content_title"><a>Combining Global Sparse Gradients with Local Gradients</a> <a class="note_content_pdf" href="/pdf?id=BkeSusCcYm" title="Download PDF" target="_blank"><img src="/static/images/pdf_icon_blue.svg" /></a> </h2></div><div class="meta_row"><span class="signatures"><a href="/profile?email=a.fikri%40ed.ac.uk" class="profile-link" data-toggle="tooltip" data-placement="bottom" title="" data-original-title="a.fikri@ed.ac.uk">Alham Fikri Aji</a>, <a href="/profile?email=kheafiel%40inf.ed.ac.uk" class="profile-link" data-toggle="tooltip" data-placement="bottom" title="" data-original-title="kheafiel@inf.ed.ac.uk">Kenneth Heafield</a></span></div><div class="clearfix"><div class="meta_row pull-left"><span class="date item">28 Sep 2018 (modified: 17 Nov 2018)</span><span class="item">ICLR 2019 Conference Withdrawn Submission</span><span class="item">Readers: <span class="readers-icon glyphicon glyphicon-globe"></span> Everyone</span></div><div class="meta_row meta_actions"></div></div><div class="note_contents"><span class="note_content_field">Abstract: </span><span class="note_content_value">Data-parallel neural network training is network-intensive, so gradient dropping was designed to exchange only large gradients.  However, gradient dropping has been shown to slow convergence.  We propose to improve convergence by having each node combine its locally computed gradient with the sparse global gradient exchanged over the network. We empirically confirm with machine translation tasks that gradient dropping with local gradients approaches convergence 48% faster than non-compressed multi-node training and 28% faster compared to vanilla gradient dropping. We also show that gradient dropping with a local gradient update does not reduce the model's final quality.</span></div><div class="note_contents"><span class="note_content_field">Keywords: </span><span class="note_content_value">Distributed training, stochastic gradient descent, machine translation</span></div><div class="note_contents"><span class="note_content_field">TL;DR: </span><span class="note_content_value">We improve gradient dropping (a technique of only exchanging large gradients on distributed training) by incorporating local gradients while doing a parameter update to reduce quality loss and further improve the training time.</span></div><div class="reply_row clearfix"><div class="item" id="reply_count">4 Replies</div></div></div><hr class="small" /><div id="note_children"><div class="note_with_children"><div id="note_S1gzFofm6m" class="note panel"><div class="title_pdf_row clearfix"><h2 class="note_content_title"><a>Submission Withdrawn by the Authors</a>  <button class="btn btn-xs btn-default permalink-button" title="Link to this comment" data-permalink-url="https://openreview.net/forum?id=BkeSusCcYm&amp;noteId=S1gzFofm6m"><span class="glyphicon glyphicon-link" aria-hidden="true"></span></button></h2></div><div class="meta_row"><span class="signatures">ICLR 2019 Conference Paper349 Authors</span></div><div class="clearfix"><div class="meta_row pull-left"><span class="date item">09 Nov 2018</span><span class="item">ICLR 2019 Conference Paper349 Withdraw Submission</span><span class="item">Readers: <span class="readers-icon glyphicon glyphicon-globe"></span> Everyone</span></div><div class="meta_row meta_actions"></div></div><div class="note_contents"><span class="note_content_field">Withdrawal confirmation: </span><span class="note_content_value">I have read and agree with the withdrawal statement on behalf of myself and my co-authors.</span></div><div class="reply_row clearfix"></div></div><div class="children"></div></div><div class="note_with_children"><div id="note_SkeY-XOanQ" class="note panel"><div class="title_pdf_row clearfix"><h2 class="note_content_title"><a>Need improvement in presentation</a>  <button class="btn btn-xs btn-default permalink-button" title="Link to this comment" data-permalink-url="https://openreview.net/forum?id=BkeSusCcYm&amp;noteId=SkeY-XOanQ"><span class="glyphicon glyphicon-link" aria-hidden="true"></span></button></h2></div><div class="meta_row"><span class="signatures">ICLR 2019 Conference Paper349 AnonReviewer2</span></div><div class="clearfix"><div class="meta_row pull-left"><span class="date item">05 Nov 2018 (modified: 07 Nov 2018)</span><span class="item">ICLR 2019 Conference Paper349 Official Review</span><span class="item">Readers: <span class="readers-icon glyphicon glyphicon-globe"></span> Everyone</span></div><div class="meta_row meta_actions"></div></div><div class="note_contents"><span class="note_content_field">Review: </span><span class="note_content_value">This paper studied the problem of reducing the training time of neural networks in a data-parallel setting where training data is stored in multiple servers. The main goal is to reduce the network traffic of exchanging the gradients at each step. Gradient dropping achieves this goal by exchanging globally only the sparse and large gradients but accumulating the small gradients locally at each step. It was shown in previous work that this approach can have slow convergence. This paper improves this technique by exploring 3 simple heuristics, SUM, PARTIAL, and ERROR, for combing the global sparse gradients and the locally computed gradients during updates. Experimental results showed that in machine translation tasks, the proposed method achieved significant faster time while the final quality of the model is not affected.

Reducing communication cost in distributed learning is a very important problem, and the paper proposes an interesting improvement upon an existing method. However, I think it’s necessary to provide more intuitions or theoretical analysis behind the introduced heuristics. And it might be better to conduct experiments or add more discussion in tasks other than machine translation. The paper is generally easy to follow but the organization needs to be improved. 

More details in terms of presentation:
- Section 2.1 and Algorithm 1 reviewed gradient dropping which is the main related work to this paper but it is in the middle of the related work section. The notations were also not clearly stated (e.g. the notation of | . |, Gt, and AllReduce). It’s better for algorithm blocks to be more self-contained with notations briefly described inside them. I suggest re-organizing it by combing Section 2.1 with Section 3 into a single technical section and providing a clear definition of the notations.
- In Section 3, the first part of Algorithm 2 is identical to Algorithm 1, so it makes sense to keep only one of them and explain only the difference. 
- In Section 4 and 5, it is unclear what the baseline method is referring to. Is it DGC or training with a single machine? The result in Figure 1 seems less important. I would suggest placing it and Section 5.1 in a less significant position. 
</span></div><div class="note_contents"><span class="note_content_field">Rating: </span><span class="note_content_value">5: Marginally below acceptance threshold</span></div><div class="note_contents"><span class="note_content_field">Confidence: </span><span class="note_content_value">4: The reviewer is confident but not absolutely certain that the evaluation is correct</span></div><div class="reply_row clearfix"></div></div><div class="children"></div></div><div class="note_with_children"><div id="note_HkgZYHSo37" class="note panel"><div class="title_pdf_row clearfix"><h2 class="note_content_title"><a>sparse gradient does not seem justified, and the speedup is small</a>  <button class="btn btn-xs btn-default permalink-button" title="Link to this comment" data-permalink-url="https://openreview.net/forum?id=BkeSusCcYm&amp;noteId=HkgZYHSo37"><span class="glyphicon glyphicon-link" aria-hidden="true"></span></button></h2></div><div class="meta_row"><span class="signatures">ICLR 2019 Conference Paper349 AnonReviewer3</span></div><div class="clearfix"><div class="meta_row pull-left"><span class="date item">03 Nov 2018 (modified: 07 Nov 2018)</span><span class="item">ICLR 2019 Conference Paper349 Official Review</span><span class="item">Readers: <span class="readers-icon glyphicon glyphicon-globe"></span> Everyone</span></div><div class="meta_row meta_actions"></div></div><div class="note_contents"><span class="note_content_field">Review: </span><span class="note_content_value">This paper proposes a 3 modes for combining local and global gradients
to better use more computing nodes. The paper is well-written and well-motivated.

I question why it is a good idea to use sparse gradient in a neural network where all coordinates (i.e. hidden units) are
exchangeable.
This paper claims that "gradient values are skewed, as most are close to zero",
but there is no references nor statistics. The only source of sparsity
for a typical MT model are those for word vectors, but in that case, perhaps it would be better to develop algorithms that specifically make use of that type of sparsity.
The quality degradation in methods like gradient dropping seems to suggest that
some arbitrary cutoff for sparsity is not a good idea.

For the results, the proposed method running on 4 nodes is compared to synchronous SGD, and
only got 25% and 45% speedups on the 2 tests.
Notably, they do not compare to asynchronous SGD with stale gradients. 
Such a test would help offer empirical support/refutation
on the assumption that the sparse gradient is a good idea in the first place.
I think these results are not good enough to justify
the extra complications in the proposed method.

The paper is well-written,
however AllReduce seems undefined, and I assumed it aggregates sparse gradients from all nodes.
I also a bit confused about the characteristics of ApplyOptimizer. In gradient dropping,
ApplyOptimizer takes a sparse gradient, but in the proposed methods, it takes a dense gradient.
I am a bit confused on how exactly the synchronous update is done, and I'd appreciate it if you can make your model of parallel updates more explicit.</span></div><div class="note_contents"><span class="note_content_field">Rating: </span><span class="note_content_value">5: Marginally below acceptance threshold</span></div><div class="note_contents"><span class="note_content_field">Confidence: </span><span class="note_content_value">3: The reviewer is fairly confident that the evaluation is correct</span></div><div class="reply_row clearfix"></div></div><div class="children"></div></div><div class="note_with_children"><div id="note_SygUFpLqhQ" class="note panel"><div class="title_pdf_row clearfix"><h2 class="note_content_title"><a>This paper does not have high enough quality and novelty</a>  <button class="btn btn-xs btn-default permalink-button" title="Link to this comment" data-permalink-url="https://openreview.net/forum?id=BkeSusCcYm&amp;noteId=SygUFpLqhQ"><span class="glyphicon glyphicon-link" aria-hidden="true"></span></button></h2></div><div class="meta_row"><span class="signatures">ICLR 2019 Conference Paper349 AnonReviewer1</span></div><div class="clearfix"><div class="meta_row pull-left"><span class="date item">03 Nov 2018 (modified: 07 Nov 2018)</span><span class="item">ICLR 2019 Conference Paper349 Official Review</span><span class="item">Readers: <span class="readers-icon glyphicon glyphicon-globe"></span> Everyone</span></div><div class="meta_row meta_actions"></div></div><div class="note_contents"><span class="note_content_field">Review: </span><span class="note_content_value">The paper looks at the problem of reducing the communication requirement for implementing the distributed optimization techniques, in particular, SGD. This problem has been looked at from multiple angles by many authors. And although there are many unanswered questions in this area, I do not see the authors providing any compelling contribution to answering those questions or providing a meaningful solution.

First of all the solution that is proposed in the paper is just a heuristic and does not seem to have a very justified basis. For example, is SUM, and PARTIAL ways of combining the local gradients the authors justify the formulas by not counting things twice. But then in the ERROR they do not care about the same issue. This tells me that it is highly probable that they have tried many things and this is what has kind of worked. I do not think this is acceptable as we would like to know why something works as opposed to just trying every possibility (which is impossible) and then finding out something works (may be only for a set of limited problems).

In addition to that, I think a much simpler version of what they are proposing has been already proposed in [1]. In [1] the authors propose to run SGD locally and then average the models every once in a while. They empirically show that such a method works well in practice. As it is clear, [1] does not do the weird heuristic of combining the gradients, but still manages to save time and converge relatively fast. So, I am not quite sure if this heuristic combining that is proposed by this paper is any useful.



[1] McMahan, H. Brendan, et al. "Communication-efficient learning of deep networks from decentralized data." </span></div><div class="note_contents"><span class="note_content_field">Rating: </span><span class="note_content_value">3: Clear rejection</span></div><div class="note_contents"><span class="note_content_field">Confidence: </span><span class="note_content_value">4: The reviewer is confident but not absolutely certain that the evaluation is correct</span></div><div class="reply_row clearfix"></div></div><div class="children"></div></div></div></main></div>
  </div>
</div>


    <!-- Footer -->
    <footer class="sitemap">
      <div class="container">
        <div class="row hidden-xs">
          <div class="col-sm-4">
            <ul class="list-unstyled">
              <li><a href="/" class="home">Home</a></li>
              <li><a href="/venues">All Venues</a></li>
            </ul>
          </div>
    
          <div class="col-sm-4">
            <ul class="list-unstyled">
              <li><a href="/about">About OpenReview</a></li>
              <li><a href="/contact">Contact</a></li>
              <li><a href="#" data-toggle="modal" data-target="#feedback-modal">Feedback</a></li>
            </ul>
          </div>
    
          <div class="col-sm-4">
            <ul class="list-unstyled">
              <li><a href="/terms">Terms of Service</a></li>
              <li><a href="/privacy">Privacy Policy</a></li>
            </ul>
          </div>
        </div>
    
        <div class="row visible-xs-block">
          <div class="col-xs-6">
            <ul class="list-unstyled">
              <li><a href="/" class="home">Home</a></li>
              <li><a href="/about">About OpenReview</a></li>
              <li><a href="#" data-toggle="modal" data-target="#feedback-modal">Feedback</a></li>
            </ul>
          </div>
    
          <div class="col-xs-6">
            <ul class="list-unstyled">
              <li><a href="/contact">Contact</a></li>
              <li><a href="/terms">Terms of Service</a></li>
              <li><a href="/privacy">Privacy Policy</a></li>
            </ul>
          </div>
        </div>
      </div>
    </footer>
    
    <footer class="sponsor">
      <div class="container">
        <div class="row">
          <div class="col-sm-10 col-sm-offset-1">
            <p class="text-center">
              OpenReview is created by the <a href="http://www.iesl.cs.umass.edu/" target="_blank">Information Extraction and Synthesis Laboratory</a>, College of Information and Computer Science, University of Massachusetts Amherst. We gratefully acknowledge the support of the OpenReview sponsors:  Google,  Facebook, NSF, the University of Massachusetts Amherst Center for Data Science, and Center for Intelligent Information Retrieval, as well as the Google Cloud Platform for donating the computing and networking services on which OpenReview.net runs.
            </p>
          </div>
        </div>
      </div>
    </footer>

  <div id="feedback-modal" class="modal fade" tabindex="-1" role="dialog">
    <div class="modal-dialog">
      <div class="modal-content">
        <div class="modal-header">
          <button type="button" class="close" data-dismiss="modal" aria-label="Close"><span aria-hidden="true">×</span></button>
          <h3 class="modal-title">Send Feedback</h3>
        </div>
        <div class="modal-body">
          <p>Enter your feedback below and we'll get back to you as soon as possible.</p>
          <form action="/feedback" method="POST">
            <div class="form-group">
              <input type="email" name="from" class="form-control" placeholder="Email" />
            </div>
            <div class="form-group">
              <input type="text" name="subject" class="form-control" placeholder="Subject" />
            </div>
            <div class="form-group">
              <textarea name="message" class="form-control feedback-input" rows="5" placeholder="Message" required=""></textarea>
            </div>
          <ul id="ui-id-2" tabindex="0" class="ui-menu ui-widget ui-widget-content" style="display: none;"></ul></form>
        </div>
        <div class="modal-footer">
          <button type="button" class="btn btn-default" data-dismiss="modal">Cancel</button>
          <button type="button" class="btn btn-primary">Send</button>
        </div>
      </div>
    </div>
  </div>

  <script type="text/javascript" async="" src="https://www.google-analytics.com/analytics.js"></script><script src="//cdnjs.cloudflare.com/ajax/libs/jquery/2.2.4/jquery.min.js"></script>
  <script>window.jQuery || document.write('&lt;script src="/static/js/vendor/jquery-2.2.4.min.js"&gt;&lt;\/script&gt;')</script>

  <script src="/static/dist/vendor.min.js"></script>
  <script src="/static/dist/templates.min.js"></script>
  <script src="/static/dist/openreview.min.js"></script>

    <script>window.legacyScripts = true;</script>


    <script async="" src="https://www.googletagmanager.com/gtag/js?id=UA-108703919-1"></script>
    <script>
      window.dataLayer = window.dataLayer || [];
      function gtag() { dataLayer.push(arguments); }
      gtag('js', new Date());
      gtag('config', 'UA-108703919-1');
    </script>


<div role="status" aria-live="assertive" aria-relevant="additions" class="ui-helper-hidden-accessible"></div></body></html>