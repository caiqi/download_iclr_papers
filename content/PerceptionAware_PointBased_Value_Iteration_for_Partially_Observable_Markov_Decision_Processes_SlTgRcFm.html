<!DOCTYPE html><html xmlns="http://www.w3.org/1999/xhtml" class="no-js"><head>
  <meta charset="utf-8" />
  <meta http-equiv="X-UA-Compatible" content="IE=edge,chrome=1" />
  <meta name="viewport" content="width=device-width, initial-scale=1" />

  <title>Perception-Aware Point-Based Value Iteration for Partially Observable Markov Decision Processes | OpenReview</title>
  <meta name="description" content="" />

      <meta name="citation_title" content="Perception-Aware Point-Based Value Iteration for Partially Observable Markov Decision Processes" />
        <meta name="citation_author" content="Anonymous" />
      <meta name="citation_publication_date" content="2018/09/27" />
      <meta name="citation_online_date" content="2018/09/27" />
      <meta name="citation_pdf_url" content="https://openreview.net/pdf?id=S1lTg3RcFm" />
      <meta name="twitter:card" content="summary" />
      <meta name="twitter:site" content="@openreviewnet" />
      <meta name="og:title" content="Perception-Aware Point-Based Value Iteration for Partially..." />
      <meta name="og:description" content="Partially observable Markov decision processes (POMDPs) are a widely-used framework to model decision-making with uncertainty about the environment and under stochastic outcome. In conventional..." />
      <meta name="og:image" content="https://openreview.net/static/images/openreview_logo_512.png" />

  <link rel="stylesheet" href="https://fonts.googleapis.com/css?family=Noto+Sans:400,400i,700,700i" />

  <link rel="stylesheet" href="/static/css/bootstrap.min.css" />
  <link rel="stylesheet" href="/static/css/jquery-ui.min.css" />
  <link rel="stylesheet" href="/static/css/main.min.css" />
</head>

<body class="forum">
  <nav class="navbar navbar-inverse navbar-fixed-top" role="navigation">
    <div class="container">
  
      <div class="navbar-header">
        <button type="button" class="navbar-toggle collapsed" data-toggle="collapse" data-target="#navbar" aria-expanded="false" aria-controls="navbar">
          <span class="sr-only">Toggle navigation</span>
          <span class="icon-bar"></span>
          <span class="icon-bar"></span>
          <span class="icon-bar"></span>
        </button>
        <a class="navbar-brand home push-link" href="/"><strong>OpenReview</strong>.net</a>
      </div>
  
      <div id="navbar" class="navbar-collapse collapse">
        <form class="navbar-form navbar-left profile-search" role="search">
          <div class="form-group has-feedback">
            <input id="search_input" type="text" class="form-control ui-autocomplete-input" placeholder="Search ICLR 2019 Conference" autocomplete="off" />
            <span class="glyphicon glyphicon-search form-control-feedback" aria-hidden="true"></span>
          </div>
  
          <input id="search_group" type="hidden" value="ICLR.cc/2019/Conference" />
          <input id="search_content" type="hidden" value="all" />
        <ul id="ui-id-1" tabindex="0" class="ui-menu ui-widget ui-widget-content ui-autocomplete ui-front" style="display: none;"></ul></form>
  
        <ul class="nav navbar-nav navbar-right">
        
            <li id="user-menu"><a href="/login">Login</a></li>
        </ul>
      </div>
  
    </div>
  </nav>

  <div id="or-banner" class="banner" style="">
  <div class="container">
    <div class="row">
      <div class="col-xs-12"><a href="/group?id=ICLR.cc/2019/Conference" title="Venue Homepage"><img class="icon" src="/static/images/arrow_left.svg" /> Go to <strong>ICLR 2019 Conference</strong> homepage</a></div>
    </div>
  </div>
</div>
<div id="flash-message-container" class="alert alert-danger" role="alert" style="display: none;">
  <div class="container">
    <div class="row">
      <div class="col-xs-12">
        <div class="alert-content">
          <button type="button" class="close" aria-label="Close"><span aria-hidden="true">×</span></button>
        </div>
      </div>
    </div>
  </div>
</div>

<div class="container">
  <div class="row">
    <div class="col-xs-12">
      <main id="content" class="clearfix openbanner-visible legacy-styles"><div id="note_S1lTg3RcFm" class="note panel"><div class="title_pdf_row clearfix"><h2 class="note_content_title"><a>Perception-Aware Point-Based Value Iteration for Partially Observable Markov Decision Processes</a> <a class="note_content_pdf" href="/pdf?id=S1lTg3RcFm" title="Download PDF" target="_blank"><img src="/static/images/pdf_icon_blue.svg" /></a> </h2></div><div class="meta_row"><span class="signatures">Anonymous</span></div><div class="clearfix"><div class="meta_row pull-left"><span class="date item">28 Sep 2018 (modified: 11 Oct 2018)</span><span class="item">ICLR 2019 Conference Blind Submission</span><span class="item">Readers: <span class="readers-icon glyphicon glyphicon-globe"></span> Everyone</span><span class="item"><a class="action-bibtex-modal" data-bibtex="@inproceedings{    &#10;anonymous2019perception-aware,    &#10;title={Perception-Aware Point-Based Value Iteration for Partially Observable Markov Decision Processes},    &#10;author={Anonymous},    &#10;booktitle={Submitted to International Conference on Learning Representations},    &#10;year={2019},    &#10;url={https://openreview.net/forum?id=S1lTg3RcFm},    &#10;note={under review}    &#10;}">Show Bibtex</a></span></div><div class="meta_row meta_actions"></div></div><div class="note_contents"><span class="note_content_field">Abstract: </span><span class="note_content_value">Partially observable Markov decision processes (POMDPs) are a widely-used framework to model decision-making with uncertainty about the environment and under stochastic outcome. In conventional POMDP models, the observations that the agent receives originate from fixed known distribution. However, in a variety of real-world scenarios the agent has an active role in its perception by selecting which observations to receive. Due to combinatorial nature of such selection process, it is computationally intractable to integrate the perception decision with the planning decision. To prevent such expansion of the action space, we propose a greedy strategy for observation selection. 
We develop a novel point-based value iteration algorithm that incorporates the greedy strategy to find near-optimal selection decision for sampled belief points. This in turn enables the solver to efficiently approximate the reachable subspace of belief simplex by essentially separating computations related to perception from planning.
Lastly, we implement the proposed solver and demonstrate its performance and computational advantage in a range of robotic scenarios where the robot simultaneously performs active perception and planning.</span></div><div class="note_contents"><span class="note_content_field">Keywords: </span><span class="note_content_value">partially observable Markov decision processes, active perception, submodular optimization, point-based value iteration, reinforcement learning</span></div><div class="note_contents"><span class="note_content_field">TL;DR: </span><span class="note_content_value">We develop a point-based value iteration solver for POMDPs with active perception and planning tasks.</span></div><div class="reply_row clearfix"><div class="item" id="reply_count">3 Replies</div></div></div><hr class="small" /><div id="note_children"><div class="note_with_children"><div id="note_BylVdUNZR7" class="note panel"><div class="title_pdf_row clearfix"><h2 class="note_content_title"><a>Algorithmic/theoretical development is sound, but assumptions are questionable.  </a>  <button class="btn btn-xs btn-default permalink-button" title="Link to this comment" data-permalink-url="https://openreview.net/forum?id=S1lTg3RcFm&amp;noteId=BylVdUNZR7"><span class="glyphicon glyphicon-link" aria-hidden="true"></span></button></h2></div><div class="meta_row"><span class="signatures">ICLR 2019 Conference Paper1121 AnonReviewer4</span></div><div class="clearfix"><div class="meta_row pull-left"><span class="date item">20 Nov 2018</span><span class="item">ICLR 2019 Conference Paper1121 Official Review</span><span class="item">Readers: <span class="readers-icon glyphicon glyphicon-globe"></span> Everyone</span></div><div class="meta_row meta_actions"></div></div><div class="note_contents"><span class="note_content_field">Review: </span><span class="note_content_value">This paper proposes a planning algorithm for a restricted class of POMDPs where the sensing decisions do not have any bearing on the hidden state evolution, or any material cost in terms of reward. A sensing decision consists of querying k out of n sensors which yield independent measurements of the hidden state. In this setting, the authors propose an optimization 2-stage optimization strategy, the first stage tries to find the optimal "planning" action in a point-based fashion, whereas the second aims to find the the sensor configuration that reduces the entropy of the post-update belief-state. The key observation is that the entropy minimization step is submodular and can be approximated greedily. This in turn translates to policy approximation bounds via information geometric arguments. 

The positive: the paper is well written (save for some contained parts), the algorithm looks to be generally sound. Altogether the paper makes good points and is an interesting read. 

The negative: 
* first, I think there is some wide-spread misuse of the term "nearly optimal". When talking about near-optimality, this usually refers to finding a (controllably) bounded approximation to the optimal policy/value function. However, here this refers to the error relative to the approximate solution produced by the 2-stage procedure of minimizing entropy, then making a planning decision. It is not clear to me to begin with that this approach would produce bounded policies/value functions. As a counter example, consider a state space consisting of two state variables S1, S2 which evolve independently with additive reward  R(S1, a)  + R(S2, a) with  R(S1, a) !=0, while R(S2, a) = 0 for all actions a. Now, there could be a sensing configuration that collapses the uncertainty over S2 completely, but does nothing over S1, and a different one that give some small reduction of uncertainty  over S1 and nothing over S2. The former may outperform the latter to any degree in terms of belief state entropy, but it will not lead to an optimal policy, since that entropy reduction is not value directed. Unless I misunderstand something, in which case the authors should clarify. 

* Second,  the particular assumptions in this paper are quite restrictive. This paper generally reads like a solution that was fit to a problem. This really hurts the story of the paper. It would be a vast improvement if the authors could find at least one plausible problem where there's a compelling case for this particular configuration of assumptions and try to evaluate how well they do on that problem relative to some reasonable baseline.

Remarks: 
* The belief state notation used in this paper impacts undue suffering upon the reader. It comes in the form of expressions with multi-level sub/superscipts and accents such as: "b prime subscript b tilde superscript a superscipt pr comma omega". This is extremely hard to parse and possibly unnecessary, as b prime subscript b tilde and b tilde are the only configurations of accents and subscripts that appear. These could just be called alpha and beta and the rest is clear from the context. 
* The claim in theorem 4, the argmax is the same under both directions of the KL divergence, is not obvious. It is definitely not true for minimization, otherwise the I-projection and the M-projection would coincide. This should be argued. Alternatively, this point can be alltogether skipped, since Pinsker's bound, which is the only place this is used, does not depend on the direction of KL.

Overall, this paper raises some nice points, but with these problems it is not a clear accept. </span></div><div class="note_contents"><span class="note_content_field">Rating: </span><span class="note_content_value">6: Marginally above acceptance threshold</span></div><div class="note_contents"><span class="note_content_field">Confidence: </span><span class="note_content_value">4: The reviewer is confident but not absolutely certain that the evaluation is correct</span></div><div class="reply_row clearfix"></div></div><div class="children"></div></div><div class="note_with_children"><div id="note_r1enwzJYn7" class="note panel"><div class="title_pdf_row clearfix"><h2 class="note_content_title"><a>Decomposition of observation acquisition and action planning in POMDPs:  Insufficient motivation and results</a>  <button class="btn btn-xs btn-default permalink-button" title="Link to this comment" data-permalink-url="https://openreview.net/forum?id=S1lTg3RcFm&amp;noteId=r1enwzJYn7"><span class="glyphicon glyphicon-link" aria-hidden="true"></span></button></h2></div><div class="meta_row"><span class="signatures">ICLR 2019 Conference Paper1121 AnonReviewer2</span></div><div class="clearfix"><div class="meta_row pull-left"><span class="date item">02 Nov 2018 (modified: 07 Nov 2018)</span><span class="item">ICLR 2019 Conference Paper1121 Official Review</span><span class="item">Readers: <span class="readers-icon glyphicon glyphicon-globe"></span> Everyone</span></div><div class="meta_row meta_actions"></div></div><div class="note_contents"><span class="note_content_field">Review: </span><span class="note_content_value">This work addresses the problem of decomposing the observation acquisition from action planning in POMDPs.  Unfortunately, the paper has two major weaknesses.  First it is hindered by a confusing motivation, and the lack of clarity on the real purpose of the work is a problem throughout. Second the experiments are insufficient given current standards in the literature.

1. Motivation:  The introduction suggests that the main motivation is to reduce the computational cost of treating all observations within planning (“one must establish a trade-off between optimality and tractability”), though later, different reasons are offered (“power, processing capability, and cost constraints”).  It seems that each of these poses different constraints, and depending on which we are most concerned with, a different approximation scheme should be selected. For example, if the real concern is tractability of the POMDP solution, I don’t see why it’s not possible to just acquire all the sensor information, and afterwards decide how to approximate the tracking (this by the way is what most point-based POMDP methods effectively do).  For the proposed AP^2-POMDP, possibly a more reasonable motivation is high cost of observation acquisition; a clean argument would have to be made about the class of problems for which this constraint is crucial, why cardinality of sensor is the right way to articulate this constraint.

2. Experimental results:  The domains selected for the experiments are too simple, given current standards in the literature.  Looking at the 1D and 2D domains is fine to illustrate specific properties of your methods.  But it does not support the claim that the proposed model is more scalable than standard POMDPs.  In such simple domains, why not include results for a point-based method?  They should work in the 1D domain, probably also in the 2D domain.  Also, the setting for the 1-D domain, with a camera in every cell, seems very artificial.  First, if sensors are expensive, why put a camera in every cell?  And if they are not expensive, then why do we need to reason about which sensor to use at each step?  And why just read from k cameras at every step?  These questions point back to the concern regarding what is the real motivation for this work.  For the 2D domain, there are not even quantitative results on cumulative reward.  To be convincing, the results would need to be on substantially more complex domains; there are several POMDP benchmarks that could be considered, e.g. those in the work of Kurniawati et al.

Other comments:
-	Assumption 1 states that the observations from sensors are mutually independent give the state and action.  Can you explain why this is reasonable?  Or whether this is a strong assumption (unlikely to be met in practice)?
-	Some of the bounds seem like they could be very loose in practice, even (in the worse case) worse than the default bound of (R_max-R_min)/(1-\gamma). For example in Thm 3, in the case where the L1 distance between the 2 beliefs is 2, this is worse than the default bound.  Did you check what is the bound for the domains in the experiments?  Is it tighter than this?
-	A key statement is on p.8: “this added complexity is significantly lower than concatening the combinatorial perception actions with the planning actions”.   It is important to support this statement, ideally with both a precise complexity analysis, and with empirical results showing the lesser performance of standard point-based methods.

Minor comment:
-	The referencing style is broken and should be fixed, in particular proper use of Author (year) in the text.
-	The derivations in the top part of p.4 (Eqn 2-4 &amp; surrounding text) are confusing, given that these apply to a standard POMDP, whereas on the previous page your present the AP^2-POMDP model. It might be better in Sec.2 to first (briefly) introduce POMDPs, with Eqn 2-4, then introduce AP^2-POMDP in Sec.3.
-	P.5: “It is worth noting that the objective function does not explicitly depend on perception actions”. This is a confusing statement; V depends on observations through b_t.  The next sentence clarifies this, but it would be better to avoid the confusing statement.
-	Alg.2:  Add a reference beside the title (unless you claim it is new).  Maybe Pineau et al. 2003.
-	P.7: “can be combined with any sampling and pruning method in other solvers” -&gt; Add references for such sampling &amp; pruning methods.


</span></div><div class="note_contents"><span class="note_content_field">Rating: </span><span class="note_content_value">4: Ok but not good enough - rejection</span></div><div class="note_contents"><span class="note_content_field">Confidence: </span><span class="note_content_value">4: The reviewer is confident but not absolutely certain that the evaluation is correct</span></div><div class="reply_row clearfix"></div></div><div class="children"></div></div><div class="note_with_children"><div id="note_SyeBQi3d3X" class="note panel"><div class="title_pdf_row clearfix"><h2 class="note_content_title"><a>review for "Perception-Aware Point-Based Value Iteration for Partially Observable Markov Decision Processes"</a>  <button class="btn btn-xs btn-default permalink-button" title="Link to this comment" data-permalink-url="https://openreview.net/forum?id=S1lTg3RcFm&amp;noteId=SyeBQi3d3X"><span class="glyphicon glyphicon-link" aria-hidden="true"></span></button></h2></div><div class="meta_row"><span class="signatures">ICLR 2019 Conference Paper1121 AnonReviewer3</span></div><div class="clearfix"><div class="meta_row pull-left"><span class="date item">02 Nov 2018 (modified: 07 Nov 2018)</span><span class="item">ICLR 2019 Conference Paper1121 Official Review</span><span class="item">Readers: <span class="readers-icon glyphicon glyphicon-globe"></span> Everyone</span></div><div class="meta_row meta_actions"></div></div><div class="note_contents"><span class="note_content_field">Review: </span><span class="note_content_value">Partially observable Markov decision processes (POMDPs) are a widely-used framework to model decision-making with uncertainty about the environment and under stochastic outcome. In conventional POMDP models, the observations that the agent receives originate from fixed known distribution. However, in a variety of real-world scenarios the agent has an active role in its perception by selecting which observations to receive. Due to the combinatorial nature of such a selection process, it is computationally intractable to integrate the perception decision with the planning decision. 

The author proposes a new form of POMDPs called AP2-POMDP, which takes active perception into account. The AP2-POMDP problem restricts the maximum number of sensors that can be selected by an agent. The agent also faces the planning problem to select the sensors. To prevent such expansion of the action space, the authors propose a greedy strategy for observation selection and obtain a near optimal bound based on submodular optimization.

The author also proposes a greedy-based scheme for the agent to find an almost optimal active perception action by minimizing the uncertainty of beliefs. The author also uses theories to prove the near-optimal guarantees of this greedy method. The author also proposes a novel perception-aware point-based value iteration to calculate the value function and obtain the policy. The author also operates an interesting simulation experiment, which shows less uncertainty of the robot when taking planning actions when using the proposed solver.

The contribution is significant in reinforcement community. The writing is in general clear. It can be improved with minor modifications, for example, explaining math equations better in English. 

My main comment for the authors is whether they have considered the scenario where the perception and the planning actions are connected. I agree with the authors that the best strategy for perception is to reduce uncertainty (and indeed, the greedy approach yields a near-optimal performance), given the restricted situation that the perception and planning are two separated processes. Nonetheless, in most real-world applications, the two processes are coupled, and therefore, we face, immediately, the trade-off between exploration and exploitation. I wonder if the authors have considered how they can extend their approach to such scenarios. 

A few minor comments:

	
(i) The authors should add a legend and perhaps, more explanation in the captions of Figure 5. The colors of the heat-map are confusing. If dark blue and dark red represent lowest and highest frequency, what about other colors? Are there obstacles placed in the grid? If so, are they placed as shown in Figure 3(b)?
	
(ii) What is the effect of k, the maximum number of sensors to be placed? Can the authors provide a figure showing the change of performance with varying k?
	
(iii) It will be more convincing if the author deploys this algorithm to real-world robots and demonstrate its effectiveness. 	

</span></div><div class="note_contents"><span class="note_content_field">Rating: </span><span class="note_content_value">7: Good paper, accept</span></div><div class="note_contents"><span class="note_content_field">Confidence: </span><span class="note_content_value">2: The reviewer is willing to defend the evaluation, but it is quite likely that the reviewer did not understand central parts of the paper</span></div><div class="reply_row clearfix"></div></div><div class="children"></div></div></div></main></div>
  </div>
</div>


    <!-- Footer -->
    <footer class="sitemap">
      <div class="container">
        <div class="row hidden-xs">
          <div class="col-sm-4">
            <ul class="list-unstyled">
              <li><a href="/" class="home">Home</a></li>
              <li><a href="/venues">All Venues</a></li>
            </ul>
          </div>
    
          <div class="col-sm-4">
            <ul class="list-unstyled">
              <li><a href="/about">About OpenReview</a></li>
              <li><a href="/contact">Contact</a></li>
              <li><a href="#" data-toggle="modal" data-target="#feedback-modal">Feedback</a></li>
            </ul>
          </div>
    
          <div class="col-sm-4">
            <ul class="list-unstyled">
              <li><a href="/terms">Terms of Service</a></li>
              <li><a href="/privacy">Privacy Policy</a></li>
            </ul>
          </div>
        </div>
    
        <div class="row visible-xs-block">
          <div class="col-xs-6">
            <ul class="list-unstyled">
              <li><a href="/" class="home">Home</a></li>
              <li><a href="/about">About OpenReview</a></li>
              <li><a href="#" data-toggle="modal" data-target="#feedback-modal">Feedback</a></li>
            </ul>
          </div>
    
          <div class="col-xs-6">
            <ul class="list-unstyled">
              <li><a href="/contact">Contact</a></li>
              <li><a href="/terms">Terms of Service</a></li>
              <li><a href="/privacy">Privacy Policy</a></li>
            </ul>
          </div>
        </div>
      </div>
    </footer>
    
    <footer class="sponsor">
      <div class="container">
        <div class="row">
          <div class="col-sm-10 col-sm-offset-1">
            <p class="text-center">
              OpenReview is created by the <a href="http://www.iesl.cs.umass.edu/" target="_blank">Information Extraction and Synthesis Laboratory</a>, College of Information and Computer Science, University of Massachusetts Amherst. We gratefully acknowledge the support of the OpenReview sponsors:  Google,  Facebook, NSF, the University of Massachusetts Amherst Center for Data Science, and Center for Intelligent Information Retrieval, as well as the Google Cloud Platform for donating the computing and networking services on which OpenReview.net runs.
            </p>
          </div>
        </div>
      </div>
    </footer>

  <div id="feedback-modal" class="modal fade" tabindex="-1" role="dialog">
    <div class="modal-dialog">
      <div class="modal-content">
        <div class="modal-header">
          <button type="button" class="close" data-dismiss="modal" aria-label="Close"><span aria-hidden="true">×</span></button>
          <h3 class="modal-title">Send Feedback</h3>
        </div>
        <div class="modal-body">
          <p>Enter your feedback below and we'll get back to you as soon as possible.</p>
          <form action="/feedback" method="POST">
            <div class="form-group">
              <input type="email" name="from" class="form-control" placeholder="Email" />
            </div>
            <div class="form-group">
              <input type="text" name="subject" class="form-control" placeholder="Subject" />
            </div>
            <div class="form-group">
              <textarea name="message" class="form-control feedback-input" rows="5" placeholder="Message" required=""></textarea>
            </div>
          <ul id="ui-id-2" tabindex="0" class="ui-menu ui-widget ui-widget-content" style="display: none;"></ul></form>
        </div>
        <div class="modal-footer">
          <button type="button" class="btn btn-default" data-dismiss="modal">Cancel</button>
          <button type="button" class="btn btn-primary">Send</button>
        </div>
      </div>
    </div>
  </div>

  <script type="text/javascript" async="" src="https://www.google-analytics.com/analytics.js"></script><script src="//cdnjs.cloudflare.com/ajax/libs/jquery/2.2.4/jquery.min.js"></script>
  <script>window.jQuery || document.write('&lt;script src="/static/js/vendor/jquery-2.2.4.min.js"&gt;&lt;\/script&gt;')</script>

  <script src="/static/dist/vendor.min.js"></script>
  <script src="/static/dist/templates.min.js"></script>
  <script src="/static/dist/openreview.min.js"></script>

    <script>window.legacyScripts = true;</script>


    <script async="" src="https://www.googletagmanager.com/gtag/js?id=UA-108703919-1"></script>
    <script>
      window.dataLayer = window.dataLayer || [];
      function gtag() { dataLayer.push(arguments); }
      gtag('js', new Date());
      gtag('config', 'UA-108703919-1');
    </script>


<div role="status" aria-live="assertive" aria-relevant="additions" class="ui-helper-hidden-accessible"></div></body></html>