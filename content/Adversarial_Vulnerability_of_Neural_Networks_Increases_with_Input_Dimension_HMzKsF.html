<!DOCTYPE html><html xmlns="http://www.w3.org/1999/xhtml" class="no-js"><head>
  <meta charset="utf-8" />
  <meta http-equiv="X-UA-Compatible" content="IE=edge,chrome=1" />
  <meta name="viewport" content="width=device-width, initial-scale=1" />

  <title>Adversarial Vulnerability of Neural Networks Increases with Input Dimension | OpenReview</title>
  <meta name="description" content="" />

      <meta name="citation_title" content="Adversarial Vulnerability of Neural Networks Increases with Input Dimension" />
        <meta name="citation_author" content="Anonymous" />
      <meta name="citation_publication_date" content="2018/09/27" />
      <meta name="citation_online_date" content="2018/09/27" />
      <meta name="citation_pdf_url" content="https://openreview.net/pdf?id=H1MzKs05F7" />
      <meta name="twitter:card" content="summary" />
      <meta name="twitter:site" content="@openreviewnet" />
      <meta name="og:title" content="Adversarial Vulnerability of Neural Networks Increases with Input..." />
      <meta name="og:description" content="Over the past four years, neural networks have been proven vulnerable to adversarial images: targeted but imperceptible image perturbations lead to drastically different predictions. We show that..." />
      <meta name="og:image" content="https://openreview.net/static/images/openreview_logo_512.png" />

  <link rel="stylesheet" href="https://fonts.googleapis.com/css?family=Noto+Sans:400,400i,700,700i" />

  <link rel="stylesheet" href="/static/css/bootstrap.min.css" />
  <link rel="stylesheet" href="/static/css/jquery-ui.min.css" />
  <link rel="stylesheet" href="/static/css/main.min.css" />
</head>

<body class="forum">
  <nav class="navbar navbar-inverse navbar-fixed-top" role="navigation">
    <div class="container">
  
      <div class="navbar-header">
        <button type="button" class="navbar-toggle collapsed" data-toggle="collapse" data-target="#navbar" aria-expanded="false" aria-controls="navbar">
          <span class="sr-only">Toggle navigation</span>
          <span class="icon-bar"></span>
          <span class="icon-bar"></span>
          <span class="icon-bar"></span>
        </button>
        <a class="navbar-brand home push-link" href="/"><strong>OpenReview</strong>.net</a>
      </div>
  
      <div id="navbar" class="navbar-collapse collapse">
        <form class="navbar-form navbar-left profile-search" role="search">
          <div class="form-group has-feedback">
            <input id="search_input" type="text" class="form-control ui-autocomplete-input" placeholder="Search ICLR 2019 Conference" autocomplete="off" />
            <span class="glyphicon glyphicon-search form-control-feedback" aria-hidden="true"></span>
          </div>
  
          <input id="search_group" type="hidden" value="ICLR.cc/2019/Conference" />
          <input id="search_content" type="hidden" value="all" />
        <ul id="ui-id-1" tabindex="0" class="ui-menu ui-widget ui-widget-content ui-autocomplete ui-front" style="display: none;"></ul></form>
  
        <ul class="nav navbar-nav navbar-right">
        
            <li id="user-menu"><a href="/login">Login</a></li>
        </ul>
      </div>
  
    </div>
  </nav>

  <div id="or-banner" class="banner" style="">
  <div class="container">
    <div class="row">
      <div class="col-xs-12"><a href="/group?id=ICLR.cc/2019/Conference" title="Venue Homepage"><img class="icon" src="/static/images/arrow_left.svg" /> Go to <strong>ICLR 2019 Conference</strong> homepage</a></div>
    </div>
  </div>
</div>
<div id="flash-message-container" class="alert alert-danger" role="alert" style="display: none;">
  <div class="container">
    <div class="row">
      <div class="col-xs-12">
        <div class="alert-content">
          <button type="button" class="close" aria-label="Close"><span aria-hidden="true">×</span></button>
        </div>
      </div>
    </div>
  </div>
</div>

<div class="container">
  <div class="row">
    <div class="col-xs-12">
      <main id="content" class="clearfix openbanner-visible legacy-styles"><div id="note_H1MzKs05F7" class="note panel"><div class="title_pdf_row clearfix"><h2 class="note_content_title"><a>Adversarial Vulnerability of Neural Networks Increases with Input Dimension</a> <a class="note_content_pdf" href="/pdf?id=H1MzKs05F7" title="Download PDF" target="_blank"><img src="/static/images/pdf_icon_blue.svg" /></a> </h2></div><div class="meta_row"><span class="signatures">Anonymous</span></div><div class="clearfix"><div class="meta_row pull-left"><span class="date item">28 Sep 2018 (modified: 19 Nov 2018)</span><span class="item">ICLR 2019 Conference Blind Submission</span><span class="item">Readers: <span class="readers-icon glyphicon glyphicon-globe"></span> Everyone</span><span class="item"><a class="action-bibtex-modal" data-bibtex="@inproceedings{    &#10;anonymous2019adversarial,    &#10;title={Adversarial Vulnerability of Neural Networks Increases with Input Dimension},    &#10;author={Anonymous},    &#10;booktitle={Submitted to International Conference on Learning Representations},    &#10;year={2019},    &#10;url={https://openreview.net/forum?id=H1MzKs05F7},    &#10;note={under review}    &#10;}">Show Bibtex</a></span><a class="note_content_pdf item" href="/revisions?id=H1MzKs05F7" target="_blank">Show Revisions</a></div><div class="meta_row meta_actions"></div></div><div class="note_contents"><span class="note_content_field">Abstract: </span><span class="note_content_value">Over the past four years, neural networks have been proven vulnerable to adversarial images: targeted but imperceptible image perturbations lead to drastically different predictions. We show that adversarial vulnerability increases with the gradients of the training objective when viewed as a function of the inputs. For most current network architectures, we prove that the L1-norm of these gradients grows as the square root of the input size. These nets therefore become increasingly vulnerable with growing image size. Our proofs rely on the network’s weight distribution at initialization, but extensive experiments confirm that our conclusions still hold after training.</span></div><div class="note_contents"><span class="note_content_field">Keywords: </span><span class="note_content_value">adversarial vulnerability, neural networks, gradients, FGSM, adversarial data-augmentation, gradient regularization, robust optimization</span></div><div class="note_contents"><span class="note_content_field">TL;DR: </span><span class="note_content_value">Neural nets have large gradients by design; that makes them adversarially vulnerable.</span></div><div class="reply_row clearfix"><div class="item" id="reply_count">13 Replies</div></div></div><hr class="small" /><div id="note_children"><div class="note_with_children"><div id="note_rkxRoqKxRm" class="note panel"><div class="title_pdf_row clearfix"><h2 class="note_content_title"><a>Overall answer to reviews</a>  <button class="btn btn-xs btn-default permalink-button" title="Link to this comment" data-permalink-url="https://openreview.net/forum?id=H1MzKs05F7&amp;noteId=rkxRoqKxRm"><span class="glyphicon glyphicon-link" aria-hidden="true"></span></button></h2></div><div class="meta_row"><span class="signatures">ICLR 2019 Conference Paper426 Authors</span></div><div class="clearfix"><div class="meta_row pull-left"><span class="date item">20 Nov 2018</span><span class="item">ICLR 2019 Conference Paper426 Official Comment</span><span class="item">Readers: <span class="readers-icon glyphicon glyphicon-globe"></span> Everyone</span></div><div class="meta_row meta_actions"></div></div><div class="note_contents"><span class="note_content_field">Comment: </span><span class="note_content_value">First, we would like to thank all the reviewers and contributors to the discussions. We are happy to see that our study raises so many questions.  We are in fact a bit surprised to be caught in such a storm, especially since nobody seems to disagree about the facts at a technical level...

We do believe our paper offers the most precise study of the effects of first-order adversarial perturbations; especially, proving independence from network structure, and re-emphasizing the direct relationship with input dimension. The limitations are clearly stated in the text. We do maintain there is a connection with adversarial attacks: if there is a vulnerability at first order, then there is a vulnerability. And we thank AnonReviewer4 for pointing out that many people do care about first-order worst-case perturbations. However, we agree that not all vulnerability comes from first-order phenomena.  

There seems to be an ongoing debate about the best terminology for the objects studied in our paper. We are happy to term them first-order inputs perturbations, first-order worst-case corruption, first-order adversarial perturbations, or anything similar that makes sense. We will be happy to receive suggestions and edit the text accordingly.  Would that be acceptable?

As to why our insights on prior vulnerability is relevant to understand the vulnerability of SOTA robust models: as mentioned by AnonReviewer3 citing [2], SOTA models still don't manage to reduce gradients on out-of-training examples (see point 5/ below). This is why we think our study is still highly relevant even after strong adversarial training. (Also see our separate thread on why prior vulnerability matters.)

As for technical remarks:

1/ More defences closer to SOTA, especially, iterative ones: the version updated today covers PGD iterative defences (see Figs. 2 &amp; 6). The conclusions are unchanged.

2/ Concerning Fig 4b of the later work [1]: Fig 4b of [1] is *after* robust adversarial training; our claims refer to before adversarial training. So [1] does not refute our claims: rather, it confirms that robust adversarial training operates via reducing gradient issues.
More precisely, to be comparable across dimensions, the x-axis of Fig 4 of [1] should be rescaled to epsilon / sqrt(d), because their epsilon is measured in l_2-norm rather than l_infty (see our Eq. (3), or their text).  After proper rescaling, Fig 4a shows that naturally trained nets are more vulnerable with growing input-dimension. This confirms our own theorems and experiments (our Fig. 3). After rescaling, the curves of Fig 4b seem to overlap: indeed this suggests that our predicted dimension-dependence vanishes *after* robust adversarial training (rather than usual training). This does not contradict our results, it comforts and completes them: usual training does not escape the prior's vulnerability, and in some situations, ill-behaved priors can be escaped by robust training.

3/ Initialization: most people use He-like initializations for a very good reason: it is essentially the only way to obtain bounded activations at init and be symmetric over the inputs of each unit. Our work thus underlines a conflict between keeping reasonably-valued activations, and having reasonably-sized input gradients. This certainly suggests future studies of other initializations. Such initializations will somehow have to break symmetry between units, and thus introduce implicit priors. Some possibilities would be to favor low frequencies in the image, or to prioritize the learning of some units wrt others by initializing them to larger weights (thus implicitly telling the network to try the large-activation units first, which in effect is a prior on the number of units). We have started to play with this, but this is a whole new research direction, while the present text is long enough...

4/ Concerning "adversarial training eventually gives robustness to the training set, but this robustness does not generalize to the test set [2]. For these models, the gradients are well behaved local to the training points (...) but the gradients aren’t well behaved for new iid samples from the data distribution."
This does not contradict our theory/experiments. On the contrary: it appears that outside the training points where gradients have been decreased, the network might keep the gradient properties from its prior, namely, naturally large gradients. This example hence rather shows that our findings on priors and the resulting gradients should be kept in mind.

Once more, we would like to thank the reviewers for the rich debate. We hope that the technical points have been addressed. We do believe our results are non-trivial and relevant. It seems that the "adversarial" terminology raises issues: we are open to consensus suggestions on this point.

[1] Are adversarial examples inevitable, anonymous, under review for ICLR 2019</span></div><div class="reply_row clearfix"></div></div><div class="children"><div class="note_with_children"><div id="note_rkgD7oKgCm" class="note panel"><div class="title_pdf_row clearfix"><h2 class="note_content_title"><a>Why prior vulnerability matters (compendium to the overall reply)</a>  <button class="btn btn-xs btn-default permalink-button" title="Link to this comment" data-permalink-url="https://openreview.net/forum?id=H1MzKs05F7&amp;noteId=rkgD7oKgCm"><span class="glyphicon glyphicon-link" aria-hidden="true"></span></button></h2></div><div class="meta_row"><span class="signatures">ICLR 2019 Conference Paper426 Authors</span></div><div class="clearfix"><div class="meta_row pull-left"><span class="date item">20 Nov 2018</span><span class="item">ICLR 2019 Conference Paper426 Official Comment</span><span class="item">Readers: <span class="readers-icon glyphicon glyphicon-globe"></span> Everyone</span></div><div class="meta_row meta_actions"></div></div><div class="note_contents"><span class="note_content_field">Comment: </span><span class="note_content_value">The reviewers acknowledge that our results --in particular our theorems and their empirical verification in Fig 3-- are novel and interesting. But some of them wonder how understanding the vulnerability of networks at initialization or after usual training helps understanding their vulnerability after (SOTA-) robust training. We try to answer these concerns with the following two points:

a/ Why understanding prior-vulnerability of neural networks may partly explain their vulnerability after robust training.

Without the right priors, even the best learning algorithms fail.
For example, if instead of using carefully designed CNNs, we used fully connected (FC) nets for image classification, we'd get much worse accuracies; especially if our FC net had the same amount of neurons than the CNN (although, theoretically, it could learn the same classifier than the CNN). The situation for adversarial vulnerability is the same: of course, smart training algorithms can help to improve robustness. And the fact that the classifiers get more robust shows that they do help up to some extent. But very ill-behaved priors (i.e. naturally vulnerable priors) will certainly harden their task.

The example pointed out by AnonReviewer3, about recent work [2] showing that adversarial training is efficient on the training set but usually fails to generalise to the test set, may actually be evidence in our favour. Think about FC nets again: they typically get 100% accuracy on the training set, but completely fail to generalise to the test set. The fact that this (essentially) does not happen to CNNs --despite using the same training algorithm!-- shows the importance of choosing well-behaved priors on our architectures/classifiers. Of course, one can argue, as in [2], that with more data, this would not happen, and hence, that adversarial vulnerability is essentially a sample-size problem. But an alternative way to get robust classifiers with the same amount of data might be to use better priors, i.e. more carefully designed architectures. Which brings us to our second point. 

b/ Our paper essentially shows that our priors are already adversarially vulnerable and usual training does not escape these priors. But how can we use those insights? What "actionable" insight does it give us?

For one thing, as argued in a/, pointing out the vulnerability of our priors gives the community a clear reason to search for better priors, so as to complement our robustification algorithms. But they give even more: our theorems, with their clear assumptions and proofs, may actually guide this search. We may for example ask: how can I design an architecture/block that escapes our assumptions? Or: what essential parts of the proofs actually "generate vulnerability"? Appendix B goes in that direction: noticing that overly large weights are an essential reason for vulnerability, it analyses what happens if we introduce average-pooling layers (that have weights of size 1/d rather than 1/sqrt(d)). This section is still preliminary (which is why it is in appendix), but it illustrates how our results, despite being only on priors, can yield concrete, actionable results to improve adversarial robustness.

[2] Adversarially Robust Generalization Requires More Data, Schmidt et al., 2018
</span></div><div class="reply_row clearfix"></div></div><div class="children"></div></div></div></div><div class="note_with_children"><div id="note_rklD0Zcn67" class="note panel"><div class="title_pdf_row clearfix"><h2 class="note_content_title"><a>An interesting approach</a>  <button class="btn btn-xs btn-default permalink-button" title="Link to this comment" data-permalink-url="https://openreview.net/forum?id=H1MzKs05F7&amp;noteId=rklD0Zcn67"><span class="glyphicon glyphicon-link" aria-hidden="true"></span></button></h2></div><div class="meta_row"><span class="signatures">ICLR 2019 Conference Paper426 AnonReviewer4</span></div><div class="clearfix"><div class="meta_row pull-left"><span class="date item">17 Nov 2018</span><span class="item">ICLR 2019 Conference Paper426 Official Review</span><span class="item">Readers: <span class="readers-icon glyphicon glyphicon-globe"></span> Everyone</span></div><div class="meta_row meta_actions"></div></div><div class="note_contents"><span class="note_content_field">Review: </span><span class="note_content_value">The paper studies how the vulnerability of a neural network model depends on its input dimension. The authors prove that for an *untrained* model, randomly initialized with Xavier initialization, the gradient of the loss wrt the input is essentially independent of the architecture and task. This implies that the major factor affecting the norm of that gradient is the input dimension. They then support their argument by experiments measuring the relation between adversarial vulnerability and gradient norm using various *trained* models (including adversarially regularized ones).

I find the main theoretical result interesting. While this is a known fact for the simple case of linear classifiers, extending it to arbitrarily deep networks is a valuable contribution. The proof crucially relies on properties of the specific initialization scheme to show that the gradient does not change too much during backproparagation through the layers. The most significant limitation of the result (which the authors kindly acknowledge) is that this result only holds at initialization. Hence it cannot distinguish between different training methods or between how different architectures evolve during training. Since the situation in adversarial robustness is much more nuanced, I am skeptical about the significance of such statements.

On the experimental side, the finding that gradient regularization improves adversarial robustness to small epsilon values has been made multiple times in the past (as the authors cite in the related work section). It is worth noting that the epsilon considered is 0.005 in L_inf (1.275/255) which is pretty small. This value corresponds to the "small-epsilon regime" where the behavior of the model is fairly linear around the original inputs and thus defenses such as FGSM-training and gradient regularization are effective.

The authors also perform an interesting experiment where they train models on downsampled ImageNet datasets and find that indeed larger input dimension leads to more vulnerable models.

While I find the results interesting, I do not see clear implications. The fact that the vulnerability of a classifier depends on the L1 norm of the input gradient is already known for any locally linear classifier (i.e. deep models too), and it is fairly clear that the L1 norm will have a dimension dependence. The fact that it does not depend on architecture or task at initialization is interesting but of limited significance in my opinion. Given that the experimental results are also not particularly novel, I recommend rejection.

Minor comments to the authors:
-- I think || x ||_* is more clear than |||x||| for the dual norm.
-- Consider using lambda for the regularization, epsilon is confusing since it is overloaded.</span></div><div class="note_contents"><span class="note_content_field">Rating: </span><span class="note_content_value">5: Marginally below acceptance threshold</span></div><div class="note_contents"><span class="note_content_field">Confidence: </span><span class="note_content_value">4: The reviewer is confident but not absolutely certain that the evaluation is correct</span></div><div class="reply_row clearfix"></div></div><div class="children"><div class="note_with_children"><div id="note_HJemastx07" class="note panel"><div class="title_pdf_row clearfix"><h2 class="note_content_title"><a>Some specific answers to your comments</a>  <button class="btn btn-xs btn-default permalink-button" title="Link to this comment" data-permalink-url="https://openreview.net/forum?id=H1MzKs05F7&amp;noteId=HJemastx07"><span class="glyphicon glyphicon-link" aria-hidden="true"></span></button></h2></div><div class="meta_row"><span class="signatures">ICLR 2019 Conference Paper426 Authors</span></div><div class="clearfix"><div class="meta_row pull-left"><span class="date item">20 Nov 2018</span><span class="item">ICLR 2019 Conference Paper426 Official Comment</span><span class="item">Readers: <span class="readers-icon glyphicon glyphicon-globe"></span> Everyone</span></div><div class="meta_row meta_actions"></div></div><div class="note_contents"><span class="note_content_field">Comment: </span><span class="note_content_value">We thank you for your careful review, and for pointing out that many people do indeed care about small worst-case l_p-perturbations, and why.

- Concerning: "I am skeptical about the significance of such statements [at initialization]." &amp; "While I find the results interesting, I do not see clear implications."

Please refer to our overall thread on, why understanding the vulnerability of priors helps understanding post (robust) training vulnerability. See also point 4/ in our overall reply.

- Concerning: "While this is a known fact for the simple case of linear classifiers..."

Even with only linear classifiers, previous published work has predicted a linear increase of vulnerability with input-dimension rather than sqrt(d), because they did not take the dimension-dependance of the weights into account.

- Concerning: "it is fairly clear that the L1 norm will have a dimension dependence"

Maybe, but it is all about getting the numbers right. Our predictions correspond to the *exact* increase-rate measured in practice.

- Concerning: "The fact that it does not depend on architecture or task at initialization is interesting but of limited significance in my opinion."

This independence on architecture at initialization shows that, if we want to get non-vulnerable priors, we need to re-think our initialization scheme and/or introduce a new architectural building block. As to why we would want non-vulnerable priors, again, please see our overall thread on the subject.

Once again, we thank you for your review and hope that our answers may help you to re-evaluate the significance of our results.</span></div><div class="reply_row clearfix"></div></div><div class="children"></div></div></div></div><div class="note_with_children"><div id="note_rylBpNERnm" class="note panel"><div class="title_pdf_row clearfix"><h2 class="note_content_title"><a>Considered question seems poorly motivated, significance of analysis and conclusions yet to be demonstrated</a>  <button class="btn btn-xs btn-default permalink-button" title="Link to this comment" data-permalink-url="https://openreview.net/forum?id=H1MzKs05F7&amp;noteId=rylBpNERnm"><span class="glyphicon glyphicon-link" aria-hidden="true"></span></button></h2></div><div class="meta_row"><span class="signatures">ICLR 2019 Conference Paper426 AnonReviewer3</span></div><div class="clearfix"><div class="meta_row pull-left"><span class="date item">06 Nov 2018 (modified: 07 Nov 2018)</span><span class="item">ICLR 2019 Conference Paper426 Official Review</span><span class="item">Readers: <span class="readers-icon glyphicon glyphicon-globe"></span> Everyone</span></div><div class="meta_row meta_actions"></div></div><div class="note_contents"><span class="note_content_field">Review: </span><span class="note_content_value">This paper argues that adversarial vulnerability of neural networks increases with input dimension. Theoretical and empirical evidence are given which connect the l_p norm of the gradient of the training objective with the existence of small-worst case l_q perturbations. This connection is made by assuming that the learned function is well approximated by a linear function local to the sampled input x. By making assumptions on the initialization scheme for some simple architectures, the authors show that the l_p norm of the gradient for randomly initialized network will be large, and provide empirical evidence that these assumptions hold after training. These assumptions imply bounds on the typical magnitude of the gradient of the loss with respect to a single input coordinate, this then implies that the overall gradient norm will depend on the input dimension.

I found this paper well written. The mathematical assumptions are presented in a clear, easy to understand manner. Also high level intuition is given around their main theorems which help the reader understand the main ideas. However, I have a  number of concerns about this work.

The first is, I do not buy the motivation for studying the "phenomenon" of small worst-case l_p perturbations. I realize this statement applies to a large body of literature, but since the publication of  [1] we are still lacking concrete motivating scenarios for the l_p action space. I would encourage the authors instead to ask the closely related but more general question of how we can improve model generalization outside the natural distribution of images, such as generalization in the presence of commonly occurring image corruptions [2]. It's possible that the analysis in this work could better our understanding model generalization in the presence of different image corruptions, indeed by making similar linearity assumptions as considered in this work, test error in additive Gaussian noise can be linked with distance to the decision boundary [3,4]. However, this particular question was not explored in this work.

Second, the work is one of many to relate the norm of the gradient with adversarial robustness (for example, this has been proposed as a defense mechanism in [5,6]). I also suspect that the main theorem relating gradient norm to initialization should easily follow for more general settings using the mean field theory developed by [7,8] (this would be particularly useful for removing assumption H1, which assumes the ReLU activation is a random variable independent of the weights). Overall, I don't see how gradient norms explain why statistical classifiers make mistakes, particularly for more realistic attacker action spaces [9]. Even for "small" l_p adversarial examples there seem to be limitations as to how much gradient norms can explain the phenomenon --- for example even max margin classifiers such as SVM's have "adversarial examples". Furthermore, adversarial training has been shown to reach a point where the model is "robust" locally to training points but this robustness does not generalize to the points in the test set [10]. In fact, for the synthetic data distributions considered in [10], it's proven that no learning algorithm can achieve robustness given insufficient training data.

Finally, the main conclusion of this work "adversarial vulnerability of neural networks increases with input dimension" is an overly general statement which needs a much more nuanced view. While experiments shown in [11] support this conclusion for naturally trained networks, it is shown that when adversarial training is applied the model is more robust when the input dimension is higher (see Figure 4 a. and b.). Perhaps the assumptions for Theorem 4 are violated for these adversarially trained models. 

1. <a href="https://arxiv.org/abs/1807.06732" target="_blank" rel="nofollow">https://arxiv.org/abs/1807.06732</a>
2. https://arxiv.org/abs/1807.01697
3. https://arxiv.org/abs/1608.08967
4. https://openreview.net/forum?id=S1xoy3CcYX&amp;noteId=BklKxJBF57.
5. https://arxiv.org/abs/1704.08847
6. https://arxiv.org/abs/1608.07690
7. https://arxiv.org/abs/1611.01232
8. https://arxiv.org/abs/1806.05393
9. https://arxiv.org/abs/1712.09665
10. https://arxiv.org/abs/1804.11285
11. https://arxiv.org/pdf/1809.02104.pdf</span></div><div class="note_contents"><span class="note_content_field">Rating: </span><span class="note_content_value">4: Ok but not good enough - rejection</span></div><div class="note_contents"><span class="note_content_field">Confidence: </span><span class="note_content_value">5: The reviewer is absolutely certain that the evaluation is correct and very familiar with the relevant literature</span></div><div class="reply_row clearfix"></div></div><div class="children"><div class="note_with_children"><div id="note_HkxbjTl6TQ" class="note panel"><div class="title_pdf_row clearfix"><h2 class="note_content_title"><a>The criticism of studying L_p perturbations is unjustified.</a>  <button class="btn btn-xs btn-default permalink-button" title="Link to this comment" data-permalink-url="https://openreview.net/forum?id=H1MzKs05F7&amp;noteId=HkxbjTl6TQ"><span class="glyphicon glyphicon-link" aria-hidden="true"></span></button></h2></div><div class="meta_row"><span class="signatures">ICLR 2019 Conference Paper426 AnonReviewer4</span></div><div class="clearfix"><div class="meta_row pull-left"><span class="date item">17 Nov 2018</span><span class="item">ICLR 2019 Conference Paper426 Official Comment</span><span class="item">Readers: <span class="readers-icon glyphicon glyphicon-globe"></span> Everyone</span></div><div class="meta_row meta_actions"></div></div><div class="note_contents"><span class="note_content_field">Comment: </span><span class="note_content_value">I understand and agree with the argument that robustness to small L_p perturbations is by no means a meaningful security guarantee. Whether this area of research is relevant to ML security is a topic of active debate that is beyond the scope of this review. However, understanding the robustness of models to small L_p perturbations is important for a variety of reasons. I will try and outline some of them below.

Clearly, the vulnerability of modern ML classifiers to imperceptible perturbations is concerning as it implies that:
a. They are not worst-case invariant to small, simple perturbations.
b. They depend on their input in ways that we would not expect/want them to.
(c. There could be security vulnerabilities that would evade human supervision.)

So the natural question to ask is: "Are L_p adversarial examples inevitable? If so, for which models/datasets?"
We  still do not know the answers to these questions. 
-- If this vulnerability is indeed inherent, then what are meaningful notions of worst-case invariances that our classifiers should satisfy? Even then, does our partial progress towards L_p robustness help us develop tools for different problems?
-- If this is simply a limitation of our current models and methods, then we will eventually be able to create L_p-robust classifiers for large datasets. This would help us better understand how to enforce invariances to our model. We could then start working towards broader families of perturbations that we want to be robust to. But before any of this happens we need to understand if we can at least solve the (conceptually) very simple problem of "Can we be robust to small Lp norms?". Moreover, if we construct models that are Lp robust, are these models more useful for standard tasks in some way? 

We could have a very lengthy discussion about the topic, but this is clearly not the right place for that. Arguably, this topic of research is of interest to a sizeable part of the ICLR community. I would thus encourage the reviewer to focus on the technical content of the paper and let the AC decide on whether L_p robustness is a topic of interest for ICLR.
</span></div><div class="reply_row clearfix"></div></div><div class="children"><div class="note_with_children"><div id="note_H1gaUtVATX" class="note panel"><div class="title_pdf_row clearfix"><h2 class="note_content_title"><a>My score holds regardless of the motivation question, here are my technical questions/concerns.</a>  <button class="btn btn-xs btn-default permalink-button" title="Link to this comment" data-permalink-url="https://openreview.net/forum?id=H1MzKs05F7&amp;noteId=H1gaUtVATX"><span class="glyphicon glyphicon-link" aria-hidden="true"></span></button></h2></div><div class="meta_row"><span class="signatures">ICLR 2019 Conference Paper426 AnonReviewer3</span></div><div class="clearfix"><div class="meta_row pull-left"><span class="date item">18 Nov 2018 (modified: 19 Nov 2018)</span><span class="item">ICLR 2019 Conference Paper426 Official Comment</span><span class="item">Readers: <span class="readers-icon glyphicon glyphicon-globe"></span> Everyone</span></div><div class="meta_row meta_actions"></div></div><div class="note_contents"><span class="note_content_field">Comment: </span><span class="note_content_value">The experiments for adversarially trained models in [1] directly contradict the title of this paper. The models trained on the higher dimensional space are more robust (see Figure 4 b.). Can the authors comment on this? My understanding is that for some settings of the weights you can show a bound such as is discussed in the paper, but there are other settings (perhaps even initializations) of the weights for which the conclusion will not hold. To me this suggests that a more appropriate title would be “improper initialization of neural networks can cause sensitivity to small perturbations". However, fixing the initialization seems unlikely to buy us much more than what adversarial training achieves, and the experiments in [1] suggest to me the conclusion of this work is limited in scope. Indeed, even adversarially trained models are still sensitive to "small" perturbations, only the epsilon at which they are sensitive to increases slightly.

As you mention, many prior works have explored gradient penalties as a way to increase robustness, and some have perhaps helped a little bit as an adversarial defense, but we hit a limit as we increase the epsilon considered for the perturbations, and its not clear whether or not this can improve upon adversarially trained models. Because of this, it’s not clear to me what actionable insights we can conclude from this work, and how this can be used to improve upon the current SOTA. In fact, it was found that adversarial training eventually gives robustness to the training set, but this robustness does not generalize to the test set [2]. For these models, the gradients are well behaved local to the training points (and thus any gradient based loss function will be minimized for the training points) but the gradients aren’t well behaved for new iid samples from the data distribution. 

Furthermore, in [2] it was shown that there is no learning algorithm can become robust to small perturbations, unless that model is trained on significantly more data. So at least for the synthetic data distribution they consider there is no  data independent initialization scheme that can achieve robustness.

Minor nit: You might be able to remove the unrealistic assumption H1 from Theorem 4 by considering the theory from [3].

1. <a href="https://arxiv.org/abs/1809.02104" target="_blank" rel="nofollow">https://arxiv.org/abs/1809.02104</a>
2. https://arxiv.org/abs/1804.11285
3. https://arxiv.org/abs/1611.01232

</span></div><div class="reply_row clearfix"></div></div><div class="children"><div class="note_with_children"><div id="note_SkgnHhtlAQ" class="note panel"><div class="title_pdf_row clearfix"><h2 class="note_content_title"><a>Specific answers to your comments &amp; concerns</a>  <button class="btn btn-xs btn-default permalink-button" title="Link to this comment" data-permalink-url="https://openreview.net/forum?id=H1MzKs05F7&amp;noteId=SkgnHhtlAQ"><span class="glyphicon glyphicon-link" aria-hidden="true"></span></button></h2></div><div class="meta_row"><span class="signatures">ICLR 2019 Conference Paper426 Authors</span></div><div class="clearfix"><div class="meta_row pull-left"><span class="date item">20 Nov 2018</span><span class="item">ICLR 2019 Conference Paper426 Official Comment</span><span class="item">Readers: <span class="readers-icon glyphicon glyphicon-globe"></span> Everyone</span></div><div class="meta_row meta_actions"></div></div><div class="note_contents"><span class="note_content_field">Comment: </span><span class="note_content_value">We thank you for your expanded reviews, comments, questions and references, which we hope to address in full.

-Concerning: "The experiments for adversarially trained models in [1] directly contradict the title of this paper"

See point 2/ of our overall reply.

- Concerning: "for some settings of the weights you can show a bound such as is discussed in the paper, but there are other settings (perhaps even initializations) of the weights for which the conclusion will not hold."

The current initialization-methods are used to avoid exploding/vanishing activations at init. Any other initialization would need to solve that issue. See point 3/ of our overall reply.

- Concerning: "fixing the initialization seems unlikely to buy us much more than what adversarial training achieves, and the experiments in [1] suggest to me the conclusion of this work is limited in scope"

Please refer to our overall thread on "why prior vulnerability matters" and how it might help understanding and harnessing the vulnerability of (robustly) trained networks.

- Concerning: "we hit a limit as we increase the epsilon considered for the perturbations"

Even for small epsilons, our networks are surprisingly vulnerable. If we don't understand the small epsilon vulnerability, then we won't understand big epsilons.

- Concerning: "it’s not clear to me what actionable insights we can conclude from this work, and how this can be used to improve upon the current SOTA."

Again, please see our thread on "why prior vulnerability matters"  and how it may help understanding and harnessing the vulnerability of (robustly) trained networks.

- Concerning: "it was found that adversarial training eventually gives robustness to the training set, but this robustness does not generalize to the test set [2]... the data distribution."

See 4/ in our overall reply.

- Concerning: "in [2] it was shown that there is no learning algorithm [that] can become robust to small perturbations, unless that model is trained on significantly more data."

Please refer to our thread on "why prior vulnerability matters". As we explain there, to get better generalisation you can either increase your amount of training data, or decrease the complexity of your model, i.e. choose better (non-vulnerable!) priors.

- Concerning assumption H1 and reference [3]:

the mean field approach of [3] relies on very strong independence approximations, namely, neglecting individual effects and replacing them with overall averaged effects with similar statistics. This amounts to disregarding most correlations. We do believe a mean-field treatment of our approach is possible, but in the end, the mean field approximations are much stronger than our assumption H1, though similar in spirit.

[1] Are adversarial examples inevitable?, 2018
[3] Deep Information Propagation, Schoenholz et al., 2017</span></div><div class="reply_row clearfix"></div></div><div class="children"></div></div></div></div><div class="note_with_children"><div id="note_B1e9VFERpQ" class="note panel"><div class="title_pdf_row clearfix"><h2 class="note_content_title"><a>Separate thread to discuss motivation</a>  <button class="btn btn-xs btn-default permalink-button" title="Link to this comment" data-permalink-url="https://openreview.net/forum?id=H1MzKs05F7&amp;noteId=B1e9VFERpQ"><span class="glyphicon glyphicon-link" aria-hidden="true"></span></button></h2></div><div class="meta_row"><span class="signatures">ICLR 2019 Conference Paper426 AnonReviewer3</span></div><div class="clearfix"><div class="meta_row pull-left"><span class="date item">18 Nov 2018</span><span class="item">ICLR 2019 Conference Paper426 Official Comment</span><span class="item">Readers: <span class="readers-icon glyphicon glyphicon-globe"></span> Everyone</span></div><div class="meta_row meta_actions"></div></div><div class="note_contents"><span class="note_content_field">Comment: </span><span class="note_content_value">I'm creating a separate thread to discuss motivation, I'm willing to evaluate this paper on the technical aspects but motivation is important and the authors of this work could have asked a very related and better motivated question and written essentially the same paper.

Why focus on small lp perturbations instead of studying the broader phenomenon that convolutional neural networks significantly underperform humans when classifying distorted images: <a href="https://arxiv.org/abs/1705.02498," target="_blank" rel="nofollow">https://arxiv.org/abs/1705.02498,</a> https://openreview.net/forum?id=HJz6tiCqYm . The fact that models are not perfect in the presence of random image corruptions is concerning as it implies that:

a. They are not even average case invariant to moderate simple perturbations.
b. They depend on their input in ways that we would not expect/want them to.
c. There are security vulnerabilities that will affect classifiers in many realistic deployed settings, say a street sign classifier misclassifying an input because it is a rainy day. 
d. We will never be robust in worst-case settings until we are first robust in the average case.

Moreover it has been known for some time that the sensitivity of models to small l2 perturbations is the same phenomenon as the sensitivity of models in the presence of large average case corruptions, see this 2016 NIPS paper: https://papers.nips.cc/paper/6331-robustness-of-classifiers-from-adversarial-to-random-noise. So your paper is essentially studying the same phenomenon that I am recommending you study, you have just chosen an odd, difficult to motivate, and difficult to evaluate metric for measuring the robustness of image classifiers. If you find small worst case perturbations surprising and interesting, but moderate average case perturbations not surprising or interesting, then I can help clarify the connection. 

Overall, my position is not so much that lp adversarial examples are uninteresting, it's just every adversarial example paper that only focuses on lp perturbations rather than studying model generalization in non-iid settings is artificially limiting the impact and scope of their work.</span></div><div class="reply_row clearfix"></div></div><div class="children"></div></div></div></div></div></div><div class="note_with_children"><div id="note_H1eawIzc3X" class="note panel"><div class="title_pdf_row clearfix"><h2 class="note_content_title"><a>A solid contribution to the study of adversarial examples.</a>  <button class="btn btn-xs btn-default permalink-button" title="Link to this comment" data-permalink-url="https://openreview.net/forum?id=H1MzKs05F7&amp;noteId=H1eawIzc3X"><span class="glyphicon glyphicon-link" aria-hidden="true"></span></button></h2></div><div class="meta_row"><span class="signatures">ICLR 2019 Conference Paper426 AnonReviewer1</span></div><div class="clearfix"><div class="meta_row pull-left"><span class="date item">03 Nov 2018 (modified: 07 Nov 2018)</span><span class="item">ICLR 2019 Conference Paper426 Official Review</span><span class="item">Readers: <span class="readers-icon glyphicon glyphicon-globe"></span> Everyone</span></div><div class="meta_row meta_actions"></div></div><div class="note_contents"><span class="note_content_field">Review: </span><span class="note_content_value">The authors provide a compelling theoretical explanation for a large class of adversarial examples.  While this explanation (rooted in the norm of gradients of neural networks being the culprit for the existence of adversarial examples) is not new, they unify several old perspectives, and convincingly argue for genuinely new scaling relationships (i.e. \sqrt(d) versus linear in d scaling of sensitivity to adversarial perturbations versus input size). They prove a number of theorems relating these scaling relationships to a broad swathe of relevant model architectures, and provide thorough empirical evidence of their work.

I can honestly find very little to complain about in this work--the prose is clear, and the proofs are correct as far as I can tell (though I found Figure 4 in the appendix (left panel) to not be hugely compelling.  More data here would be great!)

As much of the analysis hinges on the particularities of the weight distribution at initialization, could the authors comment on possible defenses to adversarial attack by altering this weight distribution? (By, for example, imposing that the average value must grow like 1/d)?</span></div><div class="note_contents"><span class="note_content_field">Rating: </span><span class="note_content_value">9: Top 15% of accepted papers, strong accept</span></div><div class="note_contents"><span class="note_content_field">Confidence: </span><span class="note_content_value">4: The reviewer is confident but not absolutely certain that the evaluation is correct</span></div><div class="reply_row clearfix"></div></div><div class="children"><div class="note_with_children"><div id="note_SJxgihtxRm" class="note panel"><div class="title_pdf_row clearfix"><h2 class="note_content_title"><a>Specific answers to your few questions</a>  <button class="btn btn-xs btn-default permalink-button" title="Link to this comment" data-permalink-url="https://openreview.net/forum?id=H1MzKs05F7&amp;noteId=SJxgihtxRm"><span class="glyphicon glyphicon-link" aria-hidden="true"></span></button></h2></div><div class="meta_row"><span class="signatures">ICLR 2019 Conference Paper426 Authors</span></div><div class="clearfix"><div class="meta_row pull-left"><span class="date item">20 Nov 2018</span><span class="item">ICLR 2019 Conference Paper426 Official Comment</span><span class="item">Readers: <span class="readers-icon glyphicon glyphicon-globe"></span> Everyone</span></div><div class="meta_row meta_actions"></div></div><div class="note_contents"><span class="note_content_field">Comment: </span><span class="note_content_value">We thank you for your review and your very positive evaluation.

Concerning Fig 4 in Appendix A:
Appendix A is preliminary work, whose goal is essentially to illustrate how our insights on the prior-vulnerability of neural networks can help us design more robust networks; in this case, by preferring average-poolings over other pooling-operations. But we agree that this section only contains preliminary results, which is why it is in appendix, not main text.

Concerning: "could the authors comment on possible defences to adversarial attack by altering this weight distribution? (By, for example, imposing that the average value must grow like 1/d)?"
Please refer to point 3/ of our overall reply, which explains what problems arise if we just change the overall weight-size at init.

We hope that this addresses your small concerns/questions and thank you, once again, for your evaluation.</span></div><div class="reply_row clearfix"></div></div><div class="children"></div></div></div></div><div class="note_with_children"><div id="note_BJgK8oPUh7" class="note panel"><div class="title_pdf_row clearfix"><h2 class="note_content_title"><a>interesting work but with limited applicability and significance demonstrated</a>  <button class="btn btn-xs btn-default permalink-button" title="Link to this comment" data-permalink-url="https://openreview.net/forum?id=H1MzKs05F7&amp;noteId=BJgK8oPUh7"><span class="glyphicon glyphicon-link" aria-hidden="true"></span></button></h2></div><div class="meta_row"><span class="signatures">ICLR 2019 Conference Paper426 AnonReviewer2</span></div><div class="clearfix"><div class="meta_row pull-left"><span class="date item">31 Oct 2018 (modified: 07 Nov 2018)</span><span class="item">ICLR 2019 Conference Paper426 Official Review</span><span class="item">Readers: <span class="readers-icon glyphicon glyphicon-globe"></span> Everyone</span></div><div class="meta_row meta_actions"></div></div><div class="note_contents"><span class="note_content_field">Review: </span><span class="note_content_value">This paper analyzes the relationship between "adversarial vulnerability" with input dimensionality of neural network. The paper proves that, under certain assumptions, as the input dimensionality increases, neural networks exhibit increasingly large gradients thus are more adversarially vulnerable. Experiments were done on neural networks trained by penalizing input gradients and FGSM-adversarial training. Similar trends on vulnerability vs dimensionality are found.

The paper is clearly written and easy to follow. I appreciate that the authors also clearly stated the limitation of the theoretical analysis.

The theoretical analyses on vulnerability and dimensionality is novel and provide some insights. But it is unlikely such analysis is significant There are a few reasons:
- This analysis only seems to work for "well-behaved" models. For models with gradient masking, obfuscated gradients or even non-differentiable models, it is not clear that how this will apply. (and I appreciate that the authors also acknowledge this in the paper.) It is unclear how this specific gradient based analysis can help the understanding of the adversarial perturbation phenomena. After all, the first order Taylor expansion argument on top of randomly initialized weights is oversimplifying the complicated problem.
- One very important special case of the point above: the analysis probably cannot cover the  adversarially PGD trained models [MMS+17] and the certifiably robust ones. Such models may have small gradients inside the box constraint, but can have large gradients between different classes.


On the empirical results, the authors made a few interesting observations, for example the close correspondence between "Adv Train" and "Grad Regu" models. 
My concern is that the experiments were done on a narrow range of models, which only have "weak" adversarial training / defenses.
Adversarial robustness is hard to achieve. What matters the most is "why the strongest model is still not robust?" not "why some weak models are not robust?" 
It is especially worrisome to me that the paper does not cover the adversarially-augmented training based iterative attacks, e.g. PGD TRAINED models [MMS+17] which is the SOTA on MNIST/CIFAR10 L_\infty robustness benchmark.
Without comprehensive analyses on SOTA robust models, it is hard to justify the validity of the theoretical analysis in this paper, and the conclusions made by the paper.
For example, re: the last sentence in the conclusion: "They hence suggest to tackle adversarial vulnerability by designing new architectures (or new architectural building blocks) rather than by new regularization techniques." The reasoning is not obvious to me given the current evidence shown in the paper.

[MMS+17] Madry A, Makelov A, Schmidt L, Tsipras D, Vladu A. Towards deep learning models resistant to adversarial attacks. arXiv preprint arXiv:1706.06083</span></div><div class="note_contents"><span class="note_content_field">Rating: </span><span class="note_content_value">5: Marginally below acceptance threshold</span></div><div class="note_contents"><span class="note_content_field">Confidence: </span><span class="note_content_value">5: The reviewer is absolutely certain that the evaluation is correct and very familiar with the relevant literature</span></div><div class="reply_row clearfix"></div></div><div class="children"><div class="note_with_children"><div id="note_S1lSrptgAm" class="note panel"><div class="title_pdf_row clearfix"><h2 class="note_content_title"><a>Specific answers to your comments &amp; concerns</a>  <button class="btn btn-xs btn-default permalink-button" title="Link to this comment" data-permalink-url="https://openreview.net/forum?id=H1MzKs05F7&amp;noteId=S1lSrptgAm"><span class="glyphicon glyphicon-link" aria-hidden="true"></span></button></h2></div><div class="meta_row"><span class="signatures">ICLR 2019 Conference Paper426 Authors</span></div><div class="clearfix"><div class="meta_row pull-left"><span class="date item">20 Nov 2018</span><span class="item">ICLR 2019 Conference Paper426 Official Comment</span><span class="item">Readers: <span class="readers-icon glyphicon glyphicon-globe"></span> Everyone</span></div><div class="meta_row meta_actions"></div></div><div class="note_contents"><span class="note_content_field">Comment: </span><span class="note_content_value">We thank you for your time, review, comments and concerns, which we hope to address in full.

- Concerning: "This analysis only seems to work for 'well-behaved' models. For models ... apply"

Indeed, we only analyse differentiable models. First, note that our results already cover many usual networks (not just a small subset). Second, we think that understanding such well-behaved models is a first step towards understanding non-differentiable ones. (For example, some non-differentiable functions can be considered differentiable at a rougher scale. But this opens a whole new research direction, while the text is long enough...)

- Concerning: "the first order Taylor expansion argument on top of randomly initialized weights is oversimplifying the complicated problem."

Not all adversarial vulnerability might be first-order, but first-order vulnerability *is* an aspect of vulnerability (not an oversimplification of a problem). If there is first-order vulnerability, then there is vulnerability. Moreover, our results actually suggest that first-order vulnerability and its relation to gradients explains an *essential* part of vulnerability (see Fig 2d, and paragraph "Validity of first order expansion").

- Concerning: "the analysis probably cannot cover the  adversarially PGD trained models [MMS+17] and the certifiably robust ones" &amp; "It is especially worrisome to me that the paper does not cover the adversarially-augmented training based iterative attacks, e.g. PGD TRAINED models"

We added experiments with PGD training on CIFAR-10 (see Fig 2 &amp; 6). Our conclusions stay unchanged. The new experiments support our claim that first-order vulnerability plays an essential role.

- Concerning: "What matters the most is 'why the strongest model is still not robust?' not 'why some weak models are not robust?'"

We think that understanding the vulnerability of "weak" models (i.e. at initialization or with usual training) may help understanding the vulnerability of SOTA-robustly trained nets. See our post on "why prior vulnerability matters".

- Concerning our last sentence:

We can reformulate it to:
"Nevertheless, they show that at least this type of first-order vulnerability is present, common, and firmly rooted *in the priors* of our current network architectures. In future, we may hence want to complement our robust regularisation techniques by new architectures (or architectural building blocks) with less vulnerable priors."
(Anything in that direction would do. We are open to propositions.)</span></div><div class="reply_row clearfix"></div></div><div class="children"></div></div></div></div></div></main></div>
  </div>
</div>


    <!-- Footer -->
    <footer class="sitemap">
      <div class="container">
        <div class="row hidden-xs">
          <div class="col-sm-4">
            <ul class="list-unstyled">
              <li><a href="/" class="home">Home</a></li>
              <li><a href="/venues">All Venues</a></li>
            </ul>
          </div>
    
          <div class="col-sm-4">
            <ul class="list-unstyled">
              <li><a href="/about">About OpenReview</a></li>
              <li><a href="/contact">Contact</a></li>
              <li><a href="#" data-toggle="modal" data-target="#feedback-modal">Feedback</a></li>
            </ul>
          </div>
    
          <div class="col-sm-4">
            <ul class="list-unstyled">
              <li><a href="/terms">Terms of Service</a></li>
              <li><a href="/privacy">Privacy Policy</a></li>
            </ul>
          </div>
        </div>
    
        <div class="row visible-xs-block">
          <div class="col-xs-6">
            <ul class="list-unstyled">
              <li><a href="/" class="home">Home</a></li>
              <li><a href="/about">About OpenReview</a></li>
              <li><a href="#" data-toggle="modal" data-target="#feedback-modal">Feedback</a></li>
            </ul>
          </div>
    
          <div class="col-xs-6">
            <ul class="list-unstyled">
              <li><a href="/contact">Contact</a></li>
              <li><a href="/terms">Terms of Service</a></li>
              <li><a href="/privacy">Privacy Policy</a></li>
            </ul>
          </div>
        </div>
      </div>
    </footer>
    
    <footer class="sponsor">
      <div class="container">
        <div class="row">
          <div class="col-sm-10 col-sm-offset-1">
            <p class="text-center">
              OpenReview is created by the <a href="http://www.iesl.cs.umass.edu/" target="_blank">Information Extraction and Synthesis Laboratory</a>, College of Information and Computer Science, University of Massachusetts Amherst. We gratefully acknowledge the support of the OpenReview sponsors:  Google,  Facebook, NSF, the University of Massachusetts Amherst Center for Data Science, and Center for Intelligent Information Retrieval, as well as the Google Cloud Platform for donating the computing and networking services on which OpenReview.net runs.
            </p>
          </div>
        </div>
      </div>
    </footer>

  <div id="feedback-modal" class="modal fade" tabindex="-1" role="dialog">
    <div class="modal-dialog">
      <div class="modal-content">
        <div class="modal-header">
          <button type="button" class="close" data-dismiss="modal" aria-label="Close"><span aria-hidden="true">×</span></button>
          <h3 class="modal-title">Send Feedback</h3>
        </div>
        <div class="modal-body">
          <p>Enter your feedback below and we'll get back to you as soon as possible.</p>
          <form action="/feedback" method="POST">
            <div class="form-group">
              <input type="email" name="from" class="form-control" placeholder="Email" />
            </div>
            <div class="form-group">
              <input type="text" name="subject" class="form-control" placeholder="Subject" />
            </div>
            <div class="form-group">
              <textarea name="message" class="form-control feedback-input" rows="5" placeholder="Message" required=""></textarea>
            </div>
          <ul id="ui-id-2" tabindex="0" class="ui-menu ui-widget ui-widget-content" style="display: none;"></ul></form>
        </div>
        <div class="modal-footer">
          <button type="button" class="btn btn-default" data-dismiss="modal">Cancel</button>
          <button type="button" class="btn btn-primary">Send</button>
        </div>
      </div>
    </div>
  </div>

  <script type="text/javascript" async="" src="https://www.google-analytics.com/analytics.js"></script><script src="//cdnjs.cloudflare.com/ajax/libs/jquery/2.2.4/jquery.min.js"></script>
  <script>window.jQuery || document.write('&lt;script src="/static/js/vendor/jquery-2.2.4.min.js"&gt;&lt;\/script&gt;')</script>

  <script src="/static/dist/vendor.min.js"></script>
  <script src="/static/dist/templates.min.js"></script>
  <script src="/static/dist/openreview.min.js"></script>

    <script>window.legacyScripts = true;</script>


    <script async="" src="https://www.googletagmanager.com/gtag/js?id=UA-108703919-1"></script>
    <script>
      window.dataLayer = window.dataLayer || [];
      function gtag() { dataLayer.push(arguments); }
      gtag('js', new Date());
      gtag('config', 'UA-108703919-1');
    </script>


<div role="status" aria-live="assertive" aria-relevant="additions" class="ui-helper-hidden-accessible"></div></body></html>