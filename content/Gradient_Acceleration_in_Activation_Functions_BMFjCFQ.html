<!DOCTYPE html><html xmlns="http://www.w3.org/1999/xhtml" class="no-js"><head>
  <meta charset="utf-8" />
  <meta http-equiv="X-UA-Compatible" content="IE=edge,chrome=1" />
  <meta name="viewport" content="width=device-width, initial-scale=1" />

  <title>Gradient Acceleration in Activation Functions | OpenReview</title>
  <meta name="description" content="" />

      <meta name="citation_title" content="Gradient Acceleration in Activation Functions" />
        <meta name="citation_author" content="Anonymous" />
      <meta name="citation_publication_date" content="2018/09/27" />
      <meta name="citation_online_date" content="2018/09/27" />
      <meta name="citation_pdf_url" content="https://openreview.net/pdf?id=B1M9FjC5FQ" />
      <meta name="twitter:card" content="summary" />
      <meta name="twitter:site" content="@openreviewnet" />
      <meta name="og:title" content="Gradient Acceleration in Activation Functions" />
      <meta name="og:description" content="Dropout has been one of standard approaches to train deep neural networks, and it is known to regularize large models to avoid overfitting. The effect of dropout has been explained by avoiding..." />
      <meta name="og:image" content="https://openreview.net/static/images/openreview_logo_512.png" />

  <link rel="stylesheet" href="https://fonts.googleapis.com/css?family=Noto+Sans:400,400i,700,700i" />

  <link rel="stylesheet" href="/static/css/bootstrap.min.css" />
  <link rel="stylesheet" href="/static/css/jquery-ui.min.css" />
  <link rel="stylesheet" href="/static/css/main.min.css" />
</head>

<body class="forum">
  <nav class="navbar navbar-inverse navbar-fixed-top" role="navigation">
    <div class="container">
  
      <div class="navbar-header">
        <button type="button" class="navbar-toggle collapsed" data-toggle="collapse" data-target="#navbar" aria-expanded="false" aria-controls="navbar">
          <span class="sr-only">Toggle navigation</span>
          <span class="icon-bar"></span>
          <span class="icon-bar"></span>
          <span class="icon-bar"></span>
        </button>
        <a class="navbar-brand home push-link" href="/"><strong>OpenReview</strong>.net</a>
      </div>
  
      <div id="navbar" class="navbar-collapse collapse">
        <form class="navbar-form navbar-left profile-search" role="search">
          <div class="form-group has-feedback">
            <input id="search_input" type="text" class="form-control ui-autocomplete-input" placeholder="Search ICLR 2019 Conference" autocomplete="off" />
            <span class="glyphicon glyphicon-search form-control-feedback" aria-hidden="true"></span>
          </div>
  
          <input id="search_group" type="hidden" value="ICLR.cc/2019/Conference" />
          <input id="search_content" type="hidden" value="all" />
        <ul id="ui-id-1" tabindex="0" class="ui-menu ui-widget ui-widget-content ui-autocomplete ui-front" style="display: none;"></ul></form>
  
        <ul class="nav navbar-nav navbar-right">
        
            <li id="user-menu"><a href="/login">Login</a></li>
        </ul>
      </div>
  
    </div>
  </nav>

  <div id="or-banner" class="banner" style="">
  <div class="container">
    <div class="row">
      <div class="col-xs-12"><a href="/group?id=ICLR.cc/2019/Conference" title="Venue Homepage"><img class="icon" src="/static/images/arrow_left.svg" /> Go to <strong>ICLR 2019 Conference</strong> homepage</a></div>
    </div>
  </div>
</div>
<div id="flash-message-container" class="alert alert-danger" role="alert" style="display: none;">
  <div class="container">
    <div class="row">
      <div class="col-xs-12">
        <div class="alert-content">
          <button type="button" class="close" aria-label="Close"><span aria-hidden="true">Ã—</span></button>
        </div>
      </div>
    </div>
  </div>
</div>

<div class="container">
  <div class="row">
    <div class="col-xs-12">
      <main id="content" class="clearfix openbanner-visible legacy-styles"><div id="note_B1M9FjC5FQ" class="note panel"><div class="title_pdf_row clearfix"><h2 class="note_content_title"><a>Gradient Acceleration in Activation Functions</a> <a class="note_content_pdf" href="/pdf?id=B1M9FjC5FQ" title="Download PDF" target="_blank"><img src="/static/images/pdf_icon_blue.svg" /></a> </h2></div><div class="meta_row"><span class="signatures">Anonymous</span></div><div class="clearfix"><div class="meta_row pull-left"><span class="date item">28 Sep 2018 (modified: 11 Oct 2018)</span><span class="item">ICLR 2019 Conference Blind Submission</span><span class="item">Readers: <span class="readers-icon glyphicon glyphicon-globe"></span> Everyone</span><span class="item"><a class="action-bibtex-modal" data-bibtex="@inproceedings{    &#10;anonymous2019gradient,    &#10;title={Gradient Acceleration in Activation Functions},    &#10;author={Anonymous},    &#10;booktitle={Submitted to International Conference on Learning Representations},    &#10;year={2019},    &#10;url={https://openreview.net/forum?id=B1M9FjC5FQ},    &#10;note={under review}    &#10;}">Show Bibtex</a></span></div><div class="meta_row meta_actions"></div></div><div class="note_contents"><span class="note_content_field">Abstract: </span><span class="note_content_value">Dropout has been one of standard approaches to train deep neural networks, and it is known to regularize large models to avoid overfitting. The effect of dropout has been explained by avoiding co-adaptation.
In this paper, however, we propose a new explanation of why dropout works and propose a new technique to design better activation functions. First, we show that dropout can be explained as an optimization technique to push the input towards the saturation area of nonlinear activation function by accelerating gradient information flowing even in the saturation area in backpropagation. Based on this explanation, we propose a new technique for activation functions, {\em gradient acceleration in activation function (GAAF)}, that accelerates gradients to flow even in the saturation area. Then, input to the activation function can climb onto the saturation area which makes the network more robust because the model converges on a flat region.  
Experiment results support our explanation of dropout and confirm that the proposed GAAF technique improves performances with expected properties.</span></div><div class="note_contents"><span class="note_content_field">Keywords: </span><span class="note_content_value">Gradient Acceleration, Saturation Areas, Dropout, Coadaptation</span></div><div class="reply_row clearfix"><div class="item" id="reply_count">6 Replies</div></div></div><hr class="small" /><div id="note_children"><div class="note_with_children"><div id="note_ryxFeM5qh7" class="note panel"><div class="title_pdf_row clearfix"><h2 class="note_content_title"><a>interesting analysis on dropout</a>  <button class="btn btn-xs btn-default permalink-button" title="Link to this comment" data-permalink-url="https://openreview.net/forum?id=B1M9FjC5FQ&amp;noteId=ryxFeM5qh7"><span class="glyphicon glyphicon-link" aria-hidden="true"></span></button></h2></div><div class="meta_row"><span class="signatures">ICLR 2019 Conference Paper471 AnonReviewer3</span></div><div class="clearfix"><div class="meta_row pull-left"><span class="date item">03 Nov 2018 (modified: 07 Nov 2018)</span><span class="item">ICLR 2019 Conference Paper471 Official Review</span><span class="item">Readers: <span class="readers-icon glyphicon glyphicon-globe"></span> Everyone</span></div><div class="meta_row meta_actions"></div></div><div class="note_contents"><span class="note_content_field">Review: </span><span class="note_content_value">This paper gives further analysis on dropout and explains why it works although Hinton et al. already showed some analysis. This paper also introduced a new gradient acceleration in activation function (GAAF).

On Table 4, the GAAF is a bit worse than dropout although GAAF converges fast. But i am not sure whether GAAF is really useful on large datasets, not on a small dataset, e.g., MINIST here. On table 5, i am not sure whether you compared with dropout or not. Is your base model already including dropout?

If you want to demonstrate that GAAF is really helpful, i think more experiments and comparisons, especially on larger datsets should be conducted.



</span></div><div class="note_contents"><span class="note_content_field">Rating: </span><span class="note_content_value">5: Marginally below acceptance threshold</span></div><div class="note_contents"><span class="note_content_field">Confidence: </span><span class="note_content_value">3: The reviewer is fairly confident that the evaluation is correct</span></div><div class="reply_row clearfix"></div></div><div class="children"><div class="note_with_children"><div id="note_rylasBk2pm" class="note panel"><div class="title_pdf_row clearfix"><h2 class="note_content_title"><a>Thanks for your review.</a>  <button class="btn btn-xs btn-default permalink-button" title="Link to this comment" data-permalink-url="https://openreview.net/forum?id=B1M9FjC5FQ&amp;noteId=rylasBk2pm"><span class="glyphicon glyphicon-link" aria-hidden="true"></span></button></h2></div><div class="meta_row"><span class="signatures">ICLR 2019 Conference Paper471 Authors</span></div><div class="clearfix"><div class="meta_row pull-left"><span class="date item">16 Nov 2018</span><span class="item">ICLR 2019 Conference Paper471 Official Comment</span><span class="item">Readers: <span class="readers-icon glyphicon glyphicon-globe"></span> Everyone</span></div><div class="meta_row meta_actions"></div></div><div class="note_contents"><span class="note_content_field">Comment: </span><span class="note_content_value">First of all, we thank you for your interests and helpful reviews.

In Table 4, we tried to show that our proposed GAAF can improve the performance of model as much as Dropout does, while the model can be converged much faster than Dropout.

In Table 5, the base model does not include dropout, because we use Convolutional Neural Networks (CNNs). In Tabel 5, we show that the effect of traditional Dropout can be reduced with Batch Normalization (BN), but GAAF works even with BN. 

Thank you for your suggestion, we will apply our method to larger datasets like ImageNet. As the effect of Dropout decreases with larger datasets, the effect of GAAF might be reduced with larger datasets. In the current submission, however, our experiments confirm that GAAF helps models optimize better, although it is with small datasets. 
</span></div><div class="reply_row clearfix"></div></div><div class="children"></div></div></div></div><div class="note_with_children"><div id="note_HkemxceqhQ" class="note panel"><div class="title_pdf_row clearfix"><h2 class="note_content_title"><a>No proper grounding of the presented argument against " avoiding co-adaptation through dropout" concept. Very weak experiments.</a>  <button class="btn btn-xs btn-default permalink-button" title="Link to this comment" data-permalink-url="https://openreview.net/forum?id=B1M9FjC5FQ&amp;noteId=HkemxceqhQ"><span class="glyphicon glyphicon-link" aria-hidden="true"></span></button></h2></div><div class="meta_row"><span class="signatures">ICLR 2019 Conference Paper471 AnonReviewer1</span></div><div class="clearfix"><div class="meta_row pull-left"><span class="date item">03 Nov 2018 (modified: 07 Nov 2018)</span><span class="item">ICLR 2019 Conference Paper471 Official Review</span><span class="item">Readers: <span class="readers-icon glyphicon glyphicon-globe"></span> Everyone</span></div><div class="meta_row meta_actions"></div></div><div class="note_contents"><span class="note_content_field">Review: </span><span class="note_content_value">The authors attempt to propose an alternative explanation for the effect of dropout in a neural network and then present a technique to improve existing activation functions.

Section 3.1 presents a experimental proof of higher co-adaptation in presence of dropout, in my opinion this is an incorrect experiment and request authors to double check. In my experience, using dropout results in sparse representations in the hidden layers which is the effect decreased co-adaptions. Also, a single experiment with MNIST data-set cannot be a proof to reject a theory.

Section 3.2 Table 2 presents a comparison between average gradient flow through layers during training where flow with dropout is higher. This is not very surprising, in my opinion, given the variance of the activation of a neuron in presence of dropout the network tries to optimize the classification cost while trying to reduce the variance. The experimental details are almost nil.

The experiments section 5 presents very weak results. Very little or no improvement and authors randomly introduce BatchNorm into one of the experiment.</span></div><div class="note_contents"><span class="note_content_field">Rating: </span><span class="note_content_value">2: Strong rejection</span></div><div class="note_contents"><span class="note_content_field">Confidence: </span><span class="note_content_value">5: The reviewer is absolutely certain that the evaluation is correct and very familiar with the relevant literature</span></div><div class="reply_row clearfix"></div></div><div class="children"><div class="note_with_children"><div id="note_BJe4SUy367" class="note panel"><div class="title_pdf_row clearfix"><h2 class="note_content_title"><a>Thank you for the feedback.</a>  <button class="btn btn-xs btn-default permalink-button" title="Link to this comment" data-permalink-url="https://openreview.net/forum?id=B1M9FjC5FQ&amp;noteId=BJe4SUy367"><span class="glyphicon glyphicon-link" aria-hidden="true"></span></button></h2></div><div class="meta_row"><span class="signatures">ICLR 2019 Conference Paper471 Authors</span></div><div class="clearfix"><div class="meta_row pull-left"><span class="date item">16 Nov 2018</span><span class="item">ICLR 2019 Conference Paper471 Official Comment</span><span class="item">Readers: <span class="readers-icon glyphicon glyphicon-globe"></span> Everyone</span></div><div class="meta_row meta_actions"></div></div><div class="note_contents"><span class="note_content_field">Comment: </span><span class="note_content_value">Thanks for your helpful review.

We do not want to reject the original explanation of Dropout, but we want to suggest another view of Dropout and within this view, we suggest a new method that can improve the modelâ€™s performance.

First of all, we think the sparse representations in hidden layers with Dropout may not be the result of avoiding co-adaptation but the result of better training. A well-optimized model can have sparse representations. We tried to show that the better optimization with Dropout can be explained by increasing gradient information flows in Section 3.

Table 2 proves our explanation about Dropout (the effect of Dropout can be explained by increasing gradient flow rather than avoiding co-adaptation), and the increasing of gradient flow is the basic idea of our proposed method.

In Tabel 5, GAAF improves the modelâ€™s accuracy compared to the base model with all three datasets. Also, we tried to show that as we know the effect of traditional Dropout can be eliminated when Batch Normalization (BN) is applied, but GAAF optimizes the model further even with BN. This is why we applied BN to this experiment. The accuracies of BN + GAAF models are higher than BN-only models.
</span></div><div class="reply_row clearfix"></div></div><div class="children"></div></div></div></div><div class="note_with_children"><div id="note_HJlSV3RF2X" class="note panel"><div class="title_pdf_row clearfix"><h2 class="note_content_title"><a>Interesting idea, but seems less precise than previous work</a>  <button class="btn btn-xs btn-default permalink-button" title="Link to this comment" data-permalink-url="https://openreview.net/forum?id=B1M9FjC5FQ&amp;noteId=HJlSV3RF2X"><span class="glyphicon glyphicon-link" aria-hidden="true"></span></button></h2></div><div class="meta_row"><span class="signatures">ICLR 2019 Conference Paper471 AnonReviewer2</span></div><div class="clearfix"><div class="meta_row pull-left"><span class="date item">02 Nov 2018 (modified: 09 Nov 2018)</span><span class="item">ICLR 2019 Conference Paper471 Official Review</span><span class="item">Readers: <span class="readers-icon glyphicon glyphicon-globe"></span> Everyone</span><a class="note_content_pdf item" href="/revisions?id=HJlSV3RF2X" target="_blank">Show Revisions</a></div><div class="meta_row meta_actions"></div></div><div class="note_contents"><span class="note_content_field">Review: </span><span class="note_content_value">This paper offers the argument that dropout works not due to preventing coadaptation, but because it gives more gradient, especially in the saturated region. However, previous works have already characterized how dropout modifies the activation function, and also the gradient in a more precise way than what is proposed in this paper. 

## Co-adaptation
co-adaptation does not seem to mean correlation among the unit activations. It is not too surprising units need more redundancy with dropout, since a highly useful feature might not always be present, but thus need to be replicated elsewhere.

Section 8 of this paper gives a definition of co-adaptation,
based on if the loss is reduced or increased based on a simultaneous change in units.
<a href="https://arxiv.org/abs/1412.4736" target="_blank" rel="nofollow">https://arxiv.org/abs/1412.4736</a>
And this work, https://arxiv.org/abs/1602.04484, reached a conclusion similar to yours
that for some notion of coadaptation, dropout might increase it.

## Gradient acceleration
It does not seem reasonable to measure "gradient information flow" simply as the norm of the gradient, which is sensitive to scales, and it is not clear if the authors accounted for scaling factor of dropout in Table 2.

The proposed resolution, to add this discontinuous step function in (7) with floor is a very interesting idea backed by good experimental results. However, I think the main effect is in adding noise, since the gradient with respect to this function is not meaningful. The main effect is optimizing with respect to the base function, but adding noise when computing the outputs. Previous work have also looked at how dropout noise modifies the effective activation function (and thus its gradient). This work, http://proceedings.mlr.press/v28/wang13a.html, give a more precise characterization instead of treating the effect as adding a function with constant gradient multiplied by an envelop. In fact, the actual gradient with dropout does involve the envelope by chain rule, but the rest is not actually constant as in GAAF. 
</span></div><div class="note_contents"><span class="note_content_field">Rating: </span><span class="note_content_value">3: Clear rejection</span></div><div class="note_contents"><span class="note_content_field">Confidence: </span><span class="note_content_value">4: The reviewer is confident but not absolutely certain that the evaluation is correct</span></div><div class="reply_row clearfix"></div></div><div class="children"><div class="note_with_children"><div id="note_r1x33UJ26m" class="note panel"><div class="title_pdf_row clearfix"><h2 class="note_content_title"><a>Thanks for your review.</a>  <button class="btn btn-xs btn-default permalink-button" title="Link to this comment" data-permalink-url="https://openreview.net/forum?id=B1M9FjC5FQ&amp;noteId=r1x33UJ26m"><span class="glyphicon glyphicon-link" aria-hidden="true"></span></button></h2></div><div class="meta_row"><span class="signatures">ICLR 2019 Conference Paper471 Authors</span></div><div class="clearfix"><div class="meta_row pull-left"><span class="date item">16 Nov 2018</span><span class="item">ICLR 2019 Conference Paper471 Official Comment</span><span class="item">Readers: <span class="readers-icon glyphicon glyphicon-globe"></span> Everyone</span></div><div class="meta_row meta_actions"></div></div><div class="note_contents"><span class="note_content_field">Comment: </span><span class="note_content_value">Thanks for your helpful review and introducing other previous works related to ours.

In section 2.3, we introduced the noisy activation function and we analyzed that drop outâ€™s effect is quite similar to noise injection in activation function. We also think that our proposed method also quite similar to noisy activation function and we expected the same effect. The noisy activation function increases gradient flow on the saturation areas with stochastic noise, while GAFF does with deterministic method. 

For the scale issue that you mentioned, we think it does not affect because we used the same model size and configurations for the Dropout model and the base model. 

Also, we think that our analysis of Dropout looks similar to previous works that you introduced, but our contribution is that in line with such analysis, we designed a new activation function which can replace the Dropout layer.
</span></div><div class="reply_row clearfix"></div></div><div class="children"></div></div></div></div></div></main></div>
  </div>
</div>


    <!-- Footer -->
    <footer class="sitemap">
      <div class="container">
        <div class="row hidden-xs">
          <div class="col-sm-4">
            <ul class="list-unstyled">
              <li><a href="/" class="home">Home</a></li>
              <li><a href="/venues">All Venues</a></li>
            </ul>
          </div>
    
          <div class="col-sm-4">
            <ul class="list-unstyled">
              <li><a href="/about">About OpenReview</a></li>
              <li><a href="/contact">Contact</a></li>
              <li><a href="#" data-toggle="modal" data-target="#feedback-modal">Feedback</a></li>
            </ul>
          </div>
    
          <div class="col-sm-4">
            <ul class="list-unstyled">
              <li><a href="/terms">Terms of Service</a></li>
              <li><a href="/privacy">Privacy Policy</a></li>
            </ul>
          </div>
        </div>
    
        <div class="row visible-xs-block">
          <div class="col-xs-6">
            <ul class="list-unstyled">
              <li><a href="/" class="home">Home</a></li>
              <li><a href="/about">About OpenReview</a></li>
              <li><a href="#" data-toggle="modal" data-target="#feedback-modal">Feedback</a></li>
            </ul>
          </div>
    
          <div class="col-xs-6">
            <ul class="list-unstyled">
              <li><a href="/contact">Contact</a></li>
              <li><a href="/terms">Terms of Service</a></li>
              <li><a href="/privacy">Privacy Policy</a></li>
            </ul>
          </div>
        </div>
      </div>
    </footer>
    
    <footer class="sponsor">
      <div class="container">
        <div class="row">
          <div class="col-sm-10 col-sm-offset-1">
            <p class="text-center">
              OpenReview is created by the <a href="http://www.iesl.cs.umass.edu/" target="_blank">Information Extraction and Synthesis Laboratory</a>, College of Information and Computer Science, University of Massachusetts Amherst. We gratefully acknowledge the support of the OpenReview sponsors:  Google,  Facebook, NSF, the University of Massachusetts Amherst Center for Data Science, and Center for Intelligent Information Retrieval, as well as the Google Cloud Platform for donating the computing and networking services on which OpenReview.net runs.
            </p>
          </div>
        </div>
      </div>
    </footer>

  <div id="feedback-modal" class="modal fade" tabindex="-1" role="dialog">
    <div class="modal-dialog">
      <div class="modal-content">
        <div class="modal-header">
          <button type="button" class="close" data-dismiss="modal" aria-label="Close"><span aria-hidden="true">Ã—</span></button>
          <h3 class="modal-title">Send Feedback</h3>
        </div>
        <div class="modal-body">
          <p>Enter your feedback below and we'll get back to you as soon as possible.</p>
          <form action="/feedback" method="POST">
            <div class="form-group">
              <input type="email" name="from" class="form-control" placeholder="Email" />
            </div>
            <div class="form-group">
              <input type="text" name="subject" class="form-control" placeholder="Subject" />
            </div>
            <div class="form-group">
              <textarea name="message" class="form-control feedback-input" rows="5" placeholder="Message" required=""></textarea>
            </div>
          <ul id="ui-id-2" tabindex="0" class="ui-menu ui-widget ui-widget-content" style="display: none;"></ul></form>
        </div>
        <div class="modal-footer">
          <button type="button" class="btn btn-default" data-dismiss="modal">Cancel</button>
          <button type="button" class="btn btn-primary">Send</button>
        </div>
      </div>
    </div>
  </div>

  <script type="text/javascript" async="" src="https://www.google-analytics.com/analytics.js"></script><script src="//cdnjs.cloudflare.com/ajax/libs/jquery/2.2.4/jquery.min.js"></script>
  <script>window.jQuery || document.write('&lt;script src="/static/js/vendor/jquery-2.2.4.min.js"&gt;&lt;\/script&gt;')</script>

  <script src="/static/dist/vendor.min.js"></script>
  <script src="/static/dist/templates.min.js"></script>
  <script src="/static/dist/openreview.min.js"></script>

    <script>window.legacyScripts = true;</script>


    <script async="" src="https://www.googletagmanager.com/gtag/js?id=UA-108703919-1"></script>
    <script>
      window.dataLayer = window.dataLayer || [];
      function gtag() { dataLayer.push(arguments); }
      gtag('js', new Date());
      gtag('config', 'UA-108703919-1');
    </script>


<div role="status" aria-live="assertive" aria-relevant="additions" class="ui-helper-hidden-accessible"></div></body></html>