<!DOCTYPE html><html xmlns="http://www.w3.org/1999/xhtml" class="no-js"><head>
  <meta charset="utf-8" />
  <meta http-equiv="X-UA-Compatible" content="IE=edge,chrome=1" />
  <meta name="viewport" content="width=device-width, initial-scale=1" />

  <title>Bayesian Modelling and Monte Carlo Inference for GAN | OpenReview</title>
  <meta name="description" content="" />

      <meta name="citation_title" content="Bayesian Modelling and Monte Carlo Inference for GAN" />
        <meta name="citation_author" content="Anonymous" />
      <meta name="citation_publication_date" content="2018/09/27" />
      <meta name="citation_online_date" content="2018/09/27" />
      <meta name="citation_pdf_url" content="https://openreview.net/pdf?id=H1l7bnR5Ym" />
      <meta name="twitter:card" content="summary" />
      <meta name="twitter:site" content="@openreviewnet" />
      <meta name="og:title" content="Bayesian Modelling and Monte Carlo Inference for GAN" />
      <meta name="og:description" content="Bayesian modelling is a principal framework to perform model aggregation, which has been a primary mechanism to combat mode collapsing in the context of Generative Adversarial Networks (GANs). In..." />
      <meta name="og:image" content="https://openreview.net/static/images/openreview_logo_512.png" />

  <link rel="stylesheet" href="https://fonts.googleapis.com/css?family=Noto+Sans:400,400i,700,700i" />

  <link rel="stylesheet" href="/static/css/bootstrap.min.css" />
  <link rel="stylesheet" href="/static/css/jquery-ui.min.css" />
  <link rel="stylesheet" href="/static/css/main.min.css" />
</head>

<body class="forum">
  <nav class="navbar navbar-inverse navbar-fixed-top" role="navigation">
    <div class="container">
  
      <div class="navbar-header">
        <button type="button" class="navbar-toggle collapsed" data-toggle="collapse" data-target="#navbar" aria-expanded="false" aria-controls="navbar">
          <span class="sr-only">Toggle navigation</span>
          <span class="icon-bar"></span>
          <span class="icon-bar"></span>
          <span class="icon-bar"></span>
        </button>
        <a class="navbar-brand home push-link" href="/"><strong>OpenReview</strong>.net</a>
      </div>
  
      <div id="navbar" class="navbar-collapse collapse">
        <form class="navbar-form navbar-left profile-search" role="search">
          <div class="form-group has-feedback">
            <input id="search_input" type="text" class="form-control ui-autocomplete-input" placeholder="Search ICLR 2019 Conference" autocomplete="off" />
            <span class="glyphicon glyphicon-search form-control-feedback" aria-hidden="true"></span>
          </div>
  
          <input id="search_group" type="hidden" value="ICLR.cc/2019/Conference" />
          <input id="search_content" type="hidden" value="all" />
        <ul id="ui-id-1" tabindex="0" class="ui-menu ui-widget ui-widget-content ui-autocomplete ui-front" style="display: none;"></ul></form>
  
        <ul class="nav navbar-nav navbar-right">
        
            <li id="user-menu"><a href="/login">Login</a></li>
        </ul>
      </div>
  
    </div>
  </nav>

  <div id="or-banner" class="banner" style="">
  <div class="container">
    <div class="row">
      <div class="col-xs-12"><a href="/group?id=ICLR.cc/2019/Conference" title="Venue Homepage"><img class="icon" src="/static/images/arrow_left.svg" /> Go to <strong>ICLR 2019 Conference</strong> homepage</a></div>
    </div>
  </div>
</div>
<div id="flash-message-container" class="alert alert-danger" role="alert" style="display: none;">
  <div class="container">
    <div class="row">
      <div class="col-xs-12">
        <div class="alert-content">
          <button type="button" class="close" aria-label="Close"><span aria-hidden="true">×</span></button>
        </div>
      </div>
    </div>
  </div>
</div>

<div class="container">
  <div class="row">
    <div class="col-xs-12">
      <main id="content" class="clearfix openbanner-visible legacy-styles"><div id="note_H1l7bnR5Ym" class="note panel"><div class="title_pdf_row clearfix"><h2 class="note_content_title"><a>Bayesian Modelling and Monte Carlo Inference for GAN</a> <a class="note_content_pdf" href="/pdf?id=H1l7bnR5Ym" title="Download PDF" target="_blank"><img src="/static/images/pdf_icon_blue.svg" /></a> </h2></div><div class="meta_row"><span class="signatures">Anonymous</span></div><div class="clearfix"><div class="meta_row pull-left"><span class="date item">28 Sep 2018 (modified: 19 Nov 2018)</span><span class="item">ICLR 2019 Conference Blind Submission</span><span class="item">Readers: <span class="readers-icon glyphicon glyphicon-globe"></span> Everyone</span><span class="item"><a class="action-bibtex-modal" data-bibtex="@inproceedings{    &#10;anonymous2019bayesian,    &#10;title={Bayesian Modelling and Monte Carlo Inference for GAN},    &#10;author={Anonymous},    &#10;booktitle={Submitted to International Conference on Learning Representations},    &#10;year={2019},    &#10;url={https://openreview.net/forum?id=H1l7bnR5Ym},    &#10;note={under review}    &#10;}">Show Bibtex</a></span><a class="note_content_pdf item" href="/revisions?id=H1l7bnR5Ym" target="_blank">Show Revisions</a></div><div class="meta_row meta_actions"></div></div><div class="note_contents"><span class="note_content_field">Abstract: </span><span class="note_content_value">Bayesian modelling is a principal framework to perform model aggregation, which has been a primary mechanism to combat mode collapsing in the context of Generative Adversarial Networks (GANs). In this paper, we propose a novel Bayesian modelling framework for GANs, which iteratively learns a distribution over generators with a carefully crafted prior.  Learning is efficiently triggered by a tailored stochastic gradient Hamiltonian Monte Carlo with novel gradient approximation to perform Bayesian inference. Our theoretical analysis further reveals that our treatment is the first Bayesian modelling framework that yields an equilibrium where generator distributions are faithful to the data distribution. Empirical evidence on synthetic high-dimensional multi-modal data and the natural image database CIFAR-10 demonstrates the superiority of our method over both start-of-the-art multi-generator GANs and other Bayesian treatment for GANs.</span></div><div class="note_contents"><span class="note_content_field">Keywords: </span><span class="note_content_value">Generative Adversarial Networks, Bayesian Deep Learning, Mode Collapse, Inception Score, Generator, Discriminator, CIFAR-10</span></div><div class="note_contents"><span class="note_content_field">TL;DR: </span><span class="note_content_value">A novel Bayesian treatment for GAN with theoretical guarantee.</span></div><div class="reply_row clearfix"><div class="item" id="reply_count">7 Replies</div></div></div><hr class="small" /><div id="note_children"><div class="note_with_children"><div id="note_B1lq-8vRa7" class="note panel"><div class="title_pdf_row clearfix"><h2 class="note_content_title"><a>Updating Inception and Frechet Inception Distance Results on CIFAR10. (Table 3 in the paper)</a>  <button class="btn btn-xs btn-default permalink-button" title="Link to this comment" data-permalink-url="https://openreview.net/forum?id=H1l7bnR5Ym&amp;noteId=B1lq-8vRa7"><span class="glyphicon glyphicon-link" aria-hidden="true"></span></button></h2></div><div class="meta_row"><span class="signatures">ICLR 2019 Conference Paper1159 Authors</span></div><div class="clearfix"><div class="meta_row pull-left"><span class="date item">18 Nov 2018</span><span class="item">ICLR 2019 Conference Paper1159 Official Comment</span><span class="item">Readers: <span class="readers-icon glyphicon glyphicon-globe"></span> Everyone</span></div><div class="meta_row meta_actions"></div></div><div class="note_contents"><span class="note_content_field">Comment: </span><span class="note_content_value">Previously, our FID results are computed using a PyTorch Implementation (<a href="https://github.com/mseitzer/pytorch-fid)." target="_blank" rel="nofollow">https://github.com/mseitzer/pytorch-fid).</a> Note that there exists a large discrepancy between the FID results conducted by PyTorch Inception model and Tensorflow model. Hence, to facilitate the comparison with previous paper, we decide to reevaluate by the official Tensorflow FID computation code (https://github.com/bioinf-jku/TTUR). 

Here are the updated results.

                       Inception scores (higher is better)
                  GAN-MM  &amp; GAN-NS &amp; WGAN &amp; LSGAN 
DCGAN    &amp;   6.53     &amp;        7.21 &amp;      7.19 &amp;      7.36 
MGAN      &amp;   7.19     &amp;        7.25 &amp;      7.18 &amp;      7.34
BGAN       &amp;   7.21    &amp;        7.37 &amp;       7.26 &amp;      7.46
ours-PSA  &amp;  7.75     &amp;       7.53  &amp;      7.28 &amp;      7.36

                             FIDs (lower is better)
                  GAN-MM  &amp; GAN-NS &amp; WGAN &amp; LSGAN 
DCGAN    &amp;      35.57 &amp;     27.68 &amp;    28.31 &amp;    29.11 
MGAN      &amp;      30.01 &amp;     27.55 &amp;    28.37 &amp;    30.72
BGAN       &amp;      29.87 &amp;     24.32 &amp;    29.87 &amp;    29.19
ours-PSA  &amp;     24.60  &amp;    23.55 &amp;     27.46 &amp;    26.90

Note that we are reporting the results with the highest ‘Inception score - 0.1 FID’ for each model. Thus the Inception scores results are also updated.
</span></div><div class="reply_row clearfix"></div></div><div class="children"></div></div><div class="note_with_children"><div id="note_Hkxok8co27" class="note panel"><div class="title_pdf_row clearfix"><h2 class="note_content_title"><a>experimental work not conclusive</a>  <button class="btn btn-xs btn-default permalink-button" title="Link to this comment" data-permalink-url="https://openreview.net/forum?id=H1l7bnR5Ym&amp;noteId=Hkxok8co27"><span class="glyphicon glyphicon-link" aria-hidden="true"></span></button></h2></div><div class="meta_row"><span class="signatures">ICLR 2019 Conference Paper1159 AnonReviewer3</span></div><div class="clearfix"><div class="meta_row pull-left"><span class="date item">04 Nov 2018 (modified: 07 Nov 2018)</span><span class="item">ICLR 2019 Conference Paper1159 Official Review</span><span class="item">Readers: <span class="readers-icon glyphicon glyphicon-globe"></span> Everyone</span></div><div class="meta_row meta_actions"></div></div><div class="note_contents"><span class="note_content_field">Review: </span><span class="note_content_value">This paper should be rejected based on the experimental work.
Experiments need to be reported for larger datasets.  Note the MGAN
paper reports results on STL-10 and ImageNet as well.

Note, your results on CIFAR-10 are quite different to those in the
MGAN paper.  Your inceptions scores are worse and FIDs are better!!  I
expect you have different configurations to their paper, but it would
be good for this to be explained.

I thought the related work section was fabulous, and as an extension
to BGAN, the paper is a very nice idea.  So I benefited a lot from reading
the paper.

I have some comments on Bayesian treatment.  In Bayesian theory, the
true distribution $p_{data}$ cannot appear in any evaluated formulas,
as you have it there in Eqn (1) which is subsequently used in your
likelihood Eqn (2).  Likelihoods are models and cannot involve "truth".

Lemma 1:  Very nice observation!!  I was trying to work that out,
once I got to Eqn (3), and you thought of it. 

Also, you do need to explain 3.2 better.  The BGAN paper, actually, is
a bit confusing from a strict Bayesian perspective, though for
different reasons.  The problem you are looking at is not a
time-series problem, so it is a bit confusing to be defining it as
such.  You talk about an iterative Bayesian model with priors and
likelihoods.  Well, maybe that can be *defined* as a probabilistic
model, but it is not in any sense a Bayesian model for the estimation
of $p_{model}$.

What you do with Equation (3) is define a distribution on
$q_g(\theta_g)$ and $q_d(\theta_d)$ (which, confusingly, involves the
"true" data distribution ... impossible for a Bayesian formulation).
You are doing a natural extension of the BGAN papers formulation in
their Eqs (1) and (2).  This, as is alluded to in Lemma 1.  Your
formulation is in terms of two conditional distributions, so
conditions should be given that their is an underlying joint
distribution that agrees with these.  Lemma 1 gives a negative result.
You have defined it as a time series problem, and apparantly one wants
this to converge, as in Gibbs sampling style.  Like BGAN, you have
just arbitrarily defined a "likelihood".

To me, this isn't a Bayesian model of the unsupervised learning task,
its a probabilistic style optimisation for it, in the sense that you are defining a probability
distribution (over $q_g(\theta_g)$ and $q_d(\theta_d)$) and sampling
from it, but its not really a "likelihood" in the formal sense.  A
likelhood defines how data is generated.  Your "likelihood" is over
model parameters, and you seem to have ignored the data likelihood,
which you define in sct 3.1 as $p_{model}()$.

Anyway, I'm happy to go with this sort of formulation, but I think you
need to call it what it is, and it is not Bayesian in the standard sense.  The theoretical
treatment needs a lot of cleaning up.  What you have defined is a
probabilistic time-series on $q_g(\theta_g)$ and $q_d(\theta_d)$.
Fair enough, thats OK.  But you need to show that it actually works in
the estimation of $p_{model}$.  Because one never has $p_{data}$, all
your Theorem 1 does is show that asymptotically, your method works.
Unfortunately, I can say the same for many crude algorithms, and most
of the existing published work.  Thus, we're left with requiring a
substantial empirical validation to demonstrate the method is useful.

Now my apologies to you: I could make somewhat related statements
about the theory of the BGAN paper, and they got to publish theirs at
ICLR!  But they did do more experimentation.

Oh, and some smaller but noticable grammar/word usage issues.</span></div><div class="note_contents"><span class="note_content_field">Rating: </span><span class="note_content_value">4: Ok but not good enough - rejection</span></div><div class="note_contents"><span class="note_content_field">Confidence: </span><span class="note_content_value">4: The reviewer is confident but not absolutely certain that the evaluation is correct</span></div><div class="reply_row clearfix"></div></div><div class="children"><div class="note_with_children"><div id="note_Hkgyn9ICTm" class="note panel"><div class="title_pdf_row clearfix"><h2 class="note_content_title"><a>Response to  AnonReviewer3</a>  <button class="btn btn-xs btn-default permalink-button" title="Link to this comment" data-permalink-url="https://openreview.net/forum?id=H1l7bnR5Ym&amp;noteId=Hkgyn9ICTm"><span class="glyphicon glyphicon-link" aria-hidden="true"></span></button></h2></div><div class="meta_row"><span class="signatures">ICLR 2019 Conference Paper1159 Authors</span></div><div class="clearfix"><div class="meta_row pull-left"><span class="date item">18 Nov 2018 (modified: 20 Nov 2018)</span><span class="item">ICLR 2019 Conference Paper1159 Official Comment</span><span class="item">Readers: <span class="readers-icon glyphicon glyphicon-globe"></span> Everyone</span></div><div class="meta_row meta_actions"></div></div><div class="note_contents"><span class="note_content_field">Comment: </span><span class="note_content_value">Dear AnonReviewer3,

Thank you for the insightful comments.
Following is our response to your concerns.

=== experiments ===

We will include results on STL-10 and ImageNet in the revision, or a later version if our machines cannot catch up the rebuttal deadline. Compared with Bayesian GAN, actually, we did a more thorough study on the choice of objective function, and our synthetic dataset is harder and more illustrative.

Here we clarify the discrepancy between our quantitative evaluation of MGAN and that of the original paper. We actually use the official open-sourced code of MGAN with the same configurations (model architectures, training data). The discrepancy comes from the inception model used to compute FID. We compute FID with PyTorch Inception model (<a href="https://github.com/mseitzer/pytorch-fid.)." target="_blank" rel="nofollow">https://github.com/mseitzer/pytorch-fid.).</a> The original MGAN paper did not say which inception model they have used. Our guess is that they used the Tensorflow inception model (https://github.com/bioinf-jku/TTUR). We observed FID computed by PyTorch model is much lower than that computed by the Tensorflow model, because of the different weights of the pre-trained models. A similar phenomenon has been recently observed for Inception Score [1]. To favor a more complete comparison, we will update our FID results by switching to the Tensorflow version.

We had posted the updated results in the comment. In our experiments, the MGAN with GAN-NS objective has the same setting with original MGAN. The Inception score and FID we get are 7.25 and 27.55 which are both worse than the scores reported in the original paper, 8.33 and 26.7. We train MGAN with the officially released code under the configuration reported in the MGAN paper (Table 4 in the appendix). The scores we reported is the best we can get via several training trials.

[1] Barratt, Shane, and Rishi Sharma. "A Note on the Inception Score." arXiv preprint arXiv:1801.01973 (2018).

=== Bayesian formulation ===

Our method has two separate Bayesian models, one for the generator and one for the discriminator. Take the Bayesian perspective for the generator as an example. The likelihood defined in the first equation of Eqn 2 gives the probability of observing some fixed discriminator distribution for some generator parameter, i.e., p(D^{(t)} | \theta_g). Composite with the prior of the generator parameter q^{(t)}(\theta_g), it is a Bayesian model from a strict perspective. Indeed, to see the correspondence of ‘model parameter’ and ‘data’  in classic Bayesian theory, our generator is the ‘model’ and the discriminator is the ‘data’. We estimate generator distribution by the observed discriminator distribution.

The novelty from classic Bayesian models is on the inference procedure. We integrate the two standard Bayesian models into a dynamical system: each Bayesian problem is solved alternatingly. From a game-theoretic point of view, each optimization problem is the best response strategy of the corresponding player, and the equilibrium presents a generator distribution that produces the target data distribution. 

=== Why time-series modelling ===

The problem is not a time-series problem. We simply solve it in an iterative manner. (akin to SGD that can iteratively solve both time-series and non-time-series problems). Our goal is to find the equilibrium of generator and discriminator distributions, where they satisfy each other’s posterior under our Bayesian criterion. It is, however, possible to find the equilibrium via an iterative scheme. We will make this part more clear in the revision.

=== A clarification about theorem 1 ===

It is indeed true that Theorem 1 only shows an analysis of the optimal solution in an asymptotic scenario. Unfortunately, it is, to our best knowledge, the best property that has been obtained in recent literature on GANs [2, 3, 4, 5, 6]. However, please note that Bayesian GAN does not even possess such asymptotic property and the difficulty of avoiding such problem as revealed by our analysis in Section 4.2. In contrast, our method is to the first Bayesian method to establish such property. 

[2] Goodfellow, Ian, et al. "Generative adversarial nets." Advances in neural information processing systems. (NIPS 2014)
[3] Hoang, Quan, et al. "MGAN: Training generative adversarial nets with multiple generators." (ICLR 2018)
[4] Arjovsky, Martin, Soumith Chintala, and Léon Bottou. "Wasserstein generative adversarial networks."(ICML 2017)
[5] Mao, Xudong, et al. "Least squares generative adversarial networks." Computer Vision (ICCV), 2017 IEEE International Conference on. IEEE, 2017.
[6] Zhao, Junbo, Michael Mathieu, and Yann LeCun. "Energy-based generative adversarial network." (ICLR 2017)
</span></div><div class="reply_row clearfix"></div></div><div class="children"></div></div></div></div><div class="note_with_children"><div id="note_ryxAXLZYhm" class="note panel"><div class="title_pdf_row clearfix"><h2 class="note_content_title"><a>Stripping the priors from Bayesian GANs</a>  <button class="btn btn-xs btn-default permalink-button" title="Link to this comment" data-permalink-url="https://openreview.net/forum?id=H1l7bnR5Ym&amp;noteId=ryxAXLZYhm"><span class="glyphicon glyphicon-link" aria-hidden="true"></span></button></h2></div><div class="meta_row"><span class="signatures">ICLR 2019 Conference Paper1159 AnonReviewer2</span></div><div class="clearfix"><div class="meta_row pull-left"><span class="date item">02 Nov 2018 (modified: 07 Nov 2018)</span><span class="item">ICLR 2019 Conference Paper1159 Official Review</span><span class="item">Readers: <span class="readers-icon glyphicon glyphicon-globe"></span> Everyone</span></div><div class="meta_row meta_actions"></div></div><div class="note_contents"><span class="note_content_field">Review: </span><span class="note_content_value">Summary
=========
The paper extends Bayesian GANs by altering the generator and discriminator parameter likelihood distributions and their respective priors. 
The authors further propose an SGHMC algorithm to collect samples of the resulting posterior distributions on each parameter set and evaluate their approach on both a synthetic and the CIFAR-10 data set. 
They claim superiority of their method, reporting a higher distance to mode centers of generated data points and better generator space coverage for the synthetic data set and better inception scores for the real world data for their method.

Review
=========
As an overall comment, I found the language poor, at times misleading.
The authors should have their manuscript proof-read for grammar and vocabulary.
Examples: 
- amazing superiority (page 1, 3rd paragraph)
- Accutally... (page 1, end of 3rd paragraph)
- the total mixture of generated data distribution (page 3, mid of 3.1)
- Similarity we define (page 3, end of 3.1)
- etc.
Over the whole manuscript, determiners are missing.

The authors start out with a general introduction to GANs and Bayesian GANs in particular, 
arguing that it is an open research question whether the generator converges to the true data generating distribution in Bayesian GANs.
I do not agree here. The Bayesian GAN defines a posterior distribution for the generator that
is proportional to the likelihood that the discriminator assigns to generated samples.
The better the generator, the higher the likelihood that the discriminator assign to these samples.
In the case of a perfect generator, here the discriminator is equally unable to distinguish real and generated samples and consequently degenerates to a constant function.
Using the same symmetry argument as the authors, one can show that this is the case for Bayesian GANs.

While defining the likelihood functions, the iterator variable t is used without introduction.

Further, I a confused by their argument of incompatibility.
First, they derive a Gibbs style update scheme based on single samples for generator and discriminator parameters using
posteriors in which the noise has been explicitly marginalized out by utilizing a Monte Carlo estimate.
Second, the used posteriors are conditional distributions with non-identical conditioning sets.
I doubt that the argument still holds under this setting.

With respect to the remaining difference between the proposed approach and Bayesian GAN,
I'd like the authors elaborate where exactly the difference between expectation of objective value
and objective value of expectation is.
Since the original GAN objectives used for crafting the likelihoods are deterministic functions,
randomness is introduced by the distributions over the generator and discriminator parameters.
I would have guessed that expectations propagate into the objective functions.

It is, however, interesting to analyze the proposed inference algorithm, especially the introduced posterior distributions.
For the discriminator, this correspond simply to the likelihood function.
For the generator, the likelihood is combined with some prior for which no closed form solution exists.
In fact, this prior changes between iterations of the inference algorithm.
The resulting gradient of the posterior decomposes into the gradient of the current objective and the sum over all previous gradients.
While this is not a prior in the Bayesian sense (i.e. in the sense of an actual prior belief), it would be interesting to have a closer look at the effect this has on the sampling method.
My educated guess is, that this conceptually adds up to the momentum term in SGHMC and thus slows down the exploration of the parameter space and results in better coverage.

The experiments are inspired by the ones done in the original Bayesian GAN publication.
I liked the developed method to measure coverage of the generator space although I find the
term of hit error misleading.
Given that the probabilistic methods all achieve a hit rate of 1, a lower hit error actually points to worse coverage.
I was surprised to see that hit error and coverage are only not consistently negatively correlated.
Adding statistics over several runs of the models (e.g. 10) would strengthen the claim of superior performance.</span></div><div class="note_contents"><span class="note_content_field">Rating: </span><span class="note_content_value">4: Ok but not good enough - rejection</span></div><div class="note_contents"><span class="note_content_field">Confidence: </span><span class="note_content_value">4: The reviewer is confident but not absolutely certain that the evaluation is correct</span></div><div class="reply_row clearfix"></div></div><div class="children"><div class="note_with_children"><div id="note_B1g1aeEapX" class="note panel"><div class="title_pdf_row clearfix"><h2 class="note_content_title"><a>Response to  AnonReviewer2</a>  <button class="btn btn-xs btn-default permalink-button" title="Link to this comment" data-permalink-url="https://openreview.net/forum?id=H1l7bnR5Ym&amp;noteId=B1g1aeEapX"><span class="glyphicon glyphicon-link" aria-hidden="true"></span></button></h2></div><div class="meta_row"><span class="signatures">ICLR 2019 Conference Paper1159 Authors</span></div><div class="clearfix"><div class="meta_row pull-left"><span class="date item">17 Nov 2018 (modified: 18 Nov 2018)</span><span class="item">ICLR 2019 Conference Paper1159 Official Comment</span><span class="item">Readers: <span class="readers-icon glyphicon glyphicon-globe"></span> Everyone</span></div><div class="meta_row meta_actions"></div></div><div class="note_contents"><span class="note_content_field">Comment: </span><span class="note_content_value">
Dear AnonReviewer2,

Thank you for the feedback. 
Following is our response to your concerns.

=== convergence of Bayesian GAN ===

The convergence of Bayesian GAN is indeed a problem, which is one of our key contributions. Bayesian GAN has a subtle difference from the original GANs during learning. To compute the posterior, Bayesian GAN cannot be learned by vanilla gradient descent methods, but is learned by SGHMC. In SGHMC framework, the gradient is always adulterated by white noises. Thus if the gradient from discriminator is always zero, the generator distribution will converge to a Gaussian distribution instead of staying unchanged.

In contrast, we fix this issue by a well-crafted prior for the generator distribution. Intuitively, the gradient from the prior helps combat with the noise and prevent degeneracy of the generator distribution towards a Gaussian distribution. Please note theorem 1 does not hold without introducing suitable prior for the generator.


=== expectation of objective value v.s. objective value of expectation ===

This difference is another very critical improvement from Bayesian GAN. We will make it more clear in the revision of the paper.

As shown in Eqn 8, to compute likelihood, Bayesian GAN takes expectation after computing the GAN objective value. While as shown in Eqn 2, we compute GAN objective value after the expectation. The subtle adjustment is crucial. Theorem 1 will not hold if the likelihood is defined as the expectation of loss value as Bayesian GAN did. Intuitively, because the expectation \E_{q_g} p_{gen}(x;\theta_g)) is equivalent to the data distribution p_model(x) produced by the generator distribution, it makes sense to compute GAN objective over it instead of the reversed order (in Bayesian GAN). Besides, it’s easy to see the gradients of the two different likelihoods is different since, for a given function f, the gradient of \sum_i f(x_i) is usually different from that of f(\sum_i x_i).

=== clarification on incompatibility ===

The incompatibility corresponds to the incompatibility between two conditional distributions that can not belong to the same joint distribution. We identify a theoretical flaw of Bayesian GAN under a very simple setting (when only using single Monte-Carlo sample) that leads to incompatible conditionals of generator and discriminator. Moreover, we are not very certain about the concern “the used posteriors are conditional distributions with non-identical conditioning sets. I doubt that the argument still holds under this setting.” Further explanation about “non-identical conditioning sets” will be appreciated.

=== relationship between hit error and coverage ===

By our definition, ‘hit error’ is the averaged distance between the generated data points and the low dimensional linear space that the ground truth mode lies in. While the ‘cover error’ measures the similarity between the distribution of projected data points and the ground truth data distribution which is uniform.

The two metrics are actually orthogonal to each other, due to the fundamental difference between projection distances (‘hit error’) and how the projections are distributed (’cover error‘). It’s possible to get the same projection distances in a scattered or dense way. It’s also possible to get the same projections from different projection distances. We will change the terminology ‘hit error’ to ‘hit distance’ to make it more clear in our revision.

=== further analyze of our inference algorithm ===

The momentum explanation seems an interesting direction to yield a formal explanation of such approximations, but we do not have a concrete analysis yet and leave it as future work. 
</span></div><div class="reply_row clearfix"></div></div><div class="children"></div></div></div></div><div class="note_with_children"><div id="note_SJxci3qu3m" class="note panel"><div class="title_pdf_row clearfix"><h2 class="note_content_title"><a>A Bayesian GAN with where data distribution is an equilibrium</a>  <button class="btn btn-xs btn-default permalink-button" title="Link to this comment" data-permalink-url="https://openreview.net/forum?id=H1l7bnR5Ym&amp;noteId=SJxci3qu3m"><span class="glyphicon glyphicon-link" aria-hidden="true"></span></button></h2></div><div class="meta_row"><span class="signatures">ICLR 2019 Conference Paper1159 AnonReviewer1</span></div><div class="clearfix"><div class="meta_row pull-left"><span class="date item">01 Nov 2018 (modified: 07 Nov 2018)</span><span class="item">ICLR 2019 Conference Paper1159 Official Review</span><span class="item">Readers: <span class="readers-icon glyphicon glyphicon-globe"></span> Everyone</span></div><div class="meta_row meta_actions"></div></div><div class="note_contents"><span class="note_content_field">Review: </span><span class="note_content_value">Mode collapse in the context of GANs occurs when the generator only learns one of the 
multiple modes of the target distribution. Mode collapsed can be tackled, for instance, using Wasserstein distance instead of Jensen-Shannon divergence. However, this sacrifices accuracy of the generated samples.

This paper is positioned in the context of Bayesian GANs (Saatsci &amp; Wilson 2017) which, by placing a posterior distribution over the generative and discriminative parameters, can potentially learn all the modes. In particular, the paper proposes a Bayesian GAN that, unlike previous Bayesian GANs, has theoretical guarantees of convergence to the real distribution.

The authors put likelihoods over the generator and discriminator with logarithms proportional to the traditional GAN objective functions. Then they choose a prior in the generative parameters which is the output of the last iteration. The prior over the discriminative parameters is a uniform improper prior (constant from minus to plus infinity). Under this specifications, they demonstrate that the true data distribution is an equilibrium under this scheme. 

For the inference, they adapt the Stochastic Gradient HMC used by Saatsci &amp; Wilson. To approximate the gradient of the discriminator, they take samples of the generator parameters. To approximate the gradient of the generator they take samples of the discriminator parameters but they also need to compute a gradient of the previous generator distribution. However, because this generator distribution is not available in close form they propose two simple approximations.

Overall, I enjoyed reading this paper. It is well written and easy to follow. The motivation is clear, and the contribution is significant. The experiments are convincing enough, comparing their method with Saatsci's Bayesian GAN and with the state of the art of GAN that deals with mode collapse. It seems an interesting improvement over the original Bayesian GAN with theoretical guarantees and an easy implementation.

Some typos:

- The authors argue that compare to point mass...
+ The authors argue that, compared to point mass...

- Theorem 1 states that any the ideal generator
+ Theorem 1 states that any ideal generator

- Assume the GAN objective and the discriminator space are symmetry
+ Assume the GAN objective and the discriminator space have symmetry

- Eqn. 8 will degenerated as a Gibbs sampling
+ Eqn. 8 will degenerate as a Gibbs sampling</span></div><div class="note_contents"><span class="note_content_field">Rating: </span><span class="note_content_value">9: Top 15% of accepted papers, strong accept</span></div><div class="note_contents"><span class="note_content_field">Confidence: </span><span class="note_content_value">4: The reviewer is confident but not absolutely certain that the evaluation is correct</span></div><div class="reply_row clearfix"></div></div><div class="children"><div class="note_with_children"><div id="note_Byg6_RX6Tm" class="note panel"><div class="title_pdf_row clearfix"><h2 class="note_content_title"><a>Response to  AnonReviewer1</a>  <button class="btn btn-xs btn-default permalink-button" title="Link to this comment" data-permalink-url="https://openreview.net/forum?id=H1l7bnR5Ym&amp;noteId=Byg6_RX6Tm"><span class="glyphicon glyphicon-link" aria-hidden="true"></span></button></h2></div><div class="meta_row"><span class="signatures">ICLR 2019 Conference Paper1159 Authors</span></div><div class="clearfix"><div class="meta_row pull-left"><span class="date item">17 Nov 2018</span><span class="item">ICLR 2019 Conference Paper1159 Official Comment</span><span class="item">Readers: <span class="readers-icon glyphicon glyphicon-globe"></span> Everyone</span></div><div class="meta_row meta_actions"></div></div><div class="note_contents"><span class="note_content_field">Comment: </span><span class="note_content_value">Dear AnonReviewer1,

Thank you for agreeing with the significance of our contribution and voting to accept our paper. We will address the typos.

We make an additional remark here, which might be interesting. Bayesian modeling has been introduced in several mini-max problems in the deep learning community, such as adversarial (robust) learning [1] and GANs. However, most prior works pose Bayesian method as a heuristic without theoretical analysis. This work presents an important initial step toward a rigorous study of modernized Bayesian approaches. 

[1] Nanyang Ye, Zhanxing Zhu. Bayesian Adversarial Learning. 32nd Annual Conference on Neural Information Processing Systems (NIPS 2018)
</span></div><div class="reply_row clearfix"></div></div><div class="children"></div></div></div></div></div></main></div>
  </div>
</div>


    <!-- Footer -->
    <footer class="sitemap">
      <div class="container">
        <div class="row hidden-xs">
          <div class="col-sm-4">
            <ul class="list-unstyled">
              <li><a href="/" class="home">Home</a></li>
              <li><a href="/venues">All Venues</a></li>
            </ul>
          </div>
    
          <div class="col-sm-4">
            <ul class="list-unstyled">
              <li><a href="/about">About OpenReview</a></li>
              <li><a href="/contact">Contact</a></li>
              <li><a href="#" data-toggle="modal" data-target="#feedback-modal">Feedback</a></li>
            </ul>
          </div>
    
          <div class="col-sm-4">
            <ul class="list-unstyled">
              <li><a href="/terms">Terms of Service</a></li>
              <li><a href="/privacy">Privacy Policy</a></li>
            </ul>
          </div>
        </div>
    
        <div class="row visible-xs-block">
          <div class="col-xs-6">
            <ul class="list-unstyled">
              <li><a href="/" class="home">Home</a></li>
              <li><a href="/about">About OpenReview</a></li>
              <li><a href="#" data-toggle="modal" data-target="#feedback-modal">Feedback</a></li>
            </ul>
          </div>
    
          <div class="col-xs-6">
            <ul class="list-unstyled">
              <li><a href="/contact">Contact</a></li>
              <li><a href="/terms">Terms of Service</a></li>
              <li><a href="/privacy">Privacy Policy</a></li>
            </ul>
          </div>
        </div>
      </div>
    </footer>
    
    <footer class="sponsor">
      <div class="container">
        <div class="row">
          <div class="col-sm-10 col-sm-offset-1">
            <p class="text-center">
              OpenReview is created by the <a href="http://www.iesl.cs.umass.edu/" target="_blank">Information Extraction and Synthesis Laboratory</a>, College of Information and Computer Science, University of Massachusetts Amherst. We gratefully acknowledge the support of the OpenReview sponsors:  Google,  Facebook, NSF, the University of Massachusetts Amherst Center for Data Science, and Center for Intelligent Information Retrieval, as well as the Google Cloud Platform for donating the computing and networking services on which OpenReview.net runs.
            </p>
          </div>
        </div>
      </div>
    </footer>

  <div id="feedback-modal" class="modal fade" tabindex="-1" role="dialog">
    <div class="modal-dialog">
      <div class="modal-content">
        <div class="modal-header">
          <button type="button" class="close" data-dismiss="modal" aria-label="Close"><span aria-hidden="true">×</span></button>
          <h3 class="modal-title">Send Feedback</h3>
        </div>
        <div class="modal-body">
          <p>Enter your feedback below and we'll get back to you as soon as possible.</p>
          <form action="/feedback" method="POST">
            <div class="form-group">
              <input type="email" name="from" class="form-control" placeholder="Email" />
            </div>
            <div class="form-group">
              <input type="text" name="subject" class="form-control" placeholder="Subject" />
            </div>
            <div class="form-group">
              <textarea name="message" class="form-control feedback-input" rows="5" placeholder="Message" required=""></textarea>
            </div>
          <ul id="ui-id-2" tabindex="0" class="ui-menu ui-widget ui-widget-content" style="display: none;"></ul></form>
        </div>
        <div class="modal-footer">
          <button type="button" class="btn btn-default" data-dismiss="modal">Cancel</button>
          <button type="button" class="btn btn-primary">Send</button>
        </div>
      </div>
    </div>
  </div>

  <script type="text/javascript" async="" src="https://www.google-analytics.com/analytics.js"></script><script src="//cdnjs.cloudflare.com/ajax/libs/jquery/2.2.4/jquery.min.js"></script>
  <script>window.jQuery || document.write('&lt;script src="/static/js/vendor/jquery-2.2.4.min.js"&gt;&lt;\/script&gt;')</script>

  <script src="/static/dist/vendor.min.js"></script>
  <script src="/static/dist/templates.min.js"></script>
  <script src="/static/dist/openreview.min.js"></script>

    <script>window.legacyScripts = true;</script>


    <script async="" src="https://www.googletagmanager.com/gtag/js?id=UA-108703919-1"></script>
    <script>
      window.dataLayer = window.dataLayer || [];
      function gtag() { dataLayer.push(arguments); }
      gtag('js', new Date());
      gtag('config', 'UA-108703919-1');
    </script>


<div role="status" aria-live="assertive" aria-relevant="additions" class="ui-helper-hidden-accessible"></div></body></html>