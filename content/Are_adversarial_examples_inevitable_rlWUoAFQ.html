<!DOCTYPE html><html xmlns="http://www.w3.org/1999/xhtml" class="no-js"><head>
  <meta charset="utf-8" />
  <meta http-equiv="X-UA-Compatible" content="IE=edge,chrome=1" />
  <meta name="viewport" content="width=device-width, initial-scale=1" />

  <title>Are adversarial examples inevitable? | OpenReview</title>
  <meta name="description" content="" />

      <meta name="citation_title" content="Are adversarial examples inevitable?" />
        <meta name="citation_author" content="Anonymous" />
      <meta name="citation_publication_date" content="2018/09/27" />
      <meta name="citation_online_date" content="2018/09/27" />
      <meta name="citation_pdf_url" content="https://openreview.net/pdf?id=r1lWUoA9FQ" />
      <meta name="twitter:card" content="summary" />
      <meta name="twitter:site" content="@openreviewnet" />
      <meta name="og:title" content="Are adversarial examples inevitable?" />
      <meta name="og:description" content="A wide range of defenses have been proposed to harden neural networks against adversarial attacks. However, a pattern has emerged in which the majority of adversarial defenses are quickly broken by..." />
      <meta name="og:image" content="https://openreview.net/static/images/openreview_logo_512.png" />

  <link rel="stylesheet" href="https://fonts.googleapis.com/css?family=Noto+Sans:400,400i,700,700i" />

  <link rel="stylesheet" href="/static/css/bootstrap.min.css" />
  <link rel="stylesheet" href="/static/css/jquery-ui.min.css" />
  <link rel="stylesheet" href="/static/css/main.min.css" />
</head>

<body class="forum">
  <nav class="navbar navbar-inverse navbar-fixed-top" role="navigation">
    <div class="container">
  
      <div class="navbar-header">
        <button type="button" class="navbar-toggle collapsed" data-toggle="collapse" data-target="#navbar" aria-expanded="false" aria-controls="navbar">
          <span class="sr-only">Toggle navigation</span>
          <span class="icon-bar"></span>
          <span class="icon-bar"></span>
          <span class="icon-bar"></span>
        </button>
        <a class="navbar-brand home push-link" href="/"><strong>OpenReview</strong>.net</a>
      </div>
  
      <div id="navbar" class="navbar-collapse collapse">
        <form class="navbar-form navbar-left profile-search" role="search">
          <div class="form-group has-feedback">
            <input id="search_input" type="text" class="form-control ui-autocomplete-input" placeholder="Search ICLR 2019 Conference" autocomplete="off" />
            <span class="glyphicon glyphicon-search form-control-feedback" aria-hidden="true"></span>
          </div>
  
          <input id="search_group" type="hidden" value="ICLR.cc/2019/Conference" />
          <input id="search_content" type="hidden" value="all" />
        <ul id="ui-id-1" tabindex="0" class="ui-menu ui-widget ui-widget-content ui-autocomplete ui-front" style="display: none;"></ul></form>
  
        <ul class="nav navbar-nav navbar-right">
        
            <li id="user-menu"><a href="/login">Login</a></li>
        </ul>
      </div>
  
    </div>
  </nav>

  <div id="or-banner" class="banner" style="">
  <div class="container">
    <div class="row">
      <div class="col-xs-12"><a href="/group?id=ICLR.cc/2019/Conference" title="Venue Homepage"><img class="icon" src="/static/images/arrow_left.svg" /> Go to <strong>ICLR 2019 Conference</strong> homepage</a></div>
    </div>
  </div>
</div>
<div id="flash-message-container" class="alert alert-danger" role="alert" style="display: none;">
  <div class="container">
    <div class="row">
      <div class="col-xs-12">
        <div class="alert-content">
          <button type="button" class="close" aria-label="Close"><span aria-hidden="true">×</span></button>
        </div>
      </div>
    </div>
  </div>
</div>

<div class="container">
  <div class="row">
    <div class="col-xs-12">
      <main id="content" class="clearfix openbanner-visible legacy-styles"><div id="note_r1lWUoA9FQ" class="note panel"><div class="title_pdf_row clearfix"><h2 class="note_content_title"><a>Are adversarial examples inevitable?</a> <a class="note_content_pdf" href="/pdf?id=r1lWUoA9FQ" title="Download PDF" target="_blank"><img src="/static/images/pdf_icon_blue.svg" /></a> </h2></div><div class="meta_row"><span class="signatures">Anonymous</span></div><div class="clearfix"><div class="meta_row pull-left"><span class="date item">28 Sep 2018 (modified: 11 Oct 2018)</span><span class="item">ICLR 2019 Conference Blind Submission</span><span class="item">Readers: <span class="readers-icon glyphicon glyphicon-globe"></span> Everyone</span><span class="item"><a class="action-bibtex-modal" data-bibtex="@inproceedings{    &#10;anonymous2019are,    &#10;title={Are adversarial examples inevitable?},    &#10;author={Anonymous},    &#10;booktitle={Submitted to International Conference on Learning Representations},    &#10;year={2019},    &#10;url={https://openreview.net/forum?id=r1lWUoA9FQ},    &#10;note={under review}    &#10;}">Show Bibtex</a></span></div><div class="meta_row meta_actions"></div></div><div class="note_contents"><span class="note_content_field">Abstract: </span><span class="note_content_value">A wide range of defenses have been proposed to harden neural networks against adversarial attacks. However, a pattern has emerged in which the majority of adversarial defenses are quickly broken by new attacks.  Given the lack of success at generating robust defenses, we are led to ask a fundamental question:  Are adversarial attacks inevitable?
This paper analyzes adversarial examples from a theoretical perspective, and identifies fundamental bounds on the susceptibility of a classifier to adversarial attacks.   We show that, for certain classes of problems, adversarial examples are inescapable.  Using experiments, we explore the implications of theoretical guarantees for real-world problems and discuss how factors such as dimensionality and image complexity limit a classifier's robustness against adversarial examples.

</span></div><div class="note_contents"><span class="note_content_field">Keywords: </span><span class="note_content_value">adversarial examples, neural networks, security</span></div><div class="note_contents"><span class="note_content_field">TL;DR: </span><span class="note_content_value">This paper identifies classes of problems for which adversarial examples are inescapable, and derives fundamental bounds on the susceptibility of any classifier to adversarial examples. </span></div><div class="reply_row clearfix"><div class="item" id="reply_count">9 Replies</div></div></div><hr class="small" /><div id="note_children"><div class="note_with_children"><div id="note_S1gW94klpX" class="note panel"><div class="title_pdf_row clearfix"><h2 class="note_content_title"><a>good insight on understanding adversarial examples</a>  <button class="btn btn-xs btn-default permalink-button" title="Link to this comment" data-permalink-url="https://openreview.net/forum?id=r1lWUoA9FQ&amp;noteId=S1gW94klpX"><span class="glyphicon glyphicon-link" aria-hidden="true"></span></button></h2></div><div class="meta_row"><span class="signatures">ICLR 2019 Conference Paper150 AnonReviewer2</span></div><div class="clearfix"><div class="meta_row pull-left"><span class="date item">07 Nov 2018</span><span class="item">ICLR 2019 Conference Paper150 Official Review</span><span class="item">Readers: <span class="readers-icon glyphicon glyphicon-globe"></span> Everyone</span></div><div class="meta_row meta_actions"></div></div><div class="note_contents"><span class="note_content_field">Review: </span><span class="note_content_value">This paper uses several lemmas in geometry to prove that adversarial examples
are hard to avoid under the assumption that there is no "don't know" class and
the distribution of each class is not too concentrated. The paper first starts
with a simple case where the data points are distributed on a sphere, and then
extends the results to the realistic case where data points are inside a cube
[0,1]^n. 

The paper uses epsilon expansion of a set as a mathematical tool, and borrows
some important lemmas from geometry to the case of adversarial learning.  In
the sphere case, the results come from a fact that high dimensional
half-spheres can almost cover all points in the sphere after an epsilon
expansion, and the results depend on dimension n. For the unit cube case, the
authors borrow a result from Talagrand, to show that the epsilon expansion of a
set can cover a large portion of the cube as long as the set distribution is
not very concentrated.  In this case, the results (for l_2 norm) do not depend
on dimension n.

Experimentally, the authors show that inputs with higher dimension can actually
get better robustness, aligning with the provided analysis.  The primary reason
that current adversarial defense does not work well on CIFAR is due to the fact
that dataset is more spread out in high dimensional space. This is a good
insight for understanding adversarial examples.

The paper is overall well written and easy to follow. The interpretation of
each lemma and proposition is clear. Although the paper mostly depend on
well-known results in geometry and the ideas used are simple, it does provide
good insight on explaining the prevalence of adversarial examples. I recommend
to accept this paper.

Question:
Is there any good method to estimate U_c for a dataset? Although it is intuitive
that CIFAR may have a smaller U_c than MNIST, is it possible to numerically
estimate this quantity? This is necessary to fully support the conclusions made
in experiments.
</span></div><div class="note_contents"><span class="note_content_field">Rating: </span><span class="note_content_value">7: Good paper, accept</span></div><div class="note_contents"><span class="note_content_field">Confidence: </span><span class="note_content_value">4: The reviewer is confident but not absolutely certain that the evaluation is correct</span></div><div class="reply_row clearfix"></div></div><div class="children"></div></div><div class="note_with_children"><div id="note_SJgNbBz03Q" class="note panel"><div class="title_pdf_row clearfix"><h2 class="note_content_title"><a>interesting maths; implications less clear</a>  <button class="btn btn-xs btn-default permalink-button" title="Link to this comment" data-permalink-url="https://openreview.net/forum?id=r1lWUoA9FQ&amp;noteId=SJgNbBz03Q"><span class="glyphicon glyphicon-link" aria-hidden="true"></span></button></h2></div><div class="meta_row"><span class="signatures">ICLR 2019 Conference Paper150 AnonReviewer3</span></div><div class="clearfix"><div class="meta_row pull-left"><span class="date item">06 Nov 2018 (modified: 07 Nov 2018)</span><span class="item">ICLR 2019 Conference Paper150 Official Review</span><span class="item">Readers: <span class="readers-icon glyphicon glyphicon-globe"></span> Everyone</span></div><div class="meta_row meta_actions"></div></div><div class="note_contents"><span class="note_content_field">Review: </span><span class="note_content_value">The paper considers the problem of adversarial examples in (mostly high-dimensional) multi-class classification problems. Although the results are not specific necessarily to very high dimensional data or two images, the paper mostly uses images as a running example, and so will I in the review. 

Assume that the data all lies in the unit box in R^n ([0, 1]^n). A multiclass classifier with K classes partitions the unit cube into K parts, each part corresponding to a given class. There are distributions \rho_c associated with each class and there is a bound on their density given by U_c and the fraction of examples of class c is f_c. And (eps, p) adversarial point y for some point x is such that |x - y|_p &lt;= \eps and the classifier classifies x &amp; y differently. 

The paper shows that under this modeling assumption adversarial examples are inevitable. The results mostly use standard (but deep) results from probability theory. The technical proofs themselves are not particular difficult (provided one has the right background). I think the overall implications are interesting, and I will recommend the paper be accepted. 

However, I also feel that this is a missed opportunity. To some extent the authors do try to have some high-level discussion about adversarial examples, but I think this could be expanded on more. For instance, why should it be assumed that an example that is \eps far should automatically have the same class label? Surely, being "eps"-far away is an equivalence relation, thus this would mean that all the hypercube would have to be labeled by the same class. This is clearly not the case. One plausible explanation is that if you take two points that are in two different classes, then any sequence of points that take one to the other with the property that each adjacent pair is at most \eps far away, must have the property that some intermediate mass have negligible chance of being a "natural" image. 

On the other hand, doesn't the fact that humans are not susceptible to most adversarial examples, imply that adversarial-example resistant classifiers exist? My own feeling is the assumption that U_c is bounded is the strongest assumption that may not hold true with real data. In any case, the paper has enough technical content to merit acceptance and I hope the open review forum will lead to a fruitful discussion about some of these questions.

--

Minor comments:
Page 6 (just after Thm 2). Isn't the bound in Eqn. (5) true for all \ell_p norms for p \geq 2? (not just \ell_2 as the sentence says)
Paras on Page 6 (just below Thm 2). It would be more pleasant if equation x could be replaced by Eq. (x) or Equation (x). 
Para in Sec 7 on Unbounded density: Clarify what norm you mean when you talk about \eps/2 perturbations.
Thm 5: Seems odd to have a theorem about MNIST. Surely the result is a lot more general!!!</span></div><div class="note_contents"><span class="note_content_field">Rating: </span><span class="note_content_value">8: Top 50% of accepted papers, clear accept</span></div><div class="note_contents"><span class="note_content_field">Confidence: </span><span class="note_content_value">4: The reviewer is confident but not absolutely certain that the evaluation is correct</span></div><div class="reply_row clearfix"></div></div><div class="children"></div></div><div class="note_with_children"><div id="note_SJgOUrbC2m" class="note panel"><div class="title_pdf_row clearfix"><h2 class="note_content_title"><a>Excellent paper; but the "uniformity-over-dimension"-type of assumption should be more highlighted</a>  <button class="btn btn-xs btn-default permalink-button" title="Link to this comment" data-permalink-url="https://openreview.net/forum?id=r1lWUoA9FQ&amp;noteId=SJgOUrbC2m"><span class="glyphicon glyphicon-link" aria-hidden="true"></span></button></h2></div><div class="meta_row"><span class="signatures">(anonymous)</span></div><div class="clearfix"><div class="meta_row pull-left"><span class="date item">06 Nov 2018</span><span class="item">ICLR 2019 Conference Paper150 Public Comment</span><span class="item">Readers: <span class="readers-icon glyphicon glyphicon-globe"></span> Everyone</span></div><div class="meta_row meta_actions"></div></div><div class="note_contents"><span class="note_content_field">Comment: </span><span class="note_content_value">Thank you very much for this very interesting paper.
I strongly suggest accepting this paper (but I did not check the proofs).

Two remarks though:

1/ I think your paper (especially abstract and conclusion) should insist on the fact that your results necessarily imply adversarial examples (with high probability) only when the input distributions satisfy a kind of "uniformity-over-dimensions" assumption (which is captured by the fact that their density functions must be bounded, and this bound should not increase to quickly with the input dimension; see your very good discussions on b-MNIST). These uniformity-like implications of your assumptions should be clear to every reader, even if he only skims through the paper; especially since they are probably quite implausible for high-dimensional image data.
Btw: your results are very much in line (though more general) with those of [1], which should be cited: you both study adversarial examples when the input distributions are subject to some kind of "uniformity-over-dimensions" assumption.

2/ In the analysis of eq. (4), I suggest a short discussion (maybe instead of or after the paragraph following Theo 5; or appendix) on what happens when pasting in eq. (3)* of the paper [2], also submitted to this conference. This eq. (3) suggests to scale the epsilon attack-threshold in p-norm as d^{1/p}. (Note that for p=2, you get the same rate \sqrt{d} than in your own discussions after Theo 5.) When pasted into your eq. (4), the fraction's numerator reduces to exp(- \pi n) (when p \leq 2). It doesn't solve the case p = 0 (because of the denominator), but at least it shows that your discussions on the special case p=2 actually hold similarly for all 2 &gt;= p &gt; 0. That suggests that equivalent results probably hold also for the case p=0.
By the way, the results of [2] neatly explain your Figure 4a. More generally, it provides an alternative (or complementary) explanation of adversarial vulnerability to yours: rather than accusing the input distributions, it accuses the classifiers themselves by showing that, independently of the input distribution, our neural network "priors" (as implied by the network architecture and weight distribution at initialisation) yield too large gradients. It might be worthwhile to contrast both approaches/explanations of adversarial vulnerability in your paper (which incidentally brings us back to point 1/ ).

*sorry, in a first version I wrote eq. (5), but it's eq. (3) I meant
[1] Adversarial Spheres, Gilmer et al.
[2] Adversarial vulnerability of neural networks increases with input dimension, submitted to ICLR 2019</span></div><div class="reply_row clearfix"></div></div><div class="children"><div class="note_with_children"><div id="note_BJgodUZR37" class="note panel"><div class="title_pdf_row clearfix"><h2 class="note_content_title"><a>Biasing the reviewers</a>  <button class="btn btn-xs btn-default permalink-button" title="Link to this comment" data-permalink-url="https://openreview.net/forum?id=r1lWUoA9FQ&amp;noteId=BJgodUZR37"><span class="glyphicon glyphicon-link" aria-hidden="true"></span></button></h2></div><div class="meta_row"><span class="signatures">(anonymous)</span></div><div class="clearfix"><div class="meta_row pull-left"><span class="date item">06 Nov 2018</span><span class="item">ICLR 2019 Conference Paper150 Public Comment</span><span class="item">Readers: <span class="readers-icon glyphicon glyphicon-globe"></span> Everyone</span></div><div class="meta_row meta_actions"></div></div><div class="note_contents"><span class="note_content_field">Comment: </span><span class="note_content_value">"I strongly suggest accepting this paper (but I did not check the proofs)."  from an anonymous poster definitely isn't helping the review process. That is a subjective opinion and this is not a social media forum. Maybe the ACs should discourage people from posting their personal opinions on accept/reject decisions here! </span></div><div class="reply_row clearfix"></div></div><div class="children"><div class="note_with_children"><div id="note_r1ghWAnAhX" class="note panel"><div class="title_pdf_row clearfix"><h2 class="note_content_title"><a>Maybe, but not sure. + Justification of my grading.</a>  <button class="btn btn-xs btn-default permalink-button" title="Link to this comment" data-permalink-url="https://openreview.net/forum?id=r1lWUoA9FQ&amp;noteId=r1ghWAnAhX"><span class="glyphicon glyphicon-link" aria-hidden="true"></span></button></h2></div><div class="meta_row"><span class="signatures">(anonymous)</span></div><div class="clearfix"><div class="meta_row pull-left"><span class="date item">06 Nov 2018</span><span class="item">ICLR 2019 Conference Paper150 Public Comment</span><span class="item">Readers: <span class="readers-icon glyphicon glyphicon-globe"></span> Everyone</span></div><div class="meta_row meta_actions"></div></div><div class="note_contents"><span class="note_content_field">Comment: </span><span class="note_content_value">The goal of my comment was more about the two remarks than about the first two sentences.
But as I obviously read the paper in quite some detail (otherwise, how could I have made these two remarks?), and as I happen to know the field quite well myself, I thought I could as well give my opinion. Of course I want to bias the reviews, precisely because I think that the paper is very good. If everyone who seriously read the paper (those are  usually at least interested in the field, which cannot always be said of assigned reviewers) did the same and left a small appreciation, we would probably get a better picture of the value of the paper to the community, than through a very few neatly written but nevertheless very noisy complete reviews. (Of course, that assumes some code of good conduct, e.g. not posting about your friends; could be enforced by authorising only comments from accounts linked to some known institution and blocking all those from the domain of conflicts of the authors)

That being said, the reason I wrote that this is an excellent paper is because I think that the overall question (are adversarial examples inevitable?) is highly relevant (yet too rarely mentioned in the literature), and that the authors provide valuable insights/contributions to its answer. Their results seem to stem from a basic learning-theory-like analysis and make some strong and probably not very realistic assumptions (boundedness of densities). But they have the merit to clearly identify and formulate mathematically the problem (which is already a big contribution), and to provide at least the start of an answer. Of course, it's not the end of the story; but almost no paper is. 
As I was trying to explain with my remarks, my only little complaint is that it does not insist enough on  the greater picture (very useful for non-experts): is it our classifiers themselves that are biased or is it our data that cannot be classified? If it is the data, what are (on a high level) the assumptions that make it inherently  vulnerable (here: the "uniformity-over-dimensions"-kind of assumption). Concerning this second question, one could argue that everything one needs (the assumptions) is in the theorem. But those assumptions are not just technical: they are an essential part of the overall message, and therefore should be mentioned even in high-level explanations.

Nevertheless, I hope that the paper will get accepted and not dismissed for reasons like "we don't know whether this description really applies  to real-world models". If we don't know, then it's a valid hypothesis and even more a reason to accept the paper. The community will have to show in future if this paper was right or wrong, and in doing so, it will inevitably sharpen its understanding of the phenomenon. That's how natural sciences work.</span></div><div class="reply_row clearfix"></div></div><div class="children"></div></div></div></div></div></div><div class="note_with_children"><div id="note_S1go00H5nm" class="note panel"><div class="title_pdf_row clearfix"><h2 class="note_content_title"><a>a good angle, limited technical contributions, inconclusive statements</a>  <button class="btn btn-xs btn-default permalink-button" title="Link to this comment" data-permalink-url="https://openreview.net/forum?id=r1lWUoA9FQ&amp;noteId=S1go00H5nm"><span class="glyphicon glyphicon-link" aria-hidden="true"></span></button></h2></div><div class="meta_row"><span class="signatures">ICLR 2019 Conference Paper150 AnonReviewer1</span></div><div class="clearfix"><div class="meta_row pull-left"><span class="date item">03 Nov 2018 (modified: 07 Nov 2018)</span><span class="item">ICLR 2019 Conference Paper150 Official Review</span><span class="item">Readers: <span class="readers-icon glyphicon glyphicon-globe"></span> Everyone</span></div><div class="meta_row meta_actions"></div></div><div class="note_contents"><span class="note_content_field">Review: </span><span class="note_content_value">This paper explores the inevitability of adversarial examples with concentration inequalities. It is motivated by the difficulties of achieving adversarial robustness in literature. It derives isoperimetric inequalities on a cube, and then discuss the adversarial robustness of data distributed inside the cube, with the assumption that the data has bounded density. These inequalities are established on different norms. The authors then discuss limitation of the proposed bounds when analyzing practical data distribution and discussed the influence of dimensionality on adversarial robustness.


Novelty of the idea:
The idea of using concentration inequalities to explain vulnerability is novel in the field of adversarial examples and is a relevant/meaningful angle on understanding this phenomenon. (Although there are concurrent works also relating concentration inequalities to adversarial robustness, they don't diminish the novelty of this work.)



On technical contributions:
In summary, this paper applies / adapts previous results in concentration inequalities to develop bounds related to adversarial examples. The bounds in Lemma 3 are on any p&gt;0, this seems to be new to my knowledge, but the technical contribution in the proof is limited.

Here are some detailed comments.

The authors claim that
"This question is complicated by the fact that simple, geometric isoperimetric inequalities fail to exist for the cube, and the shapes that achieve minimal \eps-expansion (if they exist) depend on the volume they enclose and the choice of \eps."
This statement is at least misleading, if not wrong. It is well known that geometric isoperimetric inequality does exist for cube for the L2 case (see Ledoux, M., 2001. Proposition 2.8.), and the proof procedure the author used is also very similar to the proofs in Ledoux, M., 2001.

Theorem 5's proof is confusing, if not wrong. 
This is my brief recap on the first part of Thm 5, 
If there exists eps and p such that, for all classifiers on MNIST, a random image has eps-adv with probability at least p, then for all classifiers on b-MNIST, a random image has b*eps-adv with probability at least p.
The proof in Appendix E says b-MNIST images can be classified by first downsampling. These downsampled classifiers do not cover "all classifiers on b-MNIST", so I don't see how the proof stands.
Likewise, the proof of the second part has the similar problem.
Therefore, I'm not yet convinced that Thm 5 is correct.
Also I suggest the authors use more rigorous language to present Theorem 5, in a similar fashion to previous theorems.

Re: Lemma 4, my understanding is that it is from previous literature. The authors should point out exactly where is it from (with section# and theorem#), so that readers and reviewers can more easily check the correctness of it.

The authors mention that "Intuitively, the concentration limit Uc can be interpreted as a measure of image complexity."
I think this statement is problematic. It is, at best, oversimplifying the the problem. If we assume the data lies in low-dimensional space, the volume of the support will be 0, no matter how complex the shape of the manifold is. This lead to unbounded density in the ambient dimension.
Even when considering "expanded dataset" like the authors discussed in Section 7, it is not obvious that Uc can be interpreted as image complexity. To make such a claim, more assumptions need to made and more analyses need to be done.
Similar comments applies to the "correlations between pixels" and concentration.



On the significance:
As the author themselves have already mentioned, the bounds described in the paper all depends on the bounded density of the data distribution. In practice, the density of data distribution is difficult to understand, if not impossible. Therefore it is still inconclusive whether the "inevitability" exists. But to be fair, I believe this is mostly due to the difficulty of the problem being studied.



Clarity and writing:
The skeleton of the paper is well written and easy to follow. I've pointed out some problems in my previous comments.
I also appreciate that the authors made efforts to not overclaim.

here are a few more comments:
- I personally feel Section 3 as an "warm-up" section is redundant, and the authors can consider move them to the appendix.
- In Section 6 and 7, the authors talk about when is the bound "meaningful" and "active". This part is confusing/misleading. eps=sqrt(n) is actually the maximum possible perturbation and not falls into the common "adversarial perturbation" where the perturbation does not change the semantic meaning of the image. There should be a least an additional numerical examples on small eps, so the readers have better ideas on the tightness/looseness of the bound.



References:
Ledoux, M., 2001. The concentration of measure phenomenon (No. 89). American Mathematical Soc..

</span></div><div class="note_contents"><span class="note_content_field">Rating: </span><span class="note_content_value">4: Ok but not good enough - rejection</span></div><div class="note_contents"><span class="note_content_field">Confidence: </span><span class="note_content_value">4: The reviewer is confident but not absolutely certain that the evaluation is correct</span></div><div class="reply_row clearfix"></div></div><div class="children"></div></div><div class="note_with_children"><div id="note_BygAt3Jjqm" class="note panel"><div class="title_pdf_row clearfix"><h2 class="note_content_title"><a>Any insights toward deep neural networks?</a>  <button class="btn btn-xs btn-default permalink-button" title="Link to this comment" data-permalink-url="https://openreview.net/forum?id=r1lWUoA9FQ&amp;noteId=BygAt3Jjqm"><span class="glyphicon glyphicon-link" aria-hidden="true"></span></button></h2></div><div class="meta_row"><span class="signatures"><a href="/profile?id=~Weizhi_ZHU1" class="profile-link">Weizhi ZHU</a></span></div><div class="clearfix"><div class="meta_row pull-left"><span class="date item">10 Oct 2018</span><span class="item">ICLR 2019 Conference Paper150 Public Comment</span><span class="item">Readers: <span class="readers-icon glyphicon glyphicon-globe"></span> Everyone</span></div><div class="meta_row meta_actions"></div></div><div class="note_contents"><span class="note_content_field">Comment: </span><span class="note_content_value">Hi, 

Thanks for your interesting paper. 

However, your theory seems to be adaptive to all machine learning models rather than deep networks, am I right? Do you have further insights into why shallow learning doesn't suffer severe adversarial problems but deep learning does?

Another question, the right-hand side of (2) is dimension-free if you take p&gt;=2, and becomes vol(A(\epsilon, d_p)) \geq \alpha + \sqrt{2\pi}\epsilon, which is only a little bit larger than vol(A). Can this bound support your argument, that adversarial examples are "everywhere"?

Thanks!

</span></div><div class="reply_row clearfix"></div></div><div class="children"><div class="note_with_children"><div id="note_S1ePuse2qX" class="note panel"><div class="title_pdf_row clearfix"><h2 class="note_content_title"><a>Deep nets vs shallow classifiers</a>  <button class="btn btn-xs btn-default permalink-button" title="Link to this comment" data-permalink-url="https://openreview.net/forum?id=r1lWUoA9FQ&amp;noteId=S1ePuse2qX"><span class="glyphicon glyphicon-link" aria-hidden="true"></span></button></h2></div><div class="meta_row"><span class="signatures">ICLR 2019 Conference Paper150 Authors</span></div><div class="clearfix"><div class="meta_row pull-left"><span class="date item">11 Oct 2018</span><span class="item">ICLR 2019 Conference Paper150 Official Comment</span><span class="item">Readers: <span class="readers-icon glyphicon glyphicon-globe"></span> Everyone</span></div><div class="meta_row meta_actions"></div></div><div class="note_contents"><span class="note_content_field">Comment: </span><span class="note_content_value">Weizhi,
  Thanks for taking the time to read and comment.  It seems that you’ve asked a number of different questions, so I’ve tried to address them each individually below.

Is our theory neural-net specific?
No.  Our theory is applicable to the general case of measurable classifiers.  We address the special case of neural nets experimentally, but not with analysis. Keep in mind that this enables us to address classifiers that we use in practice but are not pure neural nets.  For example, consider an adversarially hardened classification algorithm that first does median filtering, then JPEG compression, and then uses a neural classifier.  This pipeline is a measurable classifier (but not a neural net), and so our theory can say things about it.  We do think it’s interesting to study behaviors that are specific to neural nets, but that’s not what we did here.

Why do deep nets seem more susceptible than linear classifiers?
First, note that adversarially trained nets for MNIST are quite hard to fool without making severe changes to the image, and so it does not always appear that deep nets have poor robustness.  However, there are clearly datasets where the susceptibility of neural nets seems to be quite bad.
There is a reason for this apparent susceptibility:  We *choose* to use neural networks on nasty datasets with very high “complexity”.  In Section 8, we show that it is fundamentally harder to avoid adversarial examples for complex datasets (e.g, ImageNet) than, say, a nice, linearly separable SVM dataset in which the data has a large margin and is highly concentrated near the corners/sides of the unit cube. 
  If you did use a linear classifier on ImageNet, it would be subjected to the same fundamental susceptibility bounds as a neural network.  Theorem 2 guarantees that, with some minimum probability, a random image is either (a) wrongly classified, or (b) correctly classified but with adversarial examples.   Linear classifiers for ImageNet are described by the former alternative (they’re wrong a lot), while neural nets pick the latter alternative (they’re usually correct, but have adversarial examples).   
 

Dimension-free for large p:
There is an implicit dependence on dimensionality here that is easy to overlook. For large but fixed p (less than infinity - the infinite case is addressed in the paper at the end of Section 4), the radius of the unit cube goes to infinity as the dimension increases.  If one chooses epsilon to be proportional to the norm of a typical image, then epsilon increases in higher dimensions.  For this reason, if the concentration bound “U” remains fixed as the dimension increases, the theory still predicts an increase in adversarial susceptibility because of the increase in epsilon (even for large p).  
That being said, we show in Section 8 that there is not a fundamental link between adversarial robustness and dimensionality.  The shrinking of the exponential term in Theorem 2 is countered by a blow-up in the concentration bound ”U”.  As discussed above (and in Section 8), image complexity, and not dimensionality, is what affects the limits of adversarial susceptibility.

Finally, I’d point out that we are not claiming that “adversarial examples are everywhere” for any one particular problem.  There are problems that are plagued by adversarial susceptibility, and problems that are not.  Rather, we are trying to take a rigorous look at what leads to adversarial susceptibility when it is present.  </span></div><div class="reply_row clearfix"></div></div><div class="children"><div class="note_with_children"><div id="note_Hyxd147fim" class="note panel"><div class="title_pdf_row clearfix"><h2 class="note_content_title"><a>Thank you</a>  <button class="btn btn-xs btn-default permalink-button" title="Link to this comment" data-permalink-url="https://openreview.net/forum?id=r1lWUoA9FQ&amp;noteId=Hyxd147fim"><span class="glyphicon glyphicon-link" aria-hidden="true"></span></button></h2></div><div class="meta_row"><span class="signatures"><a href="/profile?id=~Weizhi_ZHU1" class="profile-link">Weizhi ZHU</a></span></div><div class="clearfix"><div class="meta_row pull-left"><span class="date item">15 Oct 2018</span><span class="item">ICLR 2019 Conference Paper150 Public Comment</span><span class="item">Readers: <span class="readers-icon glyphicon glyphicon-globe"></span> Everyone</span></div><div class="meta_row meta_actions"></div></div><div class="note_contents"><span class="note_content_field">Comment: </span><span class="note_content_value">Thanks very much for detailed and clear explanations.</span></div><div class="reply_row clearfix"></div></div><div class="children"></div></div></div></div></div></div></div></main></div>
  </div>
</div>


    <!-- Footer -->
    <footer class="sitemap">
      <div class="container">
        <div class="row hidden-xs">
          <div class="col-sm-4">
            <ul class="list-unstyled">
              <li><a href="/" class="home">Home</a></li>
              <li><a href="/venues">All Venues</a></li>
            </ul>
          </div>
    
          <div class="col-sm-4">
            <ul class="list-unstyled">
              <li><a href="/about">About OpenReview</a></li>
              <li><a href="/contact">Contact</a></li>
              <li><a href="#" data-toggle="modal" data-target="#feedback-modal">Feedback</a></li>
            </ul>
          </div>
    
          <div class="col-sm-4">
            <ul class="list-unstyled">
              <li><a href="/terms">Terms of Service</a></li>
              <li><a href="/privacy">Privacy Policy</a></li>
            </ul>
          </div>
        </div>
    
        <div class="row visible-xs-block">
          <div class="col-xs-6">
            <ul class="list-unstyled">
              <li><a href="/" class="home">Home</a></li>
              <li><a href="/about">About OpenReview</a></li>
              <li><a href="#" data-toggle="modal" data-target="#feedback-modal">Feedback</a></li>
            </ul>
          </div>
    
          <div class="col-xs-6">
            <ul class="list-unstyled">
              <li><a href="/contact">Contact</a></li>
              <li><a href="/terms">Terms of Service</a></li>
              <li><a href="/privacy">Privacy Policy</a></li>
            </ul>
          </div>
        </div>
      </div>
    </footer>
    
    <footer class="sponsor">
      <div class="container">
        <div class="row">
          <div class="col-sm-10 col-sm-offset-1">
            <p class="text-center">
              OpenReview is created by the <a href="http://www.iesl.cs.umass.edu/" target="_blank">Information Extraction and Synthesis Laboratory</a>, College of Information and Computer Science, University of Massachusetts Amherst. We gratefully acknowledge the support of the OpenReview sponsors:  Google,  Facebook, NSF, the University of Massachusetts Amherst Center for Data Science, and Center for Intelligent Information Retrieval, as well as the Google Cloud Platform for donating the computing and networking services on which OpenReview.net runs.
            </p>
          </div>
        </div>
      </div>
    </footer>

  <div id="feedback-modal" class="modal fade" tabindex="-1" role="dialog">
    <div class="modal-dialog">
      <div class="modal-content">
        <div class="modal-header">
          <button type="button" class="close" data-dismiss="modal" aria-label="Close"><span aria-hidden="true">×</span></button>
          <h3 class="modal-title">Send Feedback</h3>
        </div>
        <div class="modal-body">
          <p>Enter your feedback below and we'll get back to you as soon as possible.</p>
          <form action="/feedback" method="POST">
            <div class="form-group">
              <input type="email" name="from" class="form-control" placeholder="Email" />
            </div>
            <div class="form-group">
              <input type="text" name="subject" class="form-control" placeholder="Subject" />
            </div>
            <div class="form-group">
              <textarea name="message" class="form-control feedback-input" rows="5" placeholder="Message" required=""></textarea>
            </div>
          <ul id="ui-id-2" tabindex="0" class="ui-menu ui-widget ui-widget-content" style="display: none;"></ul></form>
        </div>
        <div class="modal-footer">
          <button type="button" class="btn btn-default" data-dismiss="modal">Cancel</button>
          <button type="button" class="btn btn-primary">Send</button>
        </div>
      </div>
    </div>
  </div>

  <script type="text/javascript" async="" src="https://www.google-analytics.com/analytics.js"></script><script src="//cdnjs.cloudflare.com/ajax/libs/jquery/2.2.4/jquery.min.js"></script>
  <script>window.jQuery || document.write('&lt;script src="/static/js/vendor/jquery-2.2.4.min.js"&gt;&lt;\/script&gt;')</script>

  <script src="/static/dist/vendor.min.js"></script>
  <script src="/static/dist/templates.min.js"></script>
  <script src="/static/dist/openreview.min.js"></script>

    <script>window.legacyScripts = true;</script>


    <script async="" src="https://www.googletagmanager.com/gtag/js?id=UA-108703919-1"></script>
    <script>
      window.dataLayer = window.dataLayer || [];
      function gtag() { dataLayer.push(arguments); }
      gtag('js', new Date());
      gtag('config', 'UA-108703919-1');
    </script>


<div role="status" aria-live="assertive" aria-relevant="additions" class="ui-helper-hidden-accessible"></div></body></html>