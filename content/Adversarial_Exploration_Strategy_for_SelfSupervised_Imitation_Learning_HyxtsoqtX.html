<!DOCTYPE html><html xmlns="http://www.w3.org/1999/xhtml" class="no-js"><head>
  <meta charset="utf-8" />
  <meta http-equiv="X-UA-Compatible" content="IE=edge,chrome=1" />
  <meta name="viewport" content="width=device-width, initial-scale=1" />

  <title>Adversarial Exploration Strategy for Self-Supervised Imitation Learning | OpenReview</title>
  <meta name="description" content="" />

      <meta name="citation_title" content="Adversarial Exploration Strategy for Self-Supervised Imitation Learning" />
        <meta name="citation_author" content="Anonymous" />
      <meta name="citation_publication_date" content="2018/09/27" />
      <meta name="citation_online_date" content="2018/09/27" />
      <meta name="citation_pdf_url" content="https://openreview.net/pdf?id=Hyxtso0qtX" />
      <meta name="twitter:card" content="summary" />
      <meta name="twitter:site" content="@openreviewnet" />
      <meta name="og:title" content="Adversarial Exploration Strategy for Self-Supervised Imitation..." />
      <meta name="og:description" content="We present an adversarial exploration strategy, a simple yet effective imitation learning scheme that incentivizes exploration of an environment without any extrinsic reward or human demonstration...." />
      <meta name="og:image" content="https://openreview.net/static/images/openreview_logo_512.png" />

  <link rel="stylesheet" href="https://fonts.googleapis.com/css?family=Noto+Sans:400,400i,700,700i" />

  <link rel="stylesheet" href="/static/css/bootstrap.min.css" />
  <link rel="stylesheet" href="/static/css/jquery-ui.min.css" />
  <link rel="stylesheet" href="/static/css/main.min.css" />
</head>

<body class="forum">
  <nav class="navbar navbar-inverse navbar-fixed-top" role="navigation">
    <div class="container">
  
      <div class="navbar-header">
        <button type="button" class="navbar-toggle collapsed" data-toggle="collapse" data-target="#navbar" aria-expanded="false" aria-controls="navbar">
          <span class="sr-only">Toggle navigation</span>
          <span class="icon-bar"></span>
          <span class="icon-bar"></span>
          <span class="icon-bar"></span>
        </button>
        <a class="navbar-brand home push-link" href="/"><strong>OpenReview</strong>.net</a>
      </div>
  
      <div id="navbar" class="navbar-collapse collapse">
        <form class="navbar-form navbar-left profile-search" role="search">
          <div class="form-group has-feedback">
            <input id="search_input" type="text" class="form-control ui-autocomplete-input" placeholder="Search ICLR 2019 Conference" autocomplete="off" />
            <span class="glyphicon glyphicon-search form-control-feedback" aria-hidden="true"></span>
          </div>
  
          <input id="search_group" type="hidden" value="ICLR.cc/2019/Conference" />
          <input id="search_content" type="hidden" value="all" />
        <ul id="ui-id-1" tabindex="0" class="ui-menu ui-widget ui-widget-content ui-autocomplete ui-front" style="display: none;"></ul></form>
  
        <ul class="nav navbar-nav navbar-right">
        
            <li id="user-menu"><a href="/login">Login</a></li>
        </ul>
      </div>
  
    </div>
  </nav>

  <div id="or-banner" class="banner" style="">
  <div class="container">
    <div class="row">
      <div class="col-xs-12"><a href="/group?id=ICLR.cc/2019/Conference" title="Venue Homepage"><img class="icon" src="/static/images/arrow_left.svg" /> Go to <strong>ICLR 2019 Conference</strong> homepage</a></div>
    </div>
  </div>
</div>
<div id="flash-message-container" class="alert alert-danger" role="alert" style="display: none;">
  <div class="container">
    <div class="row">
      <div class="col-xs-12">
        <div class="alert-content">
          <button type="button" class="close" aria-label="Close"><span aria-hidden="true">×</span></button>
        </div>
      </div>
    </div>
  </div>
</div>

<div class="container">
  <div class="row">
    <div class="col-xs-12">
      <main id="content" class="clearfix openbanner-visible legacy-styles"><div id="note_Hyxtso0qtX" class="note panel"><div class="title_pdf_row clearfix"><h2 class="note_content_title"><a>Adversarial Exploration Strategy for Self-Supervised Imitation Learning</a> <a class="note_content_pdf" href="/pdf?id=Hyxtso0qtX" title="Download PDF" target="_blank"><img src="/static/images/pdf_icon_blue.svg" /></a> </h2></div><div class="meta_row"><span class="signatures">Anonymous</span></div><div class="clearfix"><div class="meta_row pull-left"><span class="date item">28 Sep 2018 (modified: 17 Nov 2018)</span><span class="item">ICLR 2019 Conference Blind Submission</span><span class="item">Readers: <span class="readers-icon glyphicon glyphicon-globe"></span> Everyone</span><span class="item"><a class="action-bibtex-modal" data-bibtex="@inproceedings{    &#10;anonymous2019adversarial,    &#10;title={Adversarial Exploration Strategy for Self-Supervised Imitation Learning},    &#10;author={Anonymous},    &#10;booktitle={Submitted to International Conference on Learning Representations},    &#10;year={2019},    &#10;url={https://openreview.net/forum?id=Hyxtso0qtX},    &#10;note={under review}    &#10;}">Show Bibtex</a></span><a class="note_content_pdf item" href="/revisions?id=Hyxtso0qtX" target="_blank">Show Revisions</a></div><div class="meta_row meta_actions"></div></div><div class="note_contents"><span class="note_content_field">Abstract: </span><span class="note_content_value">We present an adversarial exploration strategy, a simple yet effective imitation learning scheme that incentivizes exploration of an environment without any extrinsic reward or human demonstration. Our framework consists of a deep reinforcement learning (DRL) agent and an inverse dynamics model contesting with each other. The former collects training samples for the latter, and its objective is to maximize the error of the latter. The latter is trained with samples collected by the former, and generates rewards for the former when it fails to predict the actual action taken by the former. In such a competitive setting, the DRL agent learns to generate samples that the inverse dynamics model fails to predict correctly, and the inverse dynamics model learns to adapt to the challenging samples. We further propose a reward structure that ensures the DRL agent collects only moderately hard samples and not overly hard ones that prevent the inverse model from imitating effectively. We evaluate the effectiveness of our method on several OpenAI gym robotic arm and hand manipulation tasks against a number of baseline models. Experimental results show that our method is comparable to that directly trained with expert demonstrations, and superior to the other baselines even without any human priors.</span></div><div class="note_contents"><span class="note_content_field">Keywords: </span><span class="note_content_value">adversarial exploration, self-supervised, imitation learning</span></div><div class="note_contents"><span class="note_content_field">TL;DR: </span><span class="note_content_value">A simple yet effective imitation learning scheme that incentivizes exploration of an environment without any extrinsic reward or human demonstration.</span></div><div class="reply_row clearfix"><div class="item" id="reply_count">10 Replies</div></div></div><hr class="small" /><div id="note_children"><div class="note_with_children"><div id="note_Syl-u4YshX" class="note panel"><div class="title_pdf_row clearfix"><h2 class="note_content_title"><a>good training exploration, somewhat limited scope of experimental conditions</a>  <button class="btn btn-xs btn-default permalink-button" title="Link to this comment" data-permalink-url="https://openreview.net/forum?id=Hyxtso0qtX&amp;noteId=Syl-u4YshX"><span class="glyphicon glyphicon-link" aria-hidden="true"></span></button></h2></div><div class="meta_row"><span class="signatures">ICLR 2019 Conference Paper640 AnonReviewer2</span></div><div class="clearfix"><div class="meta_row pull-left"><span class="date item">04 Nov 2018 (modified: 07 Nov 2018)</span><span class="item">ICLR 2019 Conference Paper640 Official Review</span><span class="item">Readers: <span class="readers-icon glyphicon glyphicon-globe"></span> Everyone</span></div><div class="meta_row meta_actions"></div></div><div class="note_contents"><span class="note_content_field">Review: </span><span class="note_content_value">This paper presents a system for self-supervised imitation learning using a RL agent that is rewarded for finding actions that the system does not yet predict well given the current state.  More precisely, an imitation learner I is trained to predict an action A given a desired observation state transition xt-&gt;xt+1; the training samples for I are generated using a RL policy that yields an action A to train given xt (a physics engine evaluates xt+1 from xt and A).  The RL policy is rewarded using the loss incurred by I's prediction of A, so that moderately high loss values produce highest reward.  In this way, the RL agent learns to produce effective training samples that are not too easy or hard for the learner.  The method is evaluated on five block manipulation tasks, comparing to training samples generated by other recent self-supervised methods, as well as those found using a pretrained expert model for each task.

Overall, this method exploration seems quite effective on the tasks evaluated.  I'd be curious to know more about the limits and failures of the method, e.g. in other types of environments.

Additional questions:

- p.2 mentions that the environments "are intentionally selected by us for evaluating the performance of inverse dynamics model, as each of them allows only a very limited set of chained actions".  What sort of environments would be less well fit?  Are there any failure cases of this method where other baselines perform better?

- sec 4.3 notes that the self-supervised methods are pre-trained using 30k random samples before switching to the exploration policy, but in Fig 2, the success rates do not coincide between the systems and the random baseline, at either samples=0 or samples=30k --- should they?  if not, what differences caused this?

- figs. 4, 5 and 6 all relate to the stabilizer value delta, and I have a couple questions here:  (i) for what delta does performance start to degrade?  At delta=inf, I think it should be the same as no stabilizer, while at delta=0 is the exact opposite reward (i.e. negative loss, easy samples).  (ii) delta=3 is evaluated, and performance looks decent for this in fig 6 --- but fig 4 shows that the peak PDF of "no stabilizer" is around 3 as well, yet "no stabilizer" performs poorly in Fig 5.  Why is this, if it tends to produce actions with loss around 3 in both cases?
</span></div><div class="note_contents"><span class="note_content_field">Rating: </span><span class="note_content_value">7: Good paper, accept</span></div><div class="note_contents"><span class="note_content_field">Confidence: </span><span class="note_content_value">3: The reviewer is fairly confident that the evaluation is correct</span></div><div class="reply_row clearfix"></div></div><div class="children"><div class="note_with_children"><div id="note_HJgAY-KnaX" class="note panel"><div class="title_pdf_row clearfix"><h2 class="note_content_title"><a>Response to reviewer 2 (Part 1/2)</a>  <button class="btn btn-xs btn-default permalink-button" title="Link to this comment" data-permalink-url="https://openreview.net/forum?id=Hyxtso0qtX&amp;noteId=HJgAY-KnaX"><span class="glyphicon glyphicon-link" aria-hidden="true"></span></button></h2></div><div class="meta_row"><span class="signatures">ICLR 2019 Conference Paper640 Authors</span></div><div class="clearfix"><div class="meta_row pull-left"><span class="date item">17 Nov 2018</span><span class="item">ICLR 2019 Conference Paper640 Official Comment</span><span class="item">Readers: <span class="readers-icon glyphicon glyphicon-globe"></span> Everyone</span></div><div class="meta_row meta_actions"></div></div><div class="note_contents"><span class="note_content_field">Comment: </span><span class="note_content_value">Here is the PDF version of our responses: <a href="https://www.dropbox.com/s/707hka5ba9abg54/Response_2_ICLR_2019.pdf?dl=0" target="_blank" rel="nofollow">https://www.dropbox.com/s/707hka5ba9abg54/Response_2_ICLR_2019.pdf?dl=0</a> (anonymous link)

The authors appreciate the reviewer’s the time and efforts for reviewing this paper, and would like to respond to the questions in the following paragraphs.

Q1: Overall, this method exploration seems quite effective on the tasks evaluated.  I'd be curious to know more about the limits and failures of the method, e.g., in other types of environments.

Response: We would like to bring to the reviewer’s kind attention that although our method outperforms  all the baseline methods in most of the tasks, all the methods (including ours) are unable to surpass the “Demo” baseline in the HandReach task.  The observation implies that with regard to inverse dynamics model training, the contemporary self-supervised data-collection strategies (including ours and the baseline methods) are not as effective as training directly with expert demonstrations for this task.  We consider that this task is a typical failure case for our method (as well as the other baseline methods).   The underlying rationale is presumably due to the difficulty for exploration in the high-dimensional action space of HandReach, as this is the major difference between it and the other tasks presented in our paper.  We have included more analyses on the limitations and failure cases of our method in Section 4.3 of the revised version.

Q2: p.2 mentions that the environments "are intentionally selected by us for evaluating the performance of inverse dynamics model, as each of them allows only a very limited set of chained actions".  What sort of environments would be less well fit?  Are there any failure cases of this method where other baselines perform better?

Response: We would like to thank the reviewer for raising this question.  In fact, environments that allow various valid actions for a given transition (x_t to x_{t+1}) would be less well fit for our method.  As we train the inverse dynamic model by minimizing mean-square error between the predicted action a and the ground truth action â (Eq. (5)), multiple ground truth actions for the same transition would lead to high variance in the derived gradients.  This is referred to as the “multimodality problem” and has been discussed in [1].  As the main focus of this paper is to investigate the effectiveness of the proposed adversarial exploration strategy for self-supervised imitation learning, we do not incorporate these environments and the multimodality problem in our scope of discussion to avoid confusion and potential distraction of the main subject.  

Q3: Sec 4.3 notes that the self-supervised methods are pre-trained using 30k random samples before switching to the exploration policy, but in Fig 2, the success rates do not coincide between the systems and the random baseline, at either samples=0 or samples=30k --- should they?  if not, what differences caused this?

Response: We would like to sincerely apologize for the misunderstanding caused.  In our experiments, pre-training with random samples is only applied to the HandReach task due to its high complexity in exploration.  This is also the primary subject that we intend to discuss in Section 4.3.  As a result, the success rates of all the methods are the same in the HandReach task for the first 30K samples, as you have noticed.  For the other tasks, we do not pre-train the models with random samples.  Therefore, their success rates before 30K do not coincide with that of the “Random” baseline.  </span></div><div class="reply_row clearfix"></div></div><div class="children"><div class="note_with_children"><div id="note_SkxwoWYnaQ" class="note panel"><div class="title_pdf_row clearfix"><h2 class="note_content_title"><a>Response to reviewer 2 (Part 2/2) </a>  <button class="btn btn-xs btn-default permalink-button" title="Link to this comment" data-permalink-url="https://openreview.net/forum?id=Hyxtso0qtX&amp;noteId=SkxwoWYnaQ"><span class="glyphicon glyphicon-link" aria-hidden="true"></span></button></h2></div><div class="meta_row"><span class="signatures">ICLR 2019 Conference Paper640 Authors</span></div><div class="clearfix"><div class="meta_row pull-left"><span class="date item">17 Nov 2018</span><span class="item">ICLR 2019 Conference Paper640 Official Comment</span><span class="item">Readers: <span class="readers-icon glyphicon glyphicon-globe"></span> Everyone</span></div><div class="meta_row meta_actions"></div></div><div class="note_contents"><span class="note_content_field">Comment: </span><span class="note_content_value">Q4: Figs. 4, 5, and 6 all relate to the stabilizer value delta, and I have a couple questions here:  (i) for what delta does performance start to degrade?  At delta=inf, I think it should be the same as no stabilizer, while at delta=0 is the exact opposite reward (i.e. negative loss, easy samples).  (ii) delta=3 is evaluated, and performance looks decent for this in fig 6 --- but fig 4 shows that the peak PDF of "no stabilizer" is around 3 as well, yet "no stabilizer" performs poorly in Fig 5.  Why is this, if it tends to produce actions with loss around 3 in both cases?

Response: (i) Many thanks for raising this interesting question.  We have conducted additional experiments to investigate this issue, and have analyzed the results in the updated version of our manuscript.  In Fig. 6, we compare the learning curves of the imitator for different values of δ.  For instance, Ours(0.1) corresponds to δ = 0.1.  It is observed that for most of the tasks, the success rates drop when δ is set to an overly high or low value (e.g., 100.0 or 0.0), suggesting that a moderate value of δ is necessary for the stabilization technique.  The value of δ can be adjusted dynamically by the adaptive scaling technique presented in [2], which is left as our future direction.

(ii) We would like to clarify this issue as follows.  Although the peaks of “Ours(w/o stab)” in Fig. 4 are around 3, it does not suggest that their final success rates are comparable to those of “Ours(3.0)” (with our stabilization technique) in Fig. 6.  Please note that Fig. 4 only plots the first 2K training batches of the inverse dynamics model in the entire training process.  After 2K, the peaks of “Ours(3.0)” and “Ours(w stab)” still stay close to their $\delta$ values for the rest of the training processes, while that of “Ours(w/o stab)” gradually grows to around 1K, which is prohibitively higher than reasonable values.  Such a high training loss could cause the gradient exploding problem, which typically leads to a severe performance drop [3].  This also explains why “Ours (w/o stab)” performs poorly in Fig. 5.  As a result, “Ours(3.0)” is superior to “Ours (w/o stab)” due to its relatively stabler gradients.   

Please also note that plotting the training losses of only the first 2K training batches in Fig. 4 is intended for enhancing the visualization and readability of the results.  As the training losses of  ”Ours(w/o stab)” span from low values to extraordinarily high values, it is not feasible to be compare the PDF of ”Ours(w/o stab)” directly with that of “Ours(w stab)”.  Therefore, we selected a range of data (i.e., the first 2K batches) in which the training losses of “Ours(w/o stab)” are still under 10.  We have incorporated additional figures in our supplementary material for illustrating the above observations.

[1] P. Deepak et al., "Zero-shot visual imitation," in Proc. Int. Conf. Learning Representations (ICLR), Apr.-May 2018.

[2] M. Plappert et al., "Parameter space noise for exploration." in Proc. Int. Conf. Learning Representations (ICLR), Apr.-May, 2018.

[3] R. Pascanu, T. Mikolov, and Y. Bengio, "On the difficulty of training recurrent neural networks," in Proc. Int. Conf. Machine Learning (ICML), pp.1310-1318, Jun. 2013.</span></div><div class="reply_row clearfix"></div></div><div class="children"></div></div></div></div></div></div><div class="note_with_children"><div id="note_H1l5QTm5nQ" class="note panel"><div class="title_pdf_row clearfix"><h2 class="note_content_title"><a>interesting and potentially useful paper</a>  <button class="btn btn-xs btn-default permalink-button" title="Link to this comment" data-permalink-url="https://openreview.net/forum?id=Hyxtso0qtX&amp;noteId=H1l5QTm5nQ"><span class="glyphicon glyphicon-link" aria-hidden="true"></span></button></h2></div><div class="meta_row"><span class="signatures">ICLR 2019 Conference Paper640 AnonReviewer1</span></div><div class="clearfix"><div class="meta_row pull-left"><span class="date item">03 Nov 2018 (modified: 07 Nov 2018)</span><span class="item">ICLR 2019 Conference Paper640 Official Review</span><span class="item">Readers: <span class="readers-icon glyphicon glyphicon-globe"></span> Everyone</span></div><div class="meta_row meta_actions"></div></div><div class="note_contents"><span class="note_content_field">Review: </span><span class="note_content_value">The paper proposes an exploration strategy for deep reinforcement learning agent in continuous action spaces. The core of the method is to train an inverse local model (a model that predicts the action that was taken from a pair of states) and its errors as an exploration bonus for a policy gradient agent. The intuition is that its a good self-regulating strategy similar to curiosity that leads the agents towards states that are less known by the inverse model. Seeing these states improves the . There are experiments run on the OpenAI gym comparing to other models of curiosity. The paper is well written and clear for the most part.

pros:
- the paper seems novel and results are promising
- easy to implement
cons:
- seems unstable and not clear how it would scale in a large state space where most states are going to be very difficult to learn about in the beginning like a humanoid body.
- only accounts for the immediately controllable aspects of the environment which doesn't seem to be the hard part. Understanding the rest of the environment and its relationship to the controllable part of the state seems beyond the scope of this model. Nonetheless I can imagine it helping with initial random motions. 
- from (6) the bonus seems to be unbounded and (7) doesn't seem to fix that. Is that not an issue in general ? Any intuition about that ?</span></div><div class="note_contents"><span class="note_content_field">Rating: </span><span class="note_content_value">5: Marginally below acceptance threshold</span></div><div class="note_contents"><span class="note_content_field">Confidence: </span><span class="note_content_value">3: The reviewer is fairly confident that the evaluation is correct</span></div><div class="reply_row clearfix"></div></div><div class="children"><div class="note_with_children"><div id="note_SJx05eY3TQ" class="note panel"><div class="title_pdf_row clearfix"><h2 class="note_content_title"><a>Response to reviewer 1 (Part 1/2) </a>  <button class="btn btn-xs btn-default permalink-button" title="Link to this comment" data-permalink-url="https://openreview.net/forum?id=Hyxtso0qtX&amp;noteId=SJx05eY3TQ"><span class="glyphicon glyphicon-link" aria-hidden="true"></span></button></h2></div><div class="meta_row"><span class="signatures">ICLR 2019 Conference Paper640 Authors</span></div><div class="clearfix"><div class="meta_row pull-left"><span class="date item">17 Nov 2018</span><span class="item">ICLR 2019 Conference Paper640 Official Comment</span><span class="item">Readers: <span class="readers-icon glyphicon glyphicon-globe"></span> Everyone</span></div><div class="meta_row meta_actions"></div></div><div class="note_contents"><span class="note_content_field">Comment: </span><span class="note_content_value">Here is the PDF version of our responses: <a href="https://www.dropbox.com/s/0r2hztg7af87934/Response_1_ICLR_2019.pdf?dl=0" target="_blank" rel="nofollow">https://www.dropbox.com/s/0r2hztg7af87934/Response_1_ICLR_2019.pdf?dl=0</a>  (anonymous link)

The authors appreciate the reviewer’s the time and efforts for reviewing this paper, and would like to respond to the questions in the following paragraphs.

Q1: The paper proposes an exploration strategy for deep reinforcement learning agent in continuous action spaces. 

Response: We would like to sincerely apologize for the misunderstanding caused.  In fact, our adversarial exploration strategy is a self-supervised data-collection strategy developed for training an inverse dynamics model, which is formally described in Sections 2.2 and 3.  Different from exploration strategies for deep reinforcement learning (DRL) (which aim at learning policies for maximizing the expected returns in RL tasks), our work focuses on discovering a policy for collecting a useful training dataset for an inverse dynamics model.  

Q2: Seems unstable and not clear how it would scale in a large state space where most states are going to be very difficult to learn about in the beginning like a humanoid body.

Response: We would like to address the reviewer’s concerns in the following two paragraphs.  

First, with regard to the “instability” issue, we are not quite sure which aspect the reviewer refers to, and would appreciate it if the reviewer could kindly share some more information with us.  We assume that the reviewer could be referring to either the variance of the training losses, or the variance in the learning curves of Fig. 3.  For the former case, a stabilization technique is presented in Section 3.3, and its effectiveness is analyzed and validated in Section 4.5.  For the latter case, the variance in the learning curves is mainly caused by the complexity of the high-dimensional observation spaces.  As contemporary DRL techniques also suffer from the same problems when training with raw images (i.e., high-dimensional observation spaces) [1], the high variance in the learning curves can similarly be alleviated by enhancing the model architecture or the training algorithm.  Please note that this issue also occurs in the learning curves of the other baseline methods.  Discussion of model architectures and specific training techniques for high-dimensional observation spaces, however, is beyond the scope of this paper.

Second, we agree with the reviewer that learning a policy in an environment with a large state space has been a challenging research topic.  However, in the past few years, a number of DRL methods have been proposed and achieved remarkable successes in such environments, including humanoid robotic control [2].  The successes of these methods indicate that even in an environment with a large state space, it is still possible to discover an effective policy that maximizes the expected return.  In the proposed adversarial exploration strategy, we train a DRL agent to maximize the expected losses of the inverse dynamics model (Eq. (6)).  As DRL methods have been shown effective in exploiting arbitrary reward functions in large state spaces in the literature, we consider that our method can be extended to environments with large state spaces, and exploit the losses of the inverse dynamics model to collect training samples accordingly.  
</span></div><div class="reply_row clearfix"></div></div><div class="children"><div class="note_with_children"><div id="note_SJgi2xY26Q" class="note panel"><div class="title_pdf_row clearfix"><h2 class="note_content_title"><a>Response to reviewer 1 (Part 2/2) </a>  <button class="btn btn-xs btn-default permalink-button" title="Link to this comment" data-permalink-url="https://openreview.net/forum?id=Hyxtso0qtX&amp;noteId=SJgi2xY26Q"><span class="glyphicon glyphicon-link" aria-hidden="true"></span></button></h2></div><div class="meta_row"><span class="signatures">ICLR 2019 Conference Paper640 Authors</span></div><div class="clearfix"><div class="meta_row pull-left"><span class="date item">17 Nov 2018</span><span class="item">ICLR 2019 Conference Paper640 Official Comment</span><span class="item">Readers: <span class="readers-icon glyphicon glyphicon-globe"></span> Everyone</span></div><div class="meta_row meta_actions"></div></div><div class="note_contents"><span class="note_content_field">Comment: </span><span class="note_content_value">Q3: Only accounts for the immediately controllable aspects of the environment which doesn't seem to be the hard part. Understanding the rest of the environment and its relationship to the controllable part of the state seems beyond the scope of this model. 

Response: The authors would like to thank the reviewer for raising this question, and would love to address the reviewer’s concern by clarifying it in two aspects.  First, training a satisfactory inverse dynamics model for the controllable part of the environment is essentially not straightforward.  As the comparisons presented in Section 4 reveal, there exist large variations in the success rates of different self-supervised data-collection strategies.  For example, in FetchPickAndPlace of Fig. 3, there exists a significant performance gap between our method and the “Random” baseline, indicating that different exploration strategies do lead to different learning curves.  A simple exploration strategy (e.g.,  Random) might not be as effective as the other strategies in some tasks (e.g., FetchPush, FetchPickAndPlace, and HandReach).  Second, a key feature of our adversarial exploration strategy is that it utilizes a DRL agent to keep exploring the environment and collecting difficult training samples for the inverse dynamics model.  Therefore, our method allows the inverse dynamics model to gradually expand the scope of the controllable environment.

Q4: Nonetheless I can imagine it helping with initial random motions. 

Response: We are afraid that there seems to be some misunderstanding, and are not quite sure if we understand the reviewer’s meaning of “helping with initial random motions” correctly.  We were just wondering if the reviewer could kindly clarify this question for us?

Q5: From Eq. (6) the bonus seems to be unbounded and Eq. (7) doesn't seem to fix that. Is that not an issue in general ? Any intuition about that ?

Response: Although most of the DRL works suggest that the rewards should be re-scaled or clipped within a range (e.g., from -1 to 1), the unbounded rewards do not introduce any issue during the training process of our experiments.  The empirical rationale is that the rewards received by the DRL agent are regulated by Eq. (7) to be around δ, as described in Section 4.5 and depicted in Fig. 4.  Without the stabilization technique, however, the learning curves of the inverse dynamics model degrade drastically (as illustrated in Fig. 5), even if the reward clipping technique is applied.  

[1] M. Fortunato et al., "Noisy networks for exploration," in Proc. Int. Conf. Learning Representations (ICLR), Apr.-May, 2018. 

[2] Ł. Kidziński et al., "Learning to run challenge solutions: Adapting reinforcement learning methods for neuromusculoskeletal environments," arXiv:1804.00361, Apr. 2018.</span></div><div class="reply_row clearfix"></div></div><div class="children"></div></div></div></div></div></div><div class="note_with_children"><div id="note_rkxzS_7qnm" class="note panel"><div class="title_pdf_row clearfix"><h2 class="note_content_title"><a>Cool idea, but there are issues in evaluation and results are weak overall</a>  <button class="btn btn-xs btn-default permalink-button" title="Link to this comment" data-permalink-url="https://openreview.net/forum?id=Hyxtso0qtX&amp;noteId=rkxzS_7qnm"><span class="glyphicon glyphicon-link" aria-hidden="true"></span></button></h2></div><div class="meta_row"><span class="signatures">ICLR 2019 Conference Paper640 AnonReviewer3</span></div><div class="clearfix"><div class="meta_row pull-left"><span class="date item">03 Nov 2018 (modified: 07 Nov 2018)</span><span class="item">ICLR 2019 Conference Paper640 Official Review</span><span class="item">Readers: <span class="readers-icon glyphicon glyphicon-globe"></span> Everyone</span></div><div class="meta_row meta_actions"></div></div><div class="note_contents"><span class="note_content_field">Review: </span><span class="note_content_value">The paper proposes a novel exploration strategy for self-supervised imitation learning. An inverse dynamics model is trained on the trajectories collected from a RL-trained policy. The policy is rewarded for generating trajectories on which the inverse dynamics model (IDM) currently works poorly, i.e. on which IDM predicts actions that are far (in terms of mean square error) from the actions performed by the policy. This adversarial training is performed in purely self-supervised way. The evaluation is performed by one-shot imitation of an expert trajectory using the IDM: the action is predicted from the current state of the environment and the next state in the expert’s trajectory. Experimental evaluation shows that the proposed method is superior to baseline exploration strategies for self-supervised imitation learning, including random and curiosity-based exploration. 

Overall, I find the idea quite appealing. I am not an expert in the domain and can not make comments on the novelty of the approach. I found the writing mostly clear, except for the following issues:
- the introduction has not made it crystal clear that the considered paradigm is different from e.g. DAGGER and GAIL in that expert demonstrations are used at the inference time. A much wider audience is familiar with the former methods, and this distinction should have be explained more clearly.
- Section 4.2.: “As opposed to discrete control domains, these tasks are especially challenging, as the sample complexity grows in continuous control domains.” - this sentence did not make sense to me. It basically says continuous control is challenging because it is challenging. 
- I did not understand the stabilization approach. How exactly Equation (7) forces the policy to produce “not too hard” training examples for IDM? Figure 4 shows that it is on the opposite examples with small L_I that are avoided by using \delta &gt; 0.
- Table 1 - it is a bit counterintuitive that negative numbers are better than positive numbers here. Perhaps instead of policy’s deterioration you could report the relative change, negative when the performance goes down and positive otherwise?

I do have concerns regarding the experimental evaluation:
- the “Demos” baseline approach should be explained in the main text! In Appendix S.7 I see that 1000 human demonstrations were used for training. Why 1000, and not 100 and not 10000?  How would the results change? This needs to be discussed. Without discussing this it is really unclear how the proposed method can outperform “Demos”, which it does pretty often.
- it is commendable that 20 repetitions of each experiment were performed, but I am not sure if it is ever explained in the paper what exactly the upper and lower boundaries in the figures mean. Is it the standard deviation? A confidence interval? Can you comment on the variance of the proposed approach, which seems to be very high, especially when I am looking at high-dimensional fetch-reach results?
- the results of “HandReach” experiments, where the proposed method works much worse than “Demos” are not discussed in the text at all
- overall, there is no example of the proposed method making a difference between a “working” and “non-working” system, compared to “Curiosity” and “Random”. I am wondering if improvements from 40% to 60% in such cases are really important. In 7 out of 9 plots the performance of the proposed method is less than 80% - not very impressive. "Demos" baseline doesn't perform much better, but what would happen with 10000 demonstrations?
- there is no comparison to behavioral cloning, GAIL, IRL. Would these methods perform better than learning IDM like "Demos" does?

I think that currently the paper is slightly below the threshold, due to evaluation issues discussed above and overall low performance of the proposed algorithm. I am willing to reconsider my decision if these issues are addressed.
</span></div><div class="note_contents"><span class="note_content_field">Rating: </span><span class="note_content_value">5: Marginally below acceptance threshold</span></div><div class="note_contents"><span class="note_content_field">Confidence: </span><span class="note_content_value">3: The reviewer is fairly confident that the evaluation is correct</span></div><div class="reply_row clearfix"></div></div><div class="children"><div class="note_with_children"><div id="note_S1xCH6unTX" class="note panel"><div class="title_pdf_row clearfix"><h2 class="note_content_title"><a>Response to reviewer 3 (Part 1/3)</a>  <button class="btn btn-xs btn-default permalink-button" title="Link to this comment" data-permalink-url="https://openreview.net/forum?id=Hyxtso0qtX&amp;noteId=S1xCH6unTX"><span class="glyphicon glyphicon-link" aria-hidden="true"></span></button></h2></div><div class="meta_row"><span class="signatures">ICLR 2019 Conference Paper640 Authors</span></div><div class="clearfix"><div class="meta_row pull-left"><span class="date item">17 Nov 2018</span><span class="item">ICLR 2019 Conference Paper640 Official Comment</span><span class="item">Readers: <span class="readers-icon glyphicon glyphicon-globe"></span> Everyone</span></div><div class="meta_row meta_actions"></div></div><div class="note_contents"><span class="note_content_field">Comment: </span><span class="note_content_value">Here is the PDF version of our responses: <a href="https://www.dropbox.com/s/mxhetdyy7m4nkp6/Response_3_ICLR_2019.pdf?dl=0" target="_blank" rel="nofollow">https://www.dropbox.com/s/mxhetdyy7m4nkp6/Response_3_ICLR_2019.pdf?dl=0</a> (anonymous link)

The authors appreciate the reviewer’s the time and efforts for reviewing this paper, and would like to respond to the questions in the following paragraphs.

Q1: The introduction has not made it crystal clear that the considered paradigm is different from e.g. DAGGER and GAIL in that expert demonstrations are used at the inference time. A much wider audience is familiar with the former methods, and this distinction should have be explained more clearly.

Response: The authors appreciate the thoughtful feedbacks from the reviewer, and would like to bring to the reviewer’s kind attention that in Section 1, the following sentence for describing this distinction had been included in the original manuscript:

“Self-supervised IL allows an imitator to collect training data by itself instead of using predefined extrinsic reward functions or expert supervision during training. It only needs demonstration during inference, drastically decreasing the time and effort required from human experts.”  

Q2: Section 4.2.: “As opposed to discrete control domains, these tasks are especially challenging, as the sample complexity grows in continuous control domains.” - this sentence did not make sense to me. It basically says continuous control is challenging because it is challenging. 

Response:  Thanks for sharing your thoughts with us.  We would love to clarify the meaning of this sentence as follows.  

It is challenging to train inverse dynamics models in continuous control domains because of its requirement of diverse sample data during the training phase.  The diverse training data should cover various state transitions (i.e., (x_t, x_{t+1})) as well as their corresponding actions (i.e., a_t).  To this end, an explorer (i.e., the self-supervised data-collection DRL agent) is required to visit a wide range of states and extensively attempt different actions.    Such a requirement can be easily achieved in discrete control domains, as their action spaces are not as large as those of the continuous domain counterparts, allowing an explorer to quickly travel through most of possible state transitions.  In contrast, due to the enormous number of potential actions, the sample complexity (https://en.wikipedia.org/wiki/Sample_complexity) of continuous control domains is significantly higher than that of discrete ones.  This is the reason we called it challenging in the original manuscript.  

We sincerely hope that we have adequately addressed your concerns.

Q3: I did not understand the stabilization approach. How exactly Eq. (7) forces the policy to produce “not too hard” training examples for IDM? Fig. 4 shows that it is on the opposite examples with small L_I that are avoided by using δ &gt; 0.

Response:  The main objective of Eq. (7) is to shape the rewards (i.e., the losses of the inverse dynamics model) to be the negative L1-distance to δ.  In other words, the closer the original unshaped reward is to δ, the higher the shaped reward is.  As a result, the stabilization technique presented in Eq. (7) encourages the RL agent to collect “moderately difficult” samples that regulates the losses of the inverse dynamics model to be around δ.  

Q4: Table 1 - it is a bit counterintuitive that negative numbers are better than positive numbers here. Perhaps instead of policy’s deterioration you could report the relative change, negative when the performance goes down and positive otherwise?

Response:  The authors appreciate the thoughtful feedbacks from the reviewer, and have rephrased the term as “performance change rate” and made the other necessary revisions in Section 4.4 according to the suggestions in the updated manuscript.

Q5: The “Demos” baseline approach should be explained in the main text! In Appendix S.7 I see that 1000 human demonstrations were used for training. Why 1000, and not 100 and not 10000?  How would the results change? This needs to be discussed. Without discussing this it is really unclear how the proposed method can outperform “Demos”, which it does pretty often.

Response:  We fully agree with your comments regarding the number of demonstrations.  To address your concerns, we have incorporated an additional figure illustrating the learning curves of the Demo baseline with various number of demonstrations in the supplementary material.  According to Fig. 7, we do not observe any significant difference in performance when the number of demonstrations is set to 100, 1,000, and 10,000.  This is the reason why we used 1,000 demonstrations for training this baseline method in our experiments.  </span></div><div class="reply_row clearfix"></div></div><div class="children"><div class="note_with_children"><div id="note_SkltsaunaQ" class="note panel"><div class="title_pdf_row clearfix"><h2 class="note_content_title"><a>Response to reviewer 3 (Part 2/3) </a>  <button class="btn btn-xs btn-default permalink-button" title="Link to this comment" data-permalink-url="https://openreview.net/forum?id=Hyxtso0qtX&amp;noteId=SkltsaunaQ"><span class="glyphicon glyphicon-link" aria-hidden="true"></span></button></h2></div><div class="meta_row"><span class="signatures">ICLR 2019 Conference Paper640 Authors</span></div><div class="clearfix"><div class="meta_row pull-left"><span class="date item">17 Nov 2018</span><span class="item">ICLR 2019 Conference Paper640 Official Comment</span><span class="item">Readers: <span class="readers-icon glyphicon glyphicon-globe"></span> Everyone</span></div><div class="meta_row meta_actions"></div></div><div class="note_contents"><span class="note_content_field">Comment: </span><span class="note_content_value">Q6: It is commendable that 20 repetitions of each experiment were performed, but I am not sure if it is ever explained in the paper what exactly the upper and lower boundaries in the figures mean. Is it the standard deviation? A confidence interval? 

Response:  We would like to thank the reviewer for pointing out this issue, and sincerely apologize for the confusion caused.  In Figs. 2, 3, 5, and 6, the shaded regions of the curves correspond to their confidence intervals.  We have enhanced the manuscript with additional descriptions in the revised version.

Q7: Can you comment on the variance of the proposed approach, which seems to be very high, especially when I am looking at high-dimensional fetch-reach results?

Response:  The variance in the learning curves is mainly caused by the complexity of the high-dimensional observation spaces.  Contemporary DRL techniques also suffer from the same problems when training with raw images (i.e., high-dimensional observation spaces) [1].  The high variance in the learning curves can be alleviated by enhancing the model architecture or the training algorithm.  Please note that this issue also occurs in the learning curves of the other baseline methods.  Discussion of model architectures and specific training techniques for high-dimensional observation spaces, however, is beyond the scope of this paper.  

Q8: The results of “HandReach” experiments, where the proposed method works much worse than “Demos” are not discussed in the text at all

Response:  The reason that the inverse dynamics models trained by the self-supervised data-collection strategies discussed in this paper (including ours and the other baselines) are not comparable to the Demo baseline in the HandReach task is primarily due to the high-dimensional action space.  According to our experiments, it is observed that the data collected by the self-supervised data-collection strategies only cover a very limited range of the state space in the HandReach environment.  Therefore, the inverse dynamics model trained with these data only learn to imitate trivial poses, leading to the poor success rates presented in Fig. 2.  We would like to thank the reviewer for the suggestion, and have enhanced the manuscript with the above explanation in the revised version.

Q9: Overall, there is no example of the proposed method making a difference between a “working” and “non-working” system, compared to “Curiosity” and “Random”. I am wondering if improvements from 40% to 60% in such cases are really important. In 7 out of 9 plots the performance of the proposed method is less than 80% - not very impressive.

Response:  We understand the reviewer’s concerns.  However, we do have some reservations about the perspective of the comments, and would like to discuss our point of views with the reviewer in two different aspects.  First, the main scope of this work is self-supervised data-collection strategies for training inverse dynamics models.  The baseline methods selected for comparison are mostly published in this year.  As demonstrated in Section 4, our method outperforms them significantly for most of the tasks in both low- and high-dimensional observation spaces, as well as high-dimensional action space.  Second, to the best of our knowledge, almost none of the previous works in this domain has investigated adversarial exploration strategies for training inverse dynamics models.  Our work is the first proof of concept to demonstrate that the proposed adversarial strategy does improve the training efficiency and the performance of inverse dynamics models.  We hope that the reviewer could kindly correct us if we are wrong, and consider the points discussed above.
</span></div><div class="reply_row clearfix"></div></div><div class="children"><div class="note_with_children"><div id="note_BkxL-Cdha7" class="note panel"><div class="title_pdf_row clearfix"><h2 class="note_content_title"><a>Response to reviewer 3 (Part 3/3) </a>  <button class="btn btn-xs btn-default permalink-button" title="Link to this comment" data-permalink-url="https://openreview.net/forum?id=Hyxtso0qtX&amp;noteId=BkxL-Cdha7"><span class="glyphicon glyphicon-link" aria-hidden="true"></span></button></h2></div><div class="meta_row"><span class="signatures">ICLR 2019 Conference Paper640 Authors</span></div><div class="clearfix"><div class="meta_row pull-left"><span class="date item">17 Nov 2018</span><span class="item">ICLR 2019 Conference Paper640 Official Comment</span><span class="item">Readers: <span class="readers-icon glyphicon glyphicon-globe"></span> Everyone</span></div><div class="meta_row meta_actions"></div></div><div class="note_contents"><span class="note_content_field">Comment: </span><span class="note_content_value">Q10:  "Demos" baseline doesn't perform much better, but what would happen with 10,000 demonstrations?

Response:  To address your concerns, we have incorporated an additional figure illustrating the learning curves of the Demo baseline with various number of demonstrations in in Section S10 of the supplementary material.  According to Fig. 7, we do not observe any significant difference in performance when the number of demonstrations is set to 100, 1,000, and 10,000. Hence, we use 1,000 demonstrations for training the baselines as it costs less memory than the others and has comparable performance.  We have also revised Section S10 in the supplementary material with the above explanation.

Q11: There is no comparison to behavioral cloning, GAIL, IRL. Would these methods perform better than learning IDM like "Demos" does?

Response:  We would like to bring to the reviewer’s kind attention that the primary focus of this work is self-supervised imitation learning (IL), rather than traditional IL (e.g., GAIL and IRL).  The formulation of these two branches are fundamentally different from each other.  Self-supervised IL takes demonstrations in the testing phase only.  As a result, it allows the tasks to be altered online by changing the contents (i.e., trajectories) of the demonstrations.  On the other hand, traditional IL uses demonstrations in the training phase, and does not allow online changes to the tasks in the testing phase.  Therefore, we consider that these two branches are not supposed to be compared due to their distinct problem formulations.  

[1] M. Fortunato et al., "Noisy networks for exploration," in Proc. Int. Conf. Learning Representations (ICLR), Apr.-May, 2018. </span></div><div class="reply_row clearfix"></div></div><div class="children"></div></div></div></div></div></div></div></div></div></main></div>
  </div>
</div>


    <!-- Footer -->
    <footer class="sitemap">
      <div class="container">
        <div class="row hidden-xs">
          <div class="col-sm-4">
            <ul class="list-unstyled">
              <li><a href="/" class="home">Home</a></li>
              <li><a href="/venues">All Venues</a></li>
            </ul>
          </div>
    
          <div class="col-sm-4">
            <ul class="list-unstyled">
              <li><a href="/about">About OpenReview</a></li>
              <li><a href="/contact">Contact</a></li>
              <li><a href="#" data-toggle="modal" data-target="#feedback-modal">Feedback</a></li>
            </ul>
          </div>
    
          <div class="col-sm-4">
            <ul class="list-unstyled">
              <li><a href="/terms">Terms of Service</a></li>
              <li><a href="/privacy">Privacy Policy</a></li>
            </ul>
          </div>
        </div>
    
        <div class="row visible-xs-block">
          <div class="col-xs-6">
            <ul class="list-unstyled">
              <li><a href="/" class="home">Home</a></li>
              <li><a href="/about">About OpenReview</a></li>
              <li><a href="#" data-toggle="modal" data-target="#feedback-modal">Feedback</a></li>
            </ul>
          </div>
    
          <div class="col-xs-6">
            <ul class="list-unstyled">
              <li><a href="/contact">Contact</a></li>
              <li><a href="/terms">Terms of Service</a></li>
              <li><a href="/privacy">Privacy Policy</a></li>
            </ul>
          </div>
        </div>
      </div>
    </footer>
    
    <footer class="sponsor">
      <div class="container">
        <div class="row">
          <div class="col-sm-10 col-sm-offset-1">
            <p class="text-center">
              OpenReview is created by the <a href="http://www.iesl.cs.umass.edu/" target="_blank">Information Extraction and Synthesis Laboratory</a>, College of Information and Computer Science, University of Massachusetts Amherst. We gratefully acknowledge the support of the OpenReview sponsors:  Google,  Facebook, NSF, the University of Massachusetts Amherst Center for Data Science, and Center for Intelligent Information Retrieval, as well as the Google Cloud Platform for donating the computing and networking services on which OpenReview.net runs.
            </p>
          </div>
        </div>
      </div>
    </footer>

  <div id="feedback-modal" class="modal fade" tabindex="-1" role="dialog">
    <div class="modal-dialog">
      <div class="modal-content">
        <div class="modal-header">
          <button type="button" class="close" data-dismiss="modal" aria-label="Close"><span aria-hidden="true">×</span></button>
          <h3 class="modal-title">Send Feedback</h3>
        </div>
        <div class="modal-body">
          <p>Enter your feedback below and we'll get back to you as soon as possible.</p>
          <form action="/feedback" method="POST">
            <div class="form-group">
              <input type="email" name="from" class="form-control" placeholder="Email" />
            </div>
            <div class="form-group">
              <input type="text" name="subject" class="form-control" placeholder="Subject" />
            </div>
            <div class="form-group">
              <textarea name="message" class="form-control feedback-input" rows="5" placeholder="Message" required=""></textarea>
            </div>
          <ul id="ui-id-2" tabindex="0" class="ui-menu ui-widget ui-widget-content" style="display: none;"></ul></form>
        </div>
        <div class="modal-footer">
          <button type="button" class="btn btn-default" data-dismiss="modal">Cancel</button>
          <button type="button" class="btn btn-primary">Send</button>
        </div>
      </div>
    </div>
  </div>

  <script type="text/javascript" async="" src="https://www.google-analytics.com/analytics.js"></script><script src="//cdnjs.cloudflare.com/ajax/libs/jquery/2.2.4/jquery.min.js"></script>
  <script>window.jQuery || document.write('&lt;script src="/static/js/vendor/jquery-2.2.4.min.js"&gt;&lt;\/script&gt;')</script>

  <script src="/static/dist/vendor.min.js"></script>
  <script src="/static/dist/templates.min.js"></script>
  <script src="/static/dist/openreview.min.js"></script>

    <script>window.legacyScripts = true;</script>


    <script async="" src="https://www.googletagmanager.com/gtag/js?id=UA-108703919-1"></script>
    <script>
      window.dataLayer = window.dataLayer || [];
      function gtag() { dataLayer.push(arguments); }
      gtag('js', new Date());
      gtag('config', 'UA-108703919-1');
    </script>


<div role="status" aria-live="assertive" aria-relevant="additions" class="ui-helper-hidden-accessible"></div></body></html>