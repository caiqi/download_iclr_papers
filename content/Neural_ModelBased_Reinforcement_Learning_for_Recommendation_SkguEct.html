<!DOCTYPE html><html xmlns="http://www.w3.org/1999/xhtml" class="no-js"><head>
  <meta charset="utf-8" />
  <meta http-equiv="X-UA-Compatible" content="IE=edge,chrome=1" />
  <meta name="viewport" content="width=device-width, initial-scale=1" />

  <title>Neural Model-Based Reinforcement Learning for Recommendation | OpenReview</title>
  <meta name="description" content="" />

      <meta name="citation_title" content="Neural Model-Based Reinforcement Learning for Recommendation" />
        <meta name="citation_author" content="Anonymous" />
      <meta name="citation_publication_date" content="2018/09/27" />
      <meta name="citation_online_date" content="2018/09/27" />
      <meta name="citation_pdf_url" content="https://openreview.net/pdf?id=SkguE30ct7" />
      <meta name="twitter:card" content="summary" />
      <meta name="twitter:site" content="@openreviewnet" />
      <meta name="og:title" content="Neural Model-Based Reinforcement Learning for Recommendation" />
      <meta name="og:description" content="There is a great interest in applying reinforcement learning (RL) to recommendation systems. However, in this setting, an online user is the environment; neither the reward function nor the..." />
      <meta name="og:image" content="https://openreview.net/static/images/openreview_logo_512.png" />

  <link rel="stylesheet" href="https://fonts.googleapis.com/css?family=Noto+Sans:400,400i,700,700i" />

  <link rel="stylesheet" href="/static/css/bootstrap.min.css" />
  <link rel="stylesheet" href="/static/css/jquery-ui.min.css" />
  <link rel="stylesheet" href="/static/css/main.min.css" />
</head>

<body class="forum">
  <nav class="navbar navbar-inverse navbar-fixed-top" role="navigation">
    <div class="container">
  
      <div class="navbar-header">
        <button type="button" class="navbar-toggle collapsed" data-toggle="collapse" data-target="#navbar" aria-expanded="false" aria-controls="navbar">
          <span class="sr-only">Toggle navigation</span>
          <span class="icon-bar"></span>
          <span class="icon-bar"></span>
          <span class="icon-bar"></span>
        </button>
        <a class="navbar-brand home push-link" href="/"><strong>OpenReview</strong>.net</a>
      </div>
  
      <div id="navbar" class="navbar-collapse collapse">
        <form class="navbar-form navbar-left profile-search" role="search">
          <div class="form-group has-feedback">
            <input id="search_input" type="text" class="form-control ui-autocomplete-input" placeholder="Search ICLR 2019 Conference" autocomplete="off" />
            <span class="glyphicon glyphicon-search form-control-feedback" aria-hidden="true"></span>
          </div>
  
          <input id="search_group" type="hidden" value="ICLR.cc/2019/Conference" />
          <input id="search_content" type="hidden" value="all" />
        <ul id="ui-id-1" tabindex="0" class="ui-menu ui-widget ui-widget-content ui-autocomplete ui-front" style="display: none;"></ul></form>
  
        <ul class="nav navbar-nav navbar-right">
        
            <li id="user-menu"><a href="/login">Login</a></li>
        </ul>
      </div>
  
    </div>
  </nav>

  <div id="or-banner" class="banner" style="">
  <div class="container">
    <div class="row">
      <div class="col-xs-12"><a href="/group?id=ICLR.cc/2019/Conference" title="Venue Homepage"><img class="icon" src="/static/images/arrow_left.svg" /> Go to <strong>ICLR 2019 Conference</strong> homepage</a></div>
    </div>
  </div>
</div>
<div id="flash-message-container" class="alert alert-danger" role="alert" style="display: none;">
  <div class="container">
    <div class="row">
      <div class="col-xs-12">
        <div class="alert-content">
          <button type="button" class="close" aria-label="Close"><span aria-hidden="true">×</span></button>
        </div>
      </div>
    </div>
  </div>
</div>

<div class="container">
  <div class="row">
    <div class="col-xs-12">
      <main id="content" class="clearfix openbanner-visible legacy-styles"><div id="note_SkguE30ct7" class="note panel"><div class="title_pdf_row clearfix"><h2 class="note_content_title"><a>Neural Model-Based Reinforcement Learning for Recommendation</a> <a class="note_content_pdf" href="/pdf?id=SkguE30ct7" title="Download PDF" target="_blank"><img src="/static/images/pdf_icon_blue.svg" /></a> </h2></div><div class="meta_row"><span class="signatures">Anonymous</span></div><div class="clearfix"><div class="meta_row pull-left"><span class="date item">28 Sep 2018 (modified: 11 Oct 2018)</span><span class="item">ICLR 2019 Conference Blind Submission</span><span class="item">Readers: <span class="readers-icon glyphicon glyphicon-globe"></span> Everyone</span><span class="item"><a class="action-bibtex-modal" data-bibtex="@inproceedings{    &#10;anonymous2019neural,    &#10;title={Neural Model-Based Reinforcement Learning for Recommendation},    &#10;author={Anonymous},    &#10;booktitle={Submitted to International Conference on Learning Representations},    &#10;year={2019},    &#10;url={https://openreview.net/forum?id=SkguE30ct7},    &#10;note={under review}    &#10;}">Show Bibtex</a></span></div><div class="meta_row meta_actions"></div></div><div class="note_contents"><span class="note_content_field">Abstract: </span><span class="note_content_value">There is a great interest in applying reinforcement learning (RL) to recommendation systems. However, in this setting, an online user is the environment; neither the reward function nor the environment dynamics is clearly defined, making the application of RL challenging. 
 In this paper, we propose a novel model-based reinforcement learning framework for recommendation systems, where we developed a generative adversarial network to imitate user behavior dynamics and learn her reward function. Using this user model as the simulation environment, we develop a novel DQN algorithm to obtain a combinatorial recommendation policy which can handle a large number of candidate items efficiently.  In our experiments with real data, we show this generative adversarial user model can better explain user behavior than alternatives, and the RL policy based on this model can lead to better long turn reward for the user and higher click rate for the system.</span></div><div class="note_contents"><span class="note_content_field">Keywords: </span><span class="note_content_value">Generative adversarial user model, Recommendation system, model-based reinforcement learning, combinatorial recommendation policy, deep Q-networks</span></div><div class="note_contents"><span class="note_content_field">TL;DR: </span><span class="note_content_value">A new insight of designing a RL recommendation policy based on user behavior model along with some technical highlights.</span></div><div class="reply_row clearfix"><div class="item" id="reply_count">3 Replies</div></div></div><hr class="small" /><div id="note_children"><div class="note_with_children"><div id="note_SklsdEJh2m" class="note panel"><div class="title_pdf_row clearfix"><h2 class="note_content_title"><a>Interesting problem and ideas, manuscript may not be ready for publication yet</a>  <button class="btn btn-xs btn-default permalink-button" title="Link to this comment" data-permalink-url="https://openreview.net/forum?id=SkguE30ct7&amp;noteId=SklsdEJh2m"><span class="glyphicon glyphicon-link" aria-hidden="true"></span></button></h2></div><div class="meta_row"><span class="signatures">ICLR 2019 Conference Paper1463 AnonReviewer3</span></div><div class="clearfix"><div class="meta_row pull-left"><span class="date item">04 Nov 2018 (modified: 07 Nov 2018)</span><span class="item">ICLR 2019 Conference Paper1463 Official Review</span><span class="item">Readers: <span class="readers-icon glyphicon glyphicon-globe"></span> Everyone</span></div><div class="meta_row meta_actions"></div></div><div class="note_contents"><span class="note_content_field">Review: </span><span class="note_content_value">This paper proposes to frame the recommendation problem as one of (model-based) RL. The two main innovations are: 1) a model and objective for learning the environment and reward models; 2) a cascaded DQN framework for reasoning about a combinatorial number of actions (i.e., which subset of items to recommend to the user).

The problem is clearly important and the authors' approach focuses on solving some of the current issue with deployment of RL-based recommenders. Overall the paper is relatively easy to follow, but the current version is not the easiest to understand and, in particular, it may be worth providing more intuitions (e.g., about the GAN-like setup). I also found that several decisions are not properly justified. The novelty of this paper seems reasonably high but my impression is that other/stronger baselines would make the study more convincing. Copy-editing the paper would also greatly improve readability.

Detailed comments:
- I am not clear on whether or not in the proposed model, users are "allowed" to not click on a recommendation. It sounds like the authors in fact allow it but I think that could be made clearer.

- Section 4. I am not sure that using the Generative Adversarial Network terminology is useful here. Specifically, it is not clear what is your generative model over (I imagine next state and reward?).
  
- Remark in Section 4.1: It seems like a user not clicking on something is also useful information. Why not model it?
  
- I am a bit unclear on the value of Lemma 1. Further, what are the assumptions behind it? (also what is this temperature parameter eta?)
  
- In Section 4.2, the size of your model seems to grow linearly with the number of user interactions. That seems like a major advantage of RNNs/LSTMs. In practice, I imagine you cull the history at some fixed point?
  
- What is the advantage of learning a reward? E.g., a very simple reward would be to give a positive reward if a user clicks on a recommended item and a negative reward otherwise. What does your learned reward allow beyond this?
  
- Section 4.3. I also found Section 4.3 to be relatively unclear. I find that more intuition would be helpful.

  Also, if Eq. 7 is equivalent to Eq. 8, then why is the solution of 8 used only to initialize 7? I guess it may have to do with not finding the global optimum.

- Your cascading DQN idea seems like a good one. It would be nice to check if the constraints are correctly learned. If not, this seems like it would do not better than a greedy action-by-action solution. Is that correct?
  
- In Section 6.1, it would be good to discuss the pre-processing in the main text since it's pretty important to understand the study (e.g., evaluate is impact).
  
- In 6.2, your baselines seem a bit weak. Why not compare to more recent CF models (e.g., including Session-Based RNNs which you cite earlier)?
  
- Related work: it would probably be good to survey some of the multi-arm bandit literature. There is also some CF-RL work which should be cited (perhaps there are a few things in there that should be compared to in Section 6.3 &amp; 6.4). 

- Section 6.2 and Table 1. I believe that Recall@k is most common in recommendation-systems-for-implicit-data literature. Or, are you assuming that what people do not click on are true negatives? This doesn't seem quite right as users are only allowed to click on a single item.

- In Section 6.3, could you clarify how do you learn your reward model that is used to train the various methods?

- There are many typos and grammatical errors in the paper. I would suggest that the authors carefully copy-edit the manuscript.</span></div><div class="note_contents"><span class="note_content_field">Rating: </span><span class="note_content_value">5: Marginally below acceptance threshold</span></div><div class="note_contents"><span class="note_content_field">Confidence: </span><span class="note_content_value">4: The reviewer is confident but not absolutely certain that the evaluation is correct</span></div><div class="reply_row clearfix"></div></div><div class="children"></div></div><div class="note_with_children"><div id="note_Bylww959nX" class="note panel"><div class="title_pdf_row clearfix"><h2 class="note_content_title"><a>review</a>  <button class="btn btn-xs btn-default permalink-button" title="Link to this comment" data-permalink-url="https://openreview.net/forum?id=SkguE30ct7&amp;noteId=Bylww959nX"><span class="glyphicon glyphicon-link" aria-hidden="true"></span></button></h2></div><div class="meta_row"><span class="signatures">ICLR 2019 Conference Paper1463 AnonReviewer2</span></div><div class="clearfix"><div class="meta_row pull-left"><span class="date item">03 Nov 2018 (modified: 07 Nov 2018)</span><span class="item">ICLR 2019 Conference Paper1463 Official Review</span><span class="item">Readers: <span class="readers-icon glyphicon glyphicon-globe"></span> Everyone</span></div><div class="meta_row meta_actions"></div></div><div class="note_contents"><span class="note_content_field">Review: </span><span class="note_content_value">This paper belongs to the space of treating recommendation as a reinforcement learning problem, and proposed a model-based (cascaded DQN) approach, using a generative adversarial network to simulate user rewards.

Pros:
+ proposed a set of cascading Q functions, to learn a recommendation policy
+ unified min-max optimization to learn the behavior model and the reward function
+ interesting idea of using generative adversarial networks to simulate user rewards.

Cons: 
- in Figure 6 no comparison with model-free (policy-gradient type) of approaches
- there is not a lot of detail on the value of the generative adversarial network for the user behavior dynamics, thus this prevents the reader from fully understanding the contribution
- only 2 datasets are used
- only 100 users for test users seems few
- why only 1000 active users were sampled from MovieLens?

Personally, I would prefer less details on formulating the recommendation problem as an RL problem (as there have been other papers before with a similar formulation) and more detail  on the simulation user reward model, and in general in sections 4 and 5. Also, the experiments could be strengthened.</span></div><div class="note_contents"><span class="note_content_field">Rating: </span><span class="note_content_value">6: Marginally above acceptance threshold</span></div><div class="note_contents"><span class="note_content_field">Confidence: </span><span class="note_content_value">3: The reviewer is fairly confident that the evaluation is correct</span></div><div class="reply_row clearfix"></div></div><div class="children"></div></div><div class="note_with_children"><div id="note_HygC7wF9nQ" class="note panel"><div class="title_pdf_row clearfix"><h2 class="note_content_title"><a>Review for Neural Model-Based Reinforcement Learning for Recommendation</a>  <button class="btn btn-xs btn-default permalink-button" title="Link to this comment" data-permalink-url="https://openreview.net/forum?id=SkguE30ct7&amp;noteId=HygC7wF9nQ"><span class="glyphicon glyphicon-link" aria-hidden="true"></span></button></h2></div><div class="meta_row"><span class="signatures">ICLR 2019 Conference Paper1463 AnonReviewer1</span></div><div class="clearfix"><div class="meta_row pull-left"><span class="date item">03 Nov 2018 (modified: 07 Nov 2018)</span><span class="item">ICLR 2019 Conference Paper1463 Official Review</span><span class="item">Readers: <span class="readers-icon glyphicon glyphicon-globe"></span> Everyone</span></div><div class="meta_row meta_actions"></div></div><div class="note_contents"><span class="note_content_field">Review: </span><span class="note_content_value">The authors propose a deep reinforcement learning based recommendation algorithm. Instead of manually designing reward function for RL, a generative adversarial network was proposed to learn the reward function based on user's dynamic behavior. The authors also try to provide an efficient combinatorial recommendation algorithm by designing a cascade DQN. The authors hold their experiments on the Movielens and Ant Financial news dataset. The authors adopt logistic regression (LR) and collaborative competitive filtering(CCF) as comparison baseline to evaluate recommendation performance. The authors also compared their proposed RL policy CQDN with LinUCB.

[Pros in Summary]
1. Recommendation in the deep neural network based RL is a hot topic.

2. The motivation for using a self-learned rewards function and provide efficient combinatorial recommendation is interesting.

[Cons in Summary]
1. The motivations/claimed contributions are not well supported/illustrated by the proposed algorithm or experiments.

2. Some assumptions may not be realistic.

3. The experiment is not sufficient without enough state of art baselines.

4. The writing of this paper needs improvement.

[Thoughts, Questions, and Problems in Details]
1. The idea of using a learned reward function instead of manually defined one sound sweet. But based on (7) and (8), the reward function is essentially giving more rewards for the action that the user really clicks on. How much difference is there compared with traditional manual reward design of giving a click with a reward of 1, especially given the circumstance that a lot of manual intervention is actually used in designing loss function like (7)?

Moreover, in the experiment, there is no comparison experiment evaluating the difference between using a self-learned reward function vs. a traditional manual designed reward function.

2. The assumption "in each pageview, users are recommended a page of k items and provide feedback by clicking on only one of these items; and then the system recommends a new page of k items" does not sound realistic. What if the users click on multiple items?

3. The combinatorial recommendation is useful in the recommendation setting. But it is also important to get the correct ranking order for items from the recommendation list, ie, the best item should rank on the top of the list. Is this principal guaranteed in the combinatorial recommendation proposed in this paper? It is not discussed in this paper.

4. The authors claim to provide an efficient combinatorial recommendation but fail to provide any computational complexity analysis or providing any analysis on training or serving time. Is the proposed algorithm computationally practical to be deployed in a real system?

5. The experiments are too weak because the baselines are old and state of art methods are missing from the comparison.

6. Typos and grammar errors across the paper, to name a few

"we will also estimate a user behavior model associate with the reward function"

"a model for the sequence of user clicking behavior, discussion its parametrization and parameter estimation."</span></div><div class="note_contents"><span class="note_content_field">Rating: </span><span class="note_content_value">5: Marginally below acceptance threshold</span></div><div class="note_contents"><span class="note_content_field">Confidence: </span><span class="note_content_value">5: The reviewer is absolutely certain that the evaluation is correct and very familiar with the relevant literature</span></div><div class="reply_row clearfix"></div></div><div class="children"></div></div></div></main></div>
  </div>
</div>


    <!-- Footer -->
    <footer class="sitemap">
      <div class="container">
        <div class="row hidden-xs">
          <div class="col-sm-4">
            <ul class="list-unstyled">
              <li><a href="/" class="home">Home</a></li>
              <li><a href="/venues">All Venues</a></li>
            </ul>
          </div>
    
          <div class="col-sm-4">
            <ul class="list-unstyled">
              <li><a href="/about">About OpenReview</a></li>
              <li><a href="/contact">Contact</a></li>
              <li><a href="#" data-toggle="modal" data-target="#feedback-modal">Feedback</a></li>
            </ul>
          </div>
    
          <div class="col-sm-4">
            <ul class="list-unstyled">
              <li><a href="/terms">Terms of Service</a></li>
              <li><a href="/privacy">Privacy Policy</a></li>
            </ul>
          </div>
        </div>
    
        <div class="row visible-xs-block">
          <div class="col-xs-6">
            <ul class="list-unstyled">
              <li><a href="/" class="home">Home</a></li>
              <li><a href="/about">About OpenReview</a></li>
              <li><a href="#" data-toggle="modal" data-target="#feedback-modal">Feedback</a></li>
            </ul>
          </div>
    
          <div class="col-xs-6">
            <ul class="list-unstyled">
              <li><a href="/contact">Contact</a></li>
              <li><a href="/terms">Terms of Service</a></li>
              <li><a href="/privacy">Privacy Policy</a></li>
            </ul>
          </div>
        </div>
      </div>
    </footer>
    
    <footer class="sponsor">
      <div class="container">
        <div class="row">
          <div class="col-sm-10 col-sm-offset-1">
            <p class="text-center">
              OpenReview is created by the <a href="http://www.iesl.cs.umass.edu/" target="_blank">Information Extraction and Synthesis Laboratory</a>, College of Information and Computer Science, University of Massachusetts Amherst. We gratefully acknowledge the support of the OpenReview sponsors:  Google,  Facebook, NSF, the University of Massachusetts Amherst Center for Data Science, and Center for Intelligent Information Retrieval, as well as the Google Cloud Platform for donating the computing and networking services on which OpenReview.net runs.
            </p>
          </div>
        </div>
      </div>
    </footer>

  <div id="feedback-modal" class="modal fade" tabindex="-1" role="dialog">
    <div class="modal-dialog">
      <div class="modal-content">
        <div class="modal-header">
          <button type="button" class="close" data-dismiss="modal" aria-label="Close"><span aria-hidden="true">×</span></button>
          <h3 class="modal-title">Send Feedback</h3>
        </div>
        <div class="modal-body">
          <p>Enter your feedback below and we'll get back to you as soon as possible.</p>
          <form action="/feedback" method="POST">
            <div class="form-group">
              <input type="email" name="from" class="form-control" placeholder="Email" />
            </div>
            <div class="form-group">
              <input type="text" name="subject" class="form-control" placeholder="Subject" />
            </div>
            <div class="form-group">
              <textarea name="message" class="form-control feedback-input" rows="5" placeholder="Message" required=""></textarea>
            </div>
          <ul id="ui-id-2" tabindex="0" class="ui-menu ui-widget ui-widget-content" style="display: none;"></ul></form>
        </div>
        <div class="modal-footer">
          <button type="button" class="btn btn-default" data-dismiss="modal">Cancel</button>
          <button type="button" class="btn btn-primary">Send</button>
        </div>
      </div>
    </div>
  </div>

  <script type="text/javascript" async="" src="https://www.google-analytics.com/analytics.js"></script><script src="//cdnjs.cloudflare.com/ajax/libs/jquery/2.2.4/jquery.min.js"></script>
  <script>window.jQuery || document.write('&lt;script src="/static/js/vendor/jquery-2.2.4.min.js"&gt;&lt;\/script&gt;')</script>

  <script src="/static/dist/vendor.min.js"></script>
  <script src="/static/dist/templates.min.js"></script>
  <script src="/static/dist/openreview.min.js"></script>

    <script>window.legacyScripts = true;</script>


    <script async="" src="https://www.googletagmanager.com/gtag/js?id=UA-108703919-1"></script>
    <script>
      window.dataLayer = window.dataLayer || [];
      function gtag() { dataLayer.push(arguments); }
      gtag('js', new Date());
      gtag('config', 'UA-108703919-1');
    </script>


<div role="status" aria-live="assertive" aria-relevant="additions" class="ui-helper-hidden-accessible"></div></body></html>