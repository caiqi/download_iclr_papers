<!DOCTYPE html><html xmlns="http://www.w3.org/1999/xhtml" class="no-js"><head>
  <meta charset="utf-8" />
  <meta http-equiv="X-UA-Compatible" content="IE=edge,chrome=1" />
  <meta name="viewport" content="width=device-width, initial-scale=1" />

  <title>The Neuro-Symbolic Concept Learner: Interpreting Scenes, Words, and Sentences From Natural Supervision | OpenReview</title>
  <meta name="description" content="" />

      <meta name="citation_title" content="The Neuro-Symbolic Concept Learner: Interpreting Scenes, Words, and Sentences From Natural Supervision" />
        <meta name="citation_author" content="Anonymous" />
      <meta name="citation_publication_date" content="2018/09/27" />
      <meta name="citation_online_date" content="2018/09/27" />
      <meta name="citation_pdf_url" content="https://openreview.net/pdf?id=rJgMlhRctm" />
      <meta name="twitter:card" content="summary" />
      <meta name="twitter:site" content="@openreviewnet" />
      <meta name="og:title" content="The Neuro-Symbolic Concept Learner: Interpreting Scenes, Words, and..." />
      <meta name="og:description" content="We propose the Neuro-Symbolic Concept Learner (NS-CL), a model that learns visual concepts, words, and semantic parsing of sentences without explicit supervision on any of them; instead, our model..." />
      <meta name="og:image" content="https://openreview.net/static/images/openreview_logo_512.png" />

  <link rel="stylesheet" href="https://fonts.googleapis.com/css?family=Noto+Sans:400,400i,700,700i" />

  <link rel="stylesheet" href="/static/css/bootstrap.min.css" />
  <link rel="stylesheet" href="/static/css/jquery-ui.min.css" />
  <link rel="stylesheet" href="/static/css/main.min.css" />
</head>

<body class="forum">
  <nav class="navbar navbar-inverse navbar-fixed-top" role="navigation">
    <div class="container">
  
      <div class="navbar-header">
        <button type="button" class="navbar-toggle collapsed" data-toggle="collapse" data-target="#navbar" aria-expanded="false" aria-controls="navbar">
          <span class="sr-only">Toggle navigation</span>
          <span class="icon-bar"></span>
          <span class="icon-bar"></span>
          <span class="icon-bar"></span>
        </button>
        <a class="navbar-brand home push-link" href="/"><strong>OpenReview</strong>.net</a>
      </div>
  
      <div id="navbar" class="navbar-collapse collapse">
        <form class="navbar-form navbar-left profile-search" role="search">
          <div class="form-group has-feedback">
            <input id="search_input" type="text" class="form-control ui-autocomplete-input" placeholder="Search ICLR 2019 Conference" autocomplete="off" />
            <span class="glyphicon glyphicon-search form-control-feedback" aria-hidden="true"></span>
          </div>
  
          <input id="search_group" type="hidden" value="ICLR.cc/2019/Conference" />
          <input id="search_content" type="hidden" value="all" />
        <ul id="ui-id-1" tabindex="0" class="ui-menu ui-widget ui-widget-content ui-autocomplete ui-front" style="display: none;"></ul></form>
  
        <ul class="nav navbar-nav navbar-right">
        
            <li id="user-menu"><a href="/login">Login</a></li>
        </ul>
      </div>
  
    </div>
  </nav>

  <div id="or-banner" class="banner" style="">
  <div class="container">
    <div class="row">
      <div class="col-xs-12"><a href="/group?id=ICLR.cc/2019/Conference" title="Venue Homepage"><img class="icon" src="/static/images/arrow_left.svg" /> Go to <strong>ICLR 2019 Conference</strong> homepage</a></div>
    </div>
  </div>
</div>
<div id="flash-message-container" class="alert alert-danger" role="alert" style="display: none;">
  <div class="container">
    <div class="row">
      <div class="col-xs-12">
        <div class="alert-content">
          <button type="button" class="close" aria-label="Close"><span aria-hidden="true">×</span></button>
        </div>
      </div>
    </div>
  </div>
</div>

<div class="container">
  <div class="row">
    <div class="col-xs-12">
      <main id="content" class="clearfix openbanner-visible legacy-styles"><div id="note_rJgMlhRctm" class="note panel"><div class="title_pdf_row clearfix"><h2 class="note_content_title"><a>The Neuro-Symbolic Concept Learner: Interpreting Scenes, Words, and Sentences From Natural Supervision</a> <a class="note_content_pdf" href="/pdf?id=rJgMlhRctm" title="Download PDF" target="_blank"><img src="/static/images/pdf_icon_blue.svg" /></a> </h2></div><div class="meta_row"><span class="signatures">Anonymous</span></div><div class="clearfix"><div class="meta_row pull-left"><span class="date item">28 Sep 2018 (modified: 11 Oct 2018)</span><span class="item">ICLR 2019 Conference Blind Submission</span><span class="item">Readers: <span class="readers-icon glyphicon glyphicon-globe"></span> Everyone</span><span class="item"><a class="action-bibtex-modal" data-bibtex="@inproceedings{    &#10;anonymous2019the,    &#10;title={The Neuro-Symbolic Concept Learner: Interpreting Scenes, Words, and Sentences From Natural Supervision},    &#10;author={Anonymous},    &#10;booktitle={Submitted to International Conference on Learning Representations},    &#10;year={2019},    &#10;url={https://openreview.net/forum?id=rJgMlhRctm},    &#10;note={under review}    &#10;}">Show Bibtex</a></span></div><div class="meta_row meta_actions"></div></div><div class="note_contents"><span class="note_content_field">Abstract: </span><span class="note_content_value">We propose the Neuro-Symbolic Concept Learner (NS-CL), a model that learns visual concepts, words, and semantic parsing of sentences without explicit supervision on any of them; instead, our model learns by simply looking at images and reading paired questions and answers. Our model builds an object-based scene representation and translates sentences into executable, symbolic programs. To bridge the learning of two modules, we use a neural-symbolic reasoning module that executes these programs on the latent scene representation. Analog to the human concept learning, given the parsed program, the perception module learns visual concepts based on the language description of the object being referred to. Meanwhile, the learned visual concepts facilitate learning new words and parsing new sentences. We use curriculum learning to guide searching over the large compositional space of images and language. Extensive experiments demonstrate the accuracy and efficiency of our model on learning visual concepts, word representations, and semantic parsing of sentences. Further, our method allows easy generalization to new object attributes, compositions, language concepts, scenes and questions, and even new program domains. It also empowers applications including visual question answering and bidirectional image-text retrieval.</span></div><div class="note_contents"><span class="note_content_field">Keywords: </span><span class="note_content_value">Neuro-Symbolic Representations, Concept Learning, Visual Reasoning</span></div><div class="note_contents"><span class="note_content_field">TL;DR: </span><span class="note_content_value">We present a Neuro-Symbolic Concept Learner to learn visual concepts, words, and semantic parsing of sentences without explicit annotations for any of them. </span></div><div class="reply_row clearfix"><div class="item" id="reply_count">9 Replies</div></div></div><hr class="small" /><div id="note_children"><div class="note_with_children"><div id="note_rJxoZ-_ipQ" class="note panel"><div class="title_pdf_row clearfix"><h2 class="note_content_title"><a>Our General Response</a>  <button class="btn btn-xs btn-default permalink-button" title="Link to this comment" data-permalink-url="https://openreview.net/forum?id=rJgMlhRctm&amp;noteId=rJxoZ-_ipQ"><span class="glyphicon glyphicon-link" aria-hidden="true"></span></button></h2></div><div class="meta_row"><span class="signatures">ICLR 2019 Conference Paper1053 Authors</span></div><div class="clearfix"><div class="meta_row pull-left"><span class="date item">16 Nov 2018</span><span class="item">ICLR 2019 Conference Paper1053 Official Comment</span><span class="item">Readers: <span class="readers-icon glyphicon glyphicon-globe"></span> Everyone</span></div><div class="meta_row meta_actions"></div></div><div class="note_contents"><span class="note_content_field">Comment: </span><span class="note_content_value">We thank all reviewers for their comments. In addition to the specific response below, here we summarize our goal and the changes planned to be included in the revision. 

We study concept learning---discovering both visual concepts and language concepts from natural supervision (unannotated images and question-answer pairs). With these learned concepts, our model can solve many problems, such as image captioning, retrieval, as well as VQA. But here the ability to solve VQA is really a by-product, not our end goal---learning accurate (Sec. 4.1), interpretable (Sec. 4.2), and transferrable (Sec. 4.5) concepts. 

We agree with the reviewers that it’s important to demonstrate how our model works on real images with more complex visual appearance. As suggested, we plan to include the following changes in the revision by Nov. 23:
- We will include quantitative and qualitative results on new datasets: the VQA dataset of real-world images [1] and the Minecraft dataset used by Yi et al. [2].
- We will add a systematic study regarding the data efficiency of our model, compared with other VQA baselines in Sec. 4.2.
- We will compare our model with other baselines (TbD and MAC) built upon the object-based representations.
- We will include additional discussions on limitation and future work.

Please don’t hesitate to let us know for any additional comments on the paper or on the planned changes.

[1] Antol, Stanislaw, Aishwarya Agrawal, Jiasen Lu, Margaret Mitchell, Dhruv Batra, C. Lawrence Zitnick, and Devi Parikh. "Vqa: Visual question answering." In ICCV, 2015.
[2] Yi, Kexin, Jiajun Wu, Chuang Gan, Antonio Torralba, Pushmeet Kohli, and Joshua B. Tenenbaum. "Neural-Symbolic VQA: Disentangling Reasoning from Vision and Language Understanding." In NIPS, 2018.
</span></div><div class="reply_row clearfix"></div></div><div class="children"></div></div><div class="note_with_children"><div id="note_HJeHGTF5nX" class="note panel"><div class="title_pdf_row clearfix"><h2 class="note_content_title"><a>Interesting end-to-end joint learning of visual concepts and semantic parsing but  experiments are limiting</a>  <button class="btn btn-xs btn-default permalink-button" title="Link to this comment" data-permalink-url="https://openreview.net/forum?id=rJgMlhRctm&amp;noteId=HJeHGTF5nX"><span class="glyphicon glyphicon-link" aria-hidden="true"></span></button></h2></div><div class="meta_row"><span class="signatures">ICLR 2019 Conference Paper1053 AnonReviewer2</span></div><div class="clearfix"><div class="meta_row pull-left"><span class="date item">03 Nov 2018 (modified: 07 Nov 2018)</span><span class="item">ICLR 2019 Conference Paper1053 Official Review</span><span class="item">Readers: <span class="readers-icon glyphicon glyphicon-globe"></span> Everyone</span></div><div class="meta_row meta_actions"></div></div><div class="note_contents"><span class="note_content_field">Review: </span><span class="note_content_value">
Summary:
=========
The paper proposes a joint learning of visual representation and word and semantic parsing of the sentences given paired images and paired Q/A with a model called neuro-symbolic concept learner using curriculum learning. The paper reads well and is easy to follow. The idea of jointly learning visual concepts and language is an important task. Human reasoning involves learning and recall from multiple moralities. The authors use the CLEVR dataset for evaluation.

Strength:
========
- Jointly learning the language parsing and visual representations indirectly from paired Q/A and paired images is interesting. Combining the visual learning with the visual questions answers by decomposing them into primitive symbolic operations and reasoning in symbolic space seems interesting.

- End-to-end learning of the visual concepts, Q/A decomposition into primitives and program execution was shown to be competitive to baseline methods.

Weakness:
=========
- Although, the joint learning and composition is interesting, the visual task is simplistic and it is not obvious how this would generalize into other complex VQA tasks.

- Experiments are not as rigorous as the discussion of the methods suggests. Evaluation on more datasets would have made the comparisons and drawn conclusions more stronger. Although CLEVR is suited for learning relational concepts from referential expressions, it is a toy dataset. Applicability of the proposed method on other realistic datasets would have made the paper more stronger.</span></div><div class="note_contents"><span class="note_content_field">Rating: </span><span class="note_content_value">7: Good paper, accept</span></div><div class="note_contents"><span class="note_content_field">Confidence: </span><span class="note_content_value">4: The reviewer is confident but not absolutely certain that the evaluation is correct</span></div><div class="reply_row clearfix"></div></div><div class="children"><div class="note_with_children"><div id="note_rJx2mlOjTQ" class="note panel"><div class="title_pdf_row clearfix"><h2 class="note_content_title"><a>Our Response to Reviewer 2</a>  <button class="btn btn-xs btn-default permalink-button" title="Link to this comment" data-permalink-url="https://openreview.net/forum?id=rJgMlhRctm&amp;noteId=rJx2mlOjTQ"><span class="glyphicon glyphicon-link" aria-hidden="true"></span></button></h2></div><div class="meta_row"><span class="signatures">ICLR 2019 Conference Paper1053 Authors</span></div><div class="clearfix"><div class="meta_row pull-left"><span class="date item">16 Nov 2018</span><span class="item">ICLR 2019 Conference Paper1053 Official Comment</span><span class="item">Readers: <span class="readers-icon glyphicon glyphicon-globe"></span> Everyone</span></div><div class="meta_row meta_actions"></div></div><div class="note_contents"><span class="note_content_field">Comment: </span><span class="note_content_value">Thank you very much for the encouraging and constructive comments. We agree that generalizing to more complex visual domains would be essential for our task. In the revision, we will include the results of NS-CL on new datasets, including the VQA dataset of real-world images [1] and the Minecraft dataset used by Yi et al. [2].

We have also listed all other planned changes in our general response above. Please don’t hesitate to let us know for any additional comments on the paper or on the planned changes.

[1] Antol, Stanislaw, Aishwarya Agrawal, Jiasen Lu, Margaret Mitchell, Dhruv Batra, C. Lawrence Zitnick, and Devi Parikh. "Vqa: Visual question answering." In ICCV, 2015.
[2] Yi, Kexin, Jiajun Wu, Chuang Gan, Antonio Torralba, Pushmeet Kohli, and Joshua B. Tenenbaum. "Neural-Symbolic VQA: Disentangling Reasoning from Vision and Language Understanding." In NIPS, 2018.
</span></div><div class="reply_row clearfix"></div></div><div class="children"></div></div></div></div><div class="note_with_children"><div id="note_r1g6tF8F3X" class="note panel"><div class="title_pdf_row clearfix"><h2 class="note_content_title"><a>Concern of invalid evaluation and vague demonstration of the contribution</a>  <button class="btn btn-xs btn-default permalink-button" title="Link to this comment" data-permalink-url="https://openreview.net/forum?id=rJgMlhRctm&amp;noteId=r1g6tF8F3X"><span class="glyphicon glyphicon-link" aria-hidden="true"></span></button></h2></div><div class="meta_row"><span class="signatures">ICLR 2019 Conference Paper1053 AnonReviewer3</span></div><div class="clearfix"><div class="meta_row pull-left"><span class="date item">02 Nov 2018 (modified: 07 Nov 2018)</span><span class="item">ICLR 2019 Conference Paper1053 Official Review</span><span class="item">Readers: <span class="readers-icon glyphicon glyphicon-globe"></span> Everyone</span></div><div class="meta_row meta_actions"></div></div><div class="note_contents"><span class="note_content_field">Review: </span><span class="note_content_value">To achieve the state-of-the-art on the CLEVR and the variations of this, the authors propose a method to use object-based visual representations and a differentiable quasi-symbolic executor. Since the semantic parser for a question input is not differentiable, they use REINFORCE algorithm and a technique to reduce its variance. 

Quality: 
The issue of invalid evaluation should be addressed. CLEVR dataset has train, validation, and test sets. Since the various hyper-parameters are determined with the validation set, the comparison of state-of-the-art should be done using test set. As the authors mentioned, REINFORCE algorithm may introduce high variance, this notion is critical to report valid results. However, the authors only report on the validation set in Table 2 including the main results, Table 4. For Table 5, they only specify train and test splits. Therefore, I firmly recommend the authors to report on the test set for the fair comparison with the other competitive models, and please describe how to determine the hyperparameters in all experimental settings. 
   
Clarity:
As mentioned above, please specify the experimental details regarding setting hyperparameters.
In Experiments section, the authors used less than 10% of CLEVR training images. How about to use 100% of the training examples? How about to use the same amount of training examples in the competitive models? The report is incomplete to see the differential evident from the efficient usage of training examples.

Originality and significance:
The authors argue that object-based visual representation and symbolic reasoning are the contributions of this work (excluding the recent work, NS-VQA &lt; 1 month). However, bottom-up and top-down attention work [1] shows that attention networks using object-based visual representation significantly improve VQA and image captioning performances. If the object-based visual representation alone is the primary source of improvement, it severely weakens the argument of the neuro-symbolic concept learner. Since, considering the trend of gains, the contribution of the proposing method seems to be incremental, this concern is inevitable. To defend this critic, the additional experiment to see the improvement of the other attentional model (e.g, TbD, MAC) using object-based visual representations, without any other annotations, is needed.

Pros:
- To confirm the effective learning of visual concepts, words, and semantic parsing of sentences, they insightfully exploit the nature of the CLEVR dataset for visual reasoning diagnosis.

Cons:
- Invalid evaluation to report only on the validation set, not test set.
- The unclear significance of the proposed method combining object-based visual representations and symbolic reasoning
- In the original CLEVR dataset paper, the authors said "we stress that accuracy on CLEVR is not an end goal in itself" and "..CLEVR should be used in conjunction with other VQA datasets in order to study the reasoning abilities of general VQA systems." Based on this suggestion, can this work generalize to real-world settings? This paper lacks to discuss its limitation and future direction toward the general problem settings.

Minor comments:
In 4.3, please fix the typos, "born" -&gt; "brown" and "convlutional" -&gt; "convolutional".


[1] Anderson, P., He, X., Buehler, C., Teney, D., Johnson, M., Gould, S., &amp; Zhang, L. (2018). Bottom-Up and Top-Down Attention for Image Captioning and Visual Question Answering. IEEE Computer Vision and Pattern Recognition (CVPR'18).</span></div><div class="note_contents"><span class="note_content_field">Rating: </span><span class="note_content_value">5: Marginally below acceptance threshold</span></div><div class="note_contents"><span class="note_content_field">Confidence: </span><span class="note_content_value">4: The reviewer is confident but not absolutely certain that the evaluation is correct</span></div><div class="reply_row clearfix"></div></div><div class="children"><div class="note_with_children"><div id="note_rJl4slOsa7" class="note panel"><div class="title_pdf_row clearfix"><h2 class="note_content_title"><a>Our Response to Reviewer 3 (Part 1)</a>  <button class="btn btn-xs btn-default permalink-button" title="Link to this comment" data-permalink-url="https://openreview.net/forum?id=rJgMlhRctm&amp;noteId=rJl4slOsa7"><span class="glyphicon glyphicon-link" aria-hidden="true"></span></button></h2></div><div class="meta_row"><span class="signatures">ICLR 2019 Conference Paper1053 Authors</span></div><div class="clearfix"><div class="meta_row pull-left"><span class="date item">16 Nov 2018</span><span class="item">ICLR 2019 Conference Paper1053 Official Comment</span><span class="item">Readers: <span class="readers-icon glyphicon glyphicon-globe"></span> Everyone</span></div><div class="meta_row meta_actions"></div></div><div class="note_contents"><span class="note_content_field">Comment: </span><span class="note_content_value">Thank you very much for the constructive comments. 

1. Train/test split
Our evaluation is valid and fair, because all previous papers have also reported results only on the validation set, and we follow the tradition in this paper. They did this because there are no ground-truth labels or evaluation servers provided for the CLEVR test split. Evaluation on the test split is therefore impossible. We agree that it’s important to ensure all evaluation valid, and we’ll include this clarification into the revision.

2. Object-based representations and baselines
Thanks for the suggestion. We’ll cite and discuss the paper that used object-based visual representation. We will also add additional experiments that incorporate object-based representations into TbD/MAC: Instead of the image feature extracted from a ResNet, we change the input visual feature to the reasoning neural architecture to be an object-based representation as in [1]. Please let us know if you have any suggestion regarding the comparison.

We also want to clarify that the object-based representation alone is not the main contribution of the paper. Instead, our key contribution is the integration of object-based representations and symbolic reasoning. Such combination helps us disentangle visual concept learning and language understanding, and has three advantages over alternatives, as explored in the paper:

1) Executing symbolic programs on object-based representations naturally facilitates complex reasoning that includes quantities (counting), comparisons, and relations. It also brings combinatorial generalization by design (Sec. 4.4): for example, trained on scenes with &lt;= 6 objects, our model (but not the baselines) can also perform counting on scenes with 10 objects.

2) It fully disentangles the visual concept learning and reasoning: once the visual concepts are learned, they can be systematically evaluated (Sec. 4.1) and deployed in any visual-semantic applications (such as image caption retrieval, as shown in Sec. 4.5). In contrast, earlier methods like IEP, TbD, and MAC learn visual concepts and reasoning in an entangled manner and cannot be easily adapted to new problem domains (e.g., show in Table 6, VQA baselines are only able to infer the result on a partial set of the image-caption data).

3) Symbolic execution over the object space brings full transparency. One can easily trace back the error answer and even detect adversarial (ambiguous or wrong) questions (please refer to Appendix. E for some examples).

3. Limitation and future work
We’d like to clarify that we are not targeting at a specific application such as VQA; instead, we want to build a system that learns accurate (Sec. 4.1), interpretable (Sec. 4.2), and transferrable (Sec. 4.5) concepts from natural supervision: images and question-answer pairs. To achieve this, we propose a novel framework that 1) disentangles the learning of both, but 2) bridges them with a reasoning module and 3) lets them bootstrap the learning of each other.

Toward concept learning from realistic images and complex language, the current model design suggest multiple research directions. First, our model relies on object-based representations; constructing 3D object-based representations for realistic scenes (or videos) needs further exploration [1,2]. Second, our model assumes a domain-specific language for a formal description of semantics. The integration of formal semantics into the processing of complex natural language would be meaningful future work [3,4]. We hope our paper could motivate future research in visual concept learning, language learning, and compositionality.
</span></div><div class="reply_row clearfix"></div></div><div class="children"></div></div><div class="note_with_children"><div id="note_Bkx7KxOjpX" class="note panel"><div class="title_pdf_row clearfix"><h2 class="note_content_title"><a>Our Response to Reviewer 3 (Part 2)</a>  <button class="btn btn-xs btn-default permalink-button" title="Link to this comment" data-permalink-url="https://openreview.net/forum?id=rJgMlhRctm&amp;noteId=Bkx7KxOjpX"><span class="glyphicon glyphicon-link" aria-hidden="true"></span></button></h2></div><div class="meta_row"><span class="signatures">ICLR 2019 Conference Paper1053 Authors</span></div><div class="clearfix"><div class="meta_row pull-left"><span class="date item">16 Nov 2018</span><span class="item">ICLR 2019 Conference Paper1053 Official Comment</span><span class="item">Readers: <span class="readers-icon glyphicon glyphicon-globe"></span> Everyone</span></div><div class="meta_row meta_actions"></div></div><div class="note_contents"><span class="note_content_field">Comment: </span><span class="note_content_value">4. Specific Questions
- Choice of hyperparameters.
We use the open-sourced implementation of Mask-RCNN [5] to generate object proposals. For all the training processes described in the rest of the paper, we used learning rate 1e-3 with a weight decay of 5e-4. We decay the learning rate by a factor 0.1 after 60% of the designated training epochs. The REINFORCE optimizer uses a discount factor of 0.95. In the main text, the variance of REINFORCE means the variance of the gradient estimation but not the variance of the performance (accuracy). We will also add the standard deviation of the model performance in the revision. 

- Data-efficiency
Thanks for the very nice suggestion. We have conducted a more systematic study on the data efficiency and will include it the revision. The results are

Trained on 10% of the images:
TbD: 54.2%.
MAC: 17.8%.
NS-CL: 98.9%.

Trained on 100% of the images:
TbD: 99.1%.
MAC: 98.9%.
NS-CL: 99.2%.

These results demonstrate that our model is more data-efficient.

We have also listed all other planned changes in our general response above. Please don’t hesitate to let us know for any additional comments on the paper or on the planned changes.

[1] Anderson et al. "Bottom-up and top-down attention for image captioning and visual question answering." In CVPR, 2018.
[2] Baradel et al. "Object Level Visual Reasoning in Videos." In ECCV, 2018.
[3] Artzi, Yoav, and Zettlemoyer. "Weakly supervised learning of semantic parsers for mapping instructions to actions." TACL 1 (2013): 49-62.
[4] Oh et al. "Zero-Shot Task Generalization with Multi-Task Deep Reinforcement Learning." In ICML, 2017.
[5] <a href="https://github.com/roytseng-tw/Detectron.pytorch" target="_blank" rel="nofollow">https://github.com/roytseng-tw/Detectron.pytorch</a></span></div><div class="reply_row clearfix"></div></div><div class="children"><div class="note_with_children"><div id="note_SJgbnnCgRX" class="note panel"><div class="title_pdf_row clearfix"><h2 class="note_content_title"><a>Still waiting for the results with fair comparisons w.r.t Author Feedback 2.</a>  <button class="btn btn-xs btn-default permalink-button" title="Link to this comment" data-permalink-url="https://openreview.net/forum?id=rJgMlhRctm&amp;noteId=SJgbnnCgRX"><span class="glyphicon glyphicon-link" aria-hidden="true"></span></button></h2></div><div class="meta_row"><span class="signatures">ICLR 2019 Conference Paper1053 AnonReviewer3</span></div><div class="clearfix"><div class="meta_row pull-left"><span class="date item">20 Nov 2018</span><span class="item">ICLR 2019 Conference Paper1053 Official Comment</span><span class="item">Readers: <span class="readers-icon glyphicon glyphicon-globe"></span> Everyone</span></div><div class="meta_row meta_actions"></div></div><div class="note_contents"><span class="note_content_field">Comment: </span><span class="note_content_value">Sincerely thank you for the detailed explanations and comments for a constructive rebuttal.

Re: 1. Train/test split
R1-1) I understand the current evaluation issue on CLEVR. Then, could you confirm that your hyperparameters are found using **ONLY** training split since you have used the validation split like the test split to compare state-of-the-art?
R1-2) Have you asked the authors of CLEVR regarding this issue? And, what's their response? I appreciate if you can cite the authors' reply in Appendix as a pointer to refer in future works.

Re: 2. Object-based representations and baselines
R2) With the positive results, I would like to consider increasing my rating considering the authors' argument of fair comparison.</span></div><div class="reply_row clearfix"></div></div><div class="children"></div></div></div></div></div></div><div class="note_with_children"><div id="note_Sklo1V_znQ" class="note panel"><div class="title_pdf_row clearfix"><h2 class="note_content_title"><a>Excellent paper including many cutting edge techniques </a>  <button class="btn btn-xs btn-default permalink-button" title="Link to this comment" data-permalink-url="https://openreview.net/forum?id=rJgMlhRctm&amp;noteId=Sklo1V_znQ"><span class="glyphicon glyphicon-link" aria-hidden="true"></span></button></h2></div><div class="meta_row"><span class="signatures">ICLR 2019 Conference Paper1053 AnonReviewer1</span></div><div class="clearfix"><div class="meta_row pull-left"><span class="date item">28 Oct 2018 (modified: 07 Nov 2018)</span><span class="item">ICLR 2019 Conference Paper1053 Official Review</span><span class="item">Readers: <span class="readers-icon glyphicon glyphicon-globe"></span> Everyone</span></div><div class="meta_row meta_actions"></div></div><div class="note_contents"><span class="note_content_field">Review: </span><span class="note_content_value">The paper is well written and flow well. The only thing I would like to see added is an elaboration of 
"run a semantic parsing module to translate a question into an executable program". How to do semantic parsing is far from obvious. This topic needs at least a paragraph of its own. 

This is not a requirement but an opportunity, can you explain how counting work? I think you have it at the standard level of the magic of DNN but some digging into the mechanism would be appreciated. 

In concluding maybe you can speculate how far this method can go. Compositionality? Implicit relations inferred from words and behavior? Application to video with words?   </span></div><div class="note_contents"><span class="note_content_field">Rating: </span><span class="note_content_value">9: Top 15% of accepted papers, strong accept</span></div><div class="note_contents"><span class="note_content_field">Confidence: </span><span class="note_content_value">5: The reviewer is absolutely certain that the evaluation is correct and very familiar with the relevant literature</span></div><div class="reply_row clearfix"></div></div><div class="children"><div class="note_with_children"><div id="note_SyxhAx_jpm" class="note panel"><div class="title_pdf_row clearfix"><h2 class="note_content_title"><a>Our Response to Reviewer 1</a>  <button class="btn btn-xs btn-default permalink-button" title="Link to this comment" data-permalink-url="https://openreview.net/forum?id=rJgMlhRctm&amp;noteId=SyxhAx_jpm"><span class="glyphicon glyphicon-link" aria-hidden="true"></span></button></h2></div><div class="meta_row"><span class="signatures">ICLR 2019 Conference Paper1053 Authors</span></div><div class="clearfix"><div class="meta_row pull-left"><span class="date item">16 Nov 2018</span><span class="item">ICLR 2019 Conference Paper1053 Official Comment</span><span class="item">Readers: <span class="readers-icon glyphicon glyphicon-globe"></span> Everyone</span></div><div class="meta_row meta_actions"></div></div><div class="note_contents"><span class="note_content_field">Comment: </span><span class="note_content_value">Thank you very much for the constructive comments.

1. Semantic parsing.
In short, the semantic parsing module is a neural sequence-to-tree model. Given a natural language question, the module translates into an executable program with a hierarchy of primitive operation. We present an overview in Sec. 3.1 (last paragraph of Page 4), with more implementation details in Appendix B. We’ll revise the text for better clarity.

The module begins with encoding the question into a fixed-length embedding vector using a bidirectional GRU. The decoder, taking the sentence embedding as input, recovers the hierarchy of the operations in a top-down manner: It first predicts the root token (the question type: query/count/… in the VQA case); then, conditioned on the root token, it predicts the tokens of the root’s children. The decoding algorithm runs recursively.

2. Counting.
We perform counting in a quasi-symbolic manner, based on the object-based scene representation. As an example, consider a simple program: Count(Filter(Red)), which counts the number of red objects in the scene. The operation Filter(Red) assigns each object with a value p_i, as the confidence of classifying this object as a red one. Counting is performed as: $\sum_i p_i$. During inference, we round this value to the nearest integer. More details can be found in Sec. 3,1. (Page 5) and Appendix C. We will also revise the text for better clarity.

Compared with alternatives, our method enjoys combinatorial generalization with the notion of `objects’: for example, trained on scenes with &lt;= 6 objects, our model can also perform counting on scenes with 10 objects.

3. Future direction
We thank the reviewer for the suggestions on future directions and will include the following discussions in the revision:

Compositionality. We currently view the scene as a collection of objects with latent representations. Building scene (or video) representations that also reflects the compositional nature of objects (e.g., an object is a combination of multiple primitives) will be an interesting research direction. 

Infer relations from words and behavior. Modelling actions (e.g., push and pull) as concepts is another interesting direction. People have studied the symbolic representation of skills [1] and learning word (instruction) meanings from interaction [2].

Videos and words. Our framework can also be extended to the video domain. Video techniques such as detection and tracking are needed to build the object-based representation [3]. Also, the semantic representation of sentences should be extended to include actions / interactions besides static spatial relations. 

We have also listed all other planned changes in our general response above. Please don’t hesitate to let us know for any additional comments on the paper or on the planned changes.


[1] Konidaris, George, Leslie Pack Kaelbling, and Tomas Lozano-Perez. "From skills to symbols: Learning symbolic representations for abstract high-level planning." Journal of Artificial Intelligence Research 61 (2018): 215-289.
[2] Oh, Junhyuk, Satinder Singh, Honglak Lee, and Pushmeet Kohli. "Zero-Shot Task Generalization with Multi-Task Deep Reinforcement Learning." In ICML, 2017.
[3] Baradel, Fabien, Natalia Neverova, Christian Wolf, Julien Mille, and Greg Mori. "Object Level Visual Reasoning in Videos." In ECCV, 2018.
</span></div><div class="reply_row clearfix"></div></div><div class="children"></div></div></div></div></div></main></div>
  </div>
</div>


    <!-- Footer -->
    <footer class="sitemap">
      <div class="container">
        <div class="row hidden-xs">
          <div class="col-sm-4">
            <ul class="list-unstyled">
              <li><a href="/" class="home">Home</a></li>
              <li><a href="/venues">All Venues</a></li>
            </ul>
          </div>
    
          <div class="col-sm-4">
            <ul class="list-unstyled">
              <li><a href="/about">About OpenReview</a></li>
              <li><a href="/contact">Contact</a></li>
              <li><a href="#" data-toggle="modal" data-target="#feedback-modal">Feedback</a></li>
            </ul>
          </div>
    
          <div class="col-sm-4">
            <ul class="list-unstyled">
              <li><a href="/terms">Terms of Service</a></li>
              <li><a href="/privacy">Privacy Policy</a></li>
            </ul>
          </div>
        </div>
    
        <div class="row visible-xs-block">
          <div class="col-xs-6">
            <ul class="list-unstyled">
              <li><a href="/" class="home">Home</a></li>
              <li><a href="/about">About OpenReview</a></li>
              <li><a href="#" data-toggle="modal" data-target="#feedback-modal">Feedback</a></li>
            </ul>
          </div>
    
          <div class="col-xs-6">
            <ul class="list-unstyled">
              <li><a href="/contact">Contact</a></li>
              <li><a href="/terms">Terms of Service</a></li>
              <li><a href="/privacy">Privacy Policy</a></li>
            </ul>
          </div>
        </div>
      </div>
    </footer>
    
    <footer class="sponsor">
      <div class="container">
        <div class="row">
          <div class="col-sm-10 col-sm-offset-1">
            <p class="text-center">
              OpenReview is created by the <a href="http://www.iesl.cs.umass.edu/" target="_blank">Information Extraction and Synthesis Laboratory</a>, College of Information and Computer Science, University of Massachusetts Amherst. We gratefully acknowledge the support of the OpenReview sponsors:  Google,  Facebook, NSF, the University of Massachusetts Amherst Center for Data Science, and Center for Intelligent Information Retrieval, as well as the Google Cloud Platform for donating the computing and networking services on which OpenReview.net runs.
            </p>
          </div>
        </div>
      </div>
    </footer>

  <div id="feedback-modal" class="modal fade" tabindex="-1" role="dialog">
    <div class="modal-dialog">
      <div class="modal-content">
        <div class="modal-header">
          <button type="button" class="close" data-dismiss="modal" aria-label="Close"><span aria-hidden="true">×</span></button>
          <h3 class="modal-title">Send Feedback</h3>
        </div>
        <div class="modal-body">
          <p>Enter your feedback below and we'll get back to you as soon as possible.</p>
          <form action="/feedback" method="POST">
            <div class="form-group">
              <input type="email" name="from" class="form-control" placeholder="Email" />
            </div>
            <div class="form-group">
              <input type="text" name="subject" class="form-control" placeholder="Subject" />
            </div>
            <div class="form-group">
              <textarea name="message" class="form-control feedback-input" rows="5" placeholder="Message" required=""></textarea>
            </div>
          <ul id="ui-id-2" tabindex="0" class="ui-menu ui-widget ui-widget-content" style="display: none;"></ul></form>
        </div>
        <div class="modal-footer">
          <button type="button" class="btn btn-default" data-dismiss="modal">Cancel</button>
          <button type="button" class="btn btn-primary">Send</button>
        </div>
      </div>
    </div>
  </div>

  <script type="text/javascript" async="" src="https://www.google-analytics.com/analytics.js"></script><script src="//cdnjs.cloudflare.com/ajax/libs/jquery/2.2.4/jquery.min.js"></script>
  <script>window.jQuery || document.write('&lt;script src="/static/js/vendor/jquery-2.2.4.min.js"&gt;&lt;\/script&gt;')</script>

  <script src="/static/dist/vendor.min.js"></script>
  <script src="/static/dist/templates.min.js"></script>
  <script src="/static/dist/openreview.min.js"></script>

    <script>window.legacyScripts = true;</script>


    <script async="" src="https://www.googletagmanager.com/gtag/js?id=UA-108703919-1"></script>
    <script>
      window.dataLayer = window.dataLayer || [];
      function gtag() { dataLayer.push(arguments); }
      gtag('js', new Date());
      gtag('config', 'UA-108703919-1');
    </script>


<div role="status" aria-live="assertive" aria-relevant="additions" class="ui-helper-hidden-accessible"></div></body></html>