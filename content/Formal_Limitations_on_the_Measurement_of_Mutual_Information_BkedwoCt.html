<!DOCTYPE html><html xmlns="http://www.w3.org/1999/xhtml" class="no-js"><head>
  <meta charset="utf-8" />
  <meta http-equiv="X-UA-Compatible" content="IE=edge,chrome=1" />
  <meta name="viewport" content="width=device-width, initial-scale=1" />

  <title>Formal Limitations on the Measurement of Mutual Information | OpenReview</title>
  <meta name="description" content="" />

      <meta name="citation_title" content="Formal Limitations on the Measurement of Mutual Information" />
        <meta name="citation_author" content="Anonymous" />
      <meta name="citation_publication_date" content="2018/09/27" />
      <meta name="citation_online_date" content="2018/09/27" />
      <meta name="citation_pdf_url" content="https://openreview.net/pdf?id=BkedwoC5t7" />
      <meta name="twitter:card" content="summary" />
      <meta name="twitter:site" content="@openreviewnet" />
      <meta name="og:title" content="Formal Limitations on the Measurement of Mutual Information" />
      <meta name="og:description" content="Motivated by applications to unsupervised learning, we consider the problem of measuring mutual information. Recent analysis has shown that naive kNN estimators of mutual information have serious..." />
      <meta name="og:image" content="https://openreview.net/static/images/openreview_logo_512.png" />

  <link rel="stylesheet" href="https://fonts.googleapis.com/css?family=Noto+Sans:400,400i,700,700i" />

  <link rel="stylesheet" href="/static/css/bootstrap.min.css" />
  <link rel="stylesheet" href="/static/css/jquery-ui.min.css" />
  <link rel="stylesheet" href="/static/css/main.min.css" />
</head>

<body class="forum">
  <nav class="navbar navbar-inverse navbar-fixed-top" role="navigation">
    <div class="container">
  
      <div class="navbar-header">
        <button type="button" class="navbar-toggle collapsed" data-toggle="collapse" data-target="#navbar" aria-expanded="false" aria-controls="navbar">
          <span class="sr-only">Toggle navigation</span>
          <span class="icon-bar"></span>
          <span class="icon-bar"></span>
          <span class="icon-bar"></span>
        </button>
        <a class="navbar-brand home push-link" href="/"><strong>OpenReview</strong>.net</a>
      </div>
  
      <div id="navbar" class="navbar-collapse collapse">
        <form class="navbar-form navbar-left profile-search" role="search">
          <div class="form-group has-feedback">
            <input id="search_input" type="text" class="form-control ui-autocomplete-input" placeholder="Search ICLR 2019 Conference" autocomplete="off" />
            <span class="glyphicon glyphicon-search form-control-feedback" aria-hidden="true"></span>
          </div>
  
          <input id="search_group" type="hidden" value="ICLR.cc/2019/Conference" />
          <input id="search_content" type="hidden" value="all" />
        <ul id="ui-id-1" tabindex="0" class="ui-menu ui-widget ui-widget-content ui-autocomplete ui-front" style="display: none;"></ul></form>
  
        <ul class="nav navbar-nav navbar-right">
        
            <li id="user-menu"><a href="/login">Login</a></li>
        </ul>
      </div>
  
    </div>
  </nav>

  <div id="or-banner" class="banner" style="">
  <div class="container">
    <div class="row">
      <div class="col-xs-12"><a href="/group?id=ICLR.cc/2019/Conference" title="Venue Homepage"><img class="icon" src="/static/images/arrow_left.svg" /> Go to <strong>ICLR 2019 Conference</strong> homepage</a></div>
    </div>
  </div>
</div>
<div id="flash-message-container" class="alert alert-danger" role="alert" style="display: none;">
  <div class="container">
    <div class="row">
      <div class="col-xs-12">
        <div class="alert-content">
          <button type="button" class="close" aria-label="Close"><span aria-hidden="true">×</span></button>
        </div>
      </div>
    </div>
  </div>
</div>

<div class="container">
  <div class="row">
    <div class="col-xs-12">
      <main id="content" class="clearfix openbanner-visible legacy-styles"><div id="note_BkedwoC5t7" class="note panel"><div class="title_pdf_row clearfix"><h2 class="note_content_title"><a>Formal Limitations on the Measurement of Mutual Information</a> <a class="note_content_pdf" href="/pdf?id=BkedwoC5t7" title="Download PDF" target="_blank"><img src="/static/images/pdf_icon_blue.svg" /></a> </h2></div><div class="meta_row"><span class="signatures">Anonymous</span></div><div class="clearfix"><div class="meta_row pull-left"><span class="date item">28 Sep 2018 (modified: 14 Nov 2018)</span><span class="item">ICLR 2019 Conference Blind Submission</span><span class="item">Readers: <span class="readers-icon glyphicon glyphicon-globe"></span> Everyone</span><span class="item"><a class="action-bibtex-modal" data-bibtex="@inproceedings{    &#10;anonymous2019formal,    &#10;title={Formal Limitations on the Measurement of Mutual Information},    &#10;author={Anonymous},    &#10;booktitle={Submitted to International Conference on Learning Representations},    &#10;year={2019},    &#10;url={https://openreview.net/forum?id=BkedwoC5t7},    &#10;note={under review}    &#10;}">Show Bibtex</a></span><a class="note_content_pdf item" href="/revisions?id=BkedwoC5t7" target="_blank">Show Revisions</a></div><div class="meta_row meta_actions"></div></div><div class="note_contents"><span class="note_content_field">Abstract: </span><span class="note_content_value">Motivated by applications to unsupervised learning, we consider the problem of measuring mutual information. Recent analysis has shown that naive kNN estimators of mutual information have serious statistical limitations motivating more refined methods. In this paper we prove that serious statistical limitations are inherent to any measurement method. More specifically, we show that any distribution-free high-confidence lower bound on mutual information cannot be larger than $O(\ln N)$ where $N$ is the size of the data sample. We also analyze the Donsker-Varadhan lower bound on KL divergence in particular and show that, when simple statistical considerations are taken into account, this bound can never produce a high-confidence value larger than $\ln N$. While large high-confidence lower bounds are impossible, in practice one can use estimators without formal guarantees. We suggest expressing mutual information as a difference of entropies and using cross entropy as an entropy estimator.  We observe that, although cross entropy is only an upper bound on entropy, cross-entropy estimates converge to the true cross entropy at the rate of $1/\sqrt{N}$.</span></div><div class="note_contents"><span class="note_content_field">Keywords: </span><span class="note_content_value">mutual information, predictive coding, unsupervised learning, predictive learning, generalization bounds, MINE, DIM, contrastive predictive coding</span></div><div class="note_contents"><span class="note_content_field">TL;DR: </span><span class="note_content_value">We give a theoretical analysis of the measurement and optimization of mutual information.</span></div><div class="reply_row clearfix"><div class="item" id="reply_count">12 Replies</div></div></div><hr class="small" /><div id="note_children"><div class="note_with_children"><div id="note_S1xpzQF4pQ" class="note panel"><div class="title_pdf_row clearfix"><h2 class="note_content_title"><a>Revision Submitted</a>  <button class="btn btn-xs btn-default permalink-button" title="Link to this comment" data-permalink-url="https://openreview.net/forum?id=BkedwoC5t7&amp;noteId=S1xpzQF4pQ"><span class="glyphicon glyphicon-link" aria-hidden="true"></span></button></h2></div><div class="meta_row"><span class="signatures">ICLR 2019 Conference Paper277 Authors</span></div><div class="clearfix"><div class="meta_row pull-left"><span class="date item">11 Nov 2018</span><span class="item">ICLR 2019 Conference Paper277 Official Comment</span><span class="item">Readers: <span class="readers-icon glyphicon glyphicon-globe"></span> Everyone</span></div><div class="meta_row meta_actions"></div></div><div class="note_contents"><span class="note_content_field">Comment: </span><span class="note_content_value">We just submitted a revision taking reviewer comments into account.  As explicitly noted in the revision, the results apply to continuous as well as discrete distributions.  Most other reviewer comments are addressed as well.  When I get a chance I will respond to the reviews on a more point-by-point basis.</span></div><div class="reply_row clearfix"></div></div><div class="children"></div></div><div class="note_with_children"><div id="note_H1lxFreb6X" class="note panel"><div class="title_pdf_row clearfix"><h2 class="note_content_title"><a>Timely discussion of approaches for approximating mutual information </a>  <button class="btn btn-xs btn-default permalink-button" title="Link to this comment" data-permalink-url="https://openreview.net/forum?id=BkedwoC5t7&amp;noteId=H1lxFreb6X"><span class="glyphicon glyphicon-link" aria-hidden="true"></span></button></h2></div><div class="meta_row"><span class="signatures">ICLR 2019 Conference Paper277 AnonReviewer1</span></div><div class="clearfix"><div class="meta_row pull-left"><span class="date item">08 Nov 2018 (modified: 12 Nov 2018)</span><span class="item">ICLR 2019 Conference Paper277 Official Review</span><span class="item">Readers: <span class="readers-icon glyphicon glyphicon-globe"></span> Everyone</span><a class="note_content_pdf item" href="/revisions?id=H1lxFreb6X" target="_blank">Show Revisions</a></div><div class="meta_row meta_actions"></div></div><div class="note_contents"><span class="note_content_field">Review: </span><span class="note_content_value">This paper considers the problem of estimating bounds on the mutual information. It begins by showing that popular recent estimators (e.g. MINE) are flawed, since they rely on the Donsker-Varadhan bound that cannot be estimated efficiently. They then point to entropy upper bounds as a much more feasible approach to MI approximation, and propose a framework for using them in practice. These upper bounds converge as 1/sqrt(N) to the true entropy value, making them potentially viable in practice to obtain reasonable approximations of MI. 

Given the significant recent attention payed to the MINE estimator and to MI estimation in machine learning generally, I think the message of this paper is very critical to the machine learning community. This analysis of the MINE estimator alone would warrant publication. 

There are currently some weaknesses to this paper, however, when compared to the typical ICLR paper. If the following issues are addressed I am prepared to raise my scoring of this paper:
1. Provide some intuition on how to apply this type of analysis to the continuous-valued case, or some reason why such an analysis would require a different framework. The more detail the better, if bounds analogous to Theorem 1 could be proved for continuous variables and added to the paper, that would be excellent.
2. Section 4 as written is intuitively clear, but could greatly benefit from a full rigorous analysis. There is plenty of space in the paper to do this (and the supplement is available if needed). If such an analysis can’t be done, this section should be deleted.
3. Some empirical example of MINE converging slowly in practice would greatly add to the impact of the paper.

Minor issues: 
Last sentence of Section 1 should clarify what “true entropy” means. As written it is ambiguous between the actual entropy or the cross-entropy, since both are entropies.

I understand the nested optimization problem in Section 6, but the presentation is somewhat unclearly written. More exposition here would help, along with a more clear step-by-step explanation of the practical procedure.

EDIT: The authors have addressed my concerns and I have raised my score.
</span></div><div class="note_contents"><span class="note_content_field">Rating: </span><span class="note_content_value">8: Top 50% of accepted papers, clear accept</span></div><div class="note_contents"><span class="note_content_field">Confidence: </span><span class="note_content_value">5: The reviewer is absolutely certain that the evaluation is correct and very familiar with the relevant literature</span></div><div class="reply_row clearfix"></div></div><div class="children"><div class="note_with_children"><div id="note_S1gOZ5fPT7" class="note panel"><div class="title_pdf_row clearfix"><h2 class="note_content_title"><a>Most points addressed.</a>  <button class="btn btn-xs btn-default permalink-button" title="Link to this comment" data-permalink-url="https://openreview.net/forum?id=BkedwoC5t7&amp;noteId=S1gOZ5fPT7"><span class="glyphicon glyphicon-link" aria-hidden="true"></span></button></h2></div><div class="meta_row"><span class="signatures">ICLR 2019 Conference Paper277 Authors</span></div><div class="clearfix"><div class="meta_row pull-left"><span class="date item">12 Nov 2018</span><span class="item">ICLR 2019 Conference Paper277 Official Comment</span><span class="item">Readers: <span class="readers-icon glyphicon glyphicon-globe"></span> Everyone</span></div><div class="meta_row meta_actions"></div></div><div class="note_contents"><span class="note_content_field">Comment: </span><span class="note_content_value">The revision has addressed points 1 and 2 but not 3.  An empirical evaluation is not going to happen.  It would be fun to get a pure theory paper into ICLR.</span></div><div class="reply_row clearfix"></div></div><div class="children"><div class="note_with_children"><div id="note_SkemzTGwp7" class="note panel"><div class="title_pdf_row clearfix"><h2 class="note_content_title"><a>title</a>  <button class="btn btn-xs btn-default permalink-button" title="Link to this comment" data-permalink-url="https://openreview.net/forum?id=BkedwoC5t7&amp;noteId=SkemzTGwp7"><span class="glyphicon glyphicon-link" aria-hidden="true"></span></button></h2></div><div class="meta_row"><span class="signatures">ICLR 2019 Conference Paper277 AnonReviewer1</span></div><div class="clearfix"><div class="meta_row pull-left"><span class="date item">12 Nov 2018</span><span class="item">ICLR 2019 Conference Paper277 Official Comment</span><span class="item">Readers: <span class="readers-icon glyphicon glyphicon-globe"></span> Everyone</span></div><div class="meta_row meta_actions"></div></div><div class="note_contents"><span class="note_content_field">Comment: </span><span class="note_content_value">Thanks. This looks sufficient to me, I will raise my score. </span></div><div class="reply_row clearfix"></div></div><div class="children"></div></div></div></div></div></div><div class="note_with_children"><div id="note_Syl6cQcsnX" class="note panel"><div class="title_pdf_row clearfix"><h2 class="note_content_title"><a>Promising work from theoretical standpoint</a>  <button class="btn btn-xs btn-default permalink-button" title="Link to this comment" data-permalink-url="https://openreview.net/forum?id=BkedwoC5t7&amp;noteId=Syl6cQcsnX"><span class="glyphicon glyphicon-link" aria-hidden="true"></span></button></h2></div><div class="meta_row"><span class="signatures">ICLR 2019 Conference Paper277 AnonReviewer2</span></div><div class="clearfix"><div class="meta_row pull-left"><span class="date item">04 Nov 2018 (modified: 07 Nov 2018)</span><span class="item">ICLR 2019 Conference Paper277 Official Review</span><span class="item">Readers: <span class="readers-icon glyphicon glyphicon-globe"></span> Everyone</span></div><div class="meta_row meta_actions"></div></div><div class="note_contents"><span class="note_content_field">Review: </span><span class="note_content_value">This paper is about estimating mutual information in high dimensional settings. This is a very challenging open problem, that is of interest to a diverse set of research communities.

In this paper, it is theoretically argued that the recent proposed mutual information (lower bound) estimator, MINE, that is based on the Donsker-Varadhan representation of the corresponding KL divergence expression for mutual infortmation, is fundamentally flawed for high dimensions (of discrete variables).  It is further shown that lower bounds for joint entropy are hard to obtain due to exponential sample complexity. So, the authors suggest to instead obtain an upper bound for each entropy term in the mutual information expression; cross-entropy is the suggested upper bound for an entropy term.

I have some basic questions as the following.

Since the recent KL divergence based MI estimator, MINE, is inaccurate in high dimensions, there should be at least a discussion on connections between your estimator and the classic nearest neighbor distances based estimator of Kraskov et al. and the extensions (I suppose, even for discrete variables, one can compute distances to obtain nearest neighbors efficiently). Also, there are kernel functions based estimators.

There is no discussion in the paper about the errors accumulating from individual entropy terms in the mutual information expression. Kraskov et al. talk about this problem of accumulating errors in their seminal paper and propose not to compute the entropy terms individually. What you are proposing is in contrast to their clever observations.

Does the analysis on upper bound for entropy term also apply to the conditional entropy in the mutual information expression ? I think, there are more subtleties that should be explained.

Since the proposed approach upper bounds entropy using cross entropy term (i.e. using some machine learning model like a neural network), it is even more important to show solid empirical evaluation, for synthetic as well as real world data.

There is a subtle difference between estimating mutual information and proposing an upper/lower bound for it. At present, it is not clear if the proposed upper bound of entropy would lead to an overall upper bound or lower bound for the mutual infortmation expression. The latter is important to know both in the context of optimization based on mutual information maximization (it should be lower bound in such case), as well analyzing mutual infortmation to under complex dynamics such as in brain.

</span></div><div class="note_contents"><span class="note_content_field">Rating: </span><span class="note_content_value">5: Marginally below acceptance threshold</span></div><div class="note_contents"><span class="note_content_field">Confidence: </span><span class="note_content_value">3: The reviewer is fairly confident that the evaluation is correct</span></div><div class="reply_row clearfix"></div></div><div class="children"><div class="note_with_children"><div id="note_BJlXECzvTm" class="note panel"><div class="title_pdf_row clearfix"><h2 class="note_content_title"><a>Most Points Addressed</a>  <button class="btn btn-xs btn-default permalink-button" title="Link to this comment" data-permalink-url="https://openreview.net/forum?id=BkedwoC5t7&amp;noteId=BJlXECzvTm"><span class="glyphicon glyphicon-link" aria-hidden="true"></span></button></h2></div><div class="meta_row"><span class="signatures">ICLR 2019 Conference Paper277 Authors</span></div><div class="clearfix"><div class="meta_row pull-left"><span class="date item">12 Nov 2018</span><span class="item">ICLR 2019 Conference Paper277 Official Comment</span><span class="item">Readers: <span class="readers-icon glyphicon glyphicon-globe"></span> Everyone</span></div><div class="meta_row meta_actions"></div></div><div class="note_contents"><span class="note_content_field">Comment: </span><span class="note_content_value">The revision now briefly discusses kNN methods but largely just to point out that our results apply to them also, and to the continuous case generally.

We did not explicitly discuss the idea that a difference of two quantities (a difference of entropies) accumulates more error than a direct single-quantity measurement.  However, there is a new example discussed --- the mutual information between an English sentence and its French translation --- in which the difference of entropy approach has very clear advantages.

Yes, the conditional entropy can also be upper bounded by cross-entropy.  We did not discuss this explicitly in the revision but the generalization of a theorem about distributions to a theorem about conditional distributions is generally straightforward.

I think the example of NLP translation makes the empirical point sufficiently clear as cross-entropy is the main method used in practice for training both language models and translation systems.

The revision makes it clear that we do not get either an upper or lower bound on mutual information from the cross-entropy upper bounds on entropy as this is only an upper bound and mutual information is a difference of entropies.

</span></div><div class="reply_row clearfix"></div></div><div class="children"></div></div><div class="note_with_children"><div id="note_r1e0tmqypX" class="note panel"><div class="title_pdf_row clearfix"><h2 class="note_content_title"><a>upper/lower bounds</a>  <button class="btn btn-xs btn-default permalink-button" title="Link to this comment" data-permalink-url="https://openreview.net/forum?id=BkedwoC5t7&amp;noteId=r1e0tmqypX"><span class="glyphicon glyphicon-link" aria-hidden="true"></span></button></h2></div><div class="meta_row"><span class="signatures">ICLR 2019 Conference Paper277 AnonReviewer1</span></div><div class="clearfix"><div class="meta_row pull-left"><span class="date item">07 Nov 2018</span><span class="item">ICLR 2019 Conference Paper277 Official Comment</span><span class="item">Readers: <span class="readers-icon glyphicon glyphicon-globe"></span> Everyone</span></div><div class="meta_row meta_actions"></div></div><div class="note_contents"><span class="note_content_field">Comment: </span><span class="note_content_value">"At present, it is not clear if the proposed upper bound of entropy would lead to an overall upper bound or lower bound for the mutual infortmation expression."

-The authors addressed this in their conclusion: 
Unfortunately cross entropy upper bounds on entropy fail to provide either upper or lower bounds on
mutual information—mutual information is a difference of entropies. We cannot rule out the possible
existence of superintelligent models, models beyond current expressive power, that dramatically
reduce cross-entropy loss. Lower bounds on entropy can be viewed as proofs of the non-existence
of superintelligence. We should not surprised that such proofs are infeasible.</span></div><div class="reply_row clearfix"></div></div><div class="children"></div></div></div></div><div class="note_with_children"><div id="note_rJlF2U6MoX" class="note panel"><div class="title_pdf_row clearfix"><h2 class="note_content_title"><a>ICLR 2019 Conference Paper277 AnonReviewer3</a>  <button class="btn btn-xs btn-default permalink-button" title="Link to this comment" data-permalink-url="https://openreview.net/forum?id=BkedwoC5t7&amp;noteId=rJlF2U6MoX"><span class="glyphicon glyphicon-link" aria-hidden="true"></span></button></h2></div><div class="meta_row"><span class="signatures">ICLR 2019 Conference Paper277 AnonReviewer3</span></div><div class="clearfix"><div class="meta_row pull-left"><span class="date item">16 Oct 2018 (modified: 07 Nov 2018)</span><span class="item">ICLR 2019 Conference Paper277 Official Review</span><span class="item">Readers: <span class="readers-icon glyphicon glyphicon-globe"></span> Everyone</span></div><div class="meta_row meta_actions"></div></div><div class="note_contents"><span class="note_content_field">Review: </span><span class="note_content_value">This paper studied the Donsker-Varadhan lower bound of KL-divergence. The authors show that with high probability, the DV lower bound is upper bounded by log of the sample size, so if the true KL-divergence is very large, then exponential sample size is needed to make the DV lower bound tight. The same argument holds true for any distribution-free high-confidence lower bound (such as DV lower bound) for KL divergence. Then the authors proposed to use an upper bound for entropy instead of lower bound for mutual information. 

The idea of the paper is interesting and the proof of Theorems 1 and 2 are valid. Especially I like the idea of Theorem 1, which proves that any distribution-free high-confidence lower bound for KL divergence is upper bounded by log of sample size. This idea is similar to the paper in (Gao et al 15') which shows that mutual information estimator is upper bounded by log(N).

However, this paper contains many fatal flaws, which significantly weaken the quality of this paper. Precisely,

1. The DV lower bound is just an alternative of mutual information, helping MMI predictive coding algorithm to find good coding functions C_x and C_y. The goal of MMI predictive coding is not to estimate the mutual information I(C_x(x), C_y(y)) precisely, instead, the goal is to find good coding functions. The fact that DV lower bound is small means that we can not estimate mutual information through DV lower bound, but it does not directly imply that we can not find the coding functions. I expect some experiments to show that when mutual information is large, MMI predictive coding using DV lower bound can not find good coding functions.

2. In Section 3, the formula after the proof of Outlier Risk Lemma and before Theorem 1 (btw, it is better to have numbers for these formula) seems to be problematic. The first formula shows that E_{z~q} e^{F(z)} &gt;= (1/N)e^{F_max}, then we plug it in (4). But in (4) there is negative ln of E_{z~q} e^{F(z)}, so we should have KL(P,Q) &lt;= something, correct? This may be a typo but this typo is so important such that it affect the readability a lot. Theorem 1 is correct but the paragraphs before Theorem 1 confuse the reader a lot.

3. In Section 4, are you considering classical entropy for discrete random variables, or differential entropy for continuous random variables? I assume you are considering the latter, since most people of machine learning community are interested in continuous random variables. Then your statement of I(X;Y) &lt;= H(X) is incorrect, since for continuous random variables, H(X|Y) can be negative. See (Thomas &amp; Cover, Chapter 8) for a reference.

4. Related to problem 3, if you are considering continuous random variables, then the statement I(X;Y)=H(X)+H(Y)-H(X,Y) is not always correct. There are cases that H(X) is infinite, H(Y) is infinite, H(X,Y) is infinite but I(X;Y) is finite. These cases does not only exist in mathematical books, but also exists in practice, especially when the data is located on a low-dimensional manifold embedded in a high-dimensional space. Therefore, your approach of decompose the mutual information is not always possible.

5. Regarding to your proposed optimization problem in Section 6 (also it is better to have a number), I have some concerns. Since it involves max over \Psi outside, and inf over \Theta and inf over \Phi inside, so I wonder how do you solve this problem? Can you guarantee that the solution can provide good coding functions C_x and C_y? Also, it seems that this optimization problem is proposed as an improvement over the DV lower bound method, so I wish to see some experiment showing that this method is better than the DV lower bound method, at least for some synthetic datasets.

Because of the above mentioned flaws (especially 3 and 4, and lack of experiments), I think the paper is below the standard of ICLR conference.

References:
[1] Efficient Estimation of Mutual Information for Strongly Dependent Variables, by Gao, Ver Steeg and Galstyan, AISTATS15'
[2] Elements of Information Theory, 2nd edition, by Thomas and Cover.



</span></div><div class="note_contents"><span class="note_content_field">Rating: </span><span class="note_content_value">4: Ok but not good enough - rejection</span></div><div class="note_contents"><span class="note_content_field">Confidence: </span><span class="note_content_value">4: The reviewer is confident but not absolutely certain that the evaluation is correct</span></div><div class="reply_row clearfix"></div></div><div class="children"><div class="note_with_children"><div id="note_Hke7leEw6X" class="note panel"><div class="title_pdf_row clearfix"><h2 class="note_content_title"><a>Most Points Addressed</a>  <button class="btn btn-xs btn-default permalink-button" title="Link to this comment" data-permalink-url="https://openreview.net/forum?id=BkedwoC5t7&amp;noteId=Hke7leEw6X"><span class="glyphicon glyphicon-link" aria-hidden="true"></span></button></h2></div><div class="meta_row"><span class="signatures">ICLR 2019 Conference Paper277 Authors</span></div><div class="clearfix"><div class="meta_row pull-left"><span class="date item">13 Nov 2018</span><span class="item">ICLR 2019 Conference Paper277 Official Comment</span><span class="item">Readers: <span class="readers-icon glyphicon glyphicon-globe"></span> Everyone</span></div><div class="meta_row meta_actions"></div></div><div class="note_contents"><span class="note_content_field">Comment: </span><span class="note_content_value">1. I believe that it is important to show that theory can have predictive power.  It can be useful to publish theory prior to its verification.  The theorems seems nontrivial and important on their own.  The machine translation example also seems compelling without experiments.

2. The original formulas were correct but unclear.  This section has been rewritten.

3 &amp; 4.  As noted above, the results apply to the continuous case.  The particular issue with negative differential entropy is addressed by noting the following equality
I(x,y) = sup_{C_x,C_y}  I(C_x(x),C_y(y))
where C_x and C__y range over all discrete classifications (binnings) of the continuous space.  This follows
from either the Reimann or Lebesgue definition of the integral defining expectation together with the data-processing inequality for mutual information in the discrete case.  A sophisticated discussion of continuous information theory can be found at
<a href="http://www.crmarsh.com/static/pdf/Charles_Marsh_Continuous_Entropy.pdf" target="_blank" rel="nofollow">http://www.crmarsh.com/static/pdf/Charles_Marsh_Continuous_Entropy.pdf</a>

5.  This a legitimate issue.  The optimization problem is adversarial as in GANs.  This indeed raises concerns about the stability of the optimization.  Note, however, that the adversarial objective only involves the marginal entropy H(C_y(y)) --- the parameters governing H(C_y(y)|C_x(x)) are cooperative.   In the translation example the adversarial part is the model of the background distribution of sentence codes.  My intuition is that the background model should be more stable than the conditional model yielding stable adversarial training.  Also, GANs do seem to be able to manage adversarial optimization.  The revision does not discuss this issue.

</span></div><div class="reply_row clearfix"></div></div><div class="children"></div></div><div class="note_with_children"><div id="note_Syeg-uY1pm" class="note panel"><div class="title_pdf_row clearfix"><h2 class="note_content_title"><a>discrete v. continuous</a>  <button class="btn btn-xs btn-default permalink-button" title="Link to this comment" data-permalink-url="https://openreview.net/forum?id=BkedwoC5t7&amp;noteId=Syeg-uY1pm"><span class="glyphicon glyphicon-link" aria-hidden="true"></span></button></h2></div><div class="meta_row"><span class="signatures">ICLR 2019 Conference Paper277 AnonReviewer1</span></div><div class="clearfix"><div class="meta_row pull-left"><span class="date item">07 Nov 2018</span><span class="item">ICLR 2019 Conference Paper277 Official Comment</span><span class="item">Readers: <span class="readers-icon glyphicon glyphicon-globe"></span> Everyone</span></div><div class="meta_row meta_actions"></div></div><div class="note_contents"><span class="note_content_field">Comment: </span><span class="note_content_value">"most people of machine learning community are interested in continuous random variables" 
- This may be technically true, but a very many machine learning problems involve categorical data, i.e. discrete valued data. Considering only discrete-valued data would never make it irrelevant to the "machine learning community" any more than considering only continuous-valued data would.</span></div><div class="reply_row clearfix"></div></div><div class="children"></div></div><div class="note_with_children"><div id="note_BJemYcd7jm" class="note panel"><div class="title_pdf_row clearfix"><h2 class="note_content_title"><a>Authors are focusing on discrete distributions for analysis</a>  <button class="btn btn-xs btn-default permalink-button" title="Link to this comment" data-permalink-url="https://openreview.net/forum?id=BkedwoC5t7&amp;noteId=BJemYcd7jm"><span class="glyphicon glyphicon-link" aria-hidden="true"></span></button></h2></div><div class="meta_row"><span class="signatures">ICLR 2019 Conference Paper277 Area Chair1</span></div><div class="clearfix"><div class="meta_row pull-left"><span class="date item">16 Oct 2018 (modified: 06 Nov 2018)</span><span class="item">ICLR 2019 Conference Paper277 Official Comment</span><span class="item">Readers: <span class="readers-icon glyphicon glyphicon-globe"></span> Everyone</span></div><div class="meta_row meta_actions"></div></div><div class="note_contents"><span class="note_content_field">Comment: </span><span class="note_content_value">On points (3) and (4), I think this is addressed by the statement in Section 2 just before Eq. (1): "Our theoretical analyses will assume discrete distributions."</span></div><div class="reply_row clearfix"></div></div><div class="children"><div class="note_with_children"><div id="note_rklrxl9msQ" class="note panel"><div class="title_pdf_row clearfix"><h2 class="note_content_title"><a>Some experiments are needed to justify your theory</a>  <button class="btn btn-xs btn-default permalink-button" title="Link to this comment" data-permalink-url="https://openreview.net/forum?id=BkedwoC5t7&amp;noteId=rklrxl9msQ"><span class="glyphicon glyphicon-link" aria-hidden="true"></span></button></h2></div><div class="meta_row"><span class="signatures">ICLR 2019 Conference Paper277 AnonReviewer3</span></div><div class="clearfix"><div class="meta_row pull-left"><span class="date item">17 Oct 2018</span><span class="item">ICLR 2019 Conference Paper277 Official Comment</span><span class="item">Readers: <span class="readers-icon glyphicon glyphicon-globe"></span> Everyone</span></div><div class="meta_row meta_actions"></div></div><div class="note_contents"><span class="note_content_field">Comment: </span><span class="note_content_value">If the theory works only for discrete distributions, it is not so interesting to machine learning community, unless you can show that the insights from the analysis for discrete case can be brought to continuous case. So some experiments which show that the method proposed in Section 6 has better performance are needed.</span></div><div class="reply_row clearfix"></div></div><div class="children"></div></div></div></div></div></div></div></main></div>
  </div>
</div>


    <!-- Footer -->
    <footer class="sitemap">
      <div class="container">
        <div class="row hidden-xs">
          <div class="col-sm-4">
            <ul class="list-unstyled">
              <li><a href="/" class="home">Home</a></li>
              <li><a href="/venues">All Venues</a></li>
            </ul>
          </div>
    
          <div class="col-sm-4">
            <ul class="list-unstyled">
              <li><a href="/about">About OpenReview</a></li>
              <li><a href="/contact">Contact</a></li>
              <li><a href="#" data-toggle="modal" data-target="#feedback-modal">Feedback</a></li>
            </ul>
          </div>
    
          <div class="col-sm-4">
            <ul class="list-unstyled">
              <li><a href="/terms">Terms of Service</a></li>
              <li><a href="/privacy">Privacy Policy</a></li>
            </ul>
          </div>
        </div>
    
        <div class="row visible-xs-block">
          <div class="col-xs-6">
            <ul class="list-unstyled">
              <li><a href="/" class="home">Home</a></li>
              <li><a href="/about">About OpenReview</a></li>
              <li><a href="#" data-toggle="modal" data-target="#feedback-modal">Feedback</a></li>
            </ul>
          </div>
    
          <div class="col-xs-6">
            <ul class="list-unstyled">
              <li><a href="/contact">Contact</a></li>
              <li><a href="/terms">Terms of Service</a></li>
              <li><a href="/privacy">Privacy Policy</a></li>
            </ul>
          </div>
        </div>
      </div>
    </footer>
    
    <footer class="sponsor">
      <div class="container">
        <div class="row">
          <div class="col-sm-10 col-sm-offset-1">
            <p class="text-center">
              OpenReview is created by the <a href="http://www.iesl.cs.umass.edu/" target="_blank">Information Extraction and Synthesis Laboratory</a>, College of Information and Computer Science, University of Massachusetts Amherst. We gratefully acknowledge the support of the OpenReview sponsors:  Google,  Facebook, NSF, the University of Massachusetts Amherst Center for Data Science, and Center for Intelligent Information Retrieval, as well as the Google Cloud Platform for donating the computing and networking services on which OpenReview.net runs.
            </p>
          </div>
        </div>
      </div>
    </footer>

  <div id="feedback-modal" class="modal fade" tabindex="-1" role="dialog">
    <div class="modal-dialog">
      <div class="modal-content">
        <div class="modal-header">
          <button type="button" class="close" data-dismiss="modal" aria-label="Close"><span aria-hidden="true">×</span></button>
          <h3 class="modal-title">Send Feedback</h3>
        </div>
        <div class="modal-body">
          <p>Enter your feedback below and we'll get back to you as soon as possible.</p>
          <form action="/feedback" method="POST">
            <div class="form-group">
              <input type="email" name="from" class="form-control" placeholder="Email" />
            </div>
            <div class="form-group">
              <input type="text" name="subject" class="form-control" placeholder="Subject" />
            </div>
            <div class="form-group">
              <textarea name="message" class="form-control feedback-input" rows="5" placeholder="Message" required=""></textarea>
            </div>
          <ul id="ui-id-2" tabindex="0" class="ui-menu ui-widget ui-widget-content" style="display: none;"></ul></form>
        </div>
        <div class="modal-footer">
          <button type="button" class="btn btn-default" data-dismiss="modal">Cancel</button>
          <button type="button" class="btn btn-primary">Send</button>
        </div>
      </div>
    </div>
  </div>

  <script type="text/javascript" async="" src="https://www.google-analytics.com/analytics.js"></script><script src="//cdnjs.cloudflare.com/ajax/libs/jquery/2.2.4/jquery.min.js"></script>
  <script>window.jQuery || document.write('&lt;script src="/static/js/vendor/jquery-2.2.4.min.js"&gt;&lt;\/script&gt;')</script>

  <script src="/static/dist/vendor.min.js"></script>
  <script src="/static/dist/templates.min.js"></script>
  <script src="/static/dist/openreview.min.js"></script>

    <script>window.legacyScripts = true;</script>


    <script async="" src="https://www.googletagmanager.com/gtag/js?id=UA-108703919-1"></script>
    <script>
      window.dataLayer = window.dataLayer || [];
      function gtag() { dataLayer.push(arguments); }
      gtag('js', new Date());
      gtag('config', 'UA-108703919-1');
    </script>


<div role="status" aria-live="assertive" aria-relevant="additions" class="ui-helper-hidden-accessible"></div></body></html>